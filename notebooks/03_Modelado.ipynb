{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Modelado Predictivo del Dataset\n",
    "\n",
    "En esta secci√≥n se construyen modelos predictivos para estimar la probabilidad de √©xito (*target*) de cada candidato.  \n",
    "El objetivo es:\n",
    "\n",
    "- Entrenar modelos supervisados que capturen la relaci√≥n entre variables explicativas y target\n",
    "- Evaluar la importancia de cada variable\n",
    "- Comparar distintas t√©cnicas y seleccionar la mejor\n",
    "- Preparar submodelos seg√∫n √°rea acad√©mica (Medicina, Enfermer√≠a, resto)\n",
    "\n",
    "El proceso se divide en los siguientes pasos:\n",
    "1. Preparaci√≥n de datos y divisi√≥n en entrenamiento/test  \n",
    "2. Selecci√≥n de modelos factibles  \n",
    "3. Selecci√≥n de variables importantes  \n",
    "4. Entrenamiento de submodelos  \n",
    "5. Optimizaci√≥n de hiperpar√°metros  \n",
    "6. Evaluaci√≥n del modelo sobre conjunto test\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Preparaci√≥n de Datos\n",
    "\n",
    "Antes de entrenar los modelos:\n",
    "\n",
    "- Se eliminan variables irrelevantes o identificadores\n",
    "- Se manejan valores nulos\n",
    "- Se divide el dataset en **conjunto de entrenamiento** (70‚Äì80 %) y **conjunto de test** (20‚Äì30 %)\n",
    "- Se crean subdatasets si se requiere segmentaci√≥n por √°rea acad√©mica\n",
    "\n",
    "üìå Esto asegura que el modelo no vea datos de test durante el entrenamiento, evitando sobreajuste.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Selecci√≥n de Modelos\n",
    "\n",
    "Se consideran modelos supervisados robustos para clasificaci√≥n/regresi√≥n seg√∫n el target:\n",
    "\n",
    "- **Random Forest**: Modelo basado en √°rboles con gran capacidad de generalizaci√≥n  \n",
    "- **Gradient Boosting Machines (GBM/XGBoost/LightGBM)**: Optimizaci√≥n secuencial de errores  \n",
    "- **Regresiones**: Lineales o log√≠sticas, √∫tiles para interpretaci√≥n  \n",
    "- Otros modelos factibles seg√∫n el caso (SVM, redes neuronales simples)\n",
    "\n",
    "Se busca evaluar cu√°l combina mejor:\n",
    "\n",
    "- Precisi√≥n o % acierto\n",
    "- AUC (√Årea bajo la curva ROC)\n",
    "- Interpretabilidad\n",
    "- Robustez ante outliers y variables correlacionadas\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Selecci√≥n de Variables Importantes\n",
    "\n",
    "- Se utiliza la importancia que cada modelo asigna a las variables para:\n",
    "\n",
    "  - Reducir dimensionalidad\n",
    "  - Evitar ruido\n",
    "  - Mejorar interpretabilidad\n",
    "\n",
    "- Variables con mayor impacto en la predicci√≥n ser√°n prioridad para la construcci√≥n de submodelos.\n",
    "\n",
    "üìå Ejemplo: `feature_importances_` en Random Forest o `SHAP values` para interpretabilidad avanzada.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Entrenamiento de Submodelos\n",
    "\n",
    "- Se pueden crear modelos espec√≠ficos para segmentos de inter√©s:  \n",
    "  - Medicina  \n",
    "  - Enfermer√≠a  \n",
    "  - Resto de candidatos\n",
    "\n",
    "- Esto permite capturar patrones particulares de cada √°rea que un modelo general podr√≠a diluir.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Optimizaci√≥n de Hiperpar√°metros\n",
    "\n",
    "- Se aplican t√©cnicas de tuning autom√°tico (grid search, random search o frameworks como **PyCaret**)  \n",
    "- Se buscan combinaciones que maximizan m√©tricas clave:  \n",
    "\n",
    "  - Precisi√≥n / Recall  \n",
    "  - AUC / ROC  \n",
    "  - F1-score  \n",
    "\n",
    "- El objetivo es **obtener el mejor modelo posible** sin sobreajustar a los datos de entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Evaluaci√≥n en Conjunto de Test\n",
    "\n",
    "- Una vez seleccionado el modelo √≥ptimo, se eval√∫a sobre datos no vistos:\n",
    "\n",
    "  - Comprobando m√©tricas globales: % acierto, AUC  \n",
    "  - Detectando posibles sesgos por segmento  \n",
    "  - Verificando estabilidad y consistencia de predicciones\n",
    "\n",
    "- Esto garantiza que el modelo generalice y pueda usarse para predicciones futuras.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Conclusiones del Modelado\n",
    "\n",
    "El modelado supervisado permite:\n",
    "\n",
    "- Predecir con alta fiabilidad la probabilidad de √©xito de los candidatos  \n",
    "- Entender qu√© variables tienen mayor impacto en la conversi√≥n  \n",
    "- Construir estrategias segmentadas por √°reas acad√©micas  \n",
    "- Complementar el an√°lisis de clusters previo, combinando insights no supervisados y supervisados\n",
    "\n",
    "üìå Los resultados servir√°n como base para:\n",
    "\n",
    "- Implementaci√≥n de modelos en producci√≥n\n",
    "- Definici√≥n de estrategias de admisi√≥n diferenciadas\n",
    "- An√°lisis de riesgo y priorizaci√≥n de candidatos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\datos\\\\03. Datos analizados\\\\dataset_clusterizado.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 0Ô∏è‚É£ CARGA DEL DATASET\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     19\u001b[39m ruta_dataset = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdatos\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m03. Datos analizados\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdataset_clusterizado.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruta_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m target = \u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 0.1Ô∏è‚É£ LIMPIEZA B√ÅSICA\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Eliminar IDs (no informativos)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0021755\\AppData\\Local\\anaconda3\\envs\\my_python311_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    935\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    936\u001b[39m     dialect,\n\u001b[32m    937\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    945\u001b[39m )\n\u001b[32m    946\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m948\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0021755\\AppData\\Local\\anaconda3\\envs\\my_python311_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    608\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    610\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    614\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0021755\\AppData\\Local\\anaconda3\\envs\\my_python311_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1447\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0021755\\AppData\\Local\\anaconda3\\envs\\my_python311_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1703\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1704\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1715\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1716\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0021755\\AppData\\Local\\anaconda3\\envs\\my_python311_env\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    859\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    860\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    862\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    864\u001b[39m             handle,\n\u001b[32m    865\u001b[39m             ioargs.mode,\n\u001b[32m    866\u001b[39m             encoding=ioargs.encoding,\n\u001b[32m    867\u001b[39m             errors=errors,\n\u001b[32m    868\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    869\u001b[39m         )\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    872\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '..\\\\datos\\\\03. Datos analizados\\\\dataset_clusterizado.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SCRIPT ¬∑ 03 MODELADO PREDICTIVO CON PYCARET (FINAL EXPLICABLE)\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from pycaret.classification import (\n",
    "    setup, compare_models, tune_model,\n",
    "    predict_model, plot_model, save_model\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 0Ô∏è‚É£ CARGA DEL DATASET\n",
    "# ============================================================\n",
    "\n",
    "ruta_dataset = r\"..\\datos\\03. Datos analizados\\dataset_clusterizado.csv\"\n",
    "df = pd.read_csv(ruta_dataset, sep=\";\")\n",
    "\n",
    "target = 'target'\n",
    "\n",
    "# ============================================================\n",
    "# 0.1Ô∏è‚É£ LIMPIEZA B√ÅSICA\n",
    "# ============================================================\n",
    "\n",
    "# Eliminar IDs (no informativos)\n",
    "cols_id = ['ACCOUNTID', 'ID', 'ID18__PC']\n",
    "df = df.drop(columns=[c for c in cols_id if c in df.columns])\n",
    "\n",
    "# Eliminar filas sin target\n",
    "df = df.dropna(subset=[target])\n",
    "\n",
    "# Sustituir NaN por 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# ============================================================\n",
    "# 1Ô∏è‚É£ SELECCI√ìN DE VARIABLES\n",
    "# ============================================================\n",
    "\n",
    "vars_excluir = [\n",
    "    'cluster', 'interpretacion_cluster', 'PCA1', 'PCA2',\n",
    "    'PORCENTAJE_PAGADO_FINAL', 'PAID_AMOUNT',\n",
    "    'tiempo_etapa_dias', 'tiempo_entre_etapas_dias'\n",
    "]\n",
    "\n",
    "vars_modelado = [c for c in df.columns if c not in vars_excluir + [target]]\n",
    "\n",
    "X = df[vars_modelado]\n",
    "y = df[target]\n",
    "\n",
    "print(f\"Variables utilizadas ({len(vars_modelado)}):\")\n",
    "print(vars_modelado)\n",
    "\n",
    "# ============================================================\n",
    "# 2Ô∏è‚É£ TRAIN / TEST SPLIT\n",
    "# ============================================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3Ô∏è‚É£ FILTRADO DE COLUMNAS PROBLEM√ÅTICAS\n",
    "# ============================================================\n",
    "\n",
    "object_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "const_cols = X_train.columns[X_train.nunique() <= 1].tolist()\n",
    "\n",
    "safe_cols = [c for c in X_train.columns if c not in object_cols + const_cols]\n",
    "\n",
    "train_df = X_train[safe_cols].copy()\n",
    "train_df[target] = y_train\n",
    "\n",
    "# ============================================================\n",
    "# 4Ô∏è‚É£ SETUP PYCARET (R√ÅPIDO + EXPLICABLE)\n",
    "# ============================================================\n",
    "\n",
    "setup(\n",
    "    data=train_df,\n",
    "    target=target,\n",
    "    session_id=42,\n",
    "    normalize=True,\n",
    "    transformation=False,\n",
    "    fold=3,               # r√°pido\n",
    "    verbose=False,\n",
    "    html=False\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 5Ô∏è‚É£ COMPARACI√ìN DE MODELOS EXPLICABLES\n",
    "# ============================================================\n",
    "\n",
    "best_models = compare_models(\n",
    "    include=['lr', 'dt', 'rf', 'lightgbm'],\n",
    "    sort='AUC',\n",
    "    n_select=3,\n",
    "    turbo=True\n",
    ")\n",
    "\n",
    "print(\"\\nModelos seleccionados:\")\n",
    "for m in best_models:\n",
    "    print(m)\n",
    "\n",
    "# ============================================================\n",
    "# 6Ô∏è‚É£ TUNING LIGERO\n",
    "# ============================================================\n",
    "\n",
    "tuned_models = [\n",
    "    tune_model(m, optimize='AUC', n_iter=10, choose_better=True)\n",
    "    for m in best_models\n",
    "]\n",
    "\n",
    "# ============================================================\n",
    "# 7Ô∏è‚É£ VALIDACI√ìN EN TEST + PROBABILIDADES\n",
    "# ============================================================\n",
    "\n",
    "test_df = X_test[safe_cols].copy()\n",
    "test_df[target] = y_test\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for model in tuned_models:\n",
    "    preds = predict_model(model, data=test_df)\n",
    "\n",
    "    auc = roc_auc_score(\n",
    "        preds[target],\n",
    "        preds['prediction_score']\n",
    "    )\n",
    "\n",
    "    prob_media = preds['prediction_score'].mean()\n",
    "\n",
    "    resultados.append({\n",
    "        'modelo': str(model),\n",
    "        'AUC_test': round(auc, 3),\n",
    "        'prob_media_exito': round(prob_media, 3)\n",
    "    })\n",
    "\n",
    "    print(f\"\\nModelo: {model}\")\n",
    "    print(f\"AUC test: {auc:.3f}\")\n",
    "    print(f\"Probabilidad media de √©xito: {prob_media:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8Ô∏è‚É£ IMPORTANCIA DE VARIABLES (MODELO FINAL)\n",
    "# ============================================================\n",
    "\n",
    "best_model = tuned_models[0]\n",
    "\n",
    "# Importancia nativa\n",
    "plot_model(best_model, plot='feature')\n",
    "\n",
    "# Importancia robusta (permutation)\n",
    "plot_model(best_model, plot='feature_all')\n",
    "\n",
    "# ============================================================\n",
    "# 9Ô∏è‚É£ RESUMEN FINAL DE MODELOS\n",
    "# ============================================================\n",
    "\n",
    "resumen_df = pd.DataFrame(resultados)\n",
    "print(\"\\nResumen comparativo de modelos:\")\n",
    "print(resumen_df)\n",
    "\n",
    "# ============================================================\n",
    "# üîü GUARDAR MODELO FINAL\n",
    "# ============================================================\n",
    "\n",
    "save_model(best_model, r\"..\\datos\\04. Modelos\\modelo_final_explicable\")\n",
    "\n",
    "print(\"\\nModelo final guardado correctamente.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
