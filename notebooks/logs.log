2026-01-19 10:24:16,815:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-19 10:24:16,815:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-19 10:24:16,815:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-19 10:24:16,815:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-29 15:21:31,736:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-29 15:21:31,736:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-29 15:21:31,736:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-29 15:21:31,736:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-29 15:21:35,750:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\2388759396.py:16: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-29 15:25:39,783:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\2256906154.py:17: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-29 15:26:50,165:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\1372246288.py:16: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-29 15:26:51,818:INFO:PyCaret ClassificationExperiment
2026-01-29 15:26:51,819:INFO:Logging name: clf-default-name
2026-01-29 15:26:51,819:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-29 15:26:51,820:INFO:version 3.3.2
2026-01-29 15:26:51,820:INFO:Initializing setup()
2026-01-29 15:26:51,820:INFO:self.USI: f012
2026-01-29 15:26:51,820:INFO:self._variable_keys: {'X_test', 'fold_groups_param', 'pipeline', 'fix_imbalance', 'exp_name_log', 'data', 'y_test', 'seed', 'fold_shuffle_param', 'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'logging_param', 'idx', 'y', 'target_param', 'fold_generator', 'y_train', 'gpu_param', 'USI', 'exp_id', '_available_plots', 'X', 'X_train', 'html_param', '_ml_usecase'}
2026-01-29 15:26:51,820:INFO:Checking environment
2026-01-29 15:26:51,820:INFO:python_version: 3.11.11
2026-01-29 15:26:51,821:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-29 15:26:51,821:INFO:machine: AMD64
2026-01-29 15:26:51,821:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-29 15:26:51,821:INFO:Memory: svmem(total=34009374720, available=16823705600, percent=50.5, used=17185669120, free=16823705600)
2026-01-29 15:26:51,822:INFO:Physical Core: 12
2026-01-29 15:26:51,822:INFO:Logical Core: 16
2026-01-29 15:26:51,822:INFO:Checking libraries
2026-01-29 15:26:51,822:INFO:System:
2026-01-29 15:26:51,822:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-29 15:26:51,822:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-29 15:26:51,822:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-29 15:26:51,823:INFO:PyCaret required dependencies:
2026-01-29 15:26:54,258:INFO:                 pip: 25.0
2026-01-29 15:26:54,258:INFO:          setuptools: 75.8.0
2026-01-29 15:26:54,258:INFO:             pycaret: 3.3.2
2026-01-29 15:26:54,258:INFO:             IPython: 9.9.0
2026-01-29 15:26:54,258:INFO:          ipywidgets: 8.1.8
2026-01-29 15:26:54,258:INFO:                tqdm: 4.67.1
2026-01-29 15:26:54,258:INFO:               numpy: 1.26.4
2026-01-29 15:26:54,259:INFO:              pandas: 2.1.4
2026-01-29 15:26:54,259:INFO:              jinja2: 3.1.6
2026-01-29 15:26:54,259:INFO:               scipy: 1.11.4
2026-01-29 15:26:54,259:INFO:              joblib: 1.3.2
2026-01-29 15:26:54,259:INFO:             sklearn: 1.4.2
2026-01-29 15:26:54,259:INFO:                pyod: 2.0.6
2026-01-29 15:26:54,259:INFO:            imblearn: 0.14.1
2026-01-29 15:26:54,259:INFO:   category_encoders: 2.7.0
2026-01-29 15:26:54,259:INFO:            lightgbm: 4.6.0
2026-01-29 15:26:54,259:INFO:               numba: 0.62.1
2026-01-29 15:26:54,259:INFO:            requests: 2.32.3
2026-01-29 15:26:54,259:INFO:          matplotlib: 3.7.5
2026-01-29 15:26:54,259:INFO:          scikitplot: 0.3.7
2026-01-29 15:26:54,259:INFO:         yellowbrick: 1.5
2026-01-29 15:26:54,259:INFO:              plotly: 5.24.1
2026-01-29 15:26:54,259:INFO:    plotly-resampler: Not installed
2026-01-29 15:26:54,259:INFO:             kaleido: 1.2.0
2026-01-29 15:26:54,259:INFO:           schemdraw: 0.15
2026-01-29 15:26:54,259:INFO:         statsmodels: 0.14.6
2026-01-29 15:26:54,259:INFO:              sktime: 0.26.0
2026-01-29 15:26:54,259:INFO:               tbats: 1.1.3
2026-01-29 15:26:54,259:INFO:            pmdarima: 2.0.4
2026-01-29 15:26:54,259:INFO:              psutil: 7.2.1
2026-01-29 15:26:54,260:INFO:          markupsafe: 3.0.3
2026-01-29 15:26:54,260:INFO:             pickle5: Not installed
2026-01-29 15:26:54,260:INFO:         cloudpickle: 3.0.0
2026-01-29 15:26:54,260:INFO:         deprecation: 2.1.0
2026-01-29 15:26:54,260:INFO:              xxhash: 3.6.0
2026-01-29 15:26:54,260:INFO:           wurlitzer: Not installed
2026-01-29 15:26:54,260:INFO:PyCaret optional dependencies:
2026-01-29 15:27:00,934:INFO:                shap: 0.44.1
2026-01-29 15:27:00,934:INFO:           interpret: 0.7.3
2026-01-29 15:27:00,934:INFO:                umap: 0.5.7
2026-01-29 15:27:00,934:INFO:     ydata_profiling: 4.18.1
2026-01-29 15:27:00,934:INFO:  explainerdashboard: 0.5.1
2026-01-29 15:27:00,934:INFO:             autoviz: Not installed
2026-01-29 15:27:00,936:INFO:           fairlearn: 0.7.0
2026-01-29 15:27:00,936:INFO:          deepchecks: Not installed
2026-01-29 15:27:00,936:INFO:             xgboost: Not installed
2026-01-29 15:27:00,936:INFO:            catboost: 1.2.8
2026-01-29 15:27:00,936:INFO:              kmodes: 0.12.2
2026-01-29 15:27:00,936:INFO:             mlxtend: 0.23.4
2026-01-29 15:27:00,936:INFO:       statsforecast: 1.5.0
2026-01-29 15:27:00,936:INFO:        tune_sklearn: Not installed
2026-01-29 15:27:00,936:INFO:                 ray: Not installed
2026-01-29 15:27:00,936:INFO:            hyperopt: 0.2.7
2026-01-29 15:27:00,936:INFO:              optuna: 4.6.0
2026-01-29 15:27:00,936:INFO:               skopt: 0.10.2
2026-01-29 15:27:00,938:INFO:              mlflow: 3.8.1
2026-01-29 15:27:00,938:INFO:              gradio: 6.3.0
2026-01-29 15:27:00,938:INFO:             fastapi: 0.128.0
2026-01-29 15:27:00,938:INFO:             uvicorn: 0.40.0
2026-01-29 15:27:00,938:INFO:              m2cgen: 0.10.0
2026-01-29 15:27:00,939:INFO:           evidently: 0.4.40
2026-01-29 15:27:00,939:INFO:               fugue: 0.8.7
2026-01-29 15:27:00,939:INFO:           streamlit: Not installed
2026-01-29 15:27:00,939:INFO:             prophet: Not installed
2026-01-29 15:27:00,939:INFO:None
2026-01-29 15:27:00,940:INFO:Set up data.
2026-01-29 15:27:02,745:INFO:Set up folding strategy.
2026-01-29 15:27:02,746:INFO:Set up train/test split.
2026-01-29 15:27:03,016:INFO:Set up index.
2026-01-29 15:27:03,029:INFO:Assigning column types.
2026-01-29 15:27:03,163:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-29 15:27:03,188:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 15:27:03,198:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:27:03,235:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:27:03,235:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:27:03,439:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 15:27:03,440:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:27:03,455:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:27:03,456:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:27:03,456:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-29 15:27:03,493:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:27:03,516:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:27:03,516:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:27:03,543:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:27:03,559:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:27:03,560:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:27:03,560:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-29 15:27:03,617:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:27:03,617:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:27:03,661:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:27:03,661:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:27:03,663:INFO:Preparing preprocessing pipeline...
2026-01-29 15:27:03,692:INFO:Set up simple imputation.
2026-01-29 15:27:03,845:INFO:Set up encoding of ordinal features.
2026-01-29 15:27:04,428:INFO:Set up encoding of categorical features.
2026-01-29 15:27:04,430:INFO:Set up column transformation.
2026-01-29 15:27:04,430:INFO:Set up feature normalization.
2026-01-29 15:29:42,743:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\3853355163.py:16: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 15:29:43,543:INFO:PyCaret ClassificationExperiment
2026-01-29 15:29:43,544:INFO:Logging name: clf-default-name
2026-01-29 15:29:43,544:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-29 15:29:43,544:INFO:version 3.3.2
2026-01-29 15:29:43,544:INFO:Initializing setup()
2026-01-29 15:29:43,544:INFO:self.USI: 2dfb
2026-01-29 15:29:43,545:INFO:self._variable_keys: {'X_test', 'fold_groups_param', 'pipeline', 'fix_imbalance', 'exp_name_log', 'data', 'y_test', 'seed', 'fold_shuffle_param', 'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'logging_param', 'idx', 'y', 'target_param', 'fold_generator', 'y_train', 'gpu_param', 'USI', 'exp_id', '_available_plots', 'X', 'X_train', 'html_param', '_ml_usecase'}
2026-01-29 15:29:43,545:INFO:Checking environment
2026-01-29 15:29:43,545:INFO:python_version: 3.11.11
2026-01-29 15:29:43,545:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-29 15:29:43,545:INFO:machine: AMD64
2026-01-29 15:29:43,545:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-29 15:29:43,546:INFO:Memory: svmem(total=34009374720, available=16752574464, percent=50.7, used=17256800256, free=16752574464)
2026-01-29 15:29:43,546:INFO:Physical Core: 12
2026-01-29 15:29:43,546:INFO:Logical Core: 16
2026-01-29 15:29:43,546:INFO:Checking libraries
2026-01-29 15:29:43,546:INFO:System:
2026-01-29 15:29:43,546:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-29 15:29:43,546:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-29 15:29:43,546:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-29 15:29:43,546:INFO:PyCaret required dependencies:
2026-01-29 15:29:43,546:INFO:                 pip: 25.0
2026-01-29 15:29:43,546:INFO:          setuptools: 75.8.0
2026-01-29 15:29:43,546:INFO:             pycaret: 3.3.2
2026-01-29 15:29:43,546:INFO:             IPython: 9.9.0
2026-01-29 15:29:43,546:INFO:          ipywidgets: 8.1.8
2026-01-29 15:29:43,547:INFO:                tqdm: 4.67.1
2026-01-29 15:29:43,547:INFO:               numpy: 1.26.4
2026-01-29 15:29:43,547:INFO:              pandas: 2.1.4
2026-01-29 15:29:43,547:INFO:              jinja2: 3.1.6
2026-01-29 15:29:43,547:INFO:               scipy: 1.11.4
2026-01-29 15:29:43,547:INFO:              joblib: 1.3.2
2026-01-29 15:29:43,547:INFO:             sklearn: 1.4.2
2026-01-29 15:29:43,547:INFO:                pyod: 2.0.6
2026-01-29 15:29:43,547:INFO:            imblearn: 0.14.1
2026-01-29 15:29:43,547:INFO:   category_encoders: 2.7.0
2026-01-29 15:29:43,547:INFO:            lightgbm: 4.6.0
2026-01-29 15:29:43,547:INFO:               numba: 0.62.1
2026-01-29 15:29:43,547:INFO:            requests: 2.32.3
2026-01-29 15:29:43,547:INFO:          matplotlib: 3.7.5
2026-01-29 15:29:43,547:INFO:          scikitplot: 0.3.7
2026-01-29 15:29:43,547:INFO:         yellowbrick: 1.5
2026-01-29 15:29:43,547:INFO:              plotly: 5.24.1
2026-01-29 15:29:43,547:INFO:    plotly-resampler: Not installed
2026-01-29 15:29:43,547:INFO:             kaleido: 1.2.0
2026-01-29 15:29:43,547:INFO:           schemdraw: 0.15
2026-01-29 15:29:43,548:INFO:         statsmodels: 0.14.6
2026-01-29 15:29:43,548:INFO:              sktime: 0.26.0
2026-01-29 15:29:43,548:INFO:               tbats: 1.1.3
2026-01-29 15:29:43,548:INFO:            pmdarima: 2.0.4
2026-01-29 15:29:43,548:INFO:              psutil: 7.2.1
2026-01-29 15:29:43,548:INFO:          markupsafe: 3.0.3
2026-01-29 15:29:43,548:INFO:             pickle5: Not installed
2026-01-29 15:29:43,548:INFO:         cloudpickle: 3.0.0
2026-01-29 15:29:43,548:INFO:         deprecation: 2.1.0
2026-01-29 15:29:43,548:INFO:              xxhash: 3.6.0
2026-01-29 15:29:43,548:INFO:           wurlitzer: Not installed
2026-01-29 15:29:43,548:INFO:PyCaret optional dependencies:
2026-01-29 15:29:43,548:INFO:                shap: 0.44.1
2026-01-29 15:29:43,548:INFO:           interpret: 0.7.3
2026-01-29 15:29:43,548:INFO:                umap: 0.5.7
2026-01-29 15:29:43,549:INFO:     ydata_profiling: 4.18.1
2026-01-29 15:29:43,549:INFO:  explainerdashboard: 0.5.1
2026-01-29 15:29:43,549:INFO:             autoviz: Not installed
2026-01-29 15:29:43,549:INFO:           fairlearn: 0.7.0
2026-01-29 15:29:43,549:INFO:          deepchecks: Not installed
2026-01-29 15:29:43,549:INFO:             xgboost: Not installed
2026-01-29 15:29:43,549:INFO:            catboost: 1.2.8
2026-01-29 15:29:43,549:INFO:              kmodes: 0.12.2
2026-01-29 15:29:43,549:INFO:             mlxtend: 0.23.4
2026-01-29 15:29:43,549:INFO:       statsforecast: 1.5.0
2026-01-29 15:29:43,549:INFO:        tune_sklearn: Not installed
2026-01-29 15:29:43,549:INFO:                 ray: Not installed
2026-01-29 15:29:43,549:INFO:            hyperopt: 0.2.7
2026-01-29 15:29:43,549:INFO:              optuna: 4.6.0
2026-01-29 15:29:43,549:INFO:               skopt: 0.10.2
2026-01-29 15:29:43,549:INFO:              mlflow: 3.8.1
2026-01-29 15:29:43,549:INFO:              gradio: 6.3.0
2026-01-29 15:29:43,549:INFO:             fastapi: 0.128.0
2026-01-29 15:29:43,549:INFO:             uvicorn: 0.40.0
2026-01-29 15:29:43,549:INFO:              m2cgen: 0.10.0
2026-01-29 15:29:43,549:INFO:           evidently: 0.4.40
2026-01-29 15:29:43,549:INFO:               fugue: 0.8.7
2026-01-29 15:29:43,549:INFO:           streamlit: Not installed
2026-01-29 15:29:43,549:INFO:             prophet: Not installed
2026-01-29 15:29:43,549:INFO:None
2026-01-29 15:29:43,549:INFO:Set up data.
2026-01-29 15:29:43,754:INFO:Set up folding strategy.
2026-01-29 15:29:43,754:INFO:Set up train/test split.
2026-01-29 15:29:43,990:INFO:Set up index.
2026-01-29 15:29:43,993:INFO:Assigning column types.
2026-01-29 15:29:44,136:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-29 15:29:44,174:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 15:29:44,175:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:29:44,200:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:29:44,200:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:29:44,238:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 15:29:44,239:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:29:44,263:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:29:44,264:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:29:44,265:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-29 15:29:44,304:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:29:44,327:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:29:44,327:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:29:44,359:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:29:44,395:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:29:44,395:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:29:44,396:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-29 15:29:44,471:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:29:44,472:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:29:44,530:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:29:44,530:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:29:44,545:INFO:Preparing preprocessing pipeline...
2026-01-29 15:29:44,564:INFO:Set up simple imputation.
2026-01-29 15:29:44,564:INFO:Set up column transformation.
2026-01-29 15:29:44,564:INFO:Set up feature normalization.
2026-01-29 15:29:46,181:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\numpy\core\_methods.py:176: RuntimeWarning: overflow encountered in multiply

2026-01-29 15:29:47,054:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\preprocessing\_data.py:3408: RuntimeWarning: overflow encountered in power

2026-01-29 15:29:47,057:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\numpy\core\_methods.py:152: RuntimeWarning: overflow encountered in reduce

2026-01-29 15:32:51,925:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\306957030.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 15:33:59,293:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\1040826916.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 15:34:57,609:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\4055341079.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 15:34:59,879:INFO:PyCaret ClassificationExperiment
2026-01-29 15:34:59,879:INFO:Logging name: clf-default-name
2026-01-29 15:34:59,879:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-29 15:34:59,879:INFO:version 3.3.2
2026-01-29 15:34:59,879:INFO:Initializing setup()
2026-01-29 15:34:59,879:INFO:self.USI: 732d
2026-01-29 15:34:59,879:INFO:self._variable_keys: {'X_test', 'fold_groups_param', 'pipeline', 'fix_imbalance', 'exp_name_log', 'data', 'y_test', 'seed', 'fold_shuffle_param', 'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'logging_param', 'idx', 'y', 'target_param', 'fold_generator', 'y_train', 'gpu_param', 'USI', 'exp_id', '_available_plots', 'X', 'X_train', 'html_param', '_ml_usecase'}
2026-01-29 15:34:59,879:INFO:Checking environment
2026-01-29 15:34:59,879:INFO:python_version: 3.11.11
2026-01-29 15:34:59,879:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-29 15:34:59,879:INFO:machine: AMD64
2026-01-29 15:34:59,879:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-29 15:34:59,879:INFO:Memory: svmem(total=34009374720, available=15858475008, percent=53.4, used=18150899712, free=15858475008)
2026-01-29 15:34:59,879:INFO:Physical Core: 12
2026-01-29 15:34:59,879:INFO:Logical Core: 16
2026-01-29 15:34:59,879:INFO:Checking libraries
2026-01-29 15:34:59,879:INFO:System:
2026-01-29 15:34:59,879:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-29 15:34:59,879:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-29 15:34:59,879:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-29 15:34:59,879:INFO:PyCaret required dependencies:
2026-01-29 15:34:59,879:INFO:                 pip: 25.0
2026-01-29 15:34:59,879:INFO:          setuptools: 75.8.0
2026-01-29 15:34:59,879:INFO:             pycaret: 3.3.2
2026-01-29 15:34:59,879:INFO:             IPython: 9.9.0
2026-01-29 15:34:59,879:INFO:          ipywidgets: 8.1.8
2026-01-29 15:34:59,879:INFO:                tqdm: 4.67.1
2026-01-29 15:34:59,879:INFO:               numpy: 1.26.4
2026-01-29 15:34:59,879:INFO:              pandas: 2.1.4
2026-01-29 15:34:59,879:INFO:              jinja2: 3.1.6
2026-01-29 15:34:59,879:INFO:               scipy: 1.11.4
2026-01-29 15:34:59,879:INFO:              joblib: 1.3.2
2026-01-29 15:34:59,879:INFO:             sklearn: 1.4.2
2026-01-29 15:34:59,879:INFO:                pyod: 2.0.6
2026-01-29 15:34:59,879:INFO:            imblearn: 0.14.1
2026-01-29 15:34:59,879:INFO:   category_encoders: 2.7.0
2026-01-29 15:34:59,879:INFO:            lightgbm: 4.6.0
2026-01-29 15:34:59,879:INFO:               numba: 0.62.1
2026-01-29 15:34:59,879:INFO:            requests: 2.32.3
2026-01-29 15:34:59,879:INFO:          matplotlib: 3.7.5
2026-01-29 15:34:59,879:INFO:          scikitplot: 0.3.7
2026-01-29 15:34:59,879:INFO:         yellowbrick: 1.5
2026-01-29 15:34:59,879:INFO:              plotly: 5.24.1
2026-01-29 15:34:59,879:INFO:    plotly-resampler: Not installed
2026-01-29 15:34:59,879:INFO:             kaleido: 1.2.0
2026-01-29 15:34:59,879:INFO:           schemdraw: 0.15
2026-01-29 15:34:59,879:INFO:         statsmodels: 0.14.6
2026-01-29 15:34:59,879:INFO:              sktime: 0.26.0
2026-01-29 15:34:59,879:INFO:               tbats: 1.1.3
2026-01-29 15:34:59,879:INFO:            pmdarima: 2.0.4
2026-01-29 15:34:59,879:INFO:              psutil: 7.2.1
2026-01-29 15:34:59,879:INFO:          markupsafe: 3.0.3
2026-01-29 15:34:59,879:INFO:             pickle5: Not installed
2026-01-29 15:34:59,879:INFO:         cloudpickle: 3.0.0
2026-01-29 15:34:59,879:INFO:         deprecation: 2.1.0
2026-01-29 15:34:59,879:INFO:              xxhash: 3.6.0
2026-01-29 15:34:59,879:INFO:           wurlitzer: Not installed
2026-01-29 15:34:59,879:INFO:PyCaret optional dependencies:
2026-01-29 15:34:59,879:INFO:                shap: 0.44.1
2026-01-29 15:34:59,879:INFO:           interpret: 0.7.3
2026-01-29 15:34:59,879:INFO:                umap: 0.5.7
2026-01-29 15:34:59,888:INFO:     ydata_profiling: 4.18.1
2026-01-29 15:34:59,888:INFO:  explainerdashboard: 0.5.1
2026-01-29 15:34:59,889:INFO:             autoviz: Not installed
2026-01-29 15:34:59,889:INFO:           fairlearn: 0.7.0
2026-01-29 15:34:59,890:INFO:          deepchecks: Not installed
2026-01-29 15:34:59,890:INFO:             xgboost: Not installed
2026-01-29 15:34:59,890:INFO:            catboost: 1.2.8
2026-01-29 15:34:59,890:INFO:              kmodes: 0.12.2
2026-01-29 15:34:59,890:INFO:             mlxtend: 0.23.4
2026-01-29 15:34:59,890:INFO:       statsforecast: 1.5.0
2026-01-29 15:34:59,890:INFO:        tune_sklearn: Not installed
2026-01-29 15:34:59,890:INFO:                 ray: Not installed
2026-01-29 15:34:59,890:INFO:            hyperopt: 0.2.7
2026-01-29 15:34:59,890:INFO:              optuna: 4.6.0
2026-01-29 15:34:59,890:INFO:               skopt: 0.10.2
2026-01-29 15:34:59,890:INFO:              mlflow: 3.8.1
2026-01-29 15:34:59,890:INFO:              gradio: 6.3.0
2026-01-29 15:34:59,890:INFO:             fastapi: 0.128.0
2026-01-29 15:34:59,890:INFO:             uvicorn: 0.40.0
2026-01-29 15:34:59,890:INFO:              m2cgen: 0.10.0
2026-01-29 15:34:59,890:INFO:           evidently: 0.4.40
2026-01-29 15:34:59,890:INFO:               fugue: 0.8.7
2026-01-29 15:34:59,890:INFO:           streamlit: Not installed
2026-01-29 15:34:59,890:INFO:             prophet: Not installed
2026-01-29 15:34:59,890:INFO:None
2026-01-29 15:34:59,890:INFO:Set up data.
2026-01-29 15:34:59,923:INFO:Set up folding strategy.
2026-01-29 15:34:59,923:INFO:Set up train/test split.
2026-01-29 15:35:00,060:INFO:Set up index.
2026-01-29 15:35:00,073:INFO:Assigning column types.
2026-01-29 15:35:00,090:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-29 15:35:00,140:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 15:35:00,140:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:35:00,180:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:35:00,180:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:35:00,227:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 15:35:00,228:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:35:00,257:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:35:00,258:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:35:00,259:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-29 15:35:00,303:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:35:00,329:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:35:00,329:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:35:00,373:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 15:35:00,396:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:35:00,396:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:35:00,396:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-29 15:35:00,468:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:35:00,468:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:35:00,529:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:35:00,529:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:35:00,529:INFO:Preparing preprocessing pipeline...
2026-01-29 15:35:00,546:INFO:Set up simple imputation.
2026-01-29 15:35:00,546:INFO:Set up feature normalization.
2026-01-29 15:35:00,642:INFO:Finished creating preprocessing pipeline.
2026-01-29 15:35:00,646:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-29 15:35:00,646:INFO:Creating final display dataframe.
2026-01-29 15:35:00,911:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (429278, 4)
4        Transformed data shape       (429278, 4)
5   Transformed train set shape       (343422, 4)
6    Transformed test set shape        (85856, 4)
7               Ignore features                58
8              Numeric features                 3
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13                    Normalize              True
14             Normalize method            zscore
15               Fold Generator   StratifiedKFold
16                  Fold Number                 5
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              732d
2026-01-29 15:35:00,956:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:35:00,956:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:35:01,023:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 15:35:01,023:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 15:35:01,023:INFO:setup() successfully completed in 1.17s...............
2026-01-29 15:35:01,023:INFO:Initializing compare_models()
2026-01-29 15:35:01,023:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-29 15:35:01,023:INFO:Checking exceptions
2026-01-29 15:35:01,057:INFO:Preparing display monitor
2026-01-29 15:35:01,089:INFO:Initializing Logistic Regression
2026-01-29 15:35:01,090:INFO:Total runtime is 1.8715858459472656e-05 minutes
2026-01-29 15:35:01,093:INFO:SubProcess create_model() called ==================================
2026-01-29 15:35:01,094:INFO:Initializing create_model()
2026-01-29 15:35:01,094:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:35:01,094:INFO:Checking exceptions
2026-01-29 15:35:01,094:INFO:Importing libraries
2026-01-29 15:35:01,095:INFO:Copying training dataset
2026-01-29 15:35:01,178:INFO:Defining folds
2026-01-29 15:35:01,178:INFO:Declaring metric variables
2026-01-29 15:35:01,181:INFO:Importing untrained model
2026-01-29 15:35:01,184:INFO:Logistic Regression Imported successfully
2026-01-29 15:35:01,191:INFO:Starting cross validation
2026-01-29 15:35:01,192:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:35:12,429:INFO:Calculating mean and std
2026-01-29 15:35:12,429:INFO:Creating metrics dataframe
2026-01-29 15:35:12,429:INFO:Uploading results into container
2026-01-29 15:35:12,429:INFO:Uploading model into container now
2026-01-29 15:35:12,429:INFO:_master_model_container: 1
2026-01-29 15:35:12,436:INFO:_display_container: 2
2026-01-29 15:35:12,436:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 15:35:12,436:INFO:create_model() successfully completed......................................
2026-01-29 15:35:12,639:INFO:SubProcess create_model() end ==================================
2026-01-29 15:35:12,639:INFO:Creating metrics dataframe
2026-01-29 15:35:12,643:INFO:Initializing K Neighbors Classifier
2026-01-29 15:35:12,643:INFO:Total runtime is 0.1925755222638448 minutes
2026-01-29 15:35:12,645:INFO:SubProcess create_model() called ==================================
2026-01-29 15:35:12,645:INFO:Initializing create_model()
2026-01-29 15:35:12,645:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:35:12,645:INFO:Checking exceptions
2026-01-29 15:35:12,645:INFO:Importing libraries
2026-01-29 15:35:12,646:INFO:Copying training dataset
2026-01-29 15:35:12,695:INFO:Defining folds
2026-01-29 15:35:12,706:INFO:Declaring metric variables
2026-01-29 15:35:12,708:INFO:Importing untrained model
2026-01-29 15:35:12,708:INFO:K Neighbors Classifier Imported successfully
2026-01-29 15:35:12,708:INFO:Starting cross validation
2026-01-29 15:35:12,708:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:52:50,271:INFO:Calculating mean and std
2026-01-29 15:52:50,273:INFO:Creating metrics dataframe
2026-01-29 15:52:50,276:INFO:Uploading results into container
2026-01-29 15:52:50,277:INFO:Uploading model into container now
2026-01-29 15:52:50,277:INFO:_master_model_container: 2
2026-01-29 15:52:50,278:INFO:_display_container: 2
2026-01-29 15:52:50,278:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2026-01-29 15:52:50,278:INFO:create_model() successfully completed......................................
2026-01-29 15:52:50,493:INFO:SubProcess create_model() end ==================================
2026-01-29 15:52:50,493:INFO:Creating metrics dataframe
2026-01-29 15:52:50,498:INFO:Initializing Naive Bayes
2026-01-29 15:52:50,498:INFO:Total runtime is 17.823485747973123 minutes
2026-01-29 15:52:50,501:INFO:SubProcess create_model() called ==================================
2026-01-29 15:52:50,502:INFO:Initializing create_model()
2026-01-29 15:52:50,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:52:50,502:INFO:Checking exceptions
2026-01-29 15:52:50,502:INFO:Importing libraries
2026-01-29 15:52:50,502:INFO:Copying training dataset
2026-01-29 15:52:50,576:INFO:Defining folds
2026-01-29 15:52:50,576:INFO:Declaring metric variables
2026-01-29 15:52:50,581:INFO:Importing untrained model
2026-01-29 15:52:50,581:INFO:Naive Bayes Imported successfully
2026-01-29 15:52:50,595:INFO:Starting cross validation
2026-01-29 15:52:50,596:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:52:52,044:INFO:Calculating mean and std
2026-01-29 15:52:52,047:INFO:Creating metrics dataframe
2026-01-29 15:52:52,052:INFO:Uploading results into container
2026-01-29 15:52:52,053:INFO:Uploading model into container now
2026-01-29 15:52:52,054:INFO:_master_model_container: 3
2026-01-29 15:52:52,056:INFO:_display_container: 2
2026-01-29 15:52:52,057:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2026-01-29 15:52:52,057:INFO:create_model() successfully completed......................................
2026-01-29 15:52:52,389:INFO:SubProcess create_model() end ==================================
2026-01-29 15:52:52,389:INFO:Creating metrics dataframe
2026-01-29 15:52:52,402:INFO:Initializing Decision Tree Classifier
2026-01-29 15:52:52,402:INFO:Total runtime is 17.855215458075204 minutes
2026-01-29 15:52:52,407:INFO:SubProcess create_model() called ==================================
2026-01-29 15:52:52,408:INFO:Initializing create_model()
2026-01-29 15:52:52,408:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:52:52,409:INFO:Checking exceptions
2026-01-29 15:52:52,409:INFO:Importing libraries
2026-01-29 15:52:52,409:INFO:Copying training dataset
2026-01-29 15:52:52,533:INFO:Defining folds
2026-01-29 15:52:52,533:INFO:Declaring metric variables
2026-01-29 15:52:52,537:INFO:Importing untrained model
2026-01-29 15:52:52,542:INFO:Decision Tree Classifier Imported successfully
2026-01-29 15:52:52,552:INFO:Starting cross validation
2026-01-29 15:52:52,554:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:52:57,251:INFO:Calculating mean and std
2026-01-29 15:52:57,251:INFO:Creating metrics dataframe
2026-01-29 15:52:57,251:INFO:Uploading results into container
2026-01-29 15:52:57,251:INFO:Uploading model into container now
2026-01-29 15:52:57,251:INFO:_master_model_container: 4
2026-01-29 15:52:57,251:INFO:_display_container: 2
2026-01-29 15:52:57,251:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 15:52:57,251:INFO:create_model() successfully completed......................................
2026-01-29 15:52:57,451:INFO:SubProcess create_model() end ==================================
2026-01-29 15:52:57,451:INFO:Creating metrics dataframe
2026-01-29 15:52:57,456:INFO:Initializing SVM - Linear Kernel
2026-01-29 15:52:57,456:INFO:Total runtime is 17.939451269308723 minutes
2026-01-29 15:52:57,459:INFO:SubProcess create_model() called ==================================
2026-01-29 15:52:57,459:INFO:Initializing create_model()
2026-01-29 15:52:57,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:52:57,460:INFO:Checking exceptions
2026-01-29 15:52:57,460:INFO:Importing libraries
2026-01-29 15:52:57,460:INFO:Copying training dataset
2026-01-29 15:52:57,538:INFO:Defining folds
2026-01-29 15:52:57,538:INFO:Declaring metric variables
2026-01-29 15:52:57,542:INFO:Importing untrained model
2026-01-29 15:52:57,546:INFO:SVM - Linear Kernel Imported successfully
2026-01-29 15:52:57,550:INFO:Starting cross validation
2026-01-29 15:52:57,550:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:05,859:INFO:Calculating mean and std
2026-01-29 15:53:05,860:INFO:Creating metrics dataframe
2026-01-29 15:53:05,862:INFO:Uploading results into container
2026-01-29 15:53:05,862:INFO:Uploading model into container now
2026-01-29 15:53:05,863:INFO:_master_model_container: 5
2026-01-29 15:53:05,863:INFO:_display_container: 2
2026-01-29 15:53:05,864:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=42, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2026-01-29 15:53:05,864:INFO:create_model() successfully completed......................................
2026-01-29 15:53:06,108:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:06,109:INFO:Creating metrics dataframe
2026-01-29 15:53:06,114:INFO:Initializing Ridge Classifier
2026-01-29 15:53:06,114:INFO:Total runtime is 18.083745956420895 minutes
2026-01-29 15:53:06,114:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:06,114:INFO:Initializing create_model()
2026-01-29 15:53:06,114:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:06,114:INFO:Checking exceptions
2026-01-29 15:53:06,114:INFO:Importing libraries
2026-01-29 15:53:06,114:INFO:Copying training dataset
2026-01-29 15:53:06,230:INFO:Defining folds
2026-01-29 15:53:06,230:INFO:Declaring metric variables
2026-01-29 15:53:06,235:INFO:Importing untrained model
2026-01-29 15:53:06,240:INFO:Ridge Classifier Imported successfully
2026-01-29 15:53:06,249:INFO:Starting cross validation
2026-01-29 15:53:06,251:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:14,237:INFO:Calculating mean and std
2026-01-29 15:53:14,239:INFO:Creating metrics dataframe
2026-01-29 15:53:14,242:INFO:Uploading results into container
2026-01-29 15:53:14,243:INFO:Uploading model into container now
2026-01-29 15:53:14,243:INFO:_master_model_container: 6
2026-01-29 15:53:14,243:INFO:_display_container: 2
2026-01-29 15:53:14,243:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=42, solver='auto',
                tol=0.0001)
2026-01-29 15:53:14,243:INFO:create_model() successfully completed......................................
2026-01-29 15:53:14,441:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:14,442:INFO:Creating metrics dataframe
2026-01-29 15:53:14,443:INFO:Initializing Random Forest Classifier
2026-01-29 15:53:14,443:INFO:Total runtime is 18.222563024361925 minutes
2026-01-29 15:53:14,443:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:14,443:INFO:Initializing create_model()
2026-01-29 15:53:14,443:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:14,443:INFO:Checking exceptions
2026-01-29 15:53:14,443:INFO:Importing libraries
2026-01-29 15:53:14,443:INFO:Copying training dataset
2026-01-29 15:53:14,516:INFO:Defining folds
2026-01-29 15:53:14,523:INFO:Declaring metric variables
2026-01-29 15:53:14,526:INFO:Importing untrained model
2026-01-29 15:53:14,526:INFO:Random Forest Classifier Imported successfully
2026-01-29 15:53:14,533:INFO:Starting cross validation
2026-01-29 15:53:14,533:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:24,811:INFO:Calculating mean and std
2026-01-29 15:53:24,811:INFO:Creating metrics dataframe
2026-01-29 15:53:24,811:INFO:Uploading results into container
2026-01-29 15:53:24,811:INFO:Uploading model into container now
2026-01-29 15:53:24,811:INFO:_master_model_container: 7
2026-01-29 15:53:24,811:INFO:_display_container: 2
2026-01-29 15:53:24,811:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 15:53:24,811:INFO:create_model() successfully completed......................................
2026-01-29 15:53:24,985:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:24,986:INFO:Creating metrics dataframe
2026-01-29 15:53:24,991:INFO:Initializing Quadratic Discriminant Analysis
2026-01-29 15:53:24,991:INFO:Total runtime is 18.3983648498853 minutes
2026-01-29 15:53:24,991:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:24,991:INFO:Initializing create_model()
2026-01-29 15:53:24,991:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:24,991:INFO:Checking exceptions
2026-01-29 15:53:24,991:INFO:Importing libraries
2026-01-29 15:53:24,991:INFO:Copying training dataset
2026-01-29 15:53:25,058:INFO:Defining folds
2026-01-29 15:53:25,059:INFO:Declaring metric variables
2026-01-29 15:53:25,061:INFO:Importing untrained model
2026-01-29 15:53:25,064:INFO:Quadratic Discriminant Analysis Imported successfully
2026-01-29 15:53:25,064:INFO:Starting cross validation
2026-01-29 15:53:25,064:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:25,727:INFO:Calculating mean and std
2026-01-29 15:53:25,728:INFO:Creating metrics dataframe
2026-01-29 15:53:25,730:INFO:Uploading results into container
2026-01-29 15:53:25,730:INFO:Uploading model into container now
2026-01-29 15:53:25,730:INFO:_master_model_container: 8
2026-01-29 15:53:25,732:INFO:_display_container: 2
2026-01-29 15:53:25,732:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2026-01-29 15:53:25,732:INFO:create_model() successfully completed......................................
2026-01-29 15:53:25,925:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:25,925:INFO:Creating metrics dataframe
2026-01-29 15:53:25,932:INFO:Initializing Ada Boost Classifier
2026-01-29 15:53:25,933:INFO:Total runtime is 18.414068611462906 minutes
2026-01-29 15:53:25,936:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:25,936:INFO:Initializing create_model()
2026-01-29 15:53:25,937:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:25,937:INFO:Checking exceptions
2026-01-29 15:53:25,937:INFO:Importing libraries
2026-01-29 15:53:25,937:INFO:Copying training dataset
2026-01-29 15:53:26,012:INFO:Defining folds
2026-01-29 15:53:26,012:INFO:Declaring metric variables
2026-01-29 15:53:26,016:INFO:Importing untrained model
2026-01-29 15:53:26,020:INFO:Ada Boost Classifier Imported successfully
2026-01-29 15:53:26,028:INFO:Starting cross validation
2026-01-29 15:53:26,029:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:26,189:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2026-01-29 15:53:26,215:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2026-01-29 15:53:26,237:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2026-01-29 15:53:26,264:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2026-01-29 15:53:26,282:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2026-01-29 15:53:33,521:INFO:Calculating mean and std
2026-01-29 15:53:33,524:INFO:Creating metrics dataframe
2026-01-29 15:53:33,527:INFO:Uploading results into container
2026-01-29 15:53:33,528:INFO:Uploading model into container now
2026-01-29 15:53:33,528:INFO:_master_model_container: 9
2026-01-29 15:53:33,530:INFO:_display_container: 2
2026-01-29 15:53:33,531:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=42)
2026-01-29 15:53:33,531:INFO:create_model() successfully completed......................................
2026-01-29 15:53:33,725:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:33,725:INFO:Creating metrics dataframe
2026-01-29 15:53:33,733:INFO:Initializing Gradient Boosting Classifier
2026-01-29 15:53:33,733:INFO:Total runtime is 18.544077916940047 minutes
2026-01-29 15:53:33,737:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:33,738:INFO:Initializing create_model()
2026-01-29 15:53:33,738:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:33,738:INFO:Checking exceptions
2026-01-29 15:53:33,740:INFO:Importing libraries
2026-01-29 15:53:33,740:INFO:Copying training dataset
2026-01-29 15:53:33,822:INFO:Defining folds
2026-01-29 15:53:33,822:INFO:Declaring metric variables
2026-01-29 15:53:33,826:INFO:Importing untrained model
2026-01-29 15:53:33,830:INFO:Gradient Boosting Classifier Imported successfully
2026-01-29 15:53:33,835:INFO:Starting cross validation
2026-01-29 15:53:33,836:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:41,968:INFO:Calculating mean and std
2026-01-29 15:53:41,969:INFO:Creating metrics dataframe
2026-01-29 15:53:41,971:INFO:Uploading results into container
2026-01-29 15:53:41,972:INFO:Uploading model into container now
2026-01-29 15:53:41,972:INFO:_master_model_container: 10
2026-01-29 15:53:41,973:INFO:_display_container: 2
2026-01-29 15:53:41,973:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=42, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2026-01-29 15:53:41,973:INFO:create_model() successfully completed......................................
2026-01-29 15:53:42,168:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:42,168:INFO:Creating metrics dataframe
2026-01-29 15:53:42,175:INFO:Initializing Linear Discriminant Analysis
2026-01-29 15:53:42,176:INFO:Total runtime is 18.684785914421074 minutes
2026-01-29 15:53:42,178:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:42,178:INFO:Initializing create_model()
2026-01-29 15:53:42,178:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:42,178:INFO:Checking exceptions
2026-01-29 15:53:42,178:INFO:Importing libraries
2026-01-29 15:53:42,178:INFO:Copying training dataset
2026-01-29 15:53:42,247:INFO:Defining folds
2026-01-29 15:53:42,247:INFO:Declaring metric variables
2026-01-29 15:53:42,250:INFO:Importing untrained model
2026-01-29 15:53:42,250:INFO:Linear Discriminant Analysis Imported successfully
2026-01-29 15:53:42,260:INFO:Starting cross validation
2026-01-29 15:53:42,261:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:42,938:INFO:Calculating mean and std
2026-01-29 15:53:42,939:INFO:Creating metrics dataframe
2026-01-29 15:53:42,942:INFO:Uploading results into container
2026-01-29 15:53:42,942:INFO:Uploading model into container now
2026-01-29 15:53:42,943:INFO:_master_model_container: 11
2026-01-29 15:53:42,943:INFO:_display_container: 2
2026-01-29 15:53:42,943:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2026-01-29 15:53:42,943:INFO:create_model() successfully completed......................................
2026-01-29 15:53:43,137:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:43,138:INFO:Creating metrics dataframe
2026-01-29 15:53:43,145:INFO:Initializing Extra Trees Classifier
2026-01-29 15:53:43,146:INFO:Total runtime is 18.70095623334248 minutes
2026-01-29 15:53:43,149:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:43,149:INFO:Initializing create_model()
2026-01-29 15:53:43,149:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:43,149:INFO:Checking exceptions
2026-01-29 15:53:43,150:INFO:Importing libraries
2026-01-29 15:53:43,150:INFO:Copying training dataset
2026-01-29 15:53:43,242:INFO:Defining folds
2026-01-29 15:53:43,242:INFO:Declaring metric variables
2026-01-29 15:53:43,244:INFO:Importing untrained model
2026-01-29 15:53:43,244:INFO:Extra Trees Classifier Imported successfully
2026-01-29 15:53:43,256:INFO:Starting cross validation
2026-01-29 15:53:43,258:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:47,881:INFO:Calculating mean and std
2026-01-29 15:53:47,882:INFO:Creating metrics dataframe
2026-01-29 15:53:47,886:INFO:Uploading results into container
2026-01-29 15:53:47,887:INFO:Uploading model into container now
2026-01-29 15:53:47,887:INFO:_master_model_container: 12
2026-01-29 15:53:47,888:INFO:_display_container: 2
2026-01-29 15:53:47,889:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
2026-01-29 15:53:47,889:INFO:create_model() successfully completed......................................
2026-01-29 15:53:48,126:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:48,126:INFO:Creating metrics dataframe
2026-01-29 15:53:48,135:INFO:Initializing Light Gradient Boosting Machine
2026-01-29 15:53:48,135:INFO:Total runtime is 18.78410774469375 minutes
2026-01-29 15:53:48,139:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:48,140:INFO:Initializing create_model()
2026-01-29 15:53:48,141:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:48,141:INFO:Checking exceptions
2026-01-29 15:53:48,141:INFO:Importing libraries
2026-01-29 15:53:48,141:INFO:Copying training dataset
2026-01-29 15:53:48,246:INFO:Defining folds
2026-01-29 15:53:48,246:INFO:Declaring metric variables
2026-01-29 15:53:48,250:INFO:Importing untrained model
2026-01-29 15:53:48,255:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-29 15:53:48,262:INFO:Starting cross validation
2026-01-29 15:53:48,263:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:53:50,367:INFO:Calculating mean and std
2026-01-29 15:53:50,368:INFO:Creating metrics dataframe
2026-01-29 15:53:50,371:INFO:Uploading results into container
2026-01-29 15:53:50,372:INFO:Uploading model into container now
2026-01-29 15:53:50,372:INFO:_master_model_container: 13
2026-01-29 15:53:50,373:INFO:_display_container: 2
2026-01-29 15:53:50,374:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-29 15:53:50,375:INFO:create_model() successfully completed......................................
2026-01-29 15:53:50,573:INFO:SubProcess create_model() end ==================================
2026-01-29 15:53:50,573:INFO:Creating metrics dataframe
2026-01-29 15:53:50,582:INFO:Initializing CatBoost Classifier
2026-01-29 15:53:50,582:INFO:Total runtime is 18.824895695845278 minutes
2026-01-29 15:53:50,582:INFO:SubProcess create_model() called ==================================
2026-01-29 15:53:50,582:INFO:Initializing create_model()
2026-01-29 15:53:50,582:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=catboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:53:50,582:INFO:Checking exceptions
2026-01-29 15:53:50,582:INFO:Importing libraries
2026-01-29 15:53:50,582:INFO:Copying training dataset
2026-01-29 15:53:50,668:INFO:Defining folds
2026-01-29 15:53:50,668:INFO:Declaring metric variables
2026-01-29 15:53:50,672:INFO:Importing untrained model
2026-01-29 15:53:50,675:INFO:CatBoost Classifier Imported successfully
2026-01-29 15:53:50,675:INFO:Starting cross validation
2026-01-29 15:53:50,675:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:54:46,154:INFO:Calculating mean and std
2026-01-29 15:54:46,158:INFO:Creating metrics dataframe
2026-01-29 15:54:46,158:INFO:Uploading results into container
2026-01-29 15:54:46,164:INFO:Uploading model into container now
2026-01-29 15:54:46,165:INFO:_master_model_container: 14
2026-01-29 15:54:46,165:INFO:_display_container: 2
2026-01-29 15:54:46,165:INFO:<catboost.core.CatBoostClassifier object at 0x0000024870CE2650>
2026-01-29 15:54:46,165:INFO:create_model() successfully completed......................................
2026-01-29 15:54:46,341:INFO:SubProcess create_model() end ==================================
2026-01-29 15:54:46,341:INFO:Creating metrics dataframe
2026-01-29 15:54:46,350:INFO:Initializing Dummy Classifier
2026-01-29 15:54:46,357:INFO:Total runtime is 19.754346024990074 minutes
2026-01-29 15:54:46,357:INFO:SubProcess create_model() called ==================================
2026-01-29 15:54:46,357:INFO:Initializing create_model()
2026-01-29 15:54:46,357:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024818D55690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:54:46,357:INFO:Checking exceptions
2026-01-29 15:54:46,357:INFO:Importing libraries
2026-01-29 15:54:46,357:INFO:Copying training dataset
2026-01-29 15:54:46,432:INFO:Defining folds
2026-01-29 15:54:46,432:INFO:Declaring metric variables
2026-01-29 15:54:46,436:INFO:Importing untrained model
2026-01-29 15:54:46,439:INFO:Dummy Classifier Imported successfully
2026-01-29 15:54:46,443:INFO:Starting cross validation
2026-01-29 15:54:46,443:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:54:46,724:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2026-01-29 15:54:46,745:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2026-01-29 15:54:46,757:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2026-01-29 15:54:46,776:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2026-01-29 15:54:46,808:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2026-01-29 15:54:47,025:INFO:Calculating mean and std
2026-01-29 15:54:47,025:INFO:Creating metrics dataframe
2026-01-29 15:54:47,025:INFO:Uploading results into container
2026-01-29 15:54:47,025:INFO:Uploading model into container now
2026-01-29 15:54:47,025:INFO:_master_model_container: 15
2026-01-29 15:54:47,025:INFO:_display_container: 2
2026-01-29 15:54:47,025:INFO:DummyClassifier(constant=None, random_state=42, strategy='prior')
2026-01-29 15:54:47,025:INFO:create_model() successfully completed......................................
2026-01-29 15:54:47,207:INFO:SubProcess create_model() end ==================================
2026-01-29 15:54:47,207:INFO:Creating metrics dataframe
2026-01-29 15:54:47,222:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-29 15:54:47,236:INFO:Initializing create_model()
2026-01-29 15:54:47,236:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:54:47,239:INFO:Checking exceptions
2026-01-29 15:54:47,240:INFO:Importing libraries
2026-01-29 15:54:47,240:INFO:Copying training dataset
2026-01-29 15:54:47,336:INFO:Defining folds
2026-01-29 15:54:47,337:INFO:Declaring metric variables
2026-01-29 15:54:47,337:INFO:Importing untrained model
2026-01-29 15:54:47,337:INFO:Declaring custom model
2026-01-29 15:54:47,337:INFO:Logistic Regression Imported successfully
2026-01-29 15:54:47,338:INFO:Cross validation set to False
2026-01-29 15:54:47,338:INFO:Fitting Model
2026-01-29 15:54:47,607:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 15:54:47,607:INFO:create_model() successfully completed......................................
2026-01-29 15:54:47,789:INFO:Initializing create_model()
2026-01-29 15:54:47,790:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:54:47,790:INFO:Checking exceptions
2026-01-29 15:54:47,791:INFO:Importing libraries
2026-01-29 15:54:47,791:INFO:Copying training dataset
2026-01-29 15:54:47,872:INFO:Defining folds
2026-01-29 15:54:47,872:INFO:Declaring metric variables
2026-01-29 15:54:47,872:INFO:Importing untrained model
2026-01-29 15:54:47,872:INFO:Declaring custom model
2026-01-29 15:54:47,872:INFO:Naive Bayes Imported successfully
2026-01-29 15:54:47,872:INFO:Cross validation set to False
2026-01-29 15:54:47,872:INFO:Fitting Model
2026-01-29 15:54:47,940:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2026-01-29 15:54:47,940:INFO:create_model() successfully completed......................................
2026-01-29 15:54:48,107:INFO:Initializing create_model()
2026-01-29 15:54:48,107:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:54:48,107:INFO:Checking exceptions
2026-01-29 15:54:48,118:INFO:Importing libraries
2026-01-29 15:54:48,118:INFO:Copying training dataset
2026-01-29 15:54:48,179:INFO:Defining folds
2026-01-29 15:54:48,179:INFO:Declaring metric variables
2026-01-29 15:54:48,180:INFO:Importing untrained model
2026-01-29 15:54:48,180:INFO:Declaring custom model
2026-01-29 15:54:48,180:INFO:Decision Tree Classifier Imported successfully
2026-01-29 15:54:48,180:INFO:Cross validation set to False
2026-01-29 15:54:48,180:INFO:Fitting Model
2026-01-29 15:54:48,241:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 15:54:48,241:INFO:create_model() successfully completed......................................
2026-01-29 15:54:48,425:INFO:_master_model_container: 15
2026-01-29 15:54:48,425:INFO:_display_container: 2
2026-01-29 15:54:48,425:INFO:[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), GaussianNB(priors=None, var_smoothing=1e-09), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-29 15:54:48,425:INFO:compare_models() successfully completed......................................
2026-01-29 15:54:48,425:INFO:Initializing tune_model()
2026-01-29 15:54:48,425:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 15:54:48,425:INFO:Checking exceptions
2026-01-29 15:54:48,475:INFO:Copying training dataset
2026-01-29 15:54:48,544:INFO:Checking base model
2026-01-29 15:54:48,544:INFO:Base model : Logistic Regression
2026-01-29 15:54:48,548:INFO:Declaring metric variables
2026-01-29 15:54:48,552:INFO:Defining Hyperparameters
2026-01-29 15:54:48,729:INFO:Tuning with n_jobs=-1
2026-01-29 15:54:48,729:INFO:Initializing RandomizedSearchCV
2026-01-29 15:54:52,415:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 5.682}
2026-01-29 15:54:52,415:INFO:Hyperparameter search completed
2026-01-29 15:54:52,415:INFO:SubProcess create_model() called ==================================
2026-01-29 15:54:52,415:INFO:Initializing create_model()
2026-01-29 15:54:52,415:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002480469E050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'class_weight': {}, 'C': 5.682})
2026-01-29 15:54:52,415:INFO:Checking exceptions
2026-01-29 15:54:52,415:INFO:Importing libraries
2026-01-29 15:54:52,415:INFO:Copying training dataset
2026-01-29 15:54:52,490:INFO:Defining folds
2026-01-29 15:54:52,490:INFO:Declaring metric variables
2026-01-29 15:54:52,490:INFO:Importing untrained model
2026-01-29 15:54:52,490:INFO:Declaring custom model
2026-01-29 15:54:52,490:INFO:Logistic Regression Imported successfully
2026-01-29 15:54:52,505:INFO:Starting cross validation
2026-01-29 15:54:52,509:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:54:53,488:INFO:Calculating mean and std
2026-01-29 15:54:53,489:INFO:Creating metrics dataframe
2026-01-29 15:54:53,495:INFO:Finalizing model
2026-01-29 15:54:53,827:INFO:Uploading results into container
2026-01-29 15:54:53,828:INFO:Uploading model into container now
2026-01-29 15:54:53,829:INFO:_master_model_container: 16
2026-01-29 15:54:53,829:INFO:_display_container: 3
2026-01-29 15:54:53,830:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 15:54:53,830:INFO:create_model() successfully completed......................................
2026-01-29 15:54:54,012:INFO:SubProcess create_model() end ==================================
2026-01-29 15:54:54,012:INFO:choose_better activated
2026-01-29 15:54:54,015:INFO:SubProcess create_model() called ==================================
2026-01-29 15:54:54,016:INFO:Initializing create_model()
2026-01-29 15:54:54,016:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:54:54,016:INFO:Checking exceptions
2026-01-29 15:54:54,018:INFO:Importing libraries
2026-01-29 15:54:54,019:INFO:Copying training dataset
2026-01-29 15:54:54,074:INFO:Defining folds
2026-01-29 15:54:54,074:INFO:Declaring metric variables
2026-01-29 15:54:54,074:INFO:Importing untrained model
2026-01-29 15:54:54,074:INFO:Declaring custom model
2026-01-29 15:54:54,074:INFO:Logistic Regression Imported successfully
2026-01-29 15:54:54,074:INFO:Starting cross validation
2026-01-29 15:54:54,074:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:54:54,963:INFO:Calculating mean and std
2026-01-29 15:54:54,967:INFO:Creating metrics dataframe
2026-01-29 15:54:54,969:INFO:Finalizing model
2026-01-29 15:54:55,205:INFO:Uploading results into container
2026-01-29 15:54:55,206:INFO:Uploading model into container now
2026-01-29 15:54:55,206:INFO:_master_model_container: 17
2026-01-29 15:54:55,206:INFO:_display_container: 4
2026-01-29 15:54:55,207:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 15:54:55,207:INFO:create_model() successfully completed......................................
2026-01-29 15:54:55,422:INFO:SubProcess create_model() end ==================================
2026-01-29 15:54:55,423:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 15:54:55,423:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 15:54:55,423:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2026-01-29 15:54:55,423:INFO:choose_better completed
2026-01-29 15:54:55,424:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 15:54:55,429:INFO:_master_model_container: 17
2026-01-29 15:54:55,429:INFO:_display_container: 3
2026-01-29 15:54:55,429:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 15:54:55,429:INFO:tune_model() successfully completed......................................
2026-01-29 15:54:55,641:INFO:Initializing tune_model()
2026-01-29 15:54:55,641:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 15:54:55,641:INFO:Checking exceptions
2026-01-29 15:54:55,677:INFO:Copying training dataset
2026-01-29 15:54:55,754:INFO:Checking base model
2026-01-29 15:54:55,754:INFO:Base model : Naive Bayes
2026-01-29 15:54:55,758:INFO:Declaring metric variables
2026-01-29 15:54:55,762:INFO:Defining Hyperparameters
2026-01-29 15:54:55,983:INFO:Tuning with n_jobs=-1
2026-01-29 15:54:55,983:INFO:Initializing RandomizedSearchCV
2026-01-29 15:54:57,835:INFO:best_params: {'actual_estimator__var_smoothing': 2e-07}
2026-01-29 15:54:57,836:INFO:Hyperparameter search completed
2026-01-29 15:54:57,836:INFO:SubProcess create_model() called ==================================
2026-01-29 15:54:57,837:INFO:Initializing create_model()
2026-01-29 15:54:57,837:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024804682610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'var_smoothing': 2e-07})
2026-01-29 15:54:57,837:INFO:Checking exceptions
2026-01-29 15:54:57,838:INFO:Importing libraries
2026-01-29 15:54:57,838:INFO:Copying training dataset
2026-01-29 15:54:57,990:INFO:Defining folds
2026-01-29 15:54:57,990:INFO:Declaring metric variables
2026-01-29 15:54:57,995:INFO:Importing untrained model
2026-01-29 15:54:57,995:INFO:Declaring custom model
2026-01-29 15:54:58,000:INFO:Naive Bayes Imported successfully
2026-01-29 15:54:58,010:INFO:Starting cross validation
2026-01-29 15:54:58,011:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:54:58,780:INFO:Calculating mean and std
2026-01-29 15:54:58,780:INFO:Creating metrics dataframe
2026-01-29 15:54:58,790:INFO:Finalizing model
2026-01-29 15:54:58,883:INFO:Uploading results into container
2026-01-29 15:54:58,884:INFO:Uploading model into container now
2026-01-29 15:54:58,885:INFO:_master_model_container: 18
2026-01-29 15:54:58,885:INFO:_display_container: 4
2026-01-29 15:54:58,885:INFO:GaussianNB(priors=None, var_smoothing=2e-07)
2026-01-29 15:54:58,885:INFO:create_model() successfully completed......................................
2026-01-29 15:54:59,109:INFO:SubProcess create_model() end ==================================
2026-01-29 15:54:59,110:INFO:choose_better activated
2026-01-29 15:54:59,112:INFO:SubProcess create_model() called ==================================
2026-01-29 15:54:59,112:INFO:Initializing create_model()
2026-01-29 15:54:59,112:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:54:59,112:INFO:Checking exceptions
2026-01-29 15:54:59,114:INFO:Importing libraries
2026-01-29 15:54:59,114:INFO:Copying training dataset
2026-01-29 15:54:59,188:INFO:Defining folds
2026-01-29 15:54:59,188:INFO:Declaring metric variables
2026-01-29 15:54:59,188:INFO:Importing untrained model
2026-01-29 15:54:59,188:INFO:Declaring custom model
2026-01-29 15:54:59,189:INFO:Naive Bayes Imported successfully
2026-01-29 15:54:59,189:INFO:Starting cross validation
2026-01-29 15:54:59,190:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:54:59,915:INFO:Calculating mean and std
2026-01-29 15:54:59,916:INFO:Creating metrics dataframe
2026-01-29 15:54:59,918:INFO:Finalizing model
2026-01-29 15:54:59,995:INFO:Uploading results into container
2026-01-29 15:54:59,996:INFO:Uploading model into container now
2026-01-29 15:54:59,996:INFO:_master_model_container: 19
2026-01-29 15:54:59,996:INFO:_display_container: 5
2026-01-29 15:54:59,996:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2026-01-29 15:54:59,997:INFO:create_model() successfully completed......................................
2026-01-29 15:55:00,211:INFO:SubProcess create_model() end ==================================
2026-01-29 15:55:00,211:INFO:GaussianNB(priors=None, var_smoothing=1e-09) result for AUC is 0.5369
2026-01-29 15:55:00,212:INFO:GaussianNB(priors=None, var_smoothing=2e-07) result for AUC is 0.5369
2026-01-29 15:55:00,212:INFO:GaussianNB(priors=None, var_smoothing=1e-09) is best model
2026-01-29 15:55:00,212:INFO:choose_better completed
2026-01-29 15:55:00,212:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 15:55:00,223:INFO:_master_model_container: 19
2026-01-29 15:55:00,224:INFO:_display_container: 4
2026-01-29 15:55:00,224:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2026-01-29 15:55:00,224:INFO:tune_model() successfully completed......................................
2026-01-29 15:55:00,429:INFO:Initializing tune_model()
2026-01-29 15:55:00,430:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 15:55:00,430:INFO:Checking exceptions
2026-01-29 15:55:00,466:INFO:Copying training dataset
2026-01-29 15:55:00,543:INFO:Checking base model
2026-01-29 15:55:00,544:INFO:Base model : Decision Tree Classifier
2026-01-29 15:55:00,548:INFO:Declaring metric variables
2026-01-29 15:55:00,552:INFO:Defining Hyperparameters
2026-01-29 15:55:00,802:INFO:Tuning with n_jobs=-1
2026-01-29 15:55:00,802:INFO:Initializing RandomizedSearchCV
2026-01-29 15:55:02,562:INFO:best_params: {'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.0005, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 3, 'actual_estimator__criterion': 'gini'}
2026-01-29 15:55:02,563:INFO:Hyperparameter search completed
2026-01-29 15:55:02,564:INFO:SubProcess create_model() called ==================================
2026-01-29 15:55:02,564:INFO:Initializing create_model()
2026-01-29 15:55:02,565:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024870C3D290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 9, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0005, 'max_features': 1.0, 'max_depth': 3, 'criterion': 'gini'})
2026-01-29 15:55:02,566:INFO:Checking exceptions
2026-01-29 15:55:02,566:INFO:Importing libraries
2026-01-29 15:55:02,567:INFO:Copying training dataset
2026-01-29 15:55:02,706:INFO:Defining folds
2026-01-29 15:55:02,706:INFO:Declaring metric variables
2026-01-29 15:55:02,724:INFO:Importing untrained model
2026-01-29 15:55:02,724:INFO:Declaring custom model
2026-01-29 15:55:02,729:INFO:Decision Tree Classifier Imported successfully
2026-01-29 15:55:02,737:INFO:Starting cross validation
2026-01-29 15:55:02,738:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:55:03,450:INFO:Calculating mean and std
2026-01-29 15:55:03,452:INFO:Creating metrics dataframe
2026-01-29 15:55:03,458:INFO:Finalizing model
2026-01-29 15:55:03,607:INFO:Uploading results into container
2026-01-29 15:55:03,609:INFO:Uploading model into container now
2026-01-29 15:55:03,609:INFO:_master_model_container: 20
2026-01-29 15:55:03,609:INFO:_display_container: 5
2026-01-29 15:55:03,610:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 15:55:03,610:INFO:create_model() successfully completed......................................
2026-01-29 15:55:03,874:INFO:SubProcess create_model() end ==================================
2026-01-29 15:55:03,874:INFO:choose_better activated
2026-01-29 15:55:03,880:INFO:SubProcess create_model() called ==================================
2026-01-29 15:55:03,881:INFO:Initializing create_model()
2026-01-29 15:55:03,881:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 15:55:03,882:INFO:Checking exceptions
2026-01-29 15:55:03,884:INFO:Importing libraries
2026-01-29 15:55:03,884:INFO:Copying training dataset
2026-01-29 15:55:03,986:INFO:Defining folds
2026-01-29 15:55:03,986:INFO:Declaring metric variables
2026-01-29 15:55:03,986:INFO:Importing untrained model
2026-01-29 15:55:03,986:INFO:Declaring custom model
2026-01-29 15:55:03,986:INFO:Decision Tree Classifier Imported successfully
2026-01-29 15:55:03,987:INFO:Starting cross validation
2026-01-29 15:55:03,987:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 15:55:04,751:INFO:Calculating mean and std
2026-01-29 15:55:04,751:INFO:Creating metrics dataframe
2026-01-29 15:55:04,758:INFO:Finalizing model
2026-01-29 15:55:04,852:INFO:Uploading results into container
2026-01-29 15:55:04,853:INFO:Uploading model into container now
2026-01-29 15:55:04,853:INFO:_master_model_container: 21
2026-01-29 15:55:04,853:INFO:_display_container: 6
2026-01-29 15:55:04,854:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 15:55:04,854:INFO:create_model() successfully completed......................................
2026-01-29 15:55:05,120:INFO:SubProcess create_model() end ==================================
2026-01-29 15:55:05,122:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5369
2026-01-29 15:55:05,123:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5366
2026-01-29 15:55:05,124:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-29 15:55:05,124:INFO:choose_better completed
2026-01-29 15:55:05,125:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 15:55:05,142:INFO:_master_model_container: 21
2026-01-29 15:55:05,142:INFO:_display_container: 5
2026-01-29 15:55:05,144:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 15:55:05,144:INFO:tune_model() successfully completed......................................
2026-01-29 15:55:05,552:INFO:Initializing evaluate_model()
2026-01-29 15:55:05,555:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 15:55:05,613:INFO:Initializing plot_model()
2026-01-29 15:55:05,614:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:55:05,614:INFO:Checking exceptions
2026-01-29 15:55:05,661:INFO:Preloading libraries
2026-01-29 15:55:05,661:INFO:Copying training dataset
2026-01-29 15:55:05,661:INFO:Plot type: pipeline
2026-01-29 15:55:05,825:INFO:Visual Rendered Successfully
2026-01-29 15:55:06,016:INFO:plot_model() successfully completed......................................
2026-01-29 15:55:06,019:INFO:Initializing evaluate_model()
2026-01-29 15:55:06,019:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 15:55:06,050:INFO:Initializing plot_model()
2026-01-29 15:55:06,051:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:55:06,051:INFO:Checking exceptions
2026-01-29 15:55:06,082:INFO:Preloading libraries
2026-01-29 15:55:06,082:INFO:Copying training dataset
2026-01-29 15:55:06,082:INFO:Plot type: pipeline
2026-01-29 15:55:06,148:INFO:Visual Rendered Successfully
2026-01-29 15:55:06,324:INFO:plot_model() successfully completed......................................
2026-01-29 15:55:06,326:INFO:Initializing evaluate_model()
2026-01-29 15:55:06,326:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 15:55:06,370:INFO:Initializing plot_model()
2026-01-29 15:55:06,371:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:55:06,371:INFO:Checking exceptions
2026-01-29 15:55:06,408:INFO:Preloading libraries
2026-01-29 15:55:06,408:INFO:Copying training dataset
2026-01-29 15:55:06,408:INFO:Plot type: pipeline
2026-01-29 15:55:06,476:INFO:Visual Rendered Successfully
2026-01-29 15:55:06,656:INFO:plot_model() successfully completed......................................
2026-01-29 15:55:06,662:INFO:Initializing predict_model()
2026-01-29 15:55:06,662:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000248048BC720>)
2026-01-29 15:55:06,662:INFO:Checking exceptions
2026-01-29 15:55:06,662:INFO:Preloading libraries
2026-01-29 15:55:06,664:INFO:Set up data.
2026-01-29 15:55:06,673:INFO:Set up index.
2026-01-29 15:55:07,155:INFO:Initializing predict_model()
2026-01-29 15:55:07,155:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024810D7FC40>)
2026-01-29 15:55:07,155:INFO:Checking exceptions
2026-01-29 15:55:07,155:INFO:Preloading libraries
2026-01-29 15:55:07,158:INFO:Set up data.
2026-01-29 15:55:07,164:INFO:Set up index.
2026-01-29 15:55:07,640:INFO:Initializing predict_model()
2026-01-29 15:55:07,640:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000248048BC720>)
2026-01-29 15:55:07,640:INFO:Checking exceptions
2026-01-29 15:55:07,640:INFO:Preloading libraries
2026-01-29 15:55:07,640:INFO:Set up data.
2026-01-29 15:55:07,640:INFO:Set up index.
2026-01-29 15:55:08,108:INFO:Initializing plot_model()
2026-01-29 15:55:08,109:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-29 15:55:08,109:INFO:Checking exceptions
2026-01-29 15:55:08,132:INFO:Preloading libraries
2026-01-29 15:55:08,132:INFO:Copying training dataset
2026-01-29 15:55:08,132:INFO:Plot type: feature
2026-01-29 15:55:08,386:INFO:Visual Rendered Successfully
2026-01-29 15:55:08,561:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:15,782:INFO:Initializing plot_model()
2026-01-29 15:57:15,783:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=dimension, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:15,783:INFO:Checking exceptions
2026-01-29 15:57:15,812:INFO:Preloading libraries
2026-01-29 15:57:15,812:INFO:Copying training dataset
2026-01-29 15:57:15,812:INFO:Plot type: dimension
2026-01-29 15:57:15,933:INFO:Fitting StandardScaler()
2026-01-29 15:57:16,020:INFO:Fitting PCA()
2026-01-29 15:57:16,274:INFO:Fitting & Transforming Model
2026-01-29 15:57:16,291:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\yellowbrick\features\radviz.py:199: RuntimeWarning: invalid value encountered in divide

2026-01-29 15:57:21,111:INFO:Visual Rendered Successfully
2026-01-29 15:57:21,307:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:21,319:INFO:Initializing plot_model()
2026-01-29 15:57:21,319:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:21,319:INFO:Checking exceptions
2026-01-29 15:57:21,340:INFO:Preloading libraries
2026-01-29 15:57:21,340:INFO:Copying training dataset
2026-01-29 15:57:21,340:INFO:Plot type: pipeline
2026-01-29 15:57:21,393:INFO:Visual Rendered Successfully
2026-01-29 15:57:21,588:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:27,131:INFO:Initializing plot_model()
2026-01-29 15:57:27,132:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=calibration, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:27,132:INFO:Checking exceptions
2026-01-29 15:57:27,152:INFO:Preloading libraries
2026-01-29 15:57:27,152:INFO:Copying training dataset
2026-01-29 15:57:27,152:INFO:Plot type: calibration
2026-01-29 15:57:27,159:INFO:Scoring test/hold-out set
2026-01-29 15:57:27,297:INFO:Visual Rendered Successfully
2026-01-29 15:57:27,504:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:28,477:INFO:Initializing plot_model()
2026-01-29 15:57:28,477:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:28,477:INFO:Checking exceptions
2026-01-29 15:57:28,499:INFO:Preloading libraries
2026-01-29 15:57:28,499:INFO:Copying training dataset
2026-01-29 15:57:28,499:INFO:Plot type: pipeline
2026-01-29 15:57:28,547:INFO:Visual Rendered Successfully
2026-01-29 15:57:28,750:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:29,658:INFO:Initializing plot_model()
2026-01-29 15:57:29,658:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=calibration, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:29,659:INFO:Checking exceptions
2026-01-29 15:57:29,684:INFO:Preloading libraries
2026-01-29 15:57:29,684:INFO:Copying training dataset
2026-01-29 15:57:29,684:INFO:Plot type: calibration
2026-01-29 15:57:29,691:INFO:Scoring test/hold-out set
2026-01-29 15:57:29,825:INFO:Visual Rendered Successfully
2026-01-29 15:57:30,020:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:33,354:INFO:Initializing plot_model()
2026-01-29 15:57:33,356:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=dimension, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:33,356:INFO:Checking exceptions
2026-01-29 15:57:33,377:INFO:Preloading libraries
2026-01-29 15:57:33,377:INFO:Copying training dataset
2026-01-29 15:57:33,377:INFO:Plot type: dimension
2026-01-29 15:57:33,443:INFO:Fitting StandardScaler()
2026-01-29 15:57:33,513:INFO:Fitting PCA()
2026-01-29 15:57:33,756:INFO:Fitting & Transforming Model
2026-01-29 15:57:33,771:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\yellowbrick\features\radviz.py:199: RuntimeWarning: invalid value encountered in divide

2026-01-29 15:57:41,864:INFO:Visual Rendered Successfully
2026-01-29 15:57:42,103:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:42,136:INFO:Initializing plot_model()
2026-01-29 15:57:42,136:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=tree, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:42,136:INFO:Checking exceptions
2026-01-29 15:57:43,864:INFO:Initializing plot_model()
2026-01-29 15:57:43,864:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:43,864:INFO:Checking exceptions
2026-01-29 15:57:43,884:INFO:Preloading libraries
2026-01-29 15:57:43,885:INFO:Copying training dataset
2026-01-29 15:57:43,885:INFO:Plot type: learning
2026-01-29 15:57:44,051:INFO:Fitting Model
2026-01-29 15:57:51,354:INFO:Visual Rendered Successfully
2026-01-29 15:57:51,565:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:51,615:INFO:Initializing plot_model()
2026-01-29 15:57:51,615:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=calibration, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:51,615:INFO:Checking exceptions
2026-01-29 15:57:51,646:INFO:Preloading libraries
2026-01-29 15:57:51,646:INFO:Copying training dataset
2026-01-29 15:57:51,646:INFO:Plot type: calibration
2026-01-29 15:57:51,655:INFO:Scoring test/hold-out set
2026-01-29 15:57:51,848:INFO:Visual Rendered Successfully
2026-01-29 15:57:52,080:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:53,370:INFO:Initializing plot_model()
2026-01-29 15:57:53,371:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:53,371:INFO:Checking exceptions
2026-01-29 15:57:53,391:INFO:Preloading libraries
2026-01-29 15:57:53,391:INFO:Copying training dataset
2026-01-29 15:57:53,391:INFO:Plot type: feature
2026-01-29 15:57:53,580:INFO:Visual Rendered Successfully
2026-01-29 15:57:53,778:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:55,745:INFO:Initializing plot_model()
2026-01-29 15:57:55,745:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:55,745:INFO:Checking exceptions
2026-01-29 15:57:55,770:INFO:Preloading libraries
2026-01-29 15:57:55,770:INFO:Copying training dataset
2026-01-29 15:57:55,770:INFO:Plot type: feature_all
2026-01-29 15:57:56,029:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\matplotlib\_tight_bbox.py:67: RuntimeWarning: divide by zero encountered in scalar divide

2026-01-29 15:57:56,029:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\matplotlib\_tight_bbox.py:68: RuntimeWarning: divide by zero encountered in scalar divide

2026-01-29 15:57:56,029:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\matplotlib\patches.py:739: RuntimeWarning: invalid value encountered in scalar add

2026-01-29 15:57:56,029:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\matplotlib\transforms.py:2050: RuntimeWarning: invalid value encountered in scalar add

2026-01-29 15:57:56,046:INFO:Visual Rendered Successfully
2026-01-29 15:57:56,254:INFO:plot_model() successfully completed......................................
2026-01-29 15:57:59,486:INFO:Initializing plot_model()
2026-01-29 15:57:59,487:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=boundary, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:57:59,487:INFO:Checking exceptions
2026-01-29 15:57:59,514:INFO:Preloading libraries
2026-01-29 15:57:59,514:INFO:Copying training dataset
2026-01-29 15:57:59,514:INFO:Plot type: boundary
2026-01-29 15:57:59,620:INFO:Fitting StandardScaler()
2026-01-29 15:57:59,634:INFO:Fitting PCA()
2026-01-29 15:57:59,813:INFO:Fitting Model
2026-01-29 15:58:01,610:INFO:Visual Rendered Successfully
2026-01-29 15:58:01,860:INFO:plot_model() successfully completed......................................
2026-01-29 15:58:03,737:INFO:Initializing plot_model()
2026-01-29 15:58:03,738:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=gain, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:58:03,739:INFO:Checking exceptions
2026-01-29 15:58:03,761:INFO:Preloading libraries
2026-01-29 15:58:03,761:INFO:Copying training dataset
2026-01-29 15:58:03,761:INFO:Plot type: gain
2026-01-29 15:58:03,761:INFO:Generating predictions / predict_proba on X_test
2026-01-29 15:58:03,925:INFO:Visual Rendered Successfully
2026-01-29 15:58:04,113:INFO:plot_model() successfully completed......................................
2026-01-29 15:58:05,457:INFO:Initializing plot_model()
2026-01-29 15:58:05,457:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=tree, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:58:05,457:INFO:Checking exceptions
2026-01-29 15:58:06,613:INFO:Initializing plot_model()
2026-01-29 15:58:06,613:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:58:06,613:INFO:Checking exceptions
2026-01-29 15:58:06,633:INFO:Preloading libraries
2026-01-29 15:58:06,633:INFO:Copying training dataset
2026-01-29 15:58:06,633:INFO:Plot type: learning
2026-01-29 15:58:06,829:INFO:Fitting Model
2026-01-29 15:58:13,907:INFO:Visual Rendered Successfully
2026-01-29 15:58:14,118:INFO:plot_model() successfully completed......................................
2026-01-29 15:58:14,128:INFO:Initializing plot_model()
2026-01-29 15:58:14,128:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=rfe, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:58:14,128:INFO:Checking exceptions
2026-01-29 15:58:14,166:INFO:Preloading libraries
2026-01-29 15:58:14,166:INFO:Copying training dataset
2026-01-29 15:58:14,166:INFO:Plot type: rfe
2026-01-29 15:58:14,382:INFO:Fitting Model
2026-01-29 15:58:19,436:INFO:Visual Rendered Successfully
2026-01-29 15:58:19,632:INFO:plot_model() successfully completed......................................
2026-01-29 15:58:33,959:INFO:Initializing plot_model()
2026-01-29 15:58:33,959:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=gain, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:58:33,959:INFO:Checking exceptions
2026-01-29 15:58:33,999:INFO:Preloading libraries
2026-01-29 15:58:34,000:INFO:Copying training dataset
2026-01-29 15:58:34,000:INFO:Plot type: gain
2026-01-29 15:58:34,000:INFO:Generating predictions / predict_proba on X_test
2026-01-29 15:58:34,216:INFO:Visual Rendered Successfully
2026-01-29 15:58:34,430:INFO:plot_model() successfully completed......................................
2026-01-29 15:58:36,177:INFO:Initializing plot_model()
2026-01-29 15:58:36,177:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=lift, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:58:36,177:INFO:Checking exceptions
2026-01-29 15:58:36,199:INFO:Preloading libraries
2026-01-29 15:58:36,199:INFO:Copying training dataset
2026-01-29 15:58:36,199:INFO:Plot type: lift
2026-01-29 15:58:36,199:INFO:Generating predictions / predict_proba on X_test
2026-01-29 15:58:36,361:INFO:Visual Rendered Successfully
2026-01-29 15:58:36,563:INFO:plot_model() successfully completed......................................
2026-01-29 15:58:55,094:INFO:Initializing plot_model()
2026-01-29 15:58:55,094:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=GaussianNB(priors=None, var_smoothing=1e-09), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:58:55,094:INFO:Checking exceptions
2026-01-29 15:58:55,121:INFO:Preloading libraries
2026-01-29 15:58:55,121:INFO:Copying training dataset
2026-01-29 15:58:55,121:INFO:Plot type: learning
2026-01-29 15:58:55,330:INFO:Fitting Model
2026-01-29 15:58:56,460:INFO:Visual Rendered Successfully
2026-01-29 15:58:56,670:INFO:plot_model() successfully completed......................................
2026-01-29 15:59:04,839:INFO:Initializing plot_model()
2026-01-29 15:59:04,839:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F613050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 15:59:04,839:INFO:Checking exceptions
2026-01-29 15:59:04,870:INFO:Preloading libraries
2026-01-29 15:59:04,870:INFO:Copying training dataset
2026-01-29 15:59:04,870:INFO:Plot type: learning
2026-01-29 15:59:05,056:INFO:Fitting Model
2026-01-29 15:59:06,092:INFO:Visual Rendered Successfully
2026-01-29 15:59:06,293:INFO:plot_model() successfully completed......................................
2026-01-29 16:03:10,077:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\1833453759.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 16:03:12,331:INFO:PyCaret ClassificationExperiment
2026-01-29 16:03:12,332:INFO:Logging name: clf-default-name
2026-01-29 16:03:12,332:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-29 16:03:12,332:INFO:version 3.3.2
2026-01-29 16:03:12,332:INFO:Initializing setup()
2026-01-29 16:03:12,332:INFO:self.USI: e0d6
2026-01-29 16:03:12,332:INFO:self._variable_keys: {'X_test', 'fold_groups_param', 'pipeline', 'fix_imbalance', 'exp_name_log', 'data', 'y_test', 'seed', 'fold_shuffle_param', 'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'logging_param', 'idx', 'y', 'target_param', 'fold_generator', 'y_train', 'gpu_param', 'USI', 'exp_id', '_available_plots', 'X', 'X_train', 'html_param', '_ml_usecase'}
2026-01-29 16:03:12,332:INFO:Checking environment
2026-01-29 16:03:12,332:INFO:python_version: 3.11.11
2026-01-29 16:03:12,333:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-29 16:03:12,333:INFO:machine: AMD64
2026-01-29 16:03:12,334:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-29 16:03:12,334:INFO:Memory: svmem(total=34009374720, available=12534964224, percent=63.1, used=21474410496, free=12534964224)
2026-01-29 16:03:12,334:INFO:Physical Core: 12
2026-01-29 16:03:12,334:INFO:Logical Core: 16
2026-01-29 16:03:12,334:INFO:Checking libraries
2026-01-29 16:03:12,334:INFO:System:
2026-01-29 16:03:12,334:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-29 16:03:12,334:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-29 16:03:12,335:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-29 16:03:12,335:INFO:PyCaret required dependencies:
2026-01-29 16:03:12,335:INFO:                 pip: 25.0
2026-01-29 16:03:12,335:INFO:          setuptools: 75.8.0
2026-01-29 16:03:12,335:INFO:             pycaret: 3.3.2
2026-01-29 16:03:12,335:INFO:             IPython: 9.9.0
2026-01-29 16:03:12,335:INFO:          ipywidgets: 8.1.8
2026-01-29 16:03:12,335:INFO:                tqdm: 4.67.1
2026-01-29 16:03:12,335:INFO:               numpy: 1.26.4
2026-01-29 16:03:12,335:INFO:              pandas: 2.1.4
2026-01-29 16:03:12,335:INFO:              jinja2: 3.1.6
2026-01-29 16:03:12,335:INFO:               scipy: 1.11.4
2026-01-29 16:03:12,335:INFO:              joblib: 1.3.2
2026-01-29 16:03:12,335:INFO:             sklearn: 1.4.2
2026-01-29 16:03:12,335:INFO:                pyod: 2.0.6
2026-01-29 16:03:12,335:INFO:            imblearn: 0.14.1
2026-01-29 16:03:12,335:INFO:   category_encoders: 2.7.0
2026-01-29 16:03:12,335:INFO:            lightgbm: 4.6.0
2026-01-29 16:03:12,335:INFO:               numba: 0.62.1
2026-01-29 16:03:12,335:INFO:            requests: 2.32.3
2026-01-29 16:03:12,335:INFO:          matplotlib: 3.7.5
2026-01-29 16:03:12,336:INFO:          scikitplot: 0.3.7
2026-01-29 16:03:12,336:INFO:         yellowbrick: 1.5
2026-01-29 16:03:12,336:INFO:              plotly: 5.24.1
2026-01-29 16:03:12,336:INFO:    plotly-resampler: Not installed
2026-01-29 16:03:12,336:INFO:             kaleido: 1.2.0
2026-01-29 16:03:12,336:INFO:           schemdraw: 0.15
2026-01-29 16:03:12,336:INFO:         statsmodels: 0.14.6
2026-01-29 16:03:12,336:INFO:              sktime: 0.26.0
2026-01-29 16:03:12,336:INFO:               tbats: 1.1.3
2026-01-29 16:03:12,336:INFO:            pmdarima: 2.0.4
2026-01-29 16:03:12,336:INFO:              psutil: 7.2.1
2026-01-29 16:03:12,336:INFO:          markupsafe: 3.0.3
2026-01-29 16:03:12,336:INFO:             pickle5: Not installed
2026-01-29 16:03:12,336:INFO:         cloudpickle: 3.0.0
2026-01-29 16:03:12,336:INFO:         deprecation: 2.1.0
2026-01-29 16:03:12,336:INFO:              xxhash: 3.6.0
2026-01-29 16:03:12,336:INFO:           wurlitzer: Not installed
2026-01-29 16:03:12,336:INFO:PyCaret optional dependencies:
2026-01-29 16:03:12,336:INFO:                shap: 0.44.1
2026-01-29 16:03:12,337:INFO:           interpret: 0.7.3
2026-01-29 16:03:12,338:INFO:                umap: 0.5.7
2026-01-29 16:03:12,338:INFO:     ydata_profiling: 4.18.1
2026-01-29 16:03:12,339:INFO:  explainerdashboard: 0.5.1
2026-01-29 16:03:12,339:INFO:             autoviz: Not installed
2026-01-29 16:03:12,339:INFO:           fairlearn: 0.7.0
2026-01-29 16:03:12,339:INFO:          deepchecks: Not installed
2026-01-29 16:03:12,339:INFO:             xgboost: Not installed
2026-01-29 16:03:12,339:INFO:            catboost: 1.2.8
2026-01-29 16:03:12,339:INFO:              kmodes: 0.12.2
2026-01-29 16:03:12,339:INFO:             mlxtend: 0.23.4
2026-01-29 16:03:12,339:INFO:       statsforecast: 1.5.0
2026-01-29 16:03:12,339:INFO:        tune_sklearn: Not installed
2026-01-29 16:03:12,339:INFO:                 ray: Not installed
2026-01-29 16:03:12,341:INFO:            hyperopt: 0.2.7
2026-01-29 16:03:12,341:INFO:              optuna: 4.6.0
2026-01-29 16:03:12,341:INFO:               skopt: 0.10.2
2026-01-29 16:03:12,341:INFO:              mlflow: 3.8.1
2026-01-29 16:03:12,341:INFO:              gradio: 6.3.0
2026-01-29 16:03:12,341:INFO:             fastapi: 0.128.0
2026-01-29 16:03:12,341:INFO:             uvicorn: 0.40.0
2026-01-29 16:03:12,342:INFO:              m2cgen: 0.10.0
2026-01-29 16:03:12,342:INFO:           evidently: 0.4.40
2026-01-29 16:03:12,342:INFO:               fugue: 0.8.7
2026-01-29 16:03:12,342:INFO:           streamlit: Not installed
2026-01-29 16:03:12,342:INFO:             prophet: Not installed
2026-01-29 16:03:12,342:INFO:None
2026-01-29 16:03:12,342:INFO:Set up data.
2026-01-29 16:03:12,377:INFO:Set up folding strategy.
2026-01-29 16:03:12,377:INFO:Set up train/test split.
2026-01-29 16:03:12,487:INFO:Set up index.
2026-01-29 16:03:12,495:INFO:Assigning column types.
2026-01-29 16:03:12,512:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-29 16:03:12,546:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:03:12,546:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:03:12,564:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:03:12,564:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:03:12,585:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:03:12,585:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:03:12,613:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:03:12,613:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:03:12,614:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-29 16:03:12,634:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:03:12,653:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:03:12,653:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:03:12,684:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:03:12,703:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:03:12,709:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:03:12,709:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-29 16:03:12,753:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:03:12,753:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:03:12,803:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:03:12,803:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:03:12,808:INFO:Preparing preprocessing pipeline...
2026-01-29 16:03:12,815:INFO:Set up simple imputation.
2026-01-29 16:03:12,816:INFO:Set up feature normalization.
2026-01-29 16:03:12,902:INFO:Finished creating preprocessing pipeline.
2026-01-29 16:03:12,914:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-29 16:03:12,914:INFO:Creating final display dataframe.
2026-01-29 16:03:13,181:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (429278, 4)
4        Transformed data shape       (429278, 4)
5   Transformed train set shape       (343422, 4)
6    Transformed test set shape        (85856, 4)
7               Ignore features                58
8              Numeric features                 3
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13                    Normalize              True
14             Normalize method            zscore
15               Fold Generator   StratifiedKFold
16                  Fold Number                 5
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              e0d6
2026-01-29 16:03:13,230:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:03:13,230:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:03:13,287:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:03:13,289:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:03:13,291:INFO:setup() successfully completed in 0.97s...............
2026-01-29 16:03:13,291:INFO:Initializing compare_models()
2026-01-29 16:03:13,291:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002481F5FA790>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002481F5FA790>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-29 16:03:13,293:INFO:Checking exceptions
2026-01-29 16:03:13,333:INFO:Preparing display monitor
2026-01-29 16:03:13,354:INFO:Initializing Logistic Regression
2026-01-29 16:03:13,355:INFO:Total runtime is 7.474422454833985e-06 minutes
2026-01-29 16:03:13,360:INFO:SubProcess create_model() called ==================================
2026-01-29 16:03:13,360:INFO:Initializing create_model()
2026-01-29 16:03:13,360:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002481F5FA790>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002480EE11E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:03:13,360:INFO:Checking exceptions
2026-01-29 16:03:13,361:INFO:Importing libraries
2026-01-29 16:03:13,361:INFO:Copying training dataset
2026-01-29 16:03:13,481:INFO:Defining folds
2026-01-29 16:03:13,482:INFO:Declaring metric variables
2026-01-29 16:03:13,485:INFO:Importing untrained model
2026-01-29 16:03:13,489:INFO:Logistic Regression Imported successfully
2026-01-29 16:03:13,497:INFO:Starting cross validation
2026-01-29 16:03:13,498:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:03:14,246:INFO:Calculating mean and std
2026-01-29 16:03:14,247:INFO:Creating metrics dataframe
2026-01-29 16:03:14,249:INFO:Uploading results into container
2026-01-29 16:03:14,249:INFO:Uploading model into container now
2026-01-29 16:03:14,250:INFO:_master_model_container: 1
2026-01-29 16:03:14,250:INFO:_display_container: 2
2026-01-29 16:03:14,250:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:03:14,250:INFO:create_model() successfully completed......................................
2026-01-29 16:03:14,536:INFO:SubProcess create_model() end ==================================
2026-01-29 16:03:14,536:INFO:Creating metrics dataframe
2026-01-29 16:03:14,543:INFO:Initializing K Neighbors Classifier
2026-01-29 16:03:14,544:INFO:Total runtime is 0.019810458024342857 minutes
2026-01-29 16:03:14,547:INFO:SubProcess create_model() called ==================================
2026-01-29 16:03:14,548:INFO:Initializing create_model()
2026-01-29 16:03:14,548:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002481F5FA790>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002480EE11E50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:03:14,548:INFO:Checking exceptions
2026-01-29 16:03:14,548:INFO:Importing libraries
2026-01-29 16:03:14,548:INFO:Copying training dataset
2026-01-29 16:03:14,649:INFO:Defining folds
2026-01-29 16:03:14,649:INFO:Declaring metric variables
2026-01-29 16:03:14,656:INFO:Importing untrained model
2026-01-29 16:03:14,660:INFO:K Neighbors Classifier Imported successfully
2026-01-29 16:03:14,666:INFO:Starting cross validation
2026-01-29 16:03:14,667:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:09:45,976:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\2961347337.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 16:09:47,884:INFO:PyCaret ClassificationExperiment
2026-01-29 16:09:47,884:INFO:Logging name: clf-default-name
2026-01-29 16:09:47,885:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-29 16:09:47,885:INFO:version 3.3.2
2026-01-29 16:09:47,885:INFO:Initializing setup()
2026-01-29 16:09:47,885:INFO:self.USI: a7a0
2026-01-29 16:09:47,885:INFO:self._variable_keys: {'X_test', 'fold_groups_param', 'pipeline', 'fix_imbalance', 'exp_name_log', 'data', 'y_test', 'seed', 'fold_shuffle_param', 'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'logging_param', 'idx', 'y', 'target_param', 'fold_generator', 'y_train', 'gpu_param', 'USI', 'exp_id', '_available_plots', 'X', 'X_train', 'html_param', '_ml_usecase'}
2026-01-29 16:09:47,886:INFO:Checking environment
2026-01-29 16:09:47,886:INFO:python_version: 3.11.11
2026-01-29 16:09:47,887:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-29 16:09:47,887:INFO:machine: AMD64
2026-01-29 16:09:47,887:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-29 16:09:47,887:INFO:Memory: svmem(total=34009374720, available=15785549824, percent=53.6, used=18223824896, free=15785549824)
2026-01-29 16:09:47,887:INFO:Physical Core: 12
2026-01-29 16:09:47,887:INFO:Logical Core: 16
2026-01-29 16:09:47,887:INFO:Checking libraries
2026-01-29 16:09:47,887:INFO:System:
2026-01-29 16:09:47,887:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-29 16:09:47,887:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-29 16:09:47,887:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-29 16:09:47,887:INFO:PyCaret required dependencies:
2026-01-29 16:09:47,887:INFO:                 pip: 25.0
2026-01-29 16:09:47,888:INFO:          setuptools: 75.8.0
2026-01-29 16:09:47,888:INFO:             pycaret: 3.3.2
2026-01-29 16:09:47,888:INFO:             IPython: 9.9.0
2026-01-29 16:09:47,888:INFO:          ipywidgets: 8.1.8
2026-01-29 16:09:47,888:INFO:                tqdm: 4.67.1
2026-01-29 16:09:47,888:INFO:               numpy: 1.26.4
2026-01-29 16:09:47,888:INFO:              pandas: 2.1.4
2026-01-29 16:09:47,888:INFO:              jinja2: 3.1.6
2026-01-29 16:09:47,888:INFO:               scipy: 1.11.4
2026-01-29 16:09:47,888:INFO:              joblib: 1.3.2
2026-01-29 16:09:47,888:INFO:             sklearn: 1.4.2
2026-01-29 16:09:47,888:INFO:                pyod: 2.0.6
2026-01-29 16:09:47,888:INFO:            imblearn: 0.14.1
2026-01-29 16:09:47,888:INFO:   category_encoders: 2.7.0
2026-01-29 16:09:47,888:INFO:            lightgbm: 4.6.0
2026-01-29 16:09:47,888:INFO:               numba: 0.62.1
2026-01-29 16:09:47,888:INFO:            requests: 2.32.3
2026-01-29 16:09:47,888:INFO:          matplotlib: 3.7.5
2026-01-29 16:09:47,888:INFO:          scikitplot: 0.3.7
2026-01-29 16:09:47,888:INFO:         yellowbrick: 1.5
2026-01-29 16:09:47,888:INFO:              plotly: 5.24.1
2026-01-29 16:09:47,888:INFO:    plotly-resampler: Not installed
2026-01-29 16:09:47,888:INFO:             kaleido: 1.2.0
2026-01-29 16:09:47,888:INFO:           schemdraw: 0.15
2026-01-29 16:09:47,888:INFO:         statsmodels: 0.14.6
2026-01-29 16:09:47,888:INFO:              sktime: 0.26.0
2026-01-29 16:09:47,888:INFO:               tbats: 1.1.3
2026-01-29 16:09:47,888:INFO:            pmdarima: 2.0.4
2026-01-29 16:09:47,888:INFO:              psutil: 7.2.1
2026-01-29 16:09:47,888:INFO:          markupsafe: 3.0.3
2026-01-29 16:09:47,889:INFO:             pickle5: Not installed
2026-01-29 16:09:47,889:INFO:         cloudpickle: 3.0.0
2026-01-29 16:09:47,889:INFO:         deprecation: 2.1.0
2026-01-29 16:09:47,889:INFO:              xxhash: 3.6.0
2026-01-29 16:09:47,889:INFO:           wurlitzer: Not installed
2026-01-29 16:09:47,889:INFO:PyCaret optional dependencies:
2026-01-29 16:09:47,889:INFO:                shap: 0.44.1
2026-01-29 16:09:47,889:INFO:           interpret: 0.7.3
2026-01-29 16:09:47,889:INFO:                umap: 0.5.7
2026-01-29 16:09:47,889:INFO:     ydata_profiling: 4.18.1
2026-01-29 16:09:47,889:INFO:  explainerdashboard: 0.5.1
2026-01-29 16:09:47,889:INFO:             autoviz: Not installed
2026-01-29 16:09:47,889:INFO:           fairlearn: 0.7.0
2026-01-29 16:09:47,889:INFO:          deepchecks: Not installed
2026-01-29 16:09:47,890:INFO:             xgboost: Not installed
2026-01-29 16:09:47,890:INFO:            catboost: 1.2.8
2026-01-29 16:09:47,890:INFO:              kmodes: 0.12.2
2026-01-29 16:09:47,890:INFO:             mlxtend: 0.23.4
2026-01-29 16:09:47,890:INFO:       statsforecast: 1.5.0
2026-01-29 16:09:47,890:INFO:        tune_sklearn: Not installed
2026-01-29 16:09:47,890:INFO:                 ray: Not installed
2026-01-29 16:09:47,890:INFO:            hyperopt: 0.2.7
2026-01-29 16:09:47,890:INFO:              optuna: 4.6.0
2026-01-29 16:09:47,890:INFO:               skopt: 0.10.2
2026-01-29 16:09:47,890:INFO:              mlflow: 3.8.1
2026-01-29 16:09:47,890:INFO:              gradio: 6.3.0
2026-01-29 16:09:47,890:INFO:             fastapi: 0.128.0
2026-01-29 16:09:47,890:INFO:             uvicorn: 0.40.0
2026-01-29 16:09:47,891:INFO:              m2cgen: 0.10.0
2026-01-29 16:09:47,891:INFO:           evidently: 0.4.40
2026-01-29 16:09:47,891:INFO:               fugue: 0.8.7
2026-01-29 16:09:47,891:INFO:           streamlit: Not installed
2026-01-29 16:09:47,891:INFO:             prophet: Not installed
2026-01-29 16:09:47,891:INFO:None
2026-01-29 16:09:47,891:INFO:Set up data.
2026-01-29 16:09:47,923:INFO:Set up folding strategy.
2026-01-29 16:09:47,923:INFO:Set up train/test split.
2026-01-29 16:09:48,043:INFO:Set up index.
2026-01-29 16:09:48,043:INFO:Assigning column types.
2026-01-29 16:09:48,060:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-29 16:09:48,093:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:09:48,093:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:09:48,110:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:09:48,110:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:09:48,160:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:09:48,161:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:09:48,183:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:09:48,184:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:09:48,184:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-29 16:09:48,219:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:09:48,241:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:09:48,241:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:09:48,277:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:09:48,298:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:09:48,298:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:09:48,299:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-29 16:09:48,347:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:09:48,347:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:09:48,409:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:09:48,410:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:09:48,410:INFO:Preparing preprocessing pipeline...
2026-01-29 16:09:48,410:INFO:Set up simple imputation.
2026-01-29 16:09:48,410:INFO:Set up feature normalization.
2026-01-29 16:09:48,510:INFO:Finished creating preprocessing pipeline.
2026-01-29 16:09:48,510:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-29 16:09:48,510:INFO:Creating final display dataframe.
2026-01-29 16:09:48,710:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (429278, 4)
4        Transformed data shape       (429278, 4)
5   Transformed train set shape       (343422, 4)
6    Transformed test set shape        (85856, 4)
7               Ignore features                58
8              Numeric features                 3
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13                    Normalize              True
14             Normalize method            zscore
15               Fold Generator   StratifiedKFold
16                  Fold Number                 3
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              a7a0
2026-01-29 16:09:48,767:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:09:48,767:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:09:48,826:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:09:48,826:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:09:48,831:INFO:setup() successfully completed in 0.95s...............
2026-01-29 16:09:48,831:INFO:Initializing compare_models()
2026-01-29 16:09:48,831:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-29 16:09:48,831:INFO:Checking exceptions
2026-01-29 16:09:48,860:INFO:Preparing display monitor
2026-01-29 16:09:48,876:INFO:Initializing Logistic Regression
2026-01-29 16:09:48,876:INFO:Total runtime is 0.0 minutes
2026-01-29 16:09:48,878:INFO:SubProcess create_model() called ==================================
2026-01-29 16:09:48,879:INFO:Initializing create_model()
2026-01-29 16:09:48,879:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816D43CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:09:48,879:INFO:Checking exceptions
2026-01-29 16:09:48,879:INFO:Importing libraries
2026-01-29 16:09:48,879:INFO:Copying training dataset
2026-01-29 16:09:48,949:INFO:Defining folds
2026-01-29 16:09:48,950:INFO:Declaring metric variables
2026-01-29 16:09:48,952:INFO:Importing untrained model
2026-01-29 16:09:48,954:INFO:Logistic Regression Imported successfully
2026-01-29 16:09:48,959:INFO:Starting cross validation
2026-01-29 16:09:48,960:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:09:55,362:INFO:Calculating mean and std
2026-01-29 16:09:55,362:INFO:Creating metrics dataframe
2026-01-29 16:09:55,362:INFO:Uploading results into container
2026-01-29 16:09:55,362:INFO:Uploading model into container now
2026-01-29 16:09:55,366:INFO:_master_model_container: 1
2026-01-29 16:09:55,366:INFO:_display_container: 2
2026-01-29 16:09:55,367:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:09:55,368:INFO:create_model() successfully completed......................................
2026-01-29 16:09:55,680:INFO:SubProcess create_model() end ==================================
2026-01-29 16:09:55,680:INFO:Creating metrics dataframe
2026-01-29 16:09:55,686:INFO:Initializing Decision Tree Classifier
2026-01-29 16:09:55,686:INFO:Total runtime is 0.11348706881205241 minutes
2026-01-29 16:09:55,690:INFO:SubProcess create_model() called ==================================
2026-01-29 16:09:55,690:INFO:Initializing create_model()
2026-01-29 16:09:55,691:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816D43CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:09:55,691:INFO:Checking exceptions
2026-01-29 16:09:55,691:INFO:Importing libraries
2026-01-29 16:09:55,691:INFO:Copying training dataset
2026-01-29 16:09:55,760:INFO:Defining folds
2026-01-29 16:09:55,760:INFO:Declaring metric variables
2026-01-29 16:09:55,760:INFO:Importing untrained model
2026-01-29 16:09:55,777:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:09:55,777:INFO:Starting cross validation
2026-01-29 16:09:55,777:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:10:00,720:INFO:Calculating mean and std
2026-01-29 16:10:00,725:INFO:Creating metrics dataframe
2026-01-29 16:10:00,731:INFO:Uploading results into container
2026-01-29 16:10:00,731:INFO:Uploading model into container now
2026-01-29 16:10:00,733:INFO:_master_model_container: 2
2026-01-29 16:10:00,734:INFO:_display_container: 2
2026-01-29 16:10:00,735:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:10:00,735:INFO:create_model() successfully completed......................................
2026-01-29 16:10:00,943:INFO:SubProcess create_model() end ==================================
2026-01-29 16:10:00,943:INFO:Creating metrics dataframe
2026-01-29 16:10:00,943:INFO:Initializing Random Forest Classifier
2026-01-29 16:10:00,943:INFO:Total runtime is 0.2011061708132426 minutes
2026-01-29 16:10:00,951:INFO:SubProcess create_model() called ==================================
2026-01-29 16:10:00,951:INFO:Initializing create_model()
2026-01-29 16:10:00,951:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816D43CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:10:00,951:INFO:Checking exceptions
2026-01-29 16:10:00,951:INFO:Importing libraries
2026-01-29 16:10:00,951:INFO:Copying training dataset
2026-01-29 16:10:01,011:INFO:Defining folds
2026-01-29 16:10:01,011:INFO:Declaring metric variables
2026-01-29 16:10:01,011:INFO:Importing untrained model
2026-01-29 16:10:01,011:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:10:01,026:INFO:Starting cross validation
2026-01-29 16:10:01,027:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:10:08,114:INFO:Calculating mean and std
2026-01-29 16:10:08,114:INFO:Creating metrics dataframe
2026-01-29 16:10:08,114:INFO:Uploading results into container
2026-01-29 16:10:08,114:INFO:Uploading model into container now
2026-01-29 16:10:08,114:INFO:_master_model_container: 3
2026-01-29 16:10:08,114:INFO:_display_container: 2
2026-01-29 16:10:08,114:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:10:08,114:INFO:create_model() successfully completed......................................
2026-01-29 16:10:08,309:INFO:SubProcess create_model() end ==================================
2026-01-29 16:10:08,309:INFO:Creating metrics dataframe
2026-01-29 16:10:08,326:INFO:Initializing Light Gradient Boosting Machine
2026-01-29 16:10:08,326:INFO:Total runtime is 0.32416341304779056 minutes
2026-01-29 16:10:08,326:INFO:SubProcess create_model() called ==================================
2026-01-29 16:10:08,326:INFO:Initializing create_model()
2026-01-29 16:10:08,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816D43CD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:10:08,326:INFO:Checking exceptions
2026-01-29 16:10:08,326:INFO:Importing libraries
2026-01-29 16:10:08,326:INFO:Copying training dataset
2026-01-29 16:10:08,403:INFO:Defining folds
2026-01-29 16:10:08,403:INFO:Declaring metric variables
2026-01-29 16:10:08,406:INFO:Importing untrained model
2026-01-29 16:10:08,410:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-29 16:10:08,411:INFO:Starting cross validation
2026-01-29 16:10:08,411:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:10:15,067:INFO:Calculating mean and std
2026-01-29 16:10:15,067:INFO:Creating metrics dataframe
2026-01-29 16:10:15,071:INFO:Uploading results into container
2026-01-29 16:10:15,071:INFO:Uploading model into container now
2026-01-29 16:10:15,071:INFO:_master_model_container: 4
2026-01-29 16:10:15,071:INFO:_display_container: 2
2026-01-29 16:10:15,073:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-29 16:10:15,073:INFO:create_model() successfully completed......................................
2026-01-29 16:10:15,276:INFO:SubProcess create_model() end ==================================
2026-01-29 16:10:15,276:INFO:Creating metrics dataframe
2026-01-29 16:10:15,279:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-29 16:10:15,289:INFO:Initializing create_model()
2026-01-29 16:10:15,289:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:10:15,289:INFO:Checking exceptions
2026-01-29 16:10:15,294:INFO:Importing libraries
2026-01-29 16:10:15,294:INFO:Copying training dataset
2026-01-29 16:10:15,359:INFO:Defining folds
2026-01-29 16:10:15,359:INFO:Declaring metric variables
2026-01-29 16:10:15,359:INFO:Importing untrained model
2026-01-29 16:10:15,359:INFO:Declaring custom model
2026-01-29 16:10:15,359:INFO:Logistic Regression Imported successfully
2026-01-29 16:10:15,359:INFO:Cross validation set to False
2026-01-29 16:10:15,359:INFO:Fitting Model
2026-01-29 16:10:15,576:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:10:15,576:INFO:create_model() successfully completed......................................
2026-01-29 16:10:15,776:INFO:Initializing create_model()
2026-01-29 16:10:15,776:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:10:15,776:INFO:Checking exceptions
2026-01-29 16:10:15,787:INFO:Importing libraries
2026-01-29 16:10:15,787:INFO:Copying training dataset
2026-01-29 16:10:15,843:INFO:Defining folds
2026-01-29 16:10:15,843:INFO:Declaring metric variables
2026-01-29 16:10:15,843:INFO:Importing untrained model
2026-01-29 16:10:15,843:INFO:Declaring custom model
2026-01-29 16:10:15,843:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:10:15,843:INFO:Cross validation set to False
2026-01-29 16:10:15,843:INFO:Fitting Model
2026-01-29 16:10:15,909:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:10:15,909:INFO:create_model() successfully completed......................................
2026-01-29 16:10:16,110:INFO:Initializing create_model()
2026-01-29 16:10:16,110:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:10:16,124:INFO:Checking exceptions
2026-01-29 16:10:16,126:INFO:Importing libraries
2026-01-29 16:10:16,126:INFO:Copying training dataset
2026-01-29 16:10:16,185:INFO:Defining folds
2026-01-29 16:10:16,185:INFO:Declaring metric variables
2026-01-29 16:10:16,185:INFO:Importing untrained model
2026-01-29 16:10:16,185:INFO:Declaring custom model
2026-01-29 16:10:16,186:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:10:16,186:INFO:Cross validation set to False
2026-01-29 16:10:16,186:INFO:Fitting Model
2026-01-29 16:10:17,494:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:10:17,494:INFO:create_model() successfully completed......................................
2026-01-29 16:10:17,719:INFO:_master_model_container: 4
2026-01-29 16:10:17,719:INFO:_display_container: 2
2026-01-29 16:10:17,719:INFO:[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)]
2026-01-29 16:10:17,719:INFO:compare_models() successfully completed......................................
2026-01-29 16:10:17,719:INFO:Initializing tune_model()
2026-01-29 16:10:17,719:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:10:17,719:INFO:Checking exceptions
2026-01-29 16:10:17,761:INFO:Copying training dataset
2026-01-29 16:10:17,826:INFO:Checking base model
2026-01-29 16:10:17,827:INFO:Base model : Logistic Regression
2026-01-29 16:10:17,831:INFO:Declaring metric variables
2026-01-29 16:10:17,833:INFO:Defining Hyperparameters
2026-01-29 16:10:18,043:INFO:Tuning with n_jobs=-1
2026-01-29 16:10:18,043:INFO:Initializing RandomizedSearchCV
2026-01-29 16:10:24,819:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 5.682}
2026-01-29 16:10:24,819:INFO:Hyperparameter search completed
2026-01-29 16:10:24,819:INFO:SubProcess create_model() called ==================================
2026-01-29 16:10:24,826:INFO:Initializing create_model()
2026-01-29 16:10:24,826:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816DC4BD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'class_weight': {}, 'C': 5.682})
2026-01-29 16:10:24,827:INFO:Checking exceptions
2026-01-29 16:10:24,827:INFO:Importing libraries
2026-01-29 16:10:24,827:INFO:Copying training dataset
2026-01-29 16:10:24,893:INFO:Defining folds
2026-01-29 16:10:24,893:INFO:Declaring metric variables
2026-01-29 16:10:24,893:INFO:Importing untrained model
2026-01-29 16:10:24,893:INFO:Declaring custom model
2026-01-29 16:10:24,893:INFO:Logistic Regression Imported successfully
2026-01-29 16:10:24,909:INFO:Starting cross validation
2026-01-29 16:10:24,909:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:10:25,795:INFO:Calculating mean and std
2026-01-29 16:10:25,795:INFO:Creating metrics dataframe
2026-01-29 16:10:25,809:INFO:Finalizing model
2026-01-29 16:10:26,120:INFO:Uploading results into container
2026-01-29 16:10:26,121:INFO:Uploading model into container now
2026-01-29 16:10:26,122:INFO:_master_model_container: 5
2026-01-29 16:10:26,122:INFO:_display_container: 3
2026-01-29 16:10:26,122:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:10:26,122:INFO:create_model() successfully completed......................................
2026-01-29 16:10:26,343:INFO:SubProcess create_model() end ==================================
2026-01-29 16:10:26,343:INFO:choose_better activated
2026-01-29 16:10:26,343:INFO:SubProcess create_model() called ==================================
2026-01-29 16:10:26,343:INFO:Initializing create_model()
2026-01-29 16:10:26,343:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:10:26,343:INFO:Checking exceptions
2026-01-29 16:10:26,343:INFO:Importing libraries
2026-01-29 16:10:26,343:INFO:Copying training dataset
2026-01-29 16:10:26,424:INFO:Defining folds
2026-01-29 16:10:26,426:INFO:Declaring metric variables
2026-01-29 16:10:26,426:INFO:Importing untrained model
2026-01-29 16:10:26,426:INFO:Declaring custom model
2026-01-29 16:10:26,426:INFO:Logistic Regression Imported successfully
2026-01-29 16:10:26,426:INFO:Starting cross validation
2026-01-29 16:10:26,426:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:10:27,222:INFO:Calculating mean and std
2026-01-29 16:10:27,224:INFO:Creating metrics dataframe
2026-01-29 16:10:27,226:INFO:Finalizing model
2026-01-29 16:10:27,446:INFO:Uploading results into container
2026-01-29 16:10:27,460:INFO:Uploading model into container now
2026-01-29 16:10:27,460:INFO:_master_model_container: 6
2026-01-29 16:10:27,460:INFO:_display_container: 4
2026-01-29 16:10:27,460:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:10:27,460:INFO:create_model() successfully completed......................................
2026-01-29 16:10:27,659:INFO:SubProcess create_model() end ==================================
2026-01-29 16:10:27,659:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 16:10:27,659:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 16:10:27,659:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2026-01-29 16:10:27,659:INFO:choose_better completed
2026-01-29 16:10:27,659:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:10:27,676:INFO:_master_model_container: 6
2026-01-29 16:10:27,676:INFO:_display_container: 3
2026-01-29 16:10:27,676:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:10:27,676:INFO:tune_model() successfully completed......................................
2026-01-29 16:10:27,876:INFO:Initializing tune_model()
2026-01-29 16:10:27,876:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:10:27,876:INFO:Checking exceptions
2026-01-29 16:10:27,919:INFO:Copying training dataset
2026-01-29 16:10:28,002:INFO:Checking base model
2026-01-29 16:10:28,003:INFO:Base model : Decision Tree Classifier
2026-01-29 16:10:28,008:INFO:Declaring metric variables
2026-01-29 16:10:28,012:INFO:Defining Hyperparameters
2026-01-29 16:10:28,209:INFO:Tuning with n_jobs=-1
2026-01-29 16:10:28,209:INFO:Initializing RandomizedSearchCV
2026-01-29 16:10:29,211:INFO:best_params: {'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.0005, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 3, 'actual_estimator__criterion': 'gini'}
2026-01-29 16:10:29,211:INFO:Hyperparameter search completed
2026-01-29 16:10:29,211:INFO:SubProcess create_model() called ==================================
2026-01-29 16:10:29,211:INFO:Initializing create_model()
2026-01-29 16:10:29,211:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024871ADDC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 9, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0005, 'max_features': 1.0, 'max_depth': 3, 'criterion': 'gini'})
2026-01-29 16:10:29,211:INFO:Checking exceptions
2026-01-29 16:10:29,211:INFO:Importing libraries
2026-01-29 16:10:29,211:INFO:Copying training dataset
2026-01-29 16:10:29,276:INFO:Defining folds
2026-01-29 16:10:29,276:INFO:Declaring metric variables
2026-01-29 16:10:29,276:INFO:Importing untrained model
2026-01-29 16:10:29,276:INFO:Declaring custom model
2026-01-29 16:10:29,276:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:10:29,293:INFO:Starting cross validation
2026-01-29 16:10:29,293:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:10:29,963:INFO:Calculating mean and std
2026-01-29 16:10:29,963:INFO:Creating metrics dataframe
2026-01-29 16:10:29,963:INFO:Finalizing model
2026-01-29 16:10:30,026:INFO:Uploading results into container
2026-01-29 16:10:30,026:INFO:Uploading model into container now
2026-01-29 16:10:30,026:INFO:_master_model_container: 7
2026-01-29 16:10:30,026:INFO:_display_container: 4
2026-01-29 16:10:30,026:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:10:30,026:INFO:create_model() successfully completed......................................
2026-01-29 16:10:30,264:INFO:SubProcess create_model() end ==================================
2026-01-29 16:10:30,264:INFO:choose_better activated
2026-01-29 16:10:30,276:INFO:SubProcess create_model() called ==================================
2026-01-29 16:10:30,276:INFO:Initializing create_model()
2026-01-29 16:10:30,276:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:10:30,276:INFO:Checking exceptions
2026-01-29 16:10:30,276:INFO:Importing libraries
2026-01-29 16:10:30,276:INFO:Copying training dataset
2026-01-29 16:10:30,400:INFO:Defining folds
2026-01-29 16:10:30,400:INFO:Declaring metric variables
2026-01-29 16:10:30,400:INFO:Importing untrained model
2026-01-29 16:10:30,400:INFO:Declaring custom model
2026-01-29 16:10:30,400:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:10:30,400:INFO:Starting cross validation
2026-01-29 16:10:30,400:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:10:31,146:INFO:Calculating mean and std
2026-01-29 16:10:31,147:INFO:Creating metrics dataframe
2026-01-29 16:10:31,148:INFO:Finalizing model
2026-01-29 16:10:31,226:INFO:Uploading results into container
2026-01-29 16:10:31,226:INFO:Uploading model into container now
2026-01-29 16:10:31,226:INFO:_master_model_container: 8
2026-01-29 16:10:31,226:INFO:_display_container: 5
2026-01-29 16:10:31,226:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:10:31,226:INFO:create_model() successfully completed......................................
2026-01-29 16:10:31,493:INFO:SubProcess create_model() end ==================================
2026-01-29 16:10:31,493:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5369
2026-01-29 16:10:31,493:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5366
2026-01-29 16:10:31,493:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-29 16:10:31,493:INFO:choose_better completed
2026-01-29 16:10:31,493:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:10:31,503:INFO:_master_model_container: 8
2026-01-29 16:10:31,503:INFO:_display_container: 4
2026-01-29 16:10:31,503:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:10:31,503:INFO:tune_model() successfully completed......................................
2026-01-29 16:10:31,709:INFO:Initializing tune_model()
2026-01-29 16:10:31,709:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:10:31,709:INFO:Checking exceptions
2026-01-29 16:10:31,743:INFO:Copying training dataset
2026-01-29 16:10:31,816:INFO:Checking base model
2026-01-29 16:10:31,816:INFO:Base model : Random Forest Classifier
2026-01-29 16:10:31,819:INFO:Declaring metric variables
2026-01-29 16:10:31,823:INFO:Defining Hyperparameters
2026-01-29 16:10:32,043:INFO:Tuning with n_jobs=-1
2026-01-29 16:10:32,043:INFO:Initializing RandomizedSearchCV
2026-01-29 16:10:58,903:INFO:best_params: {'actual_estimator__n_estimators': 120, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-29 16:10:58,904:INFO:Hyperparameter search completed
2026-01-29 16:10:58,904:INFO:SubProcess create_model() called ==================================
2026-01-29 16:10:58,904:INFO:Initializing create_model()
2026-01-29 16:10:58,905:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024870A8CF90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': True})
2026-01-29 16:10:58,906:INFO:Checking exceptions
2026-01-29 16:10:58,906:INFO:Importing libraries
2026-01-29 16:10:58,906:INFO:Copying training dataset
2026-01-29 16:10:58,978:INFO:Defining folds
2026-01-29 16:10:58,978:INFO:Declaring metric variables
2026-01-29 16:10:58,981:INFO:Importing untrained model
2026-01-29 16:10:58,981:INFO:Declaring custom model
2026-01-29 16:10:58,981:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:10:58,993:INFO:Starting cross validation
2026-01-29 16:10:58,994:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:11:01,930:INFO:Calculating mean and std
2026-01-29 16:11:01,930:INFO:Creating metrics dataframe
2026-01-29 16:11:01,930:INFO:Finalizing model
2026-01-29 16:11:03,712:INFO:Uploading results into container
2026-01-29 16:11:03,712:INFO:Uploading model into container now
2026-01-29 16:11:03,712:INFO:_master_model_container: 9
2026-01-29 16:11:03,712:INFO:_display_container: 5
2026-01-29 16:11:03,712:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:11:03,712:INFO:create_model() successfully completed......................................
2026-01-29 16:11:03,960:INFO:SubProcess create_model() end ==================================
2026-01-29 16:11:03,960:INFO:choose_better activated
2026-01-29 16:11:03,975:INFO:SubProcess create_model() called ==================================
2026-01-29 16:11:03,975:INFO:Initializing create_model()
2026-01-29 16:11:03,975:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:11:03,975:INFO:Checking exceptions
2026-01-29 16:11:03,975:INFO:Importing libraries
2026-01-29 16:11:03,975:INFO:Copying training dataset
2026-01-29 16:11:04,045:INFO:Defining folds
2026-01-29 16:11:04,045:INFO:Declaring metric variables
2026-01-29 16:11:04,045:INFO:Importing untrained model
2026-01-29 16:11:04,045:INFO:Declaring custom model
2026-01-29 16:11:04,045:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:11:04,045:INFO:Starting cross validation
2026-01-29 16:11:04,045:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:11:06,566:INFO:Calculating mean and std
2026-01-29 16:11:06,566:INFO:Creating metrics dataframe
2026-01-29 16:11:06,566:INFO:Finalizing model
2026-01-29 16:11:08,046:INFO:Uploading results into container
2026-01-29 16:11:08,046:INFO:Uploading model into container now
2026-01-29 16:11:08,046:INFO:_master_model_container: 10
2026-01-29 16:11:08,046:INFO:_display_container: 6
2026-01-29 16:11:08,046:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:11:08,046:INFO:create_model() successfully completed......................................
2026-01-29 16:11:08,258:INFO:SubProcess create_model() end ==================================
2026-01-29 16:11:08,258:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.5369
2026-01-29 16:11:08,258:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.5369
2026-01-29 16:11:08,258:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-29 16:11:08,258:INFO:choose_better completed
2026-01-29 16:11:08,258:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:11:08,272:INFO:_master_model_container: 10
2026-01-29 16:11:08,272:INFO:_display_container: 5
2026-01-29 16:11:08,272:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:11:08,274:INFO:tune_model() successfully completed......................................
2026-01-29 16:11:08,491:INFO:Initializing evaluate_model()
2026-01-29 16:11:08,491:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:11:08,531:INFO:Initializing plot_model()
2026-01-29 16:11:08,531:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:11:08,532:INFO:Checking exceptions
2026-01-29 16:11:08,561:INFO:Preloading libraries
2026-01-29 16:11:08,561:INFO:Copying training dataset
2026-01-29 16:11:08,561:INFO:Plot type: pipeline
2026-01-29 16:11:08,628:INFO:Visual Rendered Successfully
2026-01-29 16:11:08,862:INFO:plot_model() successfully completed......................................
2026-01-29 16:11:08,866:INFO:Initializing evaluate_model()
2026-01-29 16:11:08,867:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:11:08,917:INFO:Initializing plot_model()
2026-01-29 16:11:08,918:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:11:08,918:INFO:Checking exceptions
2026-01-29 16:11:08,964:INFO:Preloading libraries
2026-01-29 16:11:08,964:INFO:Copying training dataset
2026-01-29 16:11:08,964:INFO:Plot type: pipeline
2026-01-29 16:11:09,024:INFO:Visual Rendered Successfully
2026-01-29 16:11:09,224:INFO:plot_model() successfully completed......................................
2026-01-29 16:11:09,240:INFO:Initializing evaluate_model()
2026-01-29 16:11:09,242:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:11:09,283:INFO:Initializing plot_model()
2026-01-29 16:11:09,283:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:11:09,283:INFO:Checking exceptions
2026-01-29 16:11:09,328:INFO:Preloading libraries
2026-01-29 16:11:09,346:INFO:Copying training dataset
2026-01-29 16:11:09,346:INFO:Plot type: pipeline
2026-01-29 16:11:09,412:INFO:Visual Rendered Successfully
2026-01-29 16:11:09,627:INFO:plot_model() successfully completed......................................
2026-01-29 16:11:09,636:INFO:Initializing predict_model()
2026-01-29 16:11:09,636:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024871470B80>)
2026-01-29 16:11:09,637:INFO:Checking exceptions
2026-01-29 16:11:09,637:INFO:Preloading libraries
2026-01-29 16:11:09,638:INFO:Set up data.
2026-01-29 16:11:09,647:INFO:Set up index.
2026-01-29 16:11:10,159:INFO:Initializing predict_model()
2026-01-29 16:11:10,159:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002480475F6A0>)
2026-01-29 16:11:10,159:INFO:Checking exceptions
2026-01-29 16:11:10,159:INFO:Preloading libraries
2026-01-29 16:11:10,175:INFO:Set up data.
2026-01-29 16:11:10,177:INFO:Set up index.
2026-01-29 16:11:10,729:INFO:Initializing predict_model()
2026-01-29 16:11:10,729:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024871470680>)
2026-01-29 16:11:10,729:INFO:Checking exceptions
2026-01-29 16:11:10,729:INFO:Preloading libraries
2026-01-29 16:11:10,731:INFO:Set up data.
2026-01-29 16:11:10,740:INFO:Set up index.
2026-01-29 16:11:11,434:INFO:Initializing plot_model()
2026-01-29 16:11:11,434:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000248171E9E90>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-29 16:11:11,434:INFO:Checking exceptions
2026-01-29 16:11:11,460:INFO:Preloading libraries
2026-01-29 16:11:11,460:INFO:Copying training dataset
2026-01-29 16:11:11,460:INFO:Plot type: feature
2026-01-29 16:11:11,714:INFO:Visual Rendered Successfully
2026-01-29 16:11:11,931:INFO:plot_model() successfully completed......................................
2026-01-29 16:11:11,931:INFO:Initializing save_model()
2026-01-29 16:11:11,931:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=..\modelos\modelo_general, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-29 16:11:11,931:INFO:Adding model into prep_pipe
2026-01-29 16:13:57,407:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\1855575028.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 16:13:59,973:INFO:PyCaret ClassificationExperiment
2026-01-29 16:13:59,973:INFO:Logging name: clf-default-name
2026-01-29 16:13:59,986:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-29 16:13:59,986:INFO:version 3.3.2
2026-01-29 16:13:59,986:INFO:Initializing setup()
2026-01-29 16:13:59,986:INFO:self.USI: b0bf
2026-01-29 16:13:59,986:INFO:self._variable_keys: {'X_test', 'fold_groups_param', 'pipeline', 'fix_imbalance', 'exp_name_log', 'data', 'y_test', 'seed', 'fold_shuffle_param', 'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'logging_param', 'idx', 'y', 'target_param', 'fold_generator', 'y_train', 'gpu_param', 'USI', 'exp_id', '_available_plots', 'X', 'X_train', 'html_param', '_ml_usecase'}
2026-01-29 16:13:59,988:INFO:Checking environment
2026-01-29 16:13:59,988:INFO:python_version: 3.11.11
2026-01-29 16:13:59,988:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-29 16:13:59,988:INFO:machine: AMD64
2026-01-29 16:13:59,988:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-29 16:13:59,988:INFO:Memory: svmem(total=34009374720, available=12933509120, percent=62.0, used=21075865600, free=12933509120)
2026-01-29 16:13:59,989:INFO:Physical Core: 12
2026-01-29 16:13:59,990:INFO:Logical Core: 16
2026-01-29 16:13:59,990:INFO:Checking libraries
2026-01-29 16:13:59,990:INFO:System:
2026-01-29 16:13:59,990:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-29 16:13:59,990:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-29 16:13:59,990:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-29 16:13:59,990:INFO:PyCaret required dependencies:
2026-01-29 16:13:59,990:INFO:                 pip: 25.0
2026-01-29 16:13:59,990:INFO:          setuptools: 75.8.0
2026-01-29 16:13:59,990:INFO:             pycaret: 3.3.2
2026-01-29 16:13:59,990:INFO:             IPython: 9.9.0
2026-01-29 16:13:59,990:INFO:          ipywidgets: 8.1.8
2026-01-29 16:13:59,990:INFO:                tqdm: 4.67.1
2026-01-29 16:13:59,990:INFO:               numpy: 1.26.4
2026-01-29 16:13:59,990:INFO:              pandas: 2.1.4
2026-01-29 16:13:59,990:INFO:              jinja2: 3.1.6
2026-01-29 16:13:59,990:INFO:               scipy: 1.11.4
2026-01-29 16:13:59,990:INFO:              joblib: 1.3.2
2026-01-29 16:13:59,990:INFO:             sklearn: 1.4.2
2026-01-29 16:13:59,990:INFO:                pyod: 2.0.6
2026-01-29 16:13:59,990:INFO:            imblearn: 0.14.1
2026-01-29 16:13:59,990:INFO:   category_encoders: 2.7.0
2026-01-29 16:13:59,990:INFO:            lightgbm: 4.6.0
2026-01-29 16:13:59,990:INFO:               numba: 0.62.1
2026-01-29 16:13:59,990:INFO:            requests: 2.32.3
2026-01-29 16:13:59,990:INFO:          matplotlib: 3.7.5
2026-01-29 16:13:59,990:INFO:          scikitplot: 0.3.7
2026-01-29 16:13:59,990:INFO:         yellowbrick: 1.5
2026-01-29 16:13:59,999:INFO:              plotly: 5.24.1
2026-01-29 16:13:59,999:INFO:    plotly-resampler: Not installed
2026-01-29 16:13:59,999:INFO:             kaleido: 1.2.0
2026-01-29 16:13:59,999:INFO:           schemdraw: 0.15
2026-01-29 16:13:59,999:INFO:         statsmodels: 0.14.6
2026-01-29 16:13:59,999:INFO:              sktime: 0.26.0
2026-01-29 16:13:59,999:INFO:               tbats: 1.1.3
2026-01-29 16:13:59,999:INFO:            pmdarima: 2.0.4
2026-01-29 16:13:59,999:INFO:              psutil: 7.2.1
2026-01-29 16:13:59,999:INFO:          markupsafe: 3.0.3
2026-01-29 16:13:59,999:INFO:             pickle5: Not installed
2026-01-29 16:14:00,005:INFO:         cloudpickle: 3.0.0
2026-01-29 16:14:00,005:INFO:         deprecation: 2.1.0
2026-01-29 16:14:00,006:INFO:              xxhash: 3.6.0
2026-01-29 16:14:00,006:INFO:           wurlitzer: Not installed
2026-01-29 16:14:00,006:INFO:PyCaret optional dependencies:
2026-01-29 16:14:00,006:INFO:                shap: 0.44.1
2026-01-29 16:14:00,007:INFO:           interpret: 0.7.3
2026-01-29 16:14:00,007:INFO:                umap: 0.5.7
2026-01-29 16:14:00,007:INFO:     ydata_profiling: 4.18.1
2026-01-29 16:14:00,007:INFO:  explainerdashboard: 0.5.1
2026-01-29 16:14:00,007:INFO:             autoviz: Not installed
2026-01-29 16:14:00,007:INFO:           fairlearn: 0.7.0
2026-01-29 16:14:00,007:INFO:          deepchecks: Not installed
2026-01-29 16:14:00,007:INFO:             xgboost: Not installed
2026-01-29 16:14:00,007:INFO:            catboost: 1.2.8
2026-01-29 16:14:00,007:INFO:              kmodes: 0.12.2
2026-01-29 16:14:00,007:INFO:             mlxtend: 0.23.4
2026-01-29 16:14:00,007:INFO:       statsforecast: 1.5.0
2026-01-29 16:14:00,007:INFO:        tune_sklearn: Not installed
2026-01-29 16:14:00,007:INFO:                 ray: Not installed
2026-01-29 16:14:00,007:INFO:            hyperopt: 0.2.7
2026-01-29 16:14:00,007:INFO:              optuna: 4.6.0
2026-01-29 16:14:00,007:INFO:               skopt: 0.10.2
2026-01-29 16:14:00,007:INFO:              mlflow: 3.8.1
2026-01-29 16:14:00,007:INFO:              gradio: 6.3.0
2026-01-29 16:14:00,007:INFO:             fastapi: 0.128.0
2026-01-29 16:14:00,007:INFO:             uvicorn: 0.40.0
2026-01-29 16:14:00,007:INFO:              m2cgen: 0.10.0
2026-01-29 16:14:00,007:INFO:           evidently: 0.4.40
2026-01-29 16:14:00,007:INFO:               fugue: 0.8.7
2026-01-29 16:14:00,015:INFO:           streamlit: Not installed
2026-01-29 16:14:00,015:INFO:             prophet: Not installed
2026-01-29 16:14:00,015:INFO:None
2026-01-29 16:14:00,015:INFO:Set up data.
2026-01-29 16:14:00,090:INFO:Set up folding strategy.
2026-01-29 16:14:00,090:INFO:Set up train/test split.
2026-01-29 16:14:00,474:INFO:Set up index.
2026-01-29 16:14:00,507:INFO:Assigning column types.
2026-01-29 16:14:00,586:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-29 16:14:00,686:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:14:00,689:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:14:00,741:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:14:00,741:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:14:00,824:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:14:00,824:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:14:00,873:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:14:00,873:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:14:00,874:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-29 16:14:00,940:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:14:00,973:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:14:00,987:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:14:01,040:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:14:01,090:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:14:01,090:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:14:01,090:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-29 16:14:01,191:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:14:01,191:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:14:01,290:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:14:01,290:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:14:01,290:INFO:Preparing preprocessing pipeline...
2026-01-29 16:14:01,308:INFO:Set up simple imputation.
2026-01-29 16:14:01,308:INFO:Set up feature normalization.
2026-01-29 16:14:01,456:INFO:Finished creating preprocessing pipeline.
2026-01-29 16:14:01,473:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-29 16:14:01,473:INFO:Creating final display dataframe.
2026-01-29 16:14:01,890:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (429278, 4)
4        Transformed data shape       (429278, 4)
5   Transformed train set shape       (343422, 4)
6    Transformed test set shape        (85856, 4)
7               Ignore features                58
8              Numeric features                 3
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13                    Normalize              True
14             Normalize method            zscore
15               Fold Generator   StratifiedKFold
16                  Fold Number                 3
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              b0bf
2026-01-29 16:14:01,973:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:14:01,973:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:14:02,073:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:14:02,073:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:14:02,073:INFO:setup() successfully completed in 2.1s...............
2026-01-29 16:14:02,073:INFO:Initializing compare_models()
2026-01-29 16:14:02,073:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-29 16:14:02,073:INFO:Checking exceptions
2026-01-29 16:14:02,128:INFO:Preparing display monitor
2026-01-29 16:14:02,166:INFO:Initializing Logistic Regression
2026-01-29 16:14:02,167:INFO:Total runtime is 1.7237663269042968e-05 minutes
2026-01-29 16:14:02,172:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:02,173:INFO:Initializing create_model()
2026-01-29 16:14:02,174:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002481729F910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:02,174:INFO:Checking exceptions
2026-01-29 16:14:02,174:INFO:Importing libraries
2026-01-29 16:14:02,174:INFO:Copying training dataset
2026-01-29 16:14:02,330:INFO:Defining folds
2026-01-29 16:14:02,330:INFO:Declaring metric variables
2026-01-29 16:14:02,340:INFO:Importing untrained model
2026-01-29 16:14:02,340:INFO:Logistic Regression Imported successfully
2026-01-29 16:14:02,359:INFO:Starting cross validation
2026-01-29 16:14:02,361:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:03,601:INFO:Calculating mean and std
2026-01-29 16:14:03,601:INFO:Creating metrics dataframe
2026-01-29 16:14:03,607:INFO:Uploading results into container
2026-01-29 16:14:03,609:INFO:Uploading model into container now
2026-01-29 16:14:03,610:INFO:_master_model_container: 1
2026-01-29 16:14:03,611:INFO:_display_container: 2
2026-01-29 16:14:03,611:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:14:03,612:INFO:create_model() successfully completed......................................
2026-01-29 16:14:03,947:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:03,947:INFO:Creating metrics dataframe
2026-01-29 16:14:03,957:INFO:Initializing Decision Tree Classifier
2026-01-29 16:14:03,957:INFO:Total runtime is 0.029845261573791505 minutes
2026-01-29 16:14:03,957:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:03,957:INFO:Initializing create_model()
2026-01-29 16:14:03,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002481729F910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:03,957:INFO:Checking exceptions
2026-01-29 16:14:03,957:INFO:Importing libraries
2026-01-29 16:14:03,957:INFO:Copying training dataset
2026-01-29 16:14:04,061:INFO:Defining folds
2026-01-29 16:14:04,061:INFO:Declaring metric variables
2026-01-29 16:14:04,070:INFO:Importing untrained model
2026-01-29 16:14:04,074:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:14:04,082:INFO:Starting cross validation
2026-01-29 16:14:04,083:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:04,760:INFO:Calculating mean and std
2026-01-29 16:14:04,760:INFO:Creating metrics dataframe
2026-01-29 16:14:04,760:INFO:Uploading results into container
2026-01-29 16:14:04,760:INFO:Uploading model into container now
2026-01-29 16:14:04,765:INFO:_master_model_container: 2
2026-01-29 16:14:04,765:INFO:_display_container: 2
2026-01-29 16:14:04,766:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:14:04,766:INFO:create_model() successfully completed......................................
2026-01-29 16:14:05,022:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:05,023:INFO:Creating metrics dataframe
2026-01-29 16:14:05,023:INFO:Initializing Random Forest Classifier
2026-01-29 16:14:05,023:INFO:Total runtime is 0.047616843382517496 minutes
2026-01-29 16:14:05,023:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:05,023:INFO:Initializing create_model()
2026-01-29 16:14:05,023:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002481729F910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:05,023:INFO:Checking exceptions
2026-01-29 16:14:05,023:INFO:Importing libraries
2026-01-29 16:14:05,023:INFO:Copying training dataset
2026-01-29 16:14:05,110:INFO:Defining folds
2026-01-29 16:14:05,111:INFO:Declaring metric variables
2026-01-29 16:14:05,114:INFO:Importing untrained model
2026-01-29 16:14:05,116:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:14:05,122:INFO:Starting cross validation
2026-01-29 16:14:05,123:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:10,285:INFO:Calculating mean and std
2026-01-29 16:14:10,289:INFO:Creating metrics dataframe
2026-01-29 16:14:10,290:INFO:Uploading results into container
2026-01-29 16:14:10,290:INFO:Uploading model into container now
2026-01-29 16:14:10,294:INFO:_master_model_container: 3
2026-01-29 16:14:10,294:INFO:_display_container: 2
2026-01-29 16:14:10,296:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:14:10,296:INFO:create_model() successfully completed......................................
2026-01-29 16:14:10,540:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:10,540:INFO:Creating metrics dataframe
2026-01-29 16:14:10,559:INFO:Initializing Light Gradient Boosting Machine
2026-01-29 16:14:10,560:INFO:Total runtime is 0.1398939530054728 minutes
2026-01-29 16:14:10,560:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:10,560:INFO:Initializing create_model()
2026-01-29 16:14:10,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002481729F910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:10,560:INFO:Checking exceptions
2026-01-29 16:14:10,560:INFO:Importing libraries
2026-01-29 16:14:10,560:INFO:Copying training dataset
2026-01-29 16:14:10,640:INFO:Defining folds
2026-01-29 16:14:10,640:INFO:Declaring metric variables
2026-01-29 16:14:10,640:INFO:Importing untrained model
2026-01-29 16:14:10,640:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-29 16:14:10,655:INFO:Starting cross validation
2026-01-29 16:14:10,657:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:12,436:INFO:Calculating mean and std
2026-01-29 16:14:12,440:INFO:Creating metrics dataframe
2026-01-29 16:14:12,442:INFO:Uploading results into container
2026-01-29 16:14:12,443:INFO:Uploading model into container now
2026-01-29 16:14:12,444:INFO:_master_model_container: 4
2026-01-29 16:14:12,444:INFO:_display_container: 2
2026-01-29 16:14:12,444:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-29 16:14:12,444:INFO:create_model() successfully completed......................................
2026-01-29 16:14:12,673:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:12,673:INFO:Creating metrics dataframe
2026-01-29 16:14:12,691:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-29 16:14:12,697:INFO:Initializing create_model()
2026-01-29 16:14:12,697:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:12,697:INFO:Checking exceptions
2026-01-29 16:14:12,697:INFO:Importing libraries
2026-01-29 16:14:12,697:INFO:Copying training dataset
2026-01-29 16:14:12,802:INFO:Defining folds
2026-01-29 16:14:12,802:INFO:Declaring metric variables
2026-01-29 16:14:12,802:INFO:Importing untrained model
2026-01-29 16:14:12,802:INFO:Declaring custom model
2026-01-29 16:14:12,802:INFO:Logistic Regression Imported successfully
2026-01-29 16:14:12,804:INFO:Cross validation set to False
2026-01-29 16:14:12,804:INFO:Fitting Model
2026-01-29 16:14:13,075:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:14:13,076:INFO:create_model() successfully completed......................................
2026-01-29 16:14:13,308:INFO:Initializing create_model()
2026-01-29 16:14:13,308:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:13,308:INFO:Checking exceptions
2026-01-29 16:14:13,308:INFO:Importing libraries
2026-01-29 16:14:13,313:INFO:Copying training dataset
2026-01-29 16:14:13,373:INFO:Defining folds
2026-01-29 16:14:13,373:INFO:Declaring metric variables
2026-01-29 16:14:13,373:INFO:Importing untrained model
2026-01-29 16:14:13,373:INFO:Declaring custom model
2026-01-29 16:14:13,373:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:14:13,373:INFO:Cross validation set to False
2026-01-29 16:14:13,373:INFO:Fitting Model
2026-01-29 16:14:13,455:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:14:13,455:INFO:create_model() successfully completed......................................
2026-01-29 16:14:13,687:INFO:Initializing create_model()
2026-01-29 16:14:13,687:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:13,687:INFO:Checking exceptions
2026-01-29 16:14:13,691:INFO:Importing libraries
2026-01-29 16:14:13,692:INFO:Copying training dataset
2026-01-29 16:14:13,755:INFO:Defining folds
2026-01-29 16:14:13,755:INFO:Declaring metric variables
2026-01-29 16:14:13,756:INFO:Importing untrained model
2026-01-29 16:14:13,756:INFO:Declaring custom model
2026-01-29 16:14:13,756:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:14:13,756:INFO:Cross validation set to False
2026-01-29 16:14:13,756:INFO:Fitting Model
2026-01-29 16:14:15,632:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:14:15,632:INFO:create_model() successfully completed......................................
2026-01-29 16:14:16,010:INFO:_master_model_container: 4
2026-01-29 16:14:16,010:INFO:_display_container: 2
2026-01-29 16:14:16,010:INFO:[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)]
2026-01-29 16:14:16,010:INFO:compare_models() successfully completed......................................
2026-01-29 16:14:16,010:INFO:Initializing tune_model()
2026-01-29 16:14:16,010:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:14:16,010:INFO:Checking exceptions
2026-01-29 16:14:16,062:INFO:Copying training dataset
2026-01-29 16:14:16,144:INFO:Checking base model
2026-01-29 16:14:16,145:INFO:Base model : Logistic Regression
2026-01-29 16:14:16,149:INFO:Declaring metric variables
2026-01-29 16:14:16,152:INFO:Defining Hyperparameters
2026-01-29 16:14:16,404:INFO:Tuning with n_jobs=-1
2026-01-29 16:14:16,404:INFO:Initializing RandomizedSearchCV
2026-01-29 16:14:19,085:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 5.682}
2026-01-29 16:14:19,085:INFO:Hyperparameter search completed
2026-01-29 16:14:19,085:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:19,088:INFO:Initializing create_model()
2026-01-29 16:14:19,088:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002481711D210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'class_weight': {}, 'C': 5.682})
2026-01-29 16:14:19,088:INFO:Checking exceptions
2026-01-29 16:14:19,088:INFO:Importing libraries
2026-01-29 16:14:19,088:INFO:Copying training dataset
2026-01-29 16:14:19,189:INFO:Defining folds
2026-01-29 16:14:19,189:INFO:Declaring metric variables
2026-01-29 16:14:19,193:INFO:Importing untrained model
2026-01-29 16:14:19,193:INFO:Declaring custom model
2026-01-29 16:14:19,193:INFO:Logistic Regression Imported successfully
2026-01-29 16:14:19,205:INFO:Starting cross validation
2026-01-29 16:14:19,206:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:20,078:INFO:Calculating mean and std
2026-01-29 16:14:20,078:INFO:Creating metrics dataframe
2026-01-29 16:14:20,078:INFO:Finalizing model
2026-01-29 16:14:20,403:INFO:Uploading results into container
2026-01-29 16:14:20,414:INFO:Uploading model into container now
2026-01-29 16:14:20,414:INFO:_master_model_container: 5
2026-01-29 16:14:20,414:INFO:_display_container: 3
2026-01-29 16:14:20,414:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:14:20,414:INFO:create_model() successfully completed......................................
2026-01-29 16:14:20,655:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:20,655:INFO:choose_better activated
2026-01-29 16:14:20,655:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:20,655:INFO:Initializing create_model()
2026-01-29 16:14:20,655:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:20,655:INFO:Checking exceptions
2026-01-29 16:14:20,655:INFO:Importing libraries
2026-01-29 16:14:20,655:INFO:Copying training dataset
2026-01-29 16:14:20,725:INFO:Defining folds
2026-01-29 16:14:20,725:INFO:Declaring metric variables
2026-01-29 16:14:20,725:INFO:Importing untrained model
2026-01-29 16:14:20,725:INFO:Declaring custom model
2026-01-29 16:14:20,725:INFO:Logistic Regression Imported successfully
2026-01-29 16:14:20,725:INFO:Starting cross validation
2026-01-29 16:14:20,725:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:21,370:INFO:Calculating mean and std
2026-01-29 16:14:21,370:INFO:Creating metrics dataframe
2026-01-29 16:14:21,374:INFO:Finalizing model
2026-01-29 16:14:21,576:INFO:Uploading results into container
2026-01-29 16:14:21,576:INFO:Uploading model into container now
2026-01-29 16:14:21,576:INFO:_master_model_container: 6
2026-01-29 16:14:21,576:INFO:_display_container: 4
2026-01-29 16:14:21,576:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:14:21,576:INFO:create_model() successfully completed......................................
2026-01-29 16:14:21,774:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:21,774:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 16:14:21,774:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 16:14:21,774:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2026-01-29 16:14:21,774:INFO:choose_better completed
2026-01-29 16:14:21,774:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:14:21,792:INFO:_master_model_container: 6
2026-01-29 16:14:21,792:INFO:_display_container: 3
2026-01-29 16:14:21,792:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:14:21,792:INFO:tune_model() successfully completed......................................
2026-01-29 16:14:21,993:INFO:Initializing tune_model()
2026-01-29 16:14:21,993:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:14:21,993:INFO:Checking exceptions
2026-01-29 16:14:22,032:INFO:Copying training dataset
2026-01-29 16:14:22,078:INFO:Checking base model
2026-01-29 16:14:22,078:INFO:Base model : Decision Tree Classifier
2026-01-29 16:14:22,081:INFO:Declaring metric variables
2026-01-29 16:14:22,083:INFO:Defining Hyperparameters
2026-01-29 16:14:22,292:INFO:Tuning with n_jobs=-1
2026-01-29 16:14:22,292:INFO:Initializing RandomizedSearchCV
2026-01-29 16:14:23,116:INFO:best_params: {'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.0005, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 3, 'actual_estimator__criterion': 'gini'}
2026-01-29 16:14:23,116:INFO:Hyperparameter search completed
2026-01-29 16:14:23,116:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:23,116:INFO:Initializing create_model()
2026-01-29 16:14:23,116:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024870AE0C50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 9, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0005, 'max_features': 1.0, 'max_depth': 3, 'criterion': 'gini'})
2026-01-29 16:14:23,116:INFO:Checking exceptions
2026-01-29 16:14:23,116:INFO:Importing libraries
2026-01-29 16:14:23,116:INFO:Copying training dataset
2026-01-29 16:14:23,173:INFO:Defining folds
2026-01-29 16:14:23,173:INFO:Declaring metric variables
2026-01-29 16:14:23,173:INFO:Importing untrained model
2026-01-29 16:14:23,173:INFO:Declaring custom model
2026-01-29 16:14:23,173:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:14:23,173:INFO:Starting cross validation
2026-01-29 16:14:23,173:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:23,652:INFO:Calculating mean and std
2026-01-29 16:14:23,652:INFO:Creating metrics dataframe
2026-01-29 16:14:23,659:INFO:Finalizing model
2026-01-29 16:14:23,722:INFO:Uploading results into container
2026-01-29 16:14:23,723:INFO:Uploading model into container now
2026-01-29 16:14:23,724:INFO:_master_model_container: 7
2026-01-29 16:14:23,724:INFO:_display_container: 4
2026-01-29 16:14:23,725:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:14:23,725:INFO:create_model() successfully completed......................................
2026-01-29 16:14:23,925:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:23,925:INFO:choose_better activated
2026-01-29 16:14:23,925:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:23,925:INFO:Initializing create_model()
2026-01-29 16:14:23,925:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:23,925:INFO:Checking exceptions
2026-01-29 16:14:23,925:INFO:Importing libraries
2026-01-29 16:14:23,925:INFO:Copying training dataset
2026-01-29 16:14:23,989:INFO:Defining folds
2026-01-29 16:14:23,989:INFO:Declaring metric variables
2026-01-29 16:14:23,989:INFO:Importing untrained model
2026-01-29 16:14:23,989:INFO:Declaring custom model
2026-01-29 16:14:23,989:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:14:23,989:INFO:Starting cross validation
2026-01-29 16:14:23,989:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:24,460:INFO:Calculating mean and std
2026-01-29 16:14:24,460:INFO:Creating metrics dataframe
2026-01-29 16:14:24,460:INFO:Finalizing model
2026-01-29 16:14:24,522:INFO:Uploading results into container
2026-01-29 16:14:24,525:INFO:Uploading model into container now
2026-01-29 16:14:24,525:INFO:_master_model_container: 8
2026-01-29 16:14:24,525:INFO:_display_container: 5
2026-01-29 16:14:24,525:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:14:24,525:INFO:create_model() successfully completed......................................
2026-01-29 16:14:24,738:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:24,738:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5369
2026-01-29 16:14:24,738:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5366
2026-01-29 16:14:24,738:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-29 16:14:24,738:INFO:choose_better completed
2026-01-29 16:14:24,738:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:14:24,738:INFO:_master_model_container: 8
2026-01-29 16:14:24,738:INFO:_display_container: 4
2026-01-29 16:14:24,738:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:14:24,738:INFO:tune_model() successfully completed......................................
2026-01-29 16:14:24,953:INFO:Initializing tune_model()
2026-01-29 16:14:24,953:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:14:24,953:INFO:Checking exceptions
2026-01-29 16:14:24,975:INFO:Copying training dataset
2026-01-29 16:14:25,028:INFO:Checking base model
2026-01-29 16:14:25,028:INFO:Base model : Random Forest Classifier
2026-01-29 16:14:25,030:INFO:Declaring metric variables
2026-01-29 16:14:25,032:INFO:Defining Hyperparameters
2026-01-29 16:14:25,240:INFO:Tuning with n_jobs=-1
2026-01-29 16:14:25,240:INFO:Initializing RandomizedSearchCV
2026-01-29 16:14:47,898:INFO:best_params: {'actual_estimator__n_estimators': 120, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-29 16:14:47,900:INFO:Hyperparameter search completed
2026-01-29 16:14:47,900:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:47,900:INFO:Initializing create_model()
2026-01-29 16:14:47,902:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002481B5D2C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': True})
2026-01-29 16:14:47,902:INFO:Checking exceptions
2026-01-29 16:14:47,902:INFO:Importing libraries
2026-01-29 16:14:47,902:INFO:Copying training dataset
2026-01-29 16:14:47,975:INFO:Defining folds
2026-01-29 16:14:47,975:INFO:Declaring metric variables
2026-01-29 16:14:47,977:INFO:Importing untrained model
2026-01-29 16:14:47,977:INFO:Declaring custom model
2026-01-29 16:14:47,981:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:14:47,984:INFO:Starting cross validation
2026-01-29 16:14:47,987:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:50,600:INFO:Calculating mean and std
2026-01-29 16:14:50,600:INFO:Creating metrics dataframe
2026-01-29 16:14:50,606:INFO:Finalizing model
2026-01-29 16:14:52,198:INFO:Uploading results into container
2026-01-29 16:14:52,198:INFO:Uploading model into container now
2026-01-29 16:14:52,198:INFO:_master_model_container: 9
2026-01-29 16:14:52,198:INFO:_display_container: 5
2026-01-29 16:14:52,198:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:14:52,198:INFO:create_model() successfully completed......................................
2026-01-29 16:14:52,421:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:52,421:INFO:choose_better activated
2026-01-29 16:14:52,421:INFO:SubProcess create_model() called ==================================
2026-01-29 16:14:52,421:INFO:Initializing create_model()
2026-01-29 16:14:52,421:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:14:52,421:INFO:Checking exceptions
2026-01-29 16:14:52,421:INFO:Importing libraries
2026-01-29 16:14:52,421:INFO:Copying training dataset
2026-01-29 16:14:52,489:INFO:Defining folds
2026-01-29 16:14:52,489:INFO:Declaring metric variables
2026-01-29 16:14:52,489:INFO:Importing untrained model
2026-01-29 16:14:52,489:INFO:Declaring custom model
2026-01-29 16:14:52,489:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:14:52,489:INFO:Starting cross validation
2026-01-29 16:14:52,489:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:14:54,667:INFO:Calculating mean and std
2026-01-29 16:14:54,667:INFO:Creating metrics dataframe
2026-01-29 16:14:54,672:INFO:Finalizing model
2026-01-29 16:14:56,041:INFO:Uploading results into container
2026-01-29 16:14:56,043:INFO:Uploading model into container now
2026-01-29 16:14:56,043:INFO:_master_model_container: 10
2026-01-29 16:14:56,043:INFO:_display_container: 6
2026-01-29 16:14:56,043:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:14:56,043:INFO:create_model() successfully completed......................................
2026-01-29 16:14:56,258:INFO:SubProcess create_model() end ==================================
2026-01-29 16:14:56,269:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.5369
2026-01-29 16:14:56,269:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.5369
2026-01-29 16:14:56,269:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-29 16:14:56,269:INFO:choose_better completed
2026-01-29 16:14:56,269:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:14:56,273:INFO:_master_model_container: 10
2026-01-29 16:14:56,273:INFO:_display_container: 5
2026-01-29 16:14:56,273:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:14:56,273:INFO:tune_model() successfully completed......................................
2026-01-29 16:14:56,489:INFO:Initializing evaluate_model()
2026-01-29 16:14:56,489:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:14:56,528:INFO:Initializing plot_model()
2026-01-29 16:14:56,528:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:14:56,528:INFO:Checking exceptions
2026-01-29 16:14:56,556:INFO:Preloading libraries
2026-01-29 16:14:56,556:INFO:Copying training dataset
2026-01-29 16:14:56,556:INFO:Plot type: pipeline
2026-01-29 16:14:56,622:INFO:Visual Rendered Successfully
2026-01-29 16:14:56,822:INFO:plot_model() successfully completed......................................
2026-01-29 16:14:56,838:INFO:Initializing evaluate_model()
2026-01-29 16:14:56,838:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:14:56,889:INFO:Initializing plot_model()
2026-01-29 16:14:56,890:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:14:56,890:INFO:Checking exceptions
2026-01-29 16:14:56,929:INFO:Preloading libraries
2026-01-29 16:14:56,930:INFO:Copying training dataset
2026-01-29 16:14:56,930:INFO:Plot type: pipeline
2026-01-29 16:14:56,989:INFO:Visual Rendered Successfully
2026-01-29 16:14:57,205:INFO:plot_model() successfully completed......................................
2026-01-29 16:14:57,205:INFO:Initializing evaluate_model()
2026-01-29 16:14:57,205:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:14:57,241:INFO:Initializing plot_model()
2026-01-29 16:14:57,241:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:14:57,241:INFO:Checking exceptions
2026-01-29 16:14:57,292:INFO:Preloading libraries
2026-01-29 16:14:57,292:INFO:Copying training dataset
2026-01-29 16:14:57,292:INFO:Plot type: pipeline
2026-01-29 16:14:57,355:INFO:Visual Rendered Successfully
2026-01-29 16:14:57,574:INFO:plot_model() successfully completed......................................
2026-01-29 16:14:57,583:INFO:Initializing predict_model()
2026-01-29 16:14:57,583:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000024810D7FE20>)
2026-01-29 16:14:57,583:INFO:Checking exceptions
2026-01-29 16:14:57,583:INFO:Preloading libraries
2026-01-29 16:14:57,585:INFO:Set up data.
2026-01-29 16:14:57,594:INFO:Set up index.
2026-01-29 16:14:58,122:INFO:Initializing predict_model()
2026-01-29 16:14:58,122:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002487E4E7BA0>)
2026-01-29 16:14:58,122:INFO:Checking exceptions
2026-01-29 16:14:58,122:INFO:Preloading libraries
2026-01-29 16:14:58,122:INFO:Set up data.
2026-01-29 16:14:58,122:INFO:Set up index.
2026-01-29 16:14:58,674:INFO:Initializing predict_model()
2026-01-29 16:14:58,674:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002487E426200>)
2026-01-29 16:14:58,674:INFO:Checking exceptions
2026-01-29 16:14:58,674:INFO:Preloading libraries
2026-01-29 16:14:58,674:INFO:Set up data.
2026-01-29 16:14:58,674:INFO:Set up index.
2026-01-29 16:14:59,338:INFO:Initializing plot_model()
2026-01-29 16:14:59,338:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D82D50>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-29 16:14:59,340:INFO:Checking exceptions
2026-01-29 16:14:59,355:INFO:Preloading libraries
2026-01-29 16:14:59,355:INFO:Copying training dataset
2026-01-29 16:14:59,355:INFO:Plot type: feature
2026-01-29 16:14:59,608:INFO:Visual Rendered Successfully
2026-01-29 16:14:59,828:INFO:plot_model() successfully completed......................................
2026-01-29 16:14:59,828:INFO:Initializing save_model()
2026-01-29 16:14:59,828:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=..\datos\04. Modelos, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-29 16:14:59,828:INFO:Adding model into prep_pipe
2026-01-29 16:14:59,838:INFO:..\datos\04. Modelos.pkl saved in current working directory
2026-01-29 16:14:59,838:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(e...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=42,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2026-01-29 16:14:59,838:INFO:save_model() successfully completed......................................
2026-01-29 16:16:46,887:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\2531746454.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 16:16:48,804:INFO:PyCaret ClassificationExperiment
2026-01-29 16:16:48,804:INFO:Logging name: clf-default-name
2026-01-29 16:16:48,804:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-29 16:16:48,804:INFO:version 3.3.2
2026-01-29 16:16:48,804:INFO:Initializing setup()
2026-01-29 16:16:48,804:INFO:self.USI: 8fc6
2026-01-29 16:16:48,807:INFO:self._variable_keys: {'X_test', 'fold_groups_param', 'pipeline', 'fix_imbalance', 'exp_name_log', 'data', 'y_test', 'seed', 'fold_shuffle_param', 'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'logging_param', 'idx', 'y', 'target_param', 'fold_generator', 'y_train', 'gpu_param', 'USI', 'exp_id', '_available_plots', 'X', 'X_train', 'html_param', '_ml_usecase'}
2026-01-29 16:16:48,807:INFO:Checking environment
2026-01-29 16:16:48,807:INFO:python_version: 3.11.11
2026-01-29 16:16:48,807:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-29 16:16:48,807:INFO:machine: AMD64
2026-01-29 16:16:48,807:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-29 16:16:48,807:INFO:Memory: svmem(total=34009374720, available=13133869056, percent=61.4, used=20875505664, free=13133869056)
2026-01-29 16:16:48,807:INFO:Physical Core: 12
2026-01-29 16:16:48,807:INFO:Logical Core: 16
2026-01-29 16:16:48,807:INFO:Checking libraries
2026-01-29 16:16:48,807:INFO:System:
2026-01-29 16:16:48,807:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-29 16:16:48,807:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-29 16:16:48,807:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-29 16:16:48,807:INFO:PyCaret required dependencies:
2026-01-29 16:16:48,807:INFO:                 pip: 25.0
2026-01-29 16:16:48,807:INFO:          setuptools: 75.8.0
2026-01-29 16:16:48,807:INFO:             pycaret: 3.3.2
2026-01-29 16:16:48,807:INFO:             IPython: 9.9.0
2026-01-29 16:16:48,807:INFO:          ipywidgets: 8.1.8
2026-01-29 16:16:48,807:INFO:                tqdm: 4.67.1
2026-01-29 16:16:48,807:INFO:               numpy: 1.26.4
2026-01-29 16:16:48,807:INFO:              pandas: 2.1.4
2026-01-29 16:16:48,807:INFO:              jinja2: 3.1.6
2026-01-29 16:16:48,807:INFO:               scipy: 1.11.4
2026-01-29 16:16:48,807:INFO:              joblib: 1.3.2
2026-01-29 16:16:48,807:INFO:             sklearn: 1.4.2
2026-01-29 16:16:48,807:INFO:                pyod: 2.0.6
2026-01-29 16:16:48,807:INFO:            imblearn: 0.14.1
2026-01-29 16:16:48,807:INFO:   category_encoders: 2.7.0
2026-01-29 16:16:48,807:INFO:            lightgbm: 4.6.0
2026-01-29 16:16:48,807:INFO:               numba: 0.62.1
2026-01-29 16:16:48,807:INFO:            requests: 2.32.3
2026-01-29 16:16:48,807:INFO:          matplotlib: 3.7.5
2026-01-29 16:16:48,807:INFO:          scikitplot: 0.3.7
2026-01-29 16:16:48,807:INFO:         yellowbrick: 1.5
2026-01-29 16:16:48,807:INFO:              plotly: 5.24.1
2026-01-29 16:16:48,807:INFO:    plotly-resampler: Not installed
2026-01-29 16:16:48,807:INFO:             kaleido: 1.2.0
2026-01-29 16:16:48,807:INFO:           schemdraw: 0.15
2026-01-29 16:16:48,807:INFO:         statsmodels: 0.14.6
2026-01-29 16:16:48,807:INFO:              sktime: 0.26.0
2026-01-29 16:16:48,807:INFO:               tbats: 1.1.3
2026-01-29 16:16:48,807:INFO:            pmdarima: 2.0.4
2026-01-29 16:16:48,807:INFO:              psutil: 7.2.1
2026-01-29 16:16:48,807:INFO:          markupsafe: 3.0.3
2026-01-29 16:16:48,807:INFO:             pickle5: Not installed
2026-01-29 16:16:48,807:INFO:         cloudpickle: 3.0.0
2026-01-29 16:16:48,807:INFO:         deprecation: 2.1.0
2026-01-29 16:16:48,807:INFO:              xxhash: 3.6.0
2026-01-29 16:16:48,807:INFO:           wurlitzer: Not installed
2026-01-29 16:16:48,807:INFO:PyCaret optional dependencies:
2026-01-29 16:16:48,807:INFO:                shap: 0.44.1
2026-01-29 16:16:48,807:INFO:           interpret: 0.7.3
2026-01-29 16:16:48,807:INFO:                umap: 0.5.7
2026-01-29 16:16:48,807:INFO:     ydata_profiling: 4.18.1
2026-01-29 16:16:48,807:INFO:  explainerdashboard: 0.5.1
2026-01-29 16:16:48,807:INFO:             autoviz: Not installed
2026-01-29 16:16:48,807:INFO:           fairlearn: 0.7.0
2026-01-29 16:16:48,807:INFO:          deepchecks: Not installed
2026-01-29 16:16:48,807:INFO:             xgboost: Not installed
2026-01-29 16:16:48,807:INFO:            catboost: 1.2.8
2026-01-29 16:16:48,807:INFO:              kmodes: 0.12.2
2026-01-29 16:16:48,807:INFO:             mlxtend: 0.23.4
2026-01-29 16:16:48,807:INFO:       statsforecast: 1.5.0
2026-01-29 16:16:48,807:INFO:        tune_sklearn: Not installed
2026-01-29 16:16:48,820:INFO:                 ray: Not installed
2026-01-29 16:16:48,820:INFO:            hyperopt: 0.2.7
2026-01-29 16:16:48,820:INFO:              optuna: 4.6.0
2026-01-29 16:16:48,820:INFO:               skopt: 0.10.2
2026-01-29 16:16:48,820:INFO:              mlflow: 3.8.1
2026-01-29 16:16:48,820:INFO:              gradio: 6.3.0
2026-01-29 16:16:48,820:INFO:             fastapi: 0.128.0
2026-01-29 16:16:48,820:INFO:             uvicorn: 0.40.0
2026-01-29 16:16:48,820:INFO:              m2cgen: 0.10.0
2026-01-29 16:16:48,820:INFO:           evidently: 0.4.40
2026-01-29 16:16:48,820:INFO:               fugue: 0.8.7
2026-01-29 16:16:48,820:INFO:           streamlit: Not installed
2026-01-29 16:16:48,820:INFO:             prophet: Not installed
2026-01-29 16:16:48,820:INFO:None
2026-01-29 16:16:48,820:INFO:Set up data.
2026-01-29 16:16:48,841:INFO:Set up folding strategy.
2026-01-29 16:16:48,841:INFO:Set up train/test split.
2026-01-29 16:16:48,939:INFO:Set up index.
2026-01-29 16:16:48,939:INFO:Assigning column types.
2026-01-29 16:16:48,970:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-29 16:16:48,986:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:16:48,986:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:16:49,017:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:16:49,017:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:16:49,040:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:16:49,040:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:16:49,070:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:16:49,070:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:16:49,070:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-29 16:16:49,117:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:16:49,135:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:16:49,135:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:16:49,172:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:16:49,188:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:16:49,188:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:16:49,188:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-29 16:16:49,240:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:16:49,240:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:16:49,286:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:16:49,286:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:16:49,286:INFO:Preparing preprocessing pipeline...
2026-01-29 16:16:49,286:INFO:Set up simple imputation.
2026-01-29 16:16:49,302:INFO:Set up feature normalization.
2026-01-29 16:16:49,388:INFO:Finished creating preprocessing pipeline.
2026-01-29 16:16:49,390:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-29 16:16:49,390:INFO:Creating final display dataframe.
2026-01-29 16:16:49,588:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (429278, 4)
4        Transformed data shape       (429278, 4)
5   Transformed train set shape       (343422, 4)
6    Transformed test set shape        (85856, 4)
7               Ignore features                58
8              Numeric features                 3
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13                    Normalize              True
14             Normalize method            zscore
15               Fold Generator   StratifiedKFold
16                  Fold Number                 3
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              8fc6
2026-01-29 16:16:49,636:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:16:49,636:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:16:49,673:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:16:49,673:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:16:49,673:INFO:setup() successfully completed in 0.88s...............
2026-01-29 16:16:49,673:INFO:Initializing compare_models()
2026-01-29 16:16:49,673:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-29 16:16:49,673:INFO:Checking exceptions
2026-01-29 16:16:49,710:INFO:Preparing display monitor
2026-01-29 16:16:49,738:INFO:Initializing Logistic Regression
2026-01-29 16:16:49,738:INFO:Total runtime is 0.0 minutes
2026-01-29 16:16:49,742:INFO:SubProcess create_model() called ==================================
2026-01-29 16:16:49,742:INFO:Initializing create_model()
2026-01-29 16:16:49,742:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816CE95D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:16:49,742:INFO:Checking exceptions
2026-01-29 16:16:49,744:INFO:Importing libraries
2026-01-29 16:16:49,744:INFO:Copying training dataset
2026-01-29 16:16:49,817:INFO:Defining folds
2026-01-29 16:16:49,817:INFO:Declaring metric variables
2026-01-29 16:16:49,820:INFO:Importing untrained model
2026-01-29 16:16:49,825:INFO:Logistic Regression Imported successfully
2026-01-29 16:16:49,825:INFO:Starting cross validation
2026-01-29 16:16:49,825:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:16:50,495:INFO:Calculating mean and std
2026-01-29 16:16:50,495:INFO:Creating metrics dataframe
2026-01-29 16:16:50,495:INFO:Uploading results into container
2026-01-29 16:16:50,495:INFO:Uploading model into container now
2026-01-29 16:16:50,495:INFO:_master_model_container: 1
2026-01-29 16:16:50,495:INFO:_display_container: 2
2026-01-29 16:16:50,495:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:16:50,495:INFO:create_model() successfully completed......................................
2026-01-29 16:16:50,712:INFO:SubProcess create_model() end ==================================
2026-01-29 16:16:50,712:INFO:Creating metrics dataframe
2026-01-29 16:16:50,723:INFO:Initializing Decision Tree Classifier
2026-01-29 16:16:50,723:INFO:Total runtime is 0.016418596108754475 minutes
2026-01-29 16:16:50,723:INFO:SubProcess create_model() called ==================================
2026-01-29 16:16:50,723:INFO:Initializing create_model()
2026-01-29 16:16:50,723:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816CE95D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:16:50,723:INFO:Checking exceptions
2026-01-29 16:16:50,723:INFO:Importing libraries
2026-01-29 16:16:50,723:INFO:Copying training dataset
2026-01-29 16:16:50,787:INFO:Defining folds
2026-01-29 16:16:50,788:INFO:Declaring metric variables
2026-01-29 16:16:50,791:INFO:Importing untrained model
2026-01-29 16:16:50,791:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:16:50,791:INFO:Starting cross validation
2026-01-29 16:16:50,791:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:16:51,325:INFO:Calculating mean and std
2026-01-29 16:16:51,326:INFO:Creating metrics dataframe
2026-01-29 16:16:51,327:INFO:Uploading results into container
2026-01-29 16:16:51,328:INFO:Uploading model into container now
2026-01-29 16:16:51,328:INFO:_master_model_container: 2
2026-01-29 16:16:51,328:INFO:_display_container: 2
2026-01-29 16:16:51,329:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:16:51,329:INFO:create_model() successfully completed......................................
2026-01-29 16:16:51,550:INFO:SubProcess create_model() end ==================================
2026-01-29 16:16:51,550:INFO:Creating metrics dataframe
2026-01-29 16:16:51,556:INFO:Initializing Random Forest Classifier
2026-01-29 16:16:51,556:INFO:Total runtime is 0.030309824148813884 minutes
2026-01-29 16:16:51,558:INFO:SubProcess create_model() called ==================================
2026-01-29 16:16:51,558:INFO:Initializing create_model()
2026-01-29 16:16:51,558:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816CE95D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:16:51,558:INFO:Checking exceptions
2026-01-29 16:16:51,558:INFO:Importing libraries
2026-01-29 16:16:51,558:INFO:Copying training dataset
2026-01-29 16:16:51,631:INFO:Defining folds
2026-01-29 16:16:51,631:INFO:Declaring metric variables
2026-01-29 16:16:51,635:INFO:Importing untrained model
2026-01-29 16:16:51,638:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:16:51,643:INFO:Starting cross validation
2026-01-29 16:16:51,643:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:16:54,095:INFO:Calculating mean and std
2026-01-29 16:16:54,095:INFO:Creating metrics dataframe
2026-01-29 16:16:54,102:INFO:Uploading results into container
2026-01-29 16:16:54,102:INFO:Uploading model into container now
2026-01-29 16:16:54,103:INFO:_master_model_container: 3
2026-01-29 16:16:54,103:INFO:_display_container: 2
2026-01-29 16:16:54,104:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:16:54,105:INFO:create_model() successfully completed......................................
2026-01-29 16:16:54,317:INFO:SubProcess create_model() end ==================================
2026-01-29 16:16:54,317:INFO:Creating metrics dataframe
2026-01-29 16:16:54,321:INFO:Initializing Light Gradient Boosting Machine
2026-01-29 16:16:54,321:INFO:Total runtime is 0.07638957500457763 minutes
2026-01-29 16:16:54,321:INFO:SubProcess create_model() called ==================================
2026-01-29 16:16:54,321:INFO:Initializing create_model()
2026-01-29 16:16:54,321:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816CE95D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:16:54,321:INFO:Checking exceptions
2026-01-29 16:16:54,321:INFO:Importing libraries
2026-01-29 16:16:54,321:INFO:Copying training dataset
2026-01-29 16:16:54,390:INFO:Defining folds
2026-01-29 16:16:54,390:INFO:Declaring metric variables
2026-01-29 16:16:54,407:INFO:Importing untrained model
2026-01-29 16:16:54,407:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-29 16:16:54,407:INFO:Starting cross validation
2026-01-29 16:16:54,407:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:16:55,803:INFO:Calculating mean and std
2026-01-29 16:16:55,807:INFO:Creating metrics dataframe
2026-01-29 16:16:55,812:INFO:Uploading results into container
2026-01-29 16:16:55,813:INFO:Uploading model into container now
2026-01-29 16:16:55,813:INFO:_master_model_container: 4
2026-01-29 16:16:55,813:INFO:_display_container: 2
2026-01-29 16:16:55,813:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-29 16:16:55,813:INFO:create_model() successfully completed......................................
2026-01-29 16:16:56,004:INFO:SubProcess create_model() end ==================================
2026-01-29 16:16:56,004:INFO:Creating metrics dataframe
2026-01-29 16:16:56,020:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-29 16:16:56,025:INFO:Initializing create_model()
2026-01-29 16:16:56,025:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:16:56,025:INFO:Checking exceptions
2026-01-29 16:16:56,025:INFO:Importing libraries
2026-01-29 16:16:56,025:INFO:Copying training dataset
2026-01-29 16:16:56,071:INFO:Defining folds
2026-01-29 16:16:56,071:INFO:Declaring metric variables
2026-01-29 16:16:56,071:INFO:Importing untrained model
2026-01-29 16:16:56,071:INFO:Declaring custom model
2026-01-29 16:16:56,071:INFO:Logistic Regression Imported successfully
2026-01-29 16:16:56,071:INFO:Cross validation set to False
2026-01-29 16:16:56,071:INFO:Fitting Model
2026-01-29 16:16:56,290:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:16:56,290:INFO:create_model() successfully completed......................................
2026-01-29 16:16:56,505:INFO:Initializing create_model()
2026-01-29 16:16:56,505:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:16:56,505:INFO:Checking exceptions
2026-01-29 16:16:56,511:INFO:Importing libraries
2026-01-29 16:16:56,512:INFO:Copying training dataset
2026-01-29 16:16:56,568:INFO:Defining folds
2026-01-29 16:16:56,568:INFO:Declaring metric variables
2026-01-29 16:16:56,568:INFO:Importing untrained model
2026-01-29 16:16:56,568:INFO:Declaring custom model
2026-01-29 16:16:56,568:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:16:56,569:INFO:Cross validation set to False
2026-01-29 16:16:56,569:INFO:Fitting Model
2026-01-29 16:16:56,621:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:16:56,621:INFO:create_model() successfully completed......................................
2026-01-29 16:16:56,841:INFO:Initializing create_model()
2026-01-29 16:16:56,841:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:16:56,841:INFO:Checking exceptions
2026-01-29 16:16:56,845:INFO:Importing libraries
2026-01-29 16:16:56,845:INFO:Copying training dataset
2026-01-29 16:16:56,920:INFO:Defining folds
2026-01-29 16:16:56,920:INFO:Declaring metric variables
2026-01-29 16:16:56,920:INFO:Importing untrained model
2026-01-29 16:16:56,920:INFO:Declaring custom model
2026-01-29 16:16:56,920:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:16:56,920:INFO:Cross validation set to False
2026-01-29 16:16:56,920:INFO:Fitting Model
2026-01-29 16:16:58,137:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:16:58,137:INFO:create_model() successfully completed......................................
2026-01-29 16:16:58,359:INFO:_master_model_container: 4
2026-01-29 16:16:58,359:INFO:_display_container: 2
2026-01-29 16:16:58,359:INFO:[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)]
2026-01-29 16:16:58,359:INFO:compare_models() successfully completed......................................
2026-01-29 16:16:58,371:INFO:Initializing tune_model()
2026-01-29 16:16:58,371:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:16:58,371:INFO:Checking exceptions
2026-01-29 16:16:58,409:INFO:Copying training dataset
2026-01-29 16:16:58,490:INFO:Checking base model
2026-01-29 16:16:58,491:INFO:Base model : Logistic Regression
2026-01-29 16:16:58,493:INFO:Declaring metric variables
2026-01-29 16:16:58,495:INFO:Defining Hyperparameters
2026-01-29 16:16:58,687:INFO:Tuning with n_jobs=-1
2026-01-29 16:16:58,687:INFO:Initializing RandomizedSearchCV
2026-01-29 16:17:00,322:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 5.682}
2026-01-29 16:17:00,322:INFO:Hyperparameter search completed
2026-01-29 16:17:00,322:INFO:SubProcess create_model() called ==================================
2026-01-29 16:17:00,322:INFO:Initializing create_model()
2026-01-29 16:17:00,322:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024871B10E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'class_weight': {}, 'C': 5.682})
2026-01-29 16:17:00,322:INFO:Checking exceptions
2026-01-29 16:17:00,322:INFO:Importing libraries
2026-01-29 16:17:00,322:INFO:Copying training dataset
2026-01-29 16:17:00,387:INFO:Defining folds
2026-01-29 16:17:00,387:INFO:Declaring metric variables
2026-01-29 16:17:00,387:INFO:Importing untrained model
2026-01-29 16:17:00,387:INFO:Declaring custom model
2026-01-29 16:17:00,402:INFO:Logistic Regression Imported successfully
2026-01-29 16:17:00,407:INFO:Starting cross validation
2026-01-29 16:17:00,407:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:17:01,161:INFO:Calculating mean and std
2026-01-29 16:17:01,161:INFO:Creating metrics dataframe
2026-01-29 16:17:01,170:INFO:Finalizing model
2026-01-29 16:17:01,448:INFO:Uploading results into container
2026-01-29 16:17:01,448:INFO:Uploading model into container now
2026-01-29 16:17:01,448:INFO:_master_model_container: 5
2026-01-29 16:17:01,448:INFO:_display_container: 3
2026-01-29 16:17:01,448:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:17:01,448:INFO:create_model() successfully completed......................................
2026-01-29 16:17:01,654:INFO:SubProcess create_model() end ==================================
2026-01-29 16:17:01,654:INFO:choose_better activated
2026-01-29 16:17:01,654:INFO:SubProcess create_model() called ==================================
2026-01-29 16:17:01,654:INFO:Initializing create_model()
2026-01-29 16:17:01,654:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:17:01,654:INFO:Checking exceptions
2026-01-29 16:17:01,654:INFO:Importing libraries
2026-01-29 16:17:01,654:INFO:Copying training dataset
2026-01-29 16:17:01,721:INFO:Defining folds
2026-01-29 16:17:01,721:INFO:Declaring metric variables
2026-01-29 16:17:01,721:INFO:Importing untrained model
2026-01-29 16:17:01,721:INFO:Declaring custom model
2026-01-29 16:17:01,721:INFO:Logistic Regression Imported successfully
2026-01-29 16:17:01,721:INFO:Starting cross validation
2026-01-29 16:17:01,721:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:17:02,373:INFO:Calculating mean and std
2026-01-29 16:17:02,375:INFO:Creating metrics dataframe
2026-01-29 16:17:02,377:INFO:Finalizing model
2026-01-29 16:17:02,610:INFO:Uploading results into container
2026-01-29 16:17:02,610:INFO:Uploading model into container now
2026-01-29 16:17:02,610:INFO:_master_model_container: 6
2026-01-29 16:17:02,610:INFO:_display_container: 4
2026-01-29 16:17:02,610:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:17:02,610:INFO:create_model() successfully completed......................................
2026-01-29 16:17:02,820:INFO:SubProcess create_model() end ==================================
2026-01-29 16:17:02,820:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 16:17:02,820:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 16:17:02,820:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2026-01-29 16:17:02,820:INFO:choose_better completed
2026-01-29 16:17:02,820:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:17:02,825:INFO:_master_model_container: 6
2026-01-29 16:17:02,825:INFO:_display_container: 3
2026-01-29 16:17:02,825:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:17:02,825:INFO:tune_model() successfully completed......................................
2026-01-29 16:17:03,039:INFO:Initializing tune_model()
2026-01-29 16:17:03,039:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:17:03,040:INFO:Checking exceptions
2026-01-29 16:17:03,068:INFO:Copying training dataset
2026-01-29 16:17:03,129:INFO:Checking base model
2026-01-29 16:17:03,130:INFO:Base model : Decision Tree Classifier
2026-01-29 16:17:03,133:INFO:Declaring metric variables
2026-01-29 16:17:03,138:INFO:Defining Hyperparameters
2026-01-29 16:17:03,353:INFO:Tuning with n_jobs=-1
2026-01-29 16:17:03,353:INFO:Initializing RandomizedSearchCV
2026-01-29 16:17:04,328:INFO:best_params: {'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.0005, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 3, 'actual_estimator__criterion': 'gini'}
2026-01-29 16:17:04,328:INFO:Hyperparameter search completed
2026-01-29 16:17:04,328:INFO:SubProcess create_model() called ==================================
2026-01-29 16:17:04,328:INFO:Initializing create_model()
2026-01-29 16:17:04,328:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002487DF698D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 9, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0005, 'max_features': 1.0, 'max_depth': 3, 'criterion': 'gini'})
2026-01-29 16:17:04,328:INFO:Checking exceptions
2026-01-29 16:17:04,328:INFO:Importing libraries
2026-01-29 16:17:04,328:INFO:Copying training dataset
2026-01-29 16:17:04,387:INFO:Defining folds
2026-01-29 16:17:04,387:INFO:Declaring metric variables
2026-01-29 16:17:04,387:INFO:Importing untrained model
2026-01-29 16:17:04,387:INFO:Declaring custom model
2026-01-29 16:17:04,387:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:17:04,404:INFO:Starting cross validation
2026-01-29 16:17:04,404:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:17:04,946:INFO:Calculating mean and std
2026-01-29 16:17:04,948:INFO:Creating metrics dataframe
2026-01-29 16:17:04,954:INFO:Finalizing model
2026-01-29 16:17:05,030:INFO:Uploading results into container
2026-01-29 16:17:05,030:INFO:Uploading model into container now
2026-01-29 16:17:05,030:INFO:_master_model_container: 7
2026-01-29 16:17:05,030:INFO:_display_container: 4
2026-01-29 16:17:05,034:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:17:05,034:INFO:create_model() successfully completed......................................
2026-01-29 16:17:05,276:INFO:SubProcess create_model() end ==================================
2026-01-29 16:17:05,276:INFO:choose_better activated
2026-01-29 16:17:05,279:INFO:SubProcess create_model() called ==================================
2026-01-29 16:17:05,279:INFO:Initializing create_model()
2026-01-29 16:17:05,279:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:17:05,280:INFO:Checking exceptions
2026-01-29 16:17:05,281:INFO:Importing libraries
2026-01-29 16:17:05,282:INFO:Copying training dataset
2026-01-29 16:17:05,364:INFO:Defining folds
2026-01-29 16:17:05,364:INFO:Declaring metric variables
2026-01-29 16:17:05,365:INFO:Importing untrained model
2026-01-29 16:17:05,365:INFO:Declaring custom model
2026-01-29 16:17:05,365:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:17:05,365:INFO:Starting cross validation
2026-01-29 16:17:05,366:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:17:05,959:INFO:Calculating mean and std
2026-01-29 16:17:05,959:INFO:Creating metrics dataframe
2026-01-29 16:17:05,959:INFO:Finalizing model
2026-01-29 16:17:06,040:INFO:Uploading results into container
2026-01-29 16:17:06,040:INFO:Uploading model into container now
2026-01-29 16:17:06,040:INFO:_master_model_container: 8
2026-01-29 16:17:06,040:INFO:_display_container: 5
2026-01-29 16:17:06,040:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:17:06,040:INFO:create_model() successfully completed......................................
2026-01-29 16:17:06,270:INFO:SubProcess create_model() end ==================================
2026-01-29 16:17:06,270:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5369
2026-01-29 16:17:06,270:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5366
2026-01-29 16:17:06,270:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-29 16:17:06,270:INFO:choose_better completed
2026-01-29 16:17:06,270:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:17:06,287:INFO:_master_model_container: 8
2026-01-29 16:17:06,287:INFO:_display_container: 4
2026-01-29 16:17:06,287:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:17:06,287:INFO:tune_model() successfully completed......................................
2026-01-29 16:17:06,503:INFO:Initializing tune_model()
2026-01-29 16:17:06,503:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:17:06,503:INFO:Checking exceptions
2026-01-29 16:17:06,553:INFO:Copying training dataset
2026-01-29 16:17:06,620:INFO:Checking base model
2026-01-29 16:17:06,620:INFO:Base model : Random Forest Classifier
2026-01-29 16:17:06,626:INFO:Declaring metric variables
2026-01-29 16:17:06,630:INFO:Defining Hyperparameters
2026-01-29 16:17:06,869:INFO:Tuning with n_jobs=-1
2026-01-29 16:17:06,869:INFO:Initializing RandomizedSearchCV
2026-01-29 16:17:34,874:INFO:best_params: {'actual_estimator__n_estimators': 120, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-29 16:17:34,874:INFO:Hyperparameter search completed
2026-01-29 16:17:34,874:INFO:SubProcess create_model() called ==================================
2026-01-29 16:17:34,874:INFO:Initializing create_model()
2026-01-29 16:17:34,874:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024827B0F5D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': True})
2026-01-29 16:17:34,874:INFO:Checking exceptions
2026-01-29 16:17:34,874:INFO:Importing libraries
2026-01-29 16:17:34,874:INFO:Copying training dataset
2026-01-29 16:17:34,953:INFO:Defining folds
2026-01-29 16:17:34,953:INFO:Declaring metric variables
2026-01-29 16:17:34,956:INFO:Importing untrained model
2026-01-29 16:17:34,956:INFO:Declaring custom model
2026-01-29 16:17:34,956:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:17:34,971:INFO:Starting cross validation
2026-01-29 16:17:34,972:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:17:38,064:INFO:Calculating mean and std
2026-01-29 16:17:38,064:INFO:Creating metrics dataframe
2026-01-29 16:17:38,073:INFO:Finalizing model
2026-01-29 16:17:39,755:INFO:Uploading results into container
2026-01-29 16:17:39,756:INFO:Uploading model into container now
2026-01-29 16:17:39,756:INFO:_master_model_container: 9
2026-01-29 16:17:39,756:INFO:_display_container: 5
2026-01-29 16:17:39,756:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:17:39,756:INFO:create_model() successfully completed......................................
2026-01-29 16:17:39,984:INFO:SubProcess create_model() end ==================================
2026-01-29 16:17:39,984:INFO:choose_better activated
2026-01-29 16:17:39,984:INFO:SubProcess create_model() called ==================================
2026-01-29 16:17:39,984:INFO:Initializing create_model()
2026-01-29 16:17:39,984:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:17:39,984:INFO:Checking exceptions
2026-01-29 16:17:39,984:INFO:Importing libraries
2026-01-29 16:17:39,984:INFO:Copying training dataset
2026-01-29 16:17:40,053:INFO:Defining folds
2026-01-29 16:17:40,053:INFO:Declaring metric variables
2026-01-29 16:17:40,053:INFO:Importing untrained model
2026-01-29 16:17:40,053:INFO:Declaring custom model
2026-01-29 16:17:40,053:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:17:40,053:INFO:Starting cross validation
2026-01-29 16:17:40,053:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:17:42,552:INFO:Calculating mean and std
2026-01-29 16:17:42,552:INFO:Creating metrics dataframe
2026-01-29 16:17:42,558:INFO:Finalizing model
2026-01-29 16:17:43,833:INFO:Uploading results into container
2026-01-29 16:17:43,833:INFO:Uploading model into container now
2026-01-29 16:17:43,833:INFO:_master_model_container: 10
2026-01-29 16:17:43,833:INFO:_display_container: 6
2026-01-29 16:17:43,833:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:17:43,833:INFO:create_model() successfully completed......................................
2026-01-29 16:17:44,038:INFO:SubProcess create_model() end ==================================
2026-01-29 16:17:44,038:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.5369
2026-01-29 16:17:44,038:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.5369
2026-01-29 16:17:44,038:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-29 16:17:44,038:INFO:choose_better completed
2026-01-29 16:17:44,038:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:17:44,053:INFO:_master_model_container: 10
2026-01-29 16:17:44,053:INFO:_display_container: 5
2026-01-29 16:17:44,053:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:17:44,053:INFO:tune_model() successfully completed......................................
2026-01-29 16:17:44,254:INFO:Initializing evaluate_model()
2026-01-29 16:17:44,254:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:17:44,294:INFO:Initializing plot_model()
2026-01-29 16:17:44,294:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:17:44,295:INFO:Checking exceptions
2026-01-29 16:17:44,321:INFO:Preloading libraries
2026-01-29 16:17:44,321:INFO:Copying training dataset
2026-01-29 16:17:44,321:INFO:Plot type: pipeline
2026-01-29 16:17:44,371:INFO:Visual Rendered Successfully
2026-01-29 16:17:44,572:INFO:plot_model() successfully completed......................................
2026-01-29 16:17:44,572:INFO:Initializing evaluate_model()
2026-01-29 16:17:44,585:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:17:44,617:INFO:Initializing plot_model()
2026-01-29 16:17:44,617:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:17:44,617:INFO:Checking exceptions
2026-01-29 16:17:44,657:INFO:Preloading libraries
2026-01-29 16:17:44,658:INFO:Copying training dataset
2026-01-29 16:17:44,658:INFO:Plot type: pipeline
2026-01-29 16:17:44,750:INFO:Visual Rendered Successfully
2026-01-29 16:17:44,955:INFO:plot_model() successfully completed......................................
2026-01-29 16:17:44,955:INFO:Initializing evaluate_model()
2026-01-29 16:17:44,955:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:17:44,995:INFO:Initializing plot_model()
2026-01-29 16:17:44,995:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:17:44,995:INFO:Checking exceptions
2026-01-29 16:17:45,038:INFO:Preloading libraries
2026-01-29 16:17:45,038:INFO:Copying training dataset
2026-01-29 16:17:45,038:INFO:Plot type: pipeline
2026-01-29 16:17:45,101:INFO:Visual Rendered Successfully
2026-01-29 16:17:45,301:INFO:plot_model() successfully completed......................................
2026-01-29 16:17:45,312:INFO:Initializing predict_model()
2026-01-29 16:17:45,312:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002481E4E31A0>)
2026-01-29 16:17:45,312:INFO:Checking exceptions
2026-01-29 16:17:45,312:INFO:Preloading libraries
2026-01-29 16:17:45,314:INFO:Set up data.
2026-01-29 16:17:45,320:INFO:Set up index.
2026-01-29 16:17:45,803:INFO:Initializing predict_model()
2026-01-29 16:17:45,803:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000248172B4720>)
2026-01-29 16:17:45,803:INFO:Checking exceptions
2026-01-29 16:17:45,803:INFO:Preloading libraries
2026-01-29 16:17:45,803:INFO:Set up data.
2026-01-29 16:17:45,819:INFO:Set up index.
2026-01-29 16:17:46,305:INFO:Initializing predict_model()
2026-01-29 16:17:46,305:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002480F570EA0>)
2026-01-29 16:17:46,305:INFO:Checking exceptions
2026-01-29 16:17:46,305:INFO:Preloading libraries
2026-01-29 16:17:46,305:INFO:Set up data.
2026-01-29 16:17:46,305:INFO:Set up index.
2026-01-29 16:17:47,093:INFO:Initializing plot_model()
2026-01-29 16:17:47,093:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002480F68C810>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-29 16:17:47,093:INFO:Checking exceptions
2026-01-29 16:17:47,156:INFO:Preloading libraries
2026-01-29 16:17:47,156:INFO:Copying training dataset
2026-01-29 16:17:47,156:INFO:Plot type: feature
2026-01-29 16:17:47,476:INFO:Visual Rendered Successfully
2026-01-29 16:17:47,725:INFO:plot_model() successfully completed......................................
2026-01-29 16:17:47,725:INFO:Initializing save_model()
2026-01-29 16:17:47,725:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=..\datos\04. Modelos, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-29 16:17:47,725:INFO:Adding model into prep_pipe
2026-01-29 16:17:47,740:INFO:..\datos\04. Modelos.pkl saved in current working directory
2026-01-29 16:17:47,741:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(e...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=42,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2026-01-29 16:17:47,741:INFO:save_model() successfully completed......................................
2026-01-29 16:19:05,735:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26224\1878984716.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-29 16:19:07,449:INFO:PyCaret ClassificationExperiment
2026-01-29 16:19:07,449:INFO:Logging name: clf-default-name
2026-01-29 16:19:07,449:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-29 16:19:07,449:INFO:version 3.3.2
2026-01-29 16:19:07,451:INFO:Initializing setup()
2026-01-29 16:19:07,451:INFO:self.USI: e4f4
2026-01-29 16:19:07,451:INFO:self._variable_keys: {'X_test', 'fold_groups_param', 'pipeline', 'fix_imbalance', 'exp_name_log', 'data', 'y_test', 'seed', 'fold_shuffle_param', 'n_jobs_param', 'is_multiclass', 'gpu_n_jobs_param', 'memory', 'log_plots_param', 'logging_param', 'idx', 'y', 'target_param', 'fold_generator', 'y_train', 'gpu_param', 'USI', 'exp_id', '_available_plots', 'X', 'X_train', 'html_param', '_ml_usecase'}
2026-01-29 16:19:07,451:INFO:Checking environment
2026-01-29 16:19:07,451:INFO:python_version: 3.11.11
2026-01-29 16:19:07,451:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-29 16:19:07,451:INFO:machine: AMD64
2026-01-29 16:19:07,451:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-29 16:19:07,451:INFO:Memory: svmem(total=34009374720, available=13444653056, percent=60.5, used=20564721664, free=13444653056)
2026-01-29 16:19:07,451:INFO:Physical Core: 12
2026-01-29 16:19:07,452:INFO:Logical Core: 16
2026-01-29 16:19:07,452:INFO:Checking libraries
2026-01-29 16:19:07,452:INFO:System:
2026-01-29 16:19:07,452:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-29 16:19:07,452:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-29 16:19:07,452:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-29 16:19:07,452:INFO:PyCaret required dependencies:
2026-01-29 16:19:07,452:INFO:                 pip: 25.0
2026-01-29 16:19:07,452:INFO:          setuptools: 75.8.0
2026-01-29 16:19:07,452:INFO:             pycaret: 3.3.2
2026-01-29 16:19:07,452:INFO:             IPython: 9.9.0
2026-01-29 16:19:07,452:INFO:          ipywidgets: 8.1.8
2026-01-29 16:19:07,452:INFO:                tqdm: 4.67.1
2026-01-29 16:19:07,452:INFO:               numpy: 1.26.4
2026-01-29 16:19:07,452:INFO:              pandas: 2.1.4
2026-01-29 16:19:07,452:INFO:              jinja2: 3.1.6
2026-01-29 16:19:07,452:INFO:               scipy: 1.11.4
2026-01-29 16:19:07,452:INFO:              joblib: 1.3.2
2026-01-29 16:19:07,452:INFO:             sklearn: 1.4.2
2026-01-29 16:19:07,452:INFO:                pyod: 2.0.6
2026-01-29 16:19:07,452:INFO:            imblearn: 0.14.1
2026-01-29 16:19:07,452:INFO:   category_encoders: 2.7.0
2026-01-29 16:19:07,452:INFO:            lightgbm: 4.6.0
2026-01-29 16:19:07,452:INFO:               numba: 0.62.1
2026-01-29 16:19:07,452:INFO:            requests: 2.32.3
2026-01-29 16:19:07,452:INFO:          matplotlib: 3.7.5
2026-01-29 16:19:07,452:INFO:          scikitplot: 0.3.7
2026-01-29 16:19:07,452:INFO:         yellowbrick: 1.5
2026-01-29 16:19:07,452:INFO:              plotly: 5.24.1
2026-01-29 16:19:07,452:INFO:    plotly-resampler: Not installed
2026-01-29 16:19:07,452:INFO:             kaleido: 1.2.0
2026-01-29 16:19:07,452:INFO:           schemdraw: 0.15
2026-01-29 16:19:07,452:INFO:         statsmodels: 0.14.6
2026-01-29 16:19:07,452:INFO:              sktime: 0.26.0
2026-01-29 16:19:07,452:INFO:               tbats: 1.1.3
2026-01-29 16:19:07,452:INFO:            pmdarima: 2.0.4
2026-01-29 16:19:07,452:INFO:              psutil: 7.2.1
2026-01-29 16:19:07,452:INFO:          markupsafe: 3.0.3
2026-01-29 16:19:07,452:INFO:             pickle5: Not installed
2026-01-29 16:19:07,452:INFO:         cloudpickle: 3.0.0
2026-01-29 16:19:07,452:INFO:         deprecation: 2.1.0
2026-01-29 16:19:07,452:INFO:              xxhash: 3.6.0
2026-01-29 16:19:07,452:INFO:           wurlitzer: Not installed
2026-01-29 16:19:07,452:INFO:PyCaret optional dependencies:
2026-01-29 16:19:07,452:INFO:                shap: 0.44.1
2026-01-29 16:19:07,452:INFO:           interpret: 0.7.3
2026-01-29 16:19:07,452:INFO:                umap: 0.5.7
2026-01-29 16:19:07,452:INFO:     ydata_profiling: 4.18.1
2026-01-29 16:19:07,452:INFO:  explainerdashboard: 0.5.1
2026-01-29 16:19:07,452:INFO:             autoviz: Not installed
2026-01-29 16:19:07,452:INFO:           fairlearn: 0.7.0
2026-01-29 16:19:07,452:INFO:          deepchecks: Not installed
2026-01-29 16:19:07,452:INFO:             xgboost: Not installed
2026-01-29 16:19:07,452:INFO:            catboost: 1.2.8
2026-01-29 16:19:07,452:INFO:              kmodes: 0.12.2
2026-01-29 16:19:07,452:INFO:             mlxtend: 0.23.4
2026-01-29 16:19:07,452:INFO:       statsforecast: 1.5.0
2026-01-29 16:19:07,452:INFO:        tune_sklearn: Not installed
2026-01-29 16:19:07,452:INFO:                 ray: Not installed
2026-01-29 16:19:07,452:INFO:            hyperopt: 0.2.7
2026-01-29 16:19:07,452:INFO:              optuna: 4.6.0
2026-01-29 16:19:07,452:INFO:               skopt: 0.10.2
2026-01-29 16:19:07,452:INFO:              mlflow: 3.8.1
2026-01-29 16:19:07,452:INFO:              gradio: 6.3.0
2026-01-29 16:19:07,452:INFO:             fastapi: 0.128.0
2026-01-29 16:19:07,452:INFO:             uvicorn: 0.40.0
2026-01-29 16:19:07,452:INFO:              m2cgen: 0.10.0
2026-01-29 16:19:07,452:INFO:           evidently: 0.4.40
2026-01-29 16:19:07,452:INFO:               fugue: 0.8.7
2026-01-29 16:19:07,452:INFO:           streamlit: Not installed
2026-01-29 16:19:07,452:INFO:             prophet: Not installed
2026-01-29 16:19:07,452:INFO:None
2026-01-29 16:19:07,452:INFO:Set up data.
2026-01-29 16:19:07,485:INFO:Set up folding strategy.
2026-01-29 16:19:07,485:INFO:Set up train/test split.
2026-01-29 16:19:07,585:INFO:Set up index.
2026-01-29 16:19:07,585:INFO:Assigning column types.
2026-01-29 16:19:07,602:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-29 16:19:07,635:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:19:07,635:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:19:07,656:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:19:07,656:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:19:07,684:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-29 16:19:07,685:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:19:07,702:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:19:07,702:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:19:07,702:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-29 16:19:07,719:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:19:07,735:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:19:07,735:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:19:07,769:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-29 16:19:07,785:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:19:07,785:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:19:07,785:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-29 16:19:07,835:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:19:07,835:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:19:07,885:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:19:07,885:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:19:07,885:INFO:Preparing preprocessing pipeline...
2026-01-29 16:19:07,895:INFO:Set up simple imputation.
2026-01-29 16:19:07,895:INFO:Set up feature normalization.
2026-01-29 16:19:07,971:INFO:Finished creating preprocessing pipeline.
2026-01-29 16:19:07,971:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-29 16:19:07,971:INFO:Creating final display dataframe.
2026-01-29 16:19:08,186:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (429278, 4)
4        Transformed data shape       (429278, 4)
5   Transformed train set shape       (343422, 4)
6    Transformed test set shape        (85856, 4)
7               Ignore features                58
8              Numeric features                 3
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13                    Normalize              True
14             Normalize method            zscore
15               Fold Generator   StratifiedKFold
16                  Fold Number                 3
17                     CPU Jobs                -1
18                      Use GPU             False
19               Log Experiment             False
20              Experiment Name  clf-default-name
21                          USI              e4f4
2026-01-29 16:19:08,236:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:19:08,236:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:19:08,285:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-29 16:19:08,286:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-29 16:19:08,287:INFO:setup() successfully completed in 0.85s...............
2026-01-29 16:19:08,287:INFO:Initializing compare_models()
2026-01-29 16:19:08,287:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-29 16:19:08,287:INFO:Checking exceptions
2026-01-29 16:19:08,302:INFO:Preparing display monitor
2026-01-29 16:19:08,330:INFO:Initializing Logistic Regression
2026-01-29 16:19:08,330:INFO:Total runtime is 6.67572021484375e-06 minutes
2026-01-29 16:19:08,332:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:08,333:INFO:Initializing create_model()
2026-01-29 16:19:08,333:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002480E442290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:08,333:INFO:Checking exceptions
2026-01-29 16:19:08,333:INFO:Importing libraries
2026-01-29 16:19:08,333:INFO:Copying training dataset
2026-01-29 16:19:08,417:INFO:Defining folds
2026-01-29 16:19:08,417:INFO:Declaring metric variables
2026-01-29 16:19:08,420:INFO:Importing untrained model
2026-01-29 16:19:08,422:INFO:Logistic Regression Imported successfully
2026-01-29 16:19:08,427:INFO:Starting cross validation
2026-01-29 16:19:08,428:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:08,980:INFO:Calculating mean and std
2026-01-29 16:19:08,980:INFO:Creating metrics dataframe
2026-01-29 16:19:08,980:INFO:Uploading results into container
2026-01-29 16:19:08,980:INFO:Uploading model into container now
2026-01-29 16:19:08,984:INFO:_master_model_container: 1
2026-01-29 16:19:08,984:INFO:_display_container: 2
2026-01-29 16:19:08,984:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:19:08,984:INFO:create_model() successfully completed......................................
2026-01-29 16:19:09,185:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:09,185:INFO:Creating metrics dataframe
2026-01-29 16:19:09,185:INFO:Initializing Decision Tree Classifier
2026-01-29 16:19:09,185:INFO:Total runtime is 0.014251784483591715 minutes
2026-01-29 16:19:09,185:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:09,185:INFO:Initializing create_model()
2026-01-29 16:19:09,185:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002480E442290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:09,185:INFO:Checking exceptions
2026-01-29 16:19:09,185:INFO:Importing libraries
2026-01-29 16:19:09,185:INFO:Copying training dataset
2026-01-29 16:19:09,238:INFO:Defining folds
2026-01-29 16:19:09,251:INFO:Declaring metric variables
2026-01-29 16:19:09,253:INFO:Importing untrained model
2026-01-29 16:19:09,253:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:19:09,253:INFO:Starting cross validation
2026-01-29 16:19:09,253:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:09,718:INFO:Calculating mean and std
2026-01-29 16:19:09,719:INFO:Creating metrics dataframe
2026-01-29 16:19:09,719:INFO:Uploading results into container
2026-01-29 16:19:09,719:INFO:Uploading model into container now
2026-01-29 16:19:09,719:INFO:_master_model_container: 2
2026-01-29 16:19:09,723:INFO:_display_container: 2
2026-01-29 16:19:09,724:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:19:09,724:INFO:create_model() successfully completed......................................
2026-01-29 16:19:09,919:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:09,919:INFO:Creating metrics dataframe
2026-01-29 16:19:09,919:INFO:Initializing Random Forest Classifier
2026-01-29 16:19:09,919:INFO:Total runtime is 0.02647436459859212 minutes
2026-01-29 16:19:09,934:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:09,935:INFO:Initializing create_model()
2026-01-29 16:19:09,935:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002480E442290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:09,935:INFO:Checking exceptions
2026-01-29 16:19:09,935:INFO:Importing libraries
2026-01-29 16:19:09,935:INFO:Copying training dataset
2026-01-29 16:19:09,987:INFO:Defining folds
2026-01-29 16:19:09,987:INFO:Declaring metric variables
2026-01-29 16:19:09,987:INFO:Importing untrained model
2026-01-29 16:19:09,987:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:19:09,987:INFO:Starting cross validation
2026-01-29 16:19:09,987:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:12,204:INFO:Calculating mean and std
2026-01-29 16:19:12,204:INFO:Creating metrics dataframe
2026-01-29 16:19:12,217:INFO:Uploading results into container
2026-01-29 16:19:12,219:INFO:Uploading model into container now
2026-01-29 16:19:12,220:INFO:_master_model_container: 3
2026-01-29 16:19:12,220:INFO:_display_container: 2
2026-01-29 16:19:12,220:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:19:12,220:INFO:create_model() successfully completed......................................
2026-01-29 16:19:12,419:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:12,419:INFO:Creating metrics dataframe
2026-01-29 16:19:12,419:INFO:Initializing Light Gradient Boosting Machine
2026-01-29 16:19:12,419:INFO:Total runtime is 0.0681542714436849 minutes
2026-01-29 16:19:12,419:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:12,435:INFO:Initializing create_model()
2026-01-29 16:19:12,435:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002480E442290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:12,435:INFO:Checking exceptions
2026-01-29 16:19:12,435:INFO:Importing libraries
2026-01-29 16:19:12,435:INFO:Copying training dataset
2026-01-29 16:19:12,486:INFO:Defining folds
2026-01-29 16:19:12,486:INFO:Declaring metric variables
2026-01-29 16:19:12,486:INFO:Importing untrained model
2026-01-29 16:19:12,486:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-29 16:19:12,486:INFO:Starting cross validation
2026-01-29 16:19:12,502:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:13,632:INFO:Calculating mean and std
2026-01-29 16:19:13,632:INFO:Creating metrics dataframe
2026-01-29 16:19:13,632:INFO:Uploading results into container
2026-01-29 16:19:13,632:INFO:Uploading model into container now
2026-01-29 16:19:13,632:INFO:_master_model_container: 4
2026-01-29 16:19:13,632:INFO:_display_container: 2
2026-01-29 16:19:13,636:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-29 16:19:13,636:INFO:create_model() successfully completed......................................
2026-01-29 16:19:13,839:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:13,839:INFO:Creating metrics dataframe
2026-01-29 16:19:13,845:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-29 16:19:13,854:INFO:Initializing create_model()
2026-01-29 16:19:13,854:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:13,854:INFO:Checking exceptions
2026-01-29 16:19:13,854:INFO:Importing libraries
2026-01-29 16:19:13,854:INFO:Copying training dataset
2026-01-29 16:19:13,920:INFO:Defining folds
2026-01-29 16:19:13,920:INFO:Declaring metric variables
2026-01-29 16:19:13,920:INFO:Importing untrained model
2026-01-29 16:19:13,920:INFO:Declaring custom model
2026-01-29 16:19:13,921:INFO:Logistic Regression Imported successfully
2026-01-29 16:19:13,921:INFO:Cross validation set to False
2026-01-29 16:19:13,921:INFO:Fitting Model
2026-01-29 16:19:14,117:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:19:14,117:INFO:create_model() successfully completed......................................
2026-01-29 16:19:14,333:INFO:Initializing create_model()
2026-01-29 16:19:14,333:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:14,333:INFO:Checking exceptions
2026-01-29 16:19:14,345:INFO:Importing libraries
2026-01-29 16:19:14,345:INFO:Copying training dataset
2026-01-29 16:19:14,402:INFO:Defining folds
2026-01-29 16:19:14,402:INFO:Declaring metric variables
2026-01-29 16:19:14,402:INFO:Importing untrained model
2026-01-29 16:19:14,402:INFO:Declaring custom model
2026-01-29 16:19:14,402:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:19:14,402:INFO:Cross validation set to False
2026-01-29 16:19:14,402:INFO:Fitting Model
2026-01-29 16:19:14,469:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:19:14,469:INFO:create_model() successfully completed......................................
2026-01-29 16:19:14,671:INFO:Initializing create_model()
2026-01-29 16:19:14,671:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:14,671:INFO:Checking exceptions
2026-01-29 16:19:14,676:INFO:Importing libraries
2026-01-29 16:19:14,676:INFO:Copying training dataset
2026-01-29 16:19:14,735:INFO:Defining folds
2026-01-29 16:19:14,735:INFO:Declaring metric variables
2026-01-29 16:19:14,735:INFO:Importing untrained model
2026-01-29 16:19:14,735:INFO:Declaring custom model
2026-01-29 16:19:14,735:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:19:14,735:INFO:Cross validation set to False
2026-01-29 16:19:14,735:INFO:Fitting Model
2026-01-29 16:19:15,867:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:19:15,867:INFO:create_model() successfully completed......................................
2026-01-29 16:19:16,086:INFO:_master_model_container: 4
2026-01-29 16:19:16,086:INFO:_display_container: 2
2026-01-29 16:19:16,086:INFO:[LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)]
2026-01-29 16:19:16,086:INFO:compare_models() successfully completed......................................
2026-01-29 16:19:16,086:INFO:Initializing tune_model()
2026-01-29 16:19:16,086:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:19:16,086:INFO:Checking exceptions
2026-01-29 16:19:16,134:INFO:Copying training dataset
2026-01-29 16:19:16,196:INFO:Checking base model
2026-01-29 16:19:16,196:INFO:Base model : Logistic Regression
2026-01-29 16:19:16,199:INFO:Declaring metric variables
2026-01-29 16:19:16,201:INFO:Defining Hyperparameters
2026-01-29 16:19:16,418:INFO:Tuning with n_jobs=-1
2026-01-29 16:19:16,418:INFO:Initializing RandomizedSearchCV
2026-01-29 16:19:17,823:INFO:best_params: {'actual_estimator__class_weight': {}, 'actual_estimator__C': 5.682}
2026-01-29 16:19:17,823:INFO:Hyperparameter search completed
2026-01-29 16:19:17,823:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:17,823:INFO:Initializing create_model()
2026-01-29 16:19:17,823:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816DB8910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'class_weight': {}, 'C': 5.682})
2026-01-29 16:19:17,823:INFO:Checking exceptions
2026-01-29 16:19:17,823:INFO:Importing libraries
2026-01-29 16:19:17,823:INFO:Copying training dataset
2026-01-29 16:19:17,886:INFO:Defining folds
2026-01-29 16:19:17,886:INFO:Declaring metric variables
2026-01-29 16:19:17,886:INFO:Importing untrained model
2026-01-29 16:19:17,886:INFO:Declaring custom model
2026-01-29 16:19:17,886:INFO:Logistic Regression Imported successfully
2026-01-29 16:19:17,901:INFO:Starting cross validation
2026-01-29 16:19:17,901:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:18,608:INFO:Calculating mean and std
2026-01-29 16:19:18,608:INFO:Creating metrics dataframe
2026-01-29 16:19:18,608:INFO:Finalizing model
2026-01-29 16:19:18,909:INFO:Uploading results into container
2026-01-29 16:19:18,911:INFO:Uploading model into container now
2026-01-29 16:19:18,911:INFO:_master_model_container: 5
2026-01-29 16:19:18,911:INFO:_display_container: 3
2026-01-29 16:19:18,911:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:19:18,911:INFO:create_model() successfully completed......................................
2026-01-29 16:19:19,120:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:19,120:INFO:choose_better activated
2026-01-29 16:19:19,120:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:19,120:INFO:Initializing create_model()
2026-01-29 16:19:19,120:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:19,120:INFO:Checking exceptions
2026-01-29 16:19:19,120:INFO:Importing libraries
2026-01-29 16:19:19,120:INFO:Copying training dataset
2026-01-29 16:19:19,185:INFO:Defining folds
2026-01-29 16:19:19,185:INFO:Declaring metric variables
2026-01-29 16:19:19,185:INFO:Importing untrained model
2026-01-29 16:19:19,185:INFO:Declaring custom model
2026-01-29 16:19:19,185:INFO:Logistic Regression Imported successfully
2026-01-29 16:19:19,189:INFO:Starting cross validation
2026-01-29 16:19:19,189:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:19,920:INFO:Calculating mean and std
2026-01-29 16:19:19,920:INFO:Creating metrics dataframe
2026-01-29 16:19:19,920:INFO:Finalizing model
2026-01-29 16:19:20,154:INFO:Uploading results into container
2026-01-29 16:19:20,154:INFO:Uploading model into container now
2026-01-29 16:19:20,154:INFO:_master_model_container: 6
2026-01-29 16:19:20,154:INFO:_display_container: 4
2026-01-29 16:19:20,154:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:19:20,154:INFO:create_model() successfully completed......................................
2026-01-29 16:19:20,371:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:20,371:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 16:19:20,371:INFO:LogisticRegression(C=5.682, class_weight={}, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) result for AUC is 0.5369
2026-01-29 16:19:20,371:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False) is best model
2026-01-29 16:19:20,371:INFO:choose_better completed
2026-01-29 16:19:20,371:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:19:20,371:INFO:_master_model_container: 6
2026-01-29 16:19:20,371:INFO:_display_container: 3
2026-01-29 16:19:20,371:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-29 16:19:20,371:INFO:tune_model() successfully completed......................................
2026-01-29 16:19:20,592:INFO:Initializing tune_model()
2026-01-29 16:19:20,592:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:19:20,592:INFO:Checking exceptions
2026-01-29 16:19:20,619:INFO:Copying training dataset
2026-01-29 16:19:20,690:INFO:Checking base model
2026-01-29 16:19:20,692:INFO:Base model : Decision Tree Classifier
2026-01-29 16:19:20,694:INFO:Declaring metric variables
2026-01-29 16:19:20,697:INFO:Defining Hyperparameters
2026-01-29 16:19:20,903:INFO:Tuning with n_jobs=-1
2026-01-29 16:19:20,903:INFO:Initializing RandomizedSearchCV
2026-01-29 16:19:21,772:INFO:best_params: {'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.0005, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 3, 'actual_estimator__criterion': 'gini'}
2026-01-29 16:19:21,772:INFO:Hyperparameter search completed
2026-01-29 16:19:21,772:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:21,772:INFO:Initializing create_model()
2026-01-29 16:19:21,772:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002487E3F3ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 9, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0005, 'max_features': 1.0, 'max_depth': 3, 'criterion': 'gini'})
2026-01-29 16:19:21,772:INFO:Checking exceptions
2026-01-29 16:19:21,772:INFO:Importing libraries
2026-01-29 16:19:21,772:INFO:Copying training dataset
2026-01-29 16:19:21,834:INFO:Defining folds
2026-01-29 16:19:21,834:INFO:Declaring metric variables
2026-01-29 16:19:21,834:INFO:Importing untrained model
2026-01-29 16:19:21,834:INFO:Declaring custom model
2026-01-29 16:19:21,834:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:19:21,834:INFO:Starting cross validation
2026-01-29 16:19:21,834:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:22,491:INFO:Calculating mean and std
2026-01-29 16:19:22,491:INFO:Creating metrics dataframe
2026-01-29 16:19:22,501:INFO:Finalizing model
2026-01-29 16:19:22,568:INFO:Uploading results into container
2026-01-29 16:19:22,568:INFO:Uploading model into container now
2026-01-29 16:19:22,568:INFO:_master_model_container: 7
2026-01-29 16:19:22,568:INFO:_display_container: 4
2026-01-29 16:19:22,568:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:19:22,568:INFO:create_model() successfully completed......................................
2026-01-29 16:19:22,817:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:22,818:INFO:choose_better activated
2026-01-29 16:19:22,821:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:22,822:INFO:Initializing create_model()
2026-01-29 16:19:22,822:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:22,822:INFO:Checking exceptions
2026-01-29 16:19:22,822:INFO:Importing libraries
2026-01-29 16:19:22,822:INFO:Copying training dataset
2026-01-29 16:19:22,894:INFO:Defining folds
2026-01-29 16:19:22,894:INFO:Declaring metric variables
2026-01-29 16:19:22,894:INFO:Importing untrained model
2026-01-29 16:19:22,894:INFO:Declaring custom model
2026-01-29 16:19:22,896:INFO:Decision Tree Classifier Imported successfully
2026-01-29 16:19:22,896:INFO:Starting cross validation
2026-01-29 16:19:22,896:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:23,496:INFO:Calculating mean and std
2026-01-29 16:19:23,496:INFO:Creating metrics dataframe
2026-01-29 16:19:23,500:INFO:Finalizing model
2026-01-29 16:19:23,563:INFO:Uploading results into container
2026-01-29 16:19:23,570:INFO:Uploading model into container now
2026-01-29 16:19:23,570:INFO:_master_model_container: 8
2026-01-29 16:19:23,570:INFO:_display_container: 5
2026-01-29 16:19:23,570:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:19:23,570:INFO:create_model() successfully completed......................................
2026-01-29 16:19:23,786:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:23,786:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5369
2026-01-29 16:19:23,786:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=3, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0005, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.5366
2026-01-29 16:19:23,786:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-29 16:19:23,786:INFO:choose_better completed
2026-01-29 16:19:23,786:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:19:23,801:INFO:_master_model_container: 8
2026-01-29 16:19:23,802:INFO:_display_container: 4
2026-01-29 16:19:23,802:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-29 16:19:23,802:INFO:tune_model() successfully completed......................................
2026-01-29 16:19:24,019:INFO:Initializing tune_model()
2026-01-29 16:19:24,019:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-29 16:19:24,019:INFO:Checking exceptions
2026-01-29 16:19:24,054:INFO:Copying training dataset
2026-01-29 16:19:24,109:INFO:Checking base model
2026-01-29 16:19:24,109:INFO:Base model : Random Forest Classifier
2026-01-29 16:19:24,112:INFO:Declaring metric variables
2026-01-29 16:19:24,115:INFO:Defining Hyperparameters
2026-01-29 16:19:24,335:INFO:Tuning with n_jobs=-1
2026-01-29 16:19:24,335:INFO:Initializing RandomizedSearchCV
2026-01-29 16:19:48,174:INFO:best_params: {'actual_estimator__n_estimators': 120, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-29 16:19:48,174:INFO:Hyperparameter search completed
2026-01-29 16:19:48,174:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:48,174:INFO:Initializing create_model()
2026-01-29 16:19:48,174:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024816D828D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': True})
2026-01-29 16:19:48,174:INFO:Checking exceptions
2026-01-29 16:19:48,174:INFO:Importing libraries
2026-01-29 16:19:48,174:INFO:Copying training dataset
2026-01-29 16:19:48,289:INFO:Defining folds
2026-01-29 16:19:48,289:INFO:Declaring metric variables
2026-01-29 16:19:48,292:INFO:Importing untrained model
2026-01-29 16:19:48,292:INFO:Declaring custom model
2026-01-29 16:19:48,297:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:19:48,306:INFO:Starting cross validation
2026-01-29 16:19:48,306:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:51,069:INFO:Calculating mean and std
2026-01-29 16:19:51,069:INFO:Creating metrics dataframe
2026-01-29 16:19:51,069:INFO:Finalizing model
2026-01-29 16:19:52,812:INFO:Uploading results into container
2026-01-29 16:19:52,812:INFO:Uploading model into container now
2026-01-29 16:19:52,812:INFO:_master_model_container: 9
2026-01-29 16:19:52,812:INFO:_display_container: 5
2026-01-29 16:19:52,812:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:19:52,812:INFO:create_model() successfully completed......................................
2026-01-29 16:19:53,037:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:53,037:INFO:choose_better activated
2026-01-29 16:19:53,037:INFO:SubProcess create_model() called ==================================
2026-01-29 16:19:53,037:INFO:Initializing create_model()
2026-01-29 16:19:53,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-29 16:19:53,037:INFO:Checking exceptions
2026-01-29 16:19:53,037:INFO:Importing libraries
2026-01-29 16:19:53,037:INFO:Copying training dataset
2026-01-29 16:19:53,102:INFO:Defining folds
2026-01-29 16:19:53,102:INFO:Declaring metric variables
2026-01-29 16:19:53,102:INFO:Importing untrained model
2026-01-29 16:19:53,102:INFO:Declaring custom model
2026-01-29 16:19:53,102:INFO:Random Forest Classifier Imported successfully
2026-01-29 16:19:53,102:INFO:Starting cross validation
2026-01-29 16:19:53,102:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-29 16:19:55,455:INFO:Calculating mean and std
2026-01-29 16:19:55,455:INFO:Creating metrics dataframe
2026-01-29 16:19:55,455:INFO:Finalizing model
2026-01-29 16:19:56,823:INFO:Uploading results into container
2026-01-29 16:19:56,823:INFO:Uploading model into container now
2026-01-29 16:19:56,823:INFO:_master_model_container: 10
2026-01-29 16:19:56,823:INFO:_display_container: 6
2026-01-29 16:19:56,823:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:19:56,823:INFO:create_model() successfully completed......................................
2026-01-29 16:19:57,035:INFO:SubProcess create_model() end ==================================
2026-01-29 16:19:57,035:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.5369
2026-01-29 16:19:57,035:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.5369
2026-01-29 16:19:57,035:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-29 16:19:57,035:INFO:choose_better completed
2026-01-29 16:19:57,035:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-29 16:19:57,051:INFO:_master_model_container: 10
2026-01-29 16:19:57,051:INFO:_display_container: 5
2026-01-29 16:19:57,051:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-29 16:19:57,051:INFO:tune_model() successfully completed......................................
2026-01-29 16:19:57,302:INFO:Initializing evaluate_model()
2026-01-29 16:19:57,302:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:19:57,338:INFO:Initializing plot_model()
2026-01-29 16:19:57,339:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:19:57,339:INFO:Checking exceptions
2026-01-29 16:19:57,371:INFO:Preloading libraries
2026-01-29 16:19:57,371:INFO:Copying training dataset
2026-01-29 16:19:57,371:INFO:Plot type: pipeline
2026-01-29 16:19:57,435:INFO:Visual Rendered Successfully
2026-01-29 16:19:57,701:INFO:plot_model() successfully completed......................................
2026-01-29 16:19:57,701:INFO:Initializing evaluate_model()
2026-01-29 16:19:57,701:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:19:57,754:INFO:Initializing plot_model()
2026-01-29 16:19:57,754:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:19:57,754:INFO:Checking exceptions
2026-01-29 16:19:57,786:INFO:Preloading libraries
2026-01-29 16:19:57,786:INFO:Copying training dataset
2026-01-29 16:19:57,786:INFO:Plot type: pipeline
2026-01-29 16:19:57,895:INFO:Visual Rendered Successfully
2026-01-29 16:19:58,118:INFO:plot_model() successfully completed......................................
2026-01-29 16:19:58,126:INFO:Initializing evaluate_model()
2026-01-29 16:19:58,127:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2026-01-29 16:19:58,162:INFO:Initializing plot_model()
2026-01-29 16:19:58,163:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 16:19:58,163:INFO:Checking exceptions
2026-01-29 16:19:58,239:INFO:Preloading libraries
2026-01-29 16:19:58,239:INFO:Copying training dataset
2026-01-29 16:19:58,239:INFO:Plot type: pipeline
2026-01-29 16:19:58,325:INFO:Visual Rendered Successfully
2026-01-29 16:19:58,540:INFO:plot_model() successfully completed......................................
2026-01-29 16:19:58,551:INFO:Initializing predict_model()
2026-01-29 16:19:58,552:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000248488BA8E0>)
2026-01-29 16:19:58,552:INFO:Checking exceptions
2026-01-29 16:19:58,552:INFO:Preloading libraries
2026-01-29 16:19:58,553:INFO:Set up data.
2026-01-29 16:19:58,564:INFO:Set up index.
2026-01-29 16:19:59,152:INFO:Initializing predict_model()
2026-01-29 16:19:59,152:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002480F572AC0>)
2026-01-29 16:19:59,152:INFO:Checking exceptions
2026-01-29 16:19:59,152:INFO:Preloading libraries
2026-01-29 16:19:59,154:INFO:Set up data.
2026-01-29 16:19:59,154:INFO:Set up index.
2026-01-29 16:19:59,685:INFO:Initializing predict_model()
2026-01-29 16:19:59,685:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002487FF34220>)
2026-01-29 16:19:59,685:INFO:Checking exceptions
2026-01-29 16:19:59,685:INFO:Preloading libraries
2026-01-29 16:19:59,685:INFO:Set up data.
2026-01-29 16:19:59,685:INFO:Set up index.
2026-01-29 16:20:00,418:INFO:Initializing plot_model()
2026-01-29 16:20:00,419:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-29 16:20:00,419:INFO:Checking exceptions
2026-01-29 16:20:00,442:INFO:Preloading libraries
2026-01-29 16:20:00,442:INFO:Copying training dataset
2026-01-29 16:20:00,442:INFO:Plot type: feature
2026-01-29 16:20:00,728:INFO:Visual Rendered Successfully
2026-01-29 16:20:00,970:INFO:plot_model() successfully completed......................................
2026-01-29 16:20:00,973:INFO:Initializing save_model()
2026-01-29 16:20:00,973:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=..\datos\04. Modelos, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-29 16:20:00,973:INFO:Adding model into prep_pipe
2026-01-29 16:20:00,973:INFO:..\datos\04. Modelos.pkl saved in current working directory
2026-01-29 16:20:00,988:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(e...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=42,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2026-01-29 16:20:00,988:INFO:save_model() successfully completed......................................
2026-01-29 17:15:41,175:INFO:Initializing plot_model()
2026-01-29 17:15:41,178:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024816D447D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), plot=rfe, scale=1, save=False, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2026-01-29 17:15:41,179:INFO:Checking exceptions
2026-01-29 17:15:41,218:INFO:Preloading libraries
2026-01-29 17:15:41,219:INFO:Copying training dataset
2026-01-29 17:15:41,219:INFO:Plot type: rfe
2026-01-29 17:15:41,446:INFO:Fitting Model
2026-01-29 17:15:44,103:INFO:Visual Rendered Successfully
2026-01-29 17:15:44,349:INFO:plot_model() successfully completed......................................
2026-01-30 08:51:44,861:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-30 08:51:44,861:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-30 08:51:44,861:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-30 08:51:44,861:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-30 08:57:28,953:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\417549131.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 08:57:31,166:INFO:PyCaret ClassificationExperiment
2026-01-30 08:57:31,166:INFO:Logging name: clf-default-name
2026-01-30 08:57:31,166:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 08:57:31,166:INFO:version 3.3.2
2026-01-30 08:57:31,168:INFO:Initializing setup()
2026-01-30 08:57:31,168:INFO:self.USI: 0660
2026-01-30 08:57:31,168:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 08:57:31,170:INFO:Checking environment
2026-01-30 08:57:31,170:INFO:python_version: 3.11.11
2026-01-30 08:57:31,170:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 08:57:31,170:INFO:machine: AMD64
2026-01-30 08:57:31,170:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 08:57:31,170:INFO:Memory: svmem(total=34009374720, available=16781463552, percent=50.7, used=17227911168, free=16781463552)
2026-01-30 08:57:31,170:INFO:Physical Core: 12
2026-01-30 08:57:31,170:INFO:Logical Core: 16
2026-01-30 08:57:31,170:INFO:Checking libraries
2026-01-30 08:57:31,170:INFO:System:
2026-01-30 08:57:31,170:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 08:57:31,172:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 08:57:31,173:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 08:57:31,173:INFO:PyCaret required dependencies:
2026-01-30 08:57:32,242:INFO:                 pip: 25.0
2026-01-30 08:57:32,243:INFO:          setuptools: 75.8.0
2026-01-30 08:57:32,243:INFO:             pycaret: 3.3.2
2026-01-30 08:57:32,243:INFO:             IPython: 9.9.0
2026-01-30 08:57:32,243:INFO:          ipywidgets: 8.1.8
2026-01-30 08:57:32,243:INFO:                tqdm: 4.67.1
2026-01-30 08:57:32,243:INFO:               numpy: 1.26.4
2026-01-30 08:57:32,243:INFO:              pandas: 2.1.4
2026-01-30 08:57:32,243:INFO:              jinja2: 3.1.6
2026-01-30 08:57:32,243:INFO:               scipy: 1.11.4
2026-01-30 08:57:32,243:INFO:              joblib: 1.3.2
2026-01-30 08:57:32,243:INFO:             sklearn: 1.4.2
2026-01-30 08:57:32,243:INFO:                pyod: 2.0.6
2026-01-30 08:57:32,243:INFO:            imblearn: 0.14.1
2026-01-30 08:57:32,243:INFO:   category_encoders: 2.7.0
2026-01-30 08:57:32,243:INFO:            lightgbm: 4.6.0
2026-01-30 08:57:32,243:INFO:               numba: 0.62.1
2026-01-30 08:57:32,243:INFO:            requests: 2.32.3
2026-01-30 08:57:32,243:INFO:          matplotlib: 3.7.5
2026-01-30 08:57:32,243:INFO:          scikitplot: 0.3.7
2026-01-30 08:57:32,243:INFO:         yellowbrick: 1.5
2026-01-30 08:57:32,243:INFO:              plotly: 5.24.1
2026-01-30 08:57:32,243:INFO:    plotly-resampler: Not installed
2026-01-30 08:57:32,243:INFO:             kaleido: 1.2.0
2026-01-30 08:57:32,243:INFO:           schemdraw: 0.15
2026-01-30 08:57:32,243:INFO:         statsmodels: 0.14.6
2026-01-30 08:57:32,243:INFO:              sktime: 0.26.0
2026-01-30 08:57:32,243:INFO:               tbats: 1.1.3
2026-01-30 08:57:32,243:INFO:            pmdarima: 2.0.4
2026-01-30 08:57:32,244:INFO:              psutil: 7.2.1
2026-01-30 08:57:32,244:INFO:          markupsafe: 3.0.3
2026-01-30 08:57:32,244:INFO:             pickle5: Not installed
2026-01-30 08:57:32,244:INFO:         cloudpickle: 3.0.0
2026-01-30 08:57:32,244:INFO:         deprecation: 2.1.0
2026-01-30 08:57:32,244:INFO:              xxhash: 3.6.0
2026-01-30 08:57:32,244:INFO:           wurlitzer: Not installed
2026-01-30 08:57:32,244:INFO:PyCaret optional dependencies:
2026-01-30 08:57:39,269:INFO:                shap: 0.44.1
2026-01-30 08:57:39,269:INFO:           interpret: 0.7.3
2026-01-30 08:57:39,269:INFO:                umap: 0.5.7
2026-01-30 08:57:39,269:INFO:     ydata_profiling: 4.18.1
2026-01-30 08:57:39,270:INFO:  explainerdashboard: 0.5.1
2026-01-30 08:57:39,270:INFO:             autoviz: Not installed
2026-01-30 08:57:39,270:INFO:           fairlearn: 0.7.0
2026-01-30 08:57:39,270:INFO:          deepchecks: Not installed
2026-01-30 08:57:39,270:INFO:             xgboost: Not installed
2026-01-30 08:57:39,271:INFO:            catboost: 1.2.8
2026-01-30 08:57:39,271:INFO:              kmodes: 0.12.2
2026-01-30 08:57:39,271:INFO:             mlxtend: 0.23.4
2026-01-30 08:57:39,271:INFO:       statsforecast: 1.5.0
2026-01-30 08:57:39,271:INFO:        tune_sklearn: Not installed
2026-01-30 08:57:39,271:INFO:                 ray: Not installed
2026-01-30 08:57:39,271:INFO:            hyperopt: 0.2.7
2026-01-30 08:57:39,271:INFO:              optuna: 4.6.0
2026-01-30 08:57:39,271:INFO:               skopt: 0.10.2
2026-01-30 08:57:39,271:INFO:              mlflow: 3.8.1
2026-01-30 08:57:39,271:INFO:              gradio: 6.3.0
2026-01-30 08:57:39,271:INFO:             fastapi: 0.128.0
2026-01-30 08:57:39,271:INFO:             uvicorn: 0.40.0
2026-01-30 08:57:39,271:INFO:              m2cgen: 0.10.0
2026-01-30 08:57:39,271:INFO:           evidently: 0.4.40
2026-01-30 08:57:39,271:INFO:               fugue: 0.8.7
2026-01-30 08:57:39,271:INFO:           streamlit: Not installed
2026-01-30 08:57:39,271:INFO:             prophet: Not installed
2026-01-30 08:57:39,272:INFO:None
2026-01-30 08:57:39,272:INFO:Set up data.
2026-01-30 08:57:39,402:INFO:Set up folding strategy.
2026-01-30 08:57:39,402:INFO:Set up train/test split.
2026-01-30 08:57:39,626:INFO:Set up index.
2026-01-30 08:57:39,646:INFO:Assigning column types.
2026-01-30 08:57:39,787:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 08:57:39,824:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 08:57:39,827:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 08:57:40,010:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 08:57:40,010:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 08:57:40,239:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 08:57:40,240:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 08:57:40,272:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 08:57:40,274:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 08:57:40,274:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 08:57:40,314:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 08:57:40,324:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 08:57:40,324:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 08:57:40,374:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 08:57:40,404:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 08:57:40,404:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 08:57:40,405:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 08:57:40,475:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 08:57:40,475:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 08:57:40,540:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 08:57:40,540:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 08:57:40,540:INFO:Preparing preprocessing pipeline...
2026-01-30 08:57:40,577:INFO:Set up simple imputation.
2026-01-30 08:57:40,577:INFO:Set up feature normalization.
2026-01-30 08:57:41,019:INFO:Finished creating preprocessing pipeline.
2026-01-30 08:57:41,024:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SI...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 08:57:41,024:INFO:Creating final display dataframe.
2026-01-30 08:57:42,491:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (429278, 20)
4        Transformed data shape      (429278, 20)
5   Transformed train set shape      (300494, 20)
6    Transformed test set shape      (128784, 20)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              0660
2026-01-30 08:57:42,560:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 08:57:42,561:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 08:57:42,624:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 08:57:42,624:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 08:57:42,624:INFO:setup() successfully completed in 11.47s...............
2026-01-30 08:57:42,624:INFO:Initializing compare_models()
2026-01-30 08:57:42,624:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 08:57:42,624:INFO:Checking exceptions
2026-01-30 08:57:42,741:INFO:Preparing display monitor
2026-01-30 08:57:42,761:INFO:Initializing Logistic Regression
2026-01-30 08:57:42,762:INFO:Total runtime is 1.6895929972330728e-05 minutes
2026-01-30 08:57:42,762:INFO:SubProcess create_model() called ==================================
2026-01-30 08:57:42,762:INFO:Initializing create_model()
2026-01-30 08:57:42,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C28A7710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 08:57:42,763:INFO:Checking exceptions
2026-01-30 08:57:42,763:INFO:Importing libraries
2026-01-30 08:57:42,763:INFO:Copying training dataset
2026-01-30 08:57:42,958:INFO:Defining folds
2026-01-30 08:57:42,958:INFO:Declaring metric variables
2026-01-30 08:57:42,958:INFO:Importing untrained model
2026-01-30 08:57:42,959:INFO:Logistic Regression Imported successfully
2026-01-30 08:57:42,959:INFO:Starting cross validation
2026-01-30 08:57:42,960:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 08:57:50,916:INFO:Calculating mean and std
2026-01-30 08:57:50,916:INFO:Creating metrics dataframe
2026-01-30 08:57:50,924:INFO:Uploading results into container
2026-01-30 08:57:50,924:INFO:Uploading model into container now
2026-01-30 08:57:50,925:INFO:_master_model_container: 1
2026-01-30 08:57:50,925:INFO:_display_container: 2
2026-01-30 08:57:50,925:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 08:57:50,925:INFO:create_model() successfully completed......................................
2026-01-30 08:57:51,042:INFO:SubProcess create_model() end ==================================
2026-01-30 08:57:51,042:INFO:Creating metrics dataframe
2026-01-30 08:57:51,042:INFO:Initializing Decision Tree Classifier
2026-01-30 08:57:51,042:INFO:Total runtime is 0.1380051891009013 minutes
2026-01-30 08:57:51,042:INFO:SubProcess create_model() called ==================================
2026-01-30 08:57:51,042:INFO:Initializing create_model()
2026-01-30 08:57:51,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C28A7710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 08:57:51,042:INFO:Checking exceptions
2026-01-30 08:57:51,042:INFO:Importing libraries
2026-01-30 08:57:51,042:INFO:Copying training dataset
2026-01-30 08:57:51,173:INFO:Defining folds
2026-01-30 08:57:51,173:INFO:Declaring metric variables
2026-01-30 08:57:51,173:INFO:Importing untrained model
2026-01-30 08:57:51,173:INFO:Decision Tree Classifier Imported successfully
2026-01-30 08:57:51,173:INFO:Starting cross validation
2026-01-30 08:57:51,173:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 08:57:57,063:INFO:Calculating mean and std
2026-01-30 08:57:57,063:INFO:Creating metrics dataframe
2026-01-30 08:57:57,063:INFO:Uploading results into container
2026-01-30 08:57:57,067:INFO:Uploading model into container now
2026-01-30 08:57:57,067:INFO:_master_model_container: 2
2026-01-30 08:57:57,067:INFO:_display_container: 2
2026-01-30 08:57:57,067:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 08:57:57,067:INFO:create_model() successfully completed......................................
2026-01-30 08:57:57,173:INFO:SubProcess create_model() end ==================================
2026-01-30 08:57:57,173:INFO:Creating metrics dataframe
2026-01-30 08:57:57,188:INFO:Initializing Random Forest Classifier
2026-01-30 08:57:57,188:INFO:Total runtime is 0.2404475728670756 minutes
2026-01-30 08:57:57,188:INFO:SubProcess create_model() called ==================================
2026-01-30 08:57:57,189:INFO:Initializing create_model()
2026-01-30 08:57:57,189:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C28A7710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 08:57:57,189:INFO:Checking exceptions
2026-01-30 08:57:57,189:INFO:Importing libraries
2026-01-30 08:57:57,189:INFO:Copying training dataset
2026-01-30 08:57:57,319:INFO:Defining folds
2026-01-30 08:57:57,319:INFO:Declaring metric variables
2026-01-30 08:57:57,319:INFO:Importing untrained model
2026-01-30 08:57:57,319:INFO:Random Forest Classifier Imported successfully
2026-01-30 08:57:57,321:INFO:Starting cross validation
2026-01-30 08:57:57,321:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 08:58:09,085:INFO:Calculating mean and std
2026-01-30 08:58:09,089:INFO:Creating metrics dataframe
2026-01-30 08:58:09,091:INFO:Uploading results into container
2026-01-30 08:58:09,092:INFO:Uploading model into container now
2026-01-30 08:58:09,092:INFO:_master_model_container: 3
2026-01-30 08:58:09,092:INFO:_display_container: 2
2026-01-30 08:58:09,093:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 08:58:09,093:INFO:create_model() successfully completed......................................
2026-01-30 08:58:09,209:INFO:SubProcess create_model() end ==================================
2026-01-30 08:58:09,209:INFO:Creating metrics dataframe
2026-01-30 08:58:09,223:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 08:58:09,223:INFO:Total runtime is 0.44102354447046915 minutes
2026-01-30 08:58:09,223:INFO:SubProcess create_model() called ==================================
2026-01-30 08:58:09,223:INFO:Initializing create_model()
2026-01-30 08:58:09,223:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C28A7710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 08:58:09,223:INFO:Checking exceptions
2026-01-30 08:58:09,223:INFO:Importing libraries
2026-01-30 08:58:09,223:INFO:Copying training dataset
2026-01-30 08:58:09,367:INFO:Defining folds
2026-01-30 08:58:09,367:INFO:Declaring metric variables
2026-01-30 08:58:09,367:INFO:Importing untrained model
2026-01-30 08:58:09,367:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 08:58:09,367:INFO:Starting cross validation
2026-01-30 08:58:09,367:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 08:58:17,273:INFO:Calculating mean and std
2026-01-30 08:58:17,273:INFO:Creating metrics dataframe
2026-01-30 08:58:17,273:INFO:Uploading results into container
2026-01-30 08:58:17,273:INFO:Uploading model into container now
2026-01-30 08:58:17,273:INFO:_master_model_container: 4
2026-01-30 08:58:17,273:INFO:_display_container: 2
2026-01-30 08:58:17,273:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 08:58:17,273:INFO:create_model() successfully completed......................................
2026-01-30 08:58:17,402:INFO:SubProcess create_model() end ==================================
2026-01-30 08:58:17,403:INFO:Creating metrics dataframe
2026-01-30 08:58:17,406:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 08:58:17,406:INFO:Initializing create_model()
2026-01-30 08:58:17,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 08:58:17,406:INFO:Checking exceptions
2026-01-30 08:58:17,406:INFO:Importing libraries
2026-01-30 08:58:17,406:INFO:Copying training dataset
2026-01-30 08:58:17,740:INFO:Defining folds
2026-01-30 08:58:17,740:INFO:Declaring metric variables
2026-01-30 08:58:17,740:INFO:Importing untrained model
2026-01-30 08:58:17,740:INFO:Declaring custom model
2026-01-30 08:58:17,740:INFO:Random Forest Classifier Imported successfully
2026-01-30 08:58:17,740:INFO:Cross validation set to False
2026-01-30 08:58:17,740:INFO:Fitting Model
2026-01-30 08:58:22,099:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 08:58:22,099:INFO:create_model() successfully completed......................................
2026-01-30 08:58:22,232:INFO:Initializing create_model()
2026-01-30 08:58:22,233:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 08:58:22,233:INFO:Checking exceptions
2026-01-30 08:58:22,234:INFO:Importing libraries
2026-01-30 08:58:22,234:INFO:Copying training dataset
2026-01-30 08:58:22,473:INFO:Defining folds
2026-01-30 08:58:22,473:INFO:Declaring metric variables
2026-01-30 08:58:22,473:INFO:Importing untrained model
2026-01-30 08:58:22,473:INFO:Declaring custom model
2026-01-30 08:58:22,473:INFO:Decision Tree Classifier Imported successfully
2026-01-30 08:58:22,473:INFO:Cross validation set to False
2026-01-30 08:58:22,473:INFO:Fitting Model
2026-01-30 08:58:23,772:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 08:58:23,772:INFO:create_model() successfully completed......................................
2026-01-30 08:58:23,906:INFO:Initializing create_model()
2026-01-30 08:58:23,906:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 08:58:23,906:INFO:Checking exceptions
2026-01-30 08:58:23,906:INFO:Importing libraries
2026-01-30 08:58:23,906:INFO:Copying training dataset
2026-01-30 08:58:24,072:INFO:Defining folds
2026-01-30 08:58:24,072:INFO:Declaring metric variables
2026-01-30 08:58:24,072:INFO:Importing untrained model
2026-01-30 08:58:24,072:INFO:Declaring custom model
2026-01-30 08:58:24,072:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 08:58:24,072:INFO:Cross validation set to False
2026-01-30 08:58:24,072:INFO:Fitting Model
2026-01-30 08:58:24,608:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 08:58:24,649:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009199 seconds.
2026-01-30 08:58:24,649:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 08:58:24,649:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 08:58:24,649:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 08:58:24,649:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 08:58:24,652:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 08:58:24,652:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 08:58:25,160:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 08:58:25,160:INFO:create_model() successfully completed......................................
2026-01-30 08:58:25,322:INFO:_master_model_container: 4
2026-01-30 08:58:25,322:INFO:_display_container: 2
2026-01-30 08:58:25,322:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2026-01-30 08:58:25,322:INFO:compare_models() successfully completed......................................
2026-01-30 08:58:25,322:INFO:Initializing tune_model()
2026-01-30 08:58:25,322:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 08:58:25,322:INFO:Checking exceptions
2026-01-30 08:58:25,390:INFO:Copying training dataset
2026-01-30 08:58:25,514:INFO:Checking base model
2026-01-30 08:58:25,514:INFO:Base model : Random Forest Classifier
2026-01-30 08:58:25,515:INFO:Declaring metric variables
2026-01-30 08:58:25,516:INFO:Defining Hyperparameters
2026-01-30 08:58:25,643:INFO:Tuning with n_jobs=-1
2026-01-30 08:58:25,643:INFO:Initializing RandomizedSearchCV
2026-01-30 08:59:43,741:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 08:59:43,741:INFO:Hyperparameter search completed
2026-01-30 08:59:43,741:INFO:SubProcess create_model() called ==================================
2026-01-30 08:59:43,741:INFO:Initializing create_model()
2026-01-30 08:59:43,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06D69FCD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 08:59:43,741:INFO:Checking exceptions
2026-01-30 08:59:43,741:INFO:Importing libraries
2026-01-30 08:59:43,741:INFO:Copying training dataset
2026-01-30 08:59:43,999:INFO:Defining folds
2026-01-30 08:59:43,999:INFO:Declaring metric variables
2026-01-30 08:59:43,999:INFO:Importing untrained model
2026-01-30 08:59:43,999:INFO:Declaring custom model
2026-01-30 08:59:44,004:INFO:Random Forest Classifier Imported successfully
2026-01-30 08:59:44,004:INFO:Starting cross validation
2026-01-30 08:59:44,006:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:00:04,177:INFO:Calculating mean and std
2026-01-30 09:00:04,177:INFO:Creating metrics dataframe
2026-01-30 09:00:04,190:INFO:Finalizing model
2026-01-30 09:00:16,415:INFO:Uploading results into container
2026-01-30 09:00:16,423:INFO:Uploading model into container now
2026-01-30 09:00:16,430:INFO:_master_model_container: 5
2026-01-30 09:00:16,431:INFO:_display_container: 3
2026-01-30 09:00:16,441:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:00:16,447:INFO:create_model() successfully completed......................................
2026-01-30 09:00:16,716:INFO:SubProcess create_model() end ==================================
2026-01-30 09:00:16,717:INFO:choose_better activated
2026-01-30 09:00:16,718:INFO:SubProcess create_model() called ==================================
2026-01-30 09:00:16,719:INFO:Initializing create_model()
2026-01-30 09:00:16,719:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:00:16,719:INFO:Checking exceptions
2026-01-30 09:00:16,721:INFO:Importing libraries
2026-01-30 09:00:16,721:INFO:Copying training dataset
2026-01-30 09:00:17,238:INFO:Defining folds
2026-01-30 09:00:17,238:INFO:Declaring metric variables
2026-01-30 09:00:17,239:INFO:Importing untrained model
2026-01-30 09:00:17,239:INFO:Declaring custom model
2026-01-30 09:00:17,240:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:00:17,241:INFO:Starting cross validation
2026-01-30 09:00:17,242:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:00:34,584:INFO:Calculating mean and std
2026-01-30 09:00:34,586:INFO:Creating metrics dataframe
2026-01-30 09:00:34,590:INFO:Finalizing model
2026-01-30 09:00:44,903:INFO:Uploading results into container
2026-01-30 09:00:44,905:INFO:Uploading model into container now
2026-01-30 09:00:44,905:INFO:_master_model_container: 6
2026-01-30 09:00:44,906:INFO:_display_container: 4
2026-01-30 09:00:44,907:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:00:44,907:INFO:create_model() successfully completed......................................
2026-01-30 09:00:45,137:INFO:SubProcess create_model() end ==================================
2026-01-30 09:00:45,138:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9993
2026-01-30 09:00:45,139:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9934
2026-01-30 09:00:45,140:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 09:00:45,140:INFO:choose_better completed
2026-01-30 09:00:45,140:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:00:45,145:INFO:_master_model_container: 6
2026-01-30 09:00:45,145:INFO:_display_container: 3
2026-01-30 09:00:45,145:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:00:45,146:INFO:tune_model() successfully completed......................................
2026-01-30 09:00:45,376:INFO:Initializing tune_model()
2026-01-30 09:00:45,376:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:00:45,377:INFO:Checking exceptions
2026-01-30 09:00:45,557:INFO:Copying training dataset
2026-01-30 09:00:45,883:INFO:Checking base model
2026-01-30 09:00:45,884:INFO:Base model : Decision Tree Classifier
2026-01-30 09:00:45,885:INFO:Declaring metric variables
2026-01-30 09:00:45,885:INFO:Defining Hyperparameters
2026-01-30 09:00:46,106:INFO:Tuning with n_jobs=-1
2026-01-30 09:00:46,106:INFO:Initializing RandomizedSearchCV
2026-01-30 09:00:57,353:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 09:00:57,354:INFO:Hyperparameter search completed
2026-01-30 09:00:57,355:INFO:SubProcess create_model() called ==================================
2026-01-30 09:00:57,358:INFO:Initializing create_model()
2026-01-30 09:00:57,358:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06E84A250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 09:00:57,358:INFO:Checking exceptions
2026-01-30 09:00:57,359:INFO:Importing libraries
2026-01-30 09:00:57,359:INFO:Copying training dataset
2026-01-30 09:00:57,809:INFO:Defining folds
2026-01-30 09:00:57,810:INFO:Declaring metric variables
2026-01-30 09:00:57,810:INFO:Importing untrained model
2026-01-30 09:00:57,810:INFO:Declaring custom model
2026-01-30 09:00:57,811:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:00:57,811:INFO:Starting cross validation
2026-01-30 09:00:57,812:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:01:00,281:INFO:Calculating mean and std
2026-01-30 09:01:00,285:INFO:Creating metrics dataframe
2026-01-30 09:01:00,289:INFO:Finalizing model
2026-01-30 09:01:01,773:INFO:Uploading results into container
2026-01-30 09:01:01,774:INFO:Uploading model into container now
2026-01-30 09:01:01,775:INFO:_master_model_container: 7
2026-01-30 09:01:01,775:INFO:_display_container: 4
2026-01-30 09:01:01,776:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:01:01,776:INFO:create_model() successfully completed......................................
2026-01-30 09:01:01,993:INFO:SubProcess create_model() end ==================================
2026-01-30 09:01:01,993:INFO:choose_better activated
2026-01-30 09:01:01,995:INFO:SubProcess create_model() called ==================================
2026-01-30 09:01:01,995:INFO:Initializing create_model()
2026-01-30 09:01:01,995:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:01:01,997:INFO:Checking exceptions
2026-01-30 09:01:01,997:INFO:Importing libraries
2026-01-30 09:01:01,998:INFO:Copying training dataset
2026-01-30 09:01:02,388:INFO:Defining folds
2026-01-30 09:01:02,388:INFO:Declaring metric variables
2026-01-30 09:01:02,388:INFO:Importing untrained model
2026-01-30 09:01:02,389:INFO:Declaring custom model
2026-01-30 09:01:02,389:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:01:02,390:INFO:Starting cross validation
2026-01-30 09:01:02,391:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:01:04,980:INFO:Calculating mean and std
2026-01-30 09:01:04,981:INFO:Creating metrics dataframe
2026-01-30 09:01:04,983:INFO:Finalizing model
2026-01-30 09:01:07,011:INFO:Uploading results into container
2026-01-30 09:01:07,011:INFO:Uploading model into container now
2026-01-30 09:01:07,013:INFO:_master_model_container: 8
2026-01-30 09:01:07,013:INFO:_display_container: 5
2026-01-30 09:01:07,013:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:01:07,013:INFO:create_model() successfully completed......................................
2026-01-30 09:01:07,193:INFO:SubProcess create_model() end ==================================
2026-01-30 09:01:07,194:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9988
2026-01-30 09:01:07,194:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9876
2026-01-30 09:01:07,194:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 09:01:07,194:INFO:choose_better completed
2026-01-30 09:01:07,195:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:01:07,198:INFO:_master_model_container: 8
2026-01-30 09:01:07,198:INFO:_display_container: 4
2026-01-30 09:01:07,199:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:01:07,199:INFO:tune_model() successfully completed......................................
2026-01-30 09:01:07,369:INFO:Initializing tune_model()
2026-01-30 09:01:07,369:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:01:07,369:INFO:Checking exceptions
2026-01-30 09:01:07,510:INFO:Copying training dataset
2026-01-30 09:01:07,765:INFO:Checking base model
2026-01-30 09:01:07,766:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 09:01:07,767:INFO:Declaring metric variables
2026-01-30 09:01:07,767:INFO:Defining Hyperparameters
2026-01-30 09:01:07,955:INFO:Tuning with n_jobs=-1
2026-01-30 09:01:07,956:INFO:Initializing RandomizedSearchCV
2026-01-30 09:02:50,262:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 09:02:50,264:INFO:Hyperparameter search completed
2026-01-30 09:02:50,264:INFO:SubProcess create_model() called ==================================
2026-01-30 09:02:50,267:INFO:Initializing create_model()
2026-01-30 09:02:50,267:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06E54AC90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 09:02:50,267:INFO:Checking exceptions
2026-01-30 09:02:50,267:INFO:Importing libraries
2026-01-30 09:02:50,267:INFO:Copying training dataset
2026-01-30 09:02:50,798:INFO:Defining folds
2026-01-30 09:02:50,798:INFO:Declaring metric variables
2026-01-30 09:02:50,799:INFO:Importing untrained model
2026-01-30 09:02:50,799:INFO:Declaring custom model
2026-01-30 09:02:50,800:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:02:50,802:INFO:Starting cross validation
2026-01-30 09:02:50,804:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:03:17,057:INFO:Calculating mean and std
2026-01-30 09:03:17,059:INFO:Creating metrics dataframe
2026-01-30 09:03:17,062:INFO:Finalizing model
2026-01-30 09:03:17,824:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:03:17,824:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:03:17,824:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:03:18,485:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:03:18,486:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:03:18,487:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:03:18,493:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:03:18,661:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036028 seconds.
2026-01-30 09:03:18,661:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:03:18,661:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:03:18,662:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:03:18,664:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:03:18,676:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:03:18,676:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:03:25,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:03:25,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:03:25,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:03:26,769:INFO:Uploading results into container
2026-01-30 09:03:26,770:INFO:Uploading model into container now
2026-01-30 09:03:26,772:INFO:_master_model_container: 9
2026-01-30 09:03:26,773:INFO:_display_container: 5
2026-01-30 09:03:26,775:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:03:26,776:INFO:create_model() successfully completed......................................
2026-01-30 09:03:27,057:INFO:SubProcess create_model() end ==================================
2026-01-30 09:03:27,059:INFO:choose_better activated
2026-01-30 09:03:27,059:INFO:SubProcess create_model() called ==================================
2026-01-30 09:03:27,060:INFO:Initializing create_model()
2026-01-30 09:03:27,060:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:03:27,060:INFO:Checking exceptions
2026-01-30 09:03:27,062:INFO:Importing libraries
2026-01-30 09:03:27,062:INFO:Copying training dataset
2026-01-30 09:03:27,520:INFO:Defining folds
2026-01-30 09:03:27,520:INFO:Declaring metric variables
2026-01-30 09:03:27,520:INFO:Importing untrained model
2026-01-30 09:03:27,521:INFO:Declaring custom model
2026-01-30 09:03:27,523:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:03:27,523:INFO:Starting cross validation
2026-01-30 09:03:27,524:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:03:38,369:INFO:Calculating mean and std
2026-01-30 09:03:38,370:INFO:Creating metrics dataframe
2026-01-30 09:03:38,374:INFO:Finalizing model
2026-01-30 09:03:39,810:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:03:39,905:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023140 seconds.
2026-01-30 09:03:39,906:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:03:39,906:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:03:39,906:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:03:39,908:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:03:39,912:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:03:39,912:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:03:42,586:INFO:Uploading results into container
2026-01-30 09:03:42,587:INFO:Uploading model into container now
2026-01-30 09:03:42,588:INFO:_master_model_container: 10
2026-01-30 09:03:42,588:INFO:_display_container: 6
2026-01-30 09:03:42,590:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:03:42,590:INFO:create_model() successfully completed......................................
2026-01-30 09:03:42,843:INFO:SubProcess create_model() end ==================================
2026-01-30 09:03:42,846:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9966
2026-01-30 09:03:42,848:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9992
2026-01-30 09:03:42,849:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 09:03:42,849:INFO:choose_better completed
2026-01-30 09:03:42,856:INFO:_master_model_container: 10
2026-01-30 09:03:42,856:INFO:_display_container: 5
2026-01-30 09:03:42,858:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:03:42,858:INFO:tune_model() successfully completed......................................
2026-01-30 09:03:43,109:INFO:Initializing predict_model()
2026-01-30 09:03:43,110:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A01562A5C0>)
2026-01-30 09:03:43,110:INFO:Checking exceptions
2026-01-30 09:03:43,110:INFO:Preloading libraries
2026-01-30 09:03:43,110:INFO:Set up data.
2026-01-30 09:03:43,233:INFO:Set up index.
2026-01-30 09:03:46,032:INFO:Initializing predict_model()
2026-01-30 09:03:46,033:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A01562A5C0>)
2026-01-30 09:03:46,033:INFO:Checking exceptions
2026-01-30 09:03:46,033:INFO:Preloading libraries
2026-01-30 09:03:46,034:INFO:Set up data.
2026-01-30 09:03:46,102:INFO:Set up index.
2026-01-30 09:03:47,218:INFO:Initializing predict_model()
2026-01-30 09:03:47,218:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03C546160>)
2026-01-30 09:03:47,219:INFO:Checking exceptions
2026-01-30 09:03:47,219:INFO:Preloading libraries
2026-01-30 09:03:47,219:INFO:Set up data.
2026-01-30 09:03:47,269:INFO:Set up index.
2026-01-30 09:03:49,802:INFO:Initializing plot_model()
2026-01-30 09:03:49,803:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:03:49,803:INFO:Checking exceptions
2026-01-30 09:03:50,212:INFO:Preloading libraries
2026-01-30 09:03:50,346:INFO:Copying training dataset
2026-01-30 09:03:50,346:INFO:Plot type: feature
2026-01-30 09:03:50,346:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:03:51,430:INFO:Visual Rendered Successfully
2026-01-30 09:03:51,639:INFO:plot_model() successfully completed......................................
2026-01-30 09:03:51,652:INFO:Initializing plot_model()
2026-01-30 09:03:51,652:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C515750>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:03:51,652:INFO:Checking exceptions
2026-01-30 09:03:52,050:INFO:Preloading libraries
2026-01-30 09:03:52,174:INFO:Copying training dataset
2026-01-30 09:03:52,174:INFO:Plot type: feature_all
2026-01-30 09:03:52,636:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:03:53,616:INFO:Visual Rendered Successfully
2026-01-30 09:03:53,861:INFO:plot_model() successfully completed......................................
2026-01-30 09:03:53,888:INFO:Initializing save_model()
2026-01-30 09:03:53,889:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SI...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 09:03:53,889:INFO:Adding model into prep_pipe
2026-01-30 09:03:54,285:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 09:03:54,294:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges_...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 09:03:54,294:INFO:save_model() successfully completed......................................
2026-01-30 09:11:22,347:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\2920816646.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 09:11:24,345:INFO:PyCaret ClassificationExperiment
2026-01-30 09:11:24,345:INFO:Logging name: clf-default-name
2026-01-30 09:11:24,345:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 09:11:24,345:INFO:version 3.3.2
2026-01-30 09:11:24,345:INFO:Initializing setup()
2026-01-30 09:11:24,345:INFO:self.USI: 3c14
2026-01-30 09:11:24,345:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 09:11:24,345:INFO:Checking environment
2026-01-30 09:11:24,345:INFO:python_version: 3.11.11
2026-01-30 09:11:24,345:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 09:11:24,345:INFO:machine: AMD64
2026-01-30 09:11:24,345:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 09:11:24,345:INFO:Memory: svmem(total=34009374720, available=16202559488, percent=52.4, used=17806815232, free=16202559488)
2026-01-30 09:11:24,345:INFO:Physical Core: 12
2026-01-30 09:11:24,345:INFO:Logical Core: 16
2026-01-30 09:11:24,345:INFO:Checking libraries
2026-01-30 09:11:24,345:INFO:System:
2026-01-30 09:11:24,345:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 09:11:24,345:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 09:11:24,345:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 09:11:24,345:INFO:PyCaret required dependencies:
2026-01-30 09:11:24,345:INFO:                 pip: 25.0
2026-01-30 09:11:24,345:INFO:          setuptools: 75.8.0
2026-01-30 09:11:24,345:INFO:             pycaret: 3.3.2
2026-01-30 09:11:24,345:INFO:             IPython: 9.9.0
2026-01-30 09:11:24,345:INFO:          ipywidgets: 8.1.8
2026-01-30 09:11:24,345:INFO:                tqdm: 4.67.1
2026-01-30 09:11:24,345:INFO:               numpy: 1.26.4
2026-01-30 09:11:24,345:INFO:              pandas: 2.1.4
2026-01-30 09:11:24,345:INFO:              jinja2: 3.1.6
2026-01-30 09:11:24,345:INFO:               scipy: 1.11.4
2026-01-30 09:11:24,345:INFO:              joblib: 1.3.2
2026-01-30 09:11:24,345:INFO:             sklearn: 1.4.2
2026-01-30 09:11:24,345:INFO:                pyod: 2.0.6
2026-01-30 09:11:24,345:INFO:            imblearn: 0.14.1
2026-01-30 09:11:24,345:INFO:   category_encoders: 2.7.0
2026-01-30 09:11:24,345:INFO:            lightgbm: 4.6.0
2026-01-30 09:11:24,345:INFO:               numba: 0.62.1
2026-01-30 09:11:24,345:INFO:            requests: 2.32.3
2026-01-30 09:11:24,345:INFO:          matplotlib: 3.7.5
2026-01-30 09:11:24,345:INFO:          scikitplot: 0.3.7
2026-01-30 09:11:24,345:INFO:         yellowbrick: 1.5
2026-01-30 09:11:24,345:INFO:              plotly: 5.24.1
2026-01-30 09:11:24,345:INFO:    plotly-resampler: Not installed
2026-01-30 09:11:24,345:INFO:             kaleido: 1.2.0
2026-01-30 09:11:24,345:INFO:           schemdraw: 0.15
2026-01-30 09:11:24,345:INFO:         statsmodels: 0.14.6
2026-01-30 09:11:24,345:INFO:              sktime: 0.26.0
2026-01-30 09:11:24,345:INFO:               tbats: 1.1.3
2026-01-30 09:11:24,345:INFO:            pmdarima: 2.0.4
2026-01-30 09:11:24,345:INFO:              psutil: 7.2.1
2026-01-30 09:11:24,345:INFO:          markupsafe: 3.0.3
2026-01-30 09:11:24,345:INFO:             pickle5: Not installed
2026-01-30 09:11:24,345:INFO:         cloudpickle: 3.0.0
2026-01-30 09:11:24,345:INFO:         deprecation: 2.1.0
2026-01-30 09:11:24,345:INFO:              xxhash: 3.6.0
2026-01-30 09:11:24,345:INFO:           wurlitzer: Not installed
2026-01-30 09:11:24,345:INFO:PyCaret optional dependencies:
2026-01-30 09:11:24,345:INFO:                shap: 0.44.1
2026-01-30 09:11:24,345:INFO:           interpret: 0.7.3
2026-01-30 09:11:24,345:INFO:                umap: 0.5.7
2026-01-30 09:11:24,345:INFO:     ydata_profiling: 4.18.1
2026-01-30 09:11:24,345:INFO:  explainerdashboard: 0.5.1
2026-01-30 09:11:24,345:INFO:             autoviz: Not installed
2026-01-30 09:11:24,345:INFO:           fairlearn: 0.7.0
2026-01-30 09:11:24,345:INFO:          deepchecks: Not installed
2026-01-30 09:11:24,345:INFO:             xgboost: Not installed
2026-01-30 09:11:24,345:INFO:            catboost: 1.2.8
2026-01-30 09:11:24,345:INFO:              kmodes: 0.12.2
2026-01-30 09:11:24,345:INFO:             mlxtend: 0.23.4
2026-01-30 09:11:24,345:INFO:       statsforecast: 1.5.0
2026-01-30 09:11:24,345:INFO:        tune_sklearn: Not installed
2026-01-30 09:11:24,345:INFO:                 ray: Not installed
2026-01-30 09:11:24,345:INFO:            hyperopt: 0.2.7
2026-01-30 09:11:24,345:INFO:              optuna: 4.6.0
2026-01-30 09:11:24,345:INFO:               skopt: 0.10.2
2026-01-30 09:11:24,345:INFO:              mlflow: 3.8.1
2026-01-30 09:11:24,345:INFO:              gradio: 6.3.0
2026-01-30 09:11:24,345:INFO:             fastapi: 0.128.0
2026-01-30 09:11:24,345:INFO:             uvicorn: 0.40.0
2026-01-30 09:11:24,345:INFO:              m2cgen: 0.10.0
2026-01-30 09:11:24,345:INFO:           evidently: 0.4.40
2026-01-30 09:11:24,345:INFO:               fugue: 0.8.7
2026-01-30 09:11:24,345:INFO:           streamlit: Not installed
2026-01-30 09:11:24,345:INFO:             prophet: Not installed
2026-01-30 09:11:24,345:INFO:None
2026-01-30 09:11:24,345:INFO:Set up data.
2026-01-30 09:11:24,448:INFO:Set up folding strategy.
2026-01-30 09:11:24,449:INFO:Set up train/test split.
2026-01-30 09:11:24,608:INFO:Set up index.
2026-01-30 09:11:24,618:INFO:Assigning column types.
2026-01-30 09:11:24,717:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 09:11:24,743:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:11:24,744:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:11:24,761:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:11:24,761:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:11:24,787:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:11:24,788:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:11:24,795:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:11:24,795:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:11:24,795:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 09:11:24,828:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:11:24,846:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:11:24,847:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:11:24,862:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:11:24,886:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:11:24,886:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:11:24,886:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 09:11:24,928:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:11:24,928:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:11:24,965:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:11:24,965:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:11:24,965:INFO:Preparing preprocessing pipeline...
2026-01-30 09:11:24,987:INFO:Set up simple imputation.
2026-01-30 09:11:24,987:INFO:Set up feature normalization.
2026-01-30 09:11:25,195:INFO:Finished creating preprocessing pipeline.
2026-01-30 09:11:25,195:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SI...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 09:11:25,195:INFO:Creating final display dataframe.
2026-01-30 09:11:25,584:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (429278, 20)
4        Transformed data shape      (429278, 20)
5   Transformed train set shape      (300494, 20)
6    Transformed test set shape      (128784, 20)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              3c14
2026-01-30 09:11:25,618:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:11:25,618:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:11:25,668:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:11:25,668:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:11:25,670:INFO:setup() successfully completed in 1.33s...............
2026-01-30 09:11:25,670:INFO:Initializing compare_models()
2026-01-30 09:11:25,670:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 09:11:25,670:INFO:Checking exceptions
2026-01-30 09:11:25,762:INFO:Preparing display monitor
2026-01-30 09:11:25,762:INFO:Initializing Logistic Regression
2026-01-30 09:11:25,762:INFO:Total runtime is 0.0 minutes
2026-01-30 09:11:25,762:INFO:SubProcess create_model() called ==================================
2026-01-30 09:11:25,762:INFO:Initializing create_model()
2026-01-30 09:11:25,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03DDCEE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:11:25,762:INFO:Checking exceptions
2026-01-30 09:11:25,762:INFO:Importing libraries
2026-01-30 09:11:25,762:INFO:Copying training dataset
2026-01-30 09:11:25,930:INFO:Defining folds
2026-01-30 09:11:25,930:INFO:Declaring metric variables
2026-01-30 09:11:25,930:INFO:Importing untrained model
2026-01-30 09:11:25,930:INFO:Logistic Regression Imported successfully
2026-01-30 09:11:25,930:INFO:Starting cross validation
2026-01-30 09:11:25,930:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:11:33,531:INFO:Calculating mean and std
2026-01-30 09:11:33,531:INFO:Creating metrics dataframe
2026-01-30 09:11:33,531:INFO:Uploading results into container
2026-01-30 09:11:33,531:INFO:Uploading model into container now
2026-01-30 09:11:33,531:INFO:_master_model_container: 1
2026-01-30 09:11:33,531:INFO:_display_container: 2
2026-01-30 09:11:33,531:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 09:11:33,531:INFO:create_model() successfully completed......................................
2026-01-30 09:11:33,645:INFO:SubProcess create_model() end ==================================
2026-01-30 09:11:33,645:INFO:Creating metrics dataframe
2026-01-30 09:11:33,645:INFO:Initializing Decision Tree Classifier
2026-01-30 09:11:33,645:INFO:Total runtime is 0.13138734499613444 minutes
2026-01-30 09:11:33,645:INFO:SubProcess create_model() called ==================================
2026-01-30 09:11:33,645:INFO:Initializing create_model()
2026-01-30 09:11:33,645:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03DDCEE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:11:33,645:INFO:Checking exceptions
2026-01-30 09:11:33,645:INFO:Importing libraries
2026-01-30 09:11:33,645:INFO:Copying training dataset
2026-01-30 09:11:33,778:INFO:Defining folds
2026-01-30 09:11:33,778:INFO:Declaring metric variables
2026-01-30 09:11:33,778:INFO:Importing untrained model
2026-01-30 09:11:33,778:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:11:33,778:INFO:Starting cross validation
2026-01-30 09:11:33,778:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:11:38,720:INFO:Calculating mean and std
2026-01-30 09:11:38,720:INFO:Creating metrics dataframe
2026-01-30 09:11:38,720:INFO:Uploading results into container
2026-01-30 09:11:38,720:INFO:Uploading model into container now
2026-01-30 09:11:38,720:INFO:_master_model_container: 2
2026-01-30 09:11:38,727:INFO:_display_container: 2
2026-01-30 09:11:38,727:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:11:38,727:INFO:create_model() successfully completed......................................
2026-01-30 09:11:38,845:INFO:SubProcess create_model() end ==================================
2026-01-30 09:11:38,845:INFO:Creating metrics dataframe
2026-01-30 09:11:38,845:INFO:Initializing Random Forest Classifier
2026-01-30 09:11:38,845:INFO:Total runtime is 0.21805393298467002 minutes
2026-01-30 09:11:38,845:INFO:SubProcess create_model() called ==================================
2026-01-30 09:11:38,845:INFO:Initializing create_model()
2026-01-30 09:11:38,845:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03DDCEE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:11:38,845:INFO:Checking exceptions
2026-01-30 09:11:38,845:INFO:Importing libraries
2026-01-30 09:11:38,845:INFO:Copying training dataset
2026-01-30 09:11:38,978:INFO:Defining folds
2026-01-30 09:11:38,978:INFO:Declaring metric variables
2026-01-30 09:11:38,978:INFO:Importing untrained model
2026-01-30 09:11:38,978:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:11:38,978:INFO:Starting cross validation
2026-01-30 09:11:38,978:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:11:50,912:INFO:Calculating mean and std
2026-01-30 09:11:50,912:INFO:Creating metrics dataframe
2026-01-30 09:11:50,912:INFO:Uploading results into container
2026-01-30 09:11:50,912:INFO:Uploading model into container now
2026-01-30 09:11:50,912:INFO:_master_model_container: 3
2026-01-30 09:11:50,912:INFO:_display_container: 2
2026-01-30 09:11:50,912:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:11:50,912:INFO:create_model() successfully completed......................................
2026-01-30 09:11:51,031:INFO:SubProcess create_model() end ==================================
2026-01-30 09:11:51,031:INFO:Creating metrics dataframe
2026-01-30 09:11:51,031:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 09:11:51,031:INFO:Total runtime is 0.42115186452865605 minutes
2026-01-30 09:11:51,044:INFO:SubProcess create_model() called ==================================
2026-01-30 09:11:51,044:INFO:Initializing create_model()
2026-01-30 09:11:51,044:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03DDCEE90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:11:51,044:INFO:Checking exceptions
2026-01-30 09:11:51,044:INFO:Importing libraries
2026-01-30 09:11:51,044:INFO:Copying training dataset
2026-01-30 09:11:51,178:INFO:Defining folds
2026-01-30 09:11:51,178:INFO:Declaring metric variables
2026-01-30 09:11:51,178:INFO:Importing untrained model
2026-01-30 09:11:51,178:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:11:51,178:INFO:Starting cross validation
2026-01-30 09:11:51,178:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:11:57,631:INFO:Calculating mean and std
2026-01-30 09:11:57,631:INFO:Creating metrics dataframe
2026-01-30 09:11:57,631:INFO:Uploading results into container
2026-01-30 09:11:57,631:INFO:Uploading model into container now
2026-01-30 09:11:57,631:INFO:_master_model_container: 4
2026-01-30 09:11:57,631:INFO:_display_container: 2
2026-01-30 09:11:57,631:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:11:57,631:INFO:create_model() successfully completed......................................
2026-01-30 09:11:57,744:INFO:SubProcess create_model() end ==================================
2026-01-30 09:11:57,744:INFO:Creating metrics dataframe
2026-01-30 09:11:57,760:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 09:11:57,761:INFO:Initializing create_model()
2026-01-30 09:11:57,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:11:57,761:INFO:Checking exceptions
2026-01-30 09:11:57,761:INFO:Importing libraries
2026-01-30 09:11:57,761:INFO:Copying training dataset
2026-01-30 09:11:57,894:INFO:Defining folds
2026-01-30 09:11:57,894:INFO:Declaring metric variables
2026-01-30 09:11:57,894:INFO:Importing untrained model
2026-01-30 09:11:57,894:INFO:Declaring custom model
2026-01-30 09:11:57,894:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:11:57,894:INFO:Cross validation set to False
2026-01-30 09:11:57,894:INFO:Fitting Model
2026-01-30 09:12:02,813:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:12:02,813:INFO:create_model() successfully completed......................................
2026-01-30 09:12:02,962:INFO:Initializing create_model()
2026-01-30 09:12:02,962:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:12:02,962:INFO:Checking exceptions
2026-01-30 09:12:02,962:INFO:Importing libraries
2026-01-30 09:12:02,962:INFO:Copying training dataset
2026-01-30 09:12:03,153:INFO:Defining folds
2026-01-30 09:12:03,153:INFO:Declaring metric variables
2026-01-30 09:12:03,153:INFO:Importing untrained model
2026-01-30 09:12:03,154:INFO:Declaring custom model
2026-01-30 09:12:03,154:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:12:03,155:INFO:Cross validation set to False
2026-01-30 09:12:03,155:INFO:Fitting Model
2026-01-30 09:12:04,494:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:12:04,494:INFO:create_model() successfully completed......................................
2026-01-30 09:12:04,695:INFO:Initializing create_model()
2026-01-30 09:12:04,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:12:04,695:INFO:Checking exceptions
2026-01-30 09:12:04,697:INFO:Importing libraries
2026-01-30 09:12:04,697:INFO:Copying training dataset
2026-01-30 09:12:04,881:INFO:Defining folds
2026-01-30 09:12:04,881:INFO:Declaring metric variables
2026-01-30 09:12:04,881:INFO:Importing untrained model
2026-01-30 09:12:04,881:INFO:Declaring custom model
2026-01-30 09:12:04,881:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:12:04,881:INFO:Cross validation set to False
2026-01-30 09:12:04,881:INFO:Fitting Model
2026-01-30 09:12:05,484:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:12:05,517:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007788 seconds.
2026-01-30 09:12:05,517:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:12:05,519:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:12:05,519:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:12:05,519:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:12:05,521:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:12:05,521:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:12:06,065:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:12:06,065:INFO:create_model() successfully completed......................................
2026-01-30 09:12:06,237:INFO:_master_model_container: 4
2026-01-30 09:12:06,237:INFO:_display_container: 2
2026-01-30 09:12:06,237:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2026-01-30 09:12:06,237:INFO:compare_models() successfully completed......................................
2026-01-30 09:12:06,244:INFO:Initializing tune_model()
2026-01-30 09:12:06,244:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:12:06,244:INFO:Checking exceptions
2026-01-30 09:12:06,327:INFO:Copying training dataset
2026-01-30 09:12:06,461:INFO:Checking base model
2026-01-30 09:12:06,461:INFO:Base model : Random Forest Classifier
2026-01-30 09:12:06,461:INFO:Declaring metric variables
2026-01-30 09:12:06,461:INFO:Defining Hyperparameters
2026-01-30 09:12:06,611:INFO:Tuning with n_jobs=-1
2026-01-30 09:12:06,611:INFO:Initializing RandomizedSearchCV
2026-01-30 09:13:29,731:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 09:13:29,732:INFO:Hyperparameter search completed
2026-01-30 09:13:29,733:INFO:SubProcess create_model() called ==================================
2026-01-30 09:13:29,736:INFO:Initializing create_model()
2026-01-30 09:13:29,736:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06E702090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 09:13:29,736:INFO:Checking exceptions
2026-01-30 09:13:29,736:INFO:Importing libraries
2026-01-30 09:13:29,736:INFO:Copying training dataset
2026-01-30 09:13:29,945:INFO:Defining folds
2026-01-30 09:13:29,945:INFO:Declaring metric variables
2026-01-30 09:13:29,945:INFO:Importing untrained model
2026-01-30 09:13:29,945:INFO:Declaring custom model
2026-01-30 09:13:29,945:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:13:29,945:INFO:Starting cross validation
2026-01-30 09:13:29,945:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:13:46,361:INFO:Calculating mean and std
2026-01-30 09:13:46,362:INFO:Creating metrics dataframe
2026-01-30 09:13:46,364:INFO:Finalizing model
2026-01-30 09:13:54,279:INFO:Uploading results into container
2026-01-30 09:13:54,279:INFO:Uploading model into container now
2026-01-30 09:13:54,279:INFO:_master_model_container: 5
2026-01-30 09:13:54,279:INFO:_display_container: 3
2026-01-30 09:13:54,279:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:13:54,279:INFO:create_model() successfully completed......................................
2026-01-30 09:13:54,410:INFO:SubProcess create_model() end ==================================
2026-01-30 09:13:54,410:INFO:choose_better activated
2026-01-30 09:13:54,410:INFO:SubProcess create_model() called ==================================
2026-01-30 09:13:54,410:INFO:Initializing create_model()
2026-01-30 09:13:54,410:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:13:54,410:INFO:Checking exceptions
2026-01-30 09:13:54,410:INFO:Importing libraries
2026-01-30 09:13:54,410:INFO:Copying training dataset
2026-01-30 09:13:54,565:INFO:Defining folds
2026-01-30 09:13:54,565:INFO:Declaring metric variables
2026-01-30 09:13:54,565:INFO:Importing untrained model
2026-01-30 09:13:54,565:INFO:Declaring custom model
2026-01-30 09:13:54,565:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:13:54,566:INFO:Starting cross validation
2026-01-30 09:13:54,566:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:14:04,937:INFO:Calculating mean and std
2026-01-30 09:14:04,937:INFO:Creating metrics dataframe
2026-01-30 09:14:04,937:INFO:Finalizing model
2026-01-30 09:14:09,803:INFO:Uploading results into container
2026-01-30 09:14:09,804:INFO:Uploading model into container now
2026-01-30 09:14:09,805:INFO:_master_model_container: 6
2026-01-30 09:14:09,805:INFO:_display_container: 4
2026-01-30 09:14:09,806:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:14:09,806:INFO:create_model() successfully completed......................................
2026-01-30 09:14:09,944:INFO:SubProcess create_model() end ==================================
2026-01-30 09:14:09,945:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9993
2026-01-30 09:14:09,945:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9934
2026-01-30 09:14:09,946:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 09:14:09,946:INFO:choose_better completed
2026-01-30 09:14:09,946:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:14:09,948:INFO:_master_model_container: 6
2026-01-30 09:14:09,948:INFO:_display_container: 3
2026-01-30 09:14:09,949:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:14:09,949:INFO:tune_model() successfully completed......................................
2026-01-30 09:14:10,079:INFO:Initializing tune_model()
2026-01-30 09:14:10,079:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:14:10,079:INFO:Checking exceptions
2026-01-30 09:14:10,126:INFO:Copying training dataset
2026-01-30 09:14:10,254:INFO:Checking base model
2026-01-30 09:14:10,254:INFO:Base model : Decision Tree Classifier
2026-01-30 09:14:10,254:INFO:Declaring metric variables
2026-01-30 09:14:10,254:INFO:Defining Hyperparameters
2026-01-30 09:14:10,376:INFO:Tuning with n_jobs=-1
2026-01-30 09:14:10,376:INFO:Initializing RandomizedSearchCV
2026-01-30 09:14:15,029:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 09:14:15,031:INFO:Hyperparameter search completed
2026-01-30 09:14:15,031:INFO:SubProcess create_model() called ==================================
2026-01-30 09:14:15,031:INFO:Initializing create_model()
2026-01-30 09:14:15,031:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06E4C1350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 09:14:15,031:INFO:Checking exceptions
2026-01-30 09:14:15,031:INFO:Importing libraries
2026-01-30 09:14:15,031:INFO:Copying training dataset
2026-01-30 09:14:15,216:INFO:Defining folds
2026-01-30 09:14:15,216:INFO:Declaring metric variables
2026-01-30 09:14:15,217:INFO:Importing untrained model
2026-01-30 09:14:15,217:INFO:Declaring custom model
2026-01-30 09:14:15,218:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:14:15,218:INFO:Starting cross validation
2026-01-30 09:14:15,219:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:14:16,901:INFO:Calculating mean and std
2026-01-30 09:14:16,903:INFO:Creating metrics dataframe
2026-01-30 09:14:16,906:INFO:Finalizing model
2026-01-30 09:14:17,783:INFO:Uploading results into container
2026-01-30 09:14:17,784:INFO:Uploading model into container now
2026-01-30 09:14:17,784:INFO:_master_model_container: 7
2026-01-30 09:14:17,784:INFO:_display_container: 4
2026-01-30 09:14:17,785:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:14:17,785:INFO:create_model() successfully completed......................................
2026-01-30 09:14:17,909:INFO:SubProcess create_model() end ==================================
2026-01-30 09:14:17,915:INFO:choose_better activated
2026-01-30 09:14:17,915:INFO:SubProcess create_model() called ==================================
2026-01-30 09:14:17,915:INFO:Initializing create_model()
2026-01-30 09:14:17,915:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:14:17,915:INFO:Checking exceptions
2026-01-30 09:14:17,915:INFO:Importing libraries
2026-01-30 09:14:17,915:INFO:Copying training dataset
2026-01-30 09:14:18,064:INFO:Defining folds
2026-01-30 09:14:18,064:INFO:Declaring metric variables
2026-01-30 09:14:18,064:INFO:Importing untrained model
2026-01-30 09:14:18,064:INFO:Declaring custom model
2026-01-30 09:14:18,064:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:14:18,064:INFO:Starting cross validation
2026-01-30 09:14:18,075:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:14:20,164:INFO:Calculating mean and std
2026-01-30 09:14:20,165:INFO:Creating metrics dataframe
2026-01-30 09:14:20,166:INFO:Finalizing model
2026-01-30 09:14:21,429:INFO:Uploading results into container
2026-01-30 09:14:21,429:INFO:Uploading model into container now
2026-01-30 09:14:21,429:INFO:_master_model_container: 8
2026-01-30 09:14:21,429:INFO:_display_container: 5
2026-01-30 09:14:21,429:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:14:21,429:INFO:create_model() successfully completed......................................
2026-01-30 09:14:21,557:INFO:SubProcess create_model() end ==================================
2026-01-30 09:14:21,558:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9988
2026-01-30 09:14:21,559:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9876
2026-01-30 09:14:21,559:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 09:14:21,559:INFO:choose_better completed
2026-01-30 09:14:21,559:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:14:21,563:INFO:_master_model_container: 8
2026-01-30 09:14:21,563:INFO:_display_container: 4
2026-01-30 09:14:21,563:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:14:21,563:INFO:tune_model() successfully completed......................................
2026-01-30 09:14:21,684:INFO:Initializing tune_model()
2026-01-30 09:14:21,684:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:14:21,684:INFO:Checking exceptions
2026-01-30 09:14:21,747:INFO:Copying training dataset
2026-01-30 09:14:21,867:INFO:Checking base model
2026-01-30 09:14:21,868:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 09:14:21,868:INFO:Declaring metric variables
2026-01-30 09:14:21,869:INFO:Defining Hyperparameters
2026-01-30 09:14:21,975:INFO:Tuning with n_jobs=-1
2026-01-30 09:14:21,975:INFO:Initializing RandomizedSearchCV
2026-01-30 09:14:53,219:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 09:14:53,219:INFO:Hyperparameter search completed
2026-01-30 09:14:53,225:INFO:SubProcess create_model() called ==================================
2026-01-30 09:14:53,226:INFO:Initializing create_model()
2026-01-30 09:14:53,226:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C8B3D810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 09:14:53,226:INFO:Checking exceptions
2026-01-30 09:14:53,226:INFO:Importing libraries
2026-01-30 09:14:53,227:INFO:Copying training dataset
2026-01-30 09:14:53,410:INFO:Defining folds
2026-01-30 09:14:53,410:INFO:Declaring metric variables
2026-01-30 09:14:53,410:INFO:Importing untrained model
2026-01-30 09:14:53,410:INFO:Declaring custom model
2026-01-30 09:14:53,410:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:14:53,410:INFO:Starting cross validation
2026-01-30 09:14:53,410:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:15:02,138:INFO:Calculating mean and std
2026-01-30 09:15:02,142:INFO:Creating metrics dataframe
2026-01-30 09:15:02,146:INFO:Finalizing model
2026-01-30 09:15:02,511:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:15:02,511:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:15:02,511:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:15:02,674:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:15:02,674:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:15:02,674:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:15:02,675:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:15:02,713:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006680 seconds.
2026-01-30 09:15:02,713:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:15:02,713:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:15:02,714:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:15:02,715:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:15:02,718:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:15:02,718:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:15:04,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:15:04,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:15:05,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:15:05,318:INFO:Uploading results into container
2026-01-30 09:15:05,319:INFO:Uploading model into container now
2026-01-30 09:15:05,320:INFO:_master_model_container: 9
2026-01-30 09:15:05,321:INFO:_display_container: 5
2026-01-30 09:15:05,322:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:15:05,323:INFO:create_model() successfully completed......................................
2026-01-30 09:15:05,510:INFO:SubProcess create_model() end ==================================
2026-01-30 09:15:05,510:INFO:choose_better activated
2026-01-30 09:15:05,510:INFO:SubProcess create_model() called ==================================
2026-01-30 09:15:05,510:INFO:Initializing create_model()
2026-01-30 09:15:05,510:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:15:05,510:INFO:Checking exceptions
2026-01-30 09:15:05,512:INFO:Importing libraries
2026-01-30 09:15:05,512:INFO:Copying training dataset
2026-01-30 09:15:05,698:INFO:Defining folds
2026-01-30 09:15:05,699:INFO:Declaring metric variables
2026-01-30 09:15:05,699:INFO:Importing untrained model
2026-01-30 09:15:05,699:INFO:Declaring custom model
2026-01-30 09:15:05,700:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:15:05,700:INFO:Starting cross validation
2026-01-30 09:15:05,700:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:15:10,180:INFO:Calculating mean and std
2026-01-30 09:15:10,180:INFO:Creating metrics dataframe
2026-01-30 09:15:10,182:INFO:Finalizing model
2026-01-30 09:15:10,714:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:15:10,746:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006799 seconds.
2026-01-30 09:15:10,746:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:15:10,746:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:15:10,746:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:15:10,747:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:15:10,749:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:15:10,749:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:15:11,521:INFO:Uploading results into container
2026-01-30 09:15:11,521:INFO:Uploading model into container now
2026-01-30 09:15:11,523:INFO:_master_model_container: 10
2026-01-30 09:15:11,523:INFO:_display_container: 6
2026-01-30 09:15:11,524:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:15:11,525:INFO:create_model() successfully completed......................................
2026-01-30 09:15:11,709:INFO:SubProcess create_model() end ==================================
2026-01-30 09:15:11,713:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9966
2026-01-30 09:15:11,713:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9992
2026-01-30 09:15:11,713:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 09:15:11,713:INFO:choose_better completed
2026-01-30 09:15:11,718:INFO:_master_model_container: 10
2026-01-30 09:15:11,718:INFO:_display_container: 5
2026-01-30 09:15:11,718:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:15:11,718:INFO:tune_model() successfully completed......................................
2026-01-30 09:15:11,893:INFO:Initializing predict_model()
2026-01-30 09:15:11,907:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0D01FAF20>)
2026-01-30 09:15:11,907:INFO:Checking exceptions
2026-01-30 09:15:11,907:INFO:Preloading libraries
2026-01-30 09:15:11,907:INFO:Set up data.
2026-01-30 09:15:11,925:INFO:Set up index.
2026-01-30 09:15:12,810:INFO:Initializing predict_model()
2026-01-30 09:15:12,810:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A06E850F40>)
2026-01-30 09:15:12,810:INFO:Checking exceptions
2026-01-30 09:15:12,810:INFO:Preloading libraries
2026-01-30 09:15:12,811:INFO:Set up data.
2026-01-30 09:15:12,826:INFO:Set up index.
2026-01-30 09:15:13,291:INFO:Initializing predict_model()
2026-01-30 09:15:13,291:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A06E7DB100>)
2026-01-30 09:15:13,291:INFO:Checking exceptions
2026-01-30 09:15:13,291:INFO:Preloading libraries
2026-01-30 09:15:13,291:INFO:Set up data.
2026-01-30 09:15:13,327:INFO:Set up index.
2026-01-30 09:15:14,342:INFO:Initializing plot_model()
2026-01-30 09:15:14,342:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:15:14,342:INFO:Checking exceptions
2026-01-30 09:15:14,430:INFO:Preloading libraries
2026-01-30 09:15:14,463:INFO:Copying training dataset
2026-01-30 09:15:14,464:INFO:Plot type: feature
2026-01-30 09:15:14,464:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:15:14,709:INFO:Visual Rendered Successfully
2026-01-30 09:15:14,826:INFO:plot_model() successfully completed......................................
2026-01-30 09:15:14,847:INFO:Initializing plot_model()
2026-01-30 09:15:14,847:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C3A4050>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:15:14,847:INFO:Checking exceptions
2026-01-30 09:15:14,925:INFO:Preloading libraries
2026-01-30 09:15:14,964:INFO:Copying training dataset
2026-01-30 09:15:14,964:INFO:Plot type: feature_all
2026-01-30 09:15:15,089:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:15:15,392:INFO:Visual Rendered Successfully
2026-01-30 09:15:15,511:INFO:plot_model() successfully completed......................................
2026-01-30 09:15:15,524:INFO:Initializing save_model()
2026-01-30 09:15:15,524:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SI...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 09:15:15,524:INFO:Adding model into prep_pipe
2026-01-30 09:15:15,641:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 09:15:15,646:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges_...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 09:15:15,646:INFO:save_model() successfully completed......................................
2026-01-30 09:17:59,181:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\729519084.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 09:18:01,251:INFO:PyCaret ClassificationExperiment
2026-01-30 09:18:01,251:INFO:Logging name: clf-default-name
2026-01-30 09:18:01,252:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 09:18:01,252:INFO:version 3.3.2
2026-01-30 09:18:01,252:INFO:Initializing setup()
2026-01-30 09:18:01,252:INFO:self.USI: 66cd
2026-01-30 09:18:01,252:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 09:18:01,252:INFO:Checking environment
2026-01-30 09:18:01,253:INFO:python_version: 3.11.11
2026-01-30 09:18:01,254:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 09:18:01,254:INFO:machine: AMD64
2026-01-30 09:18:01,254:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 09:18:01,254:INFO:Memory: svmem(total=34009374720, available=12782919680, percent=62.4, used=21226455040, free=12782919680)
2026-01-30 09:18:01,254:INFO:Physical Core: 12
2026-01-30 09:18:01,254:INFO:Logical Core: 16
2026-01-30 09:18:01,254:INFO:Checking libraries
2026-01-30 09:18:01,254:INFO:System:
2026-01-30 09:18:01,254:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 09:18:01,254:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 09:18:01,254:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 09:18:01,254:INFO:PyCaret required dependencies:
2026-01-30 09:18:01,254:INFO:                 pip: 25.0
2026-01-30 09:18:01,254:INFO:          setuptools: 75.8.0
2026-01-30 09:18:01,255:INFO:             pycaret: 3.3.2
2026-01-30 09:18:01,255:INFO:             IPython: 9.9.0
2026-01-30 09:18:01,255:INFO:          ipywidgets: 8.1.8
2026-01-30 09:18:01,255:INFO:                tqdm: 4.67.1
2026-01-30 09:18:01,255:INFO:               numpy: 1.26.4
2026-01-30 09:18:01,255:INFO:              pandas: 2.1.4
2026-01-30 09:18:01,255:INFO:              jinja2: 3.1.6
2026-01-30 09:18:01,255:INFO:               scipy: 1.11.4
2026-01-30 09:18:01,255:INFO:              joblib: 1.3.2
2026-01-30 09:18:01,255:INFO:             sklearn: 1.4.2
2026-01-30 09:18:01,255:INFO:                pyod: 2.0.6
2026-01-30 09:18:01,255:INFO:            imblearn: 0.14.1
2026-01-30 09:18:01,255:INFO:   category_encoders: 2.7.0
2026-01-30 09:18:01,255:INFO:            lightgbm: 4.6.0
2026-01-30 09:18:01,255:INFO:               numba: 0.62.1
2026-01-30 09:18:01,255:INFO:            requests: 2.32.3
2026-01-30 09:18:01,255:INFO:          matplotlib: 3.7.5
2026-01-30 09:18:01,255:INFO:          scikitplot: 0.3.7
2026-01-30 09:18:01,255:INFO:         yellowbrick: 1.5
2026-01-30 09:18:01,255:INFO:              plotly: 5.24.1
2026-01-30 09:18:01,255:INFO:    plotly-resampler: Not installed
2026-01-30 09:18:01,255:INFO:             kaleido: 1.2.0
2026-01-30 09:18:01,255:INFO:           schemdraw: 0.15
2026-01-30 09:18:01,255:INFO:         statsmodels: 0.14.6
2026-01-30 09:18:01,255:INFO:              sktime: 0.26.0
2026-01-30 09:18:01,255:INFO:               tbats: 1.1.3
2026-01-30 09:18:01,255:INFO:            pmdarima: 2.0.4
2026-01-30 09:18:01,255:INFO:              psutil: 7.2.1
2026-01-30 09:18:01,255:INFO:          markupsafe: 3.0.3
2026-01-30 09:18:01,255:INFO:             pickle5: Not installed
2026-01-30 09:18:01,255:INFO:         cloudpickle: 3.0.0
2026-01-30 09:18:01,256:INFO:         deprecation: 2.1.0
2026-01-30 09:18:01,256:INFO:              xxhash: 3.6.0
2026-01-30 09:18:01,256:INFO:           wurlitzer: Not installed
2026-01-30 09:18:01,256:INFO:PyCaret optional dependencies:
2026-01-30 09:18:01,256:INFO:                shap: 0.44.1
2026-01-30 09:18:01,257:INFO:           interpret: 0.7.3
2026-01-30 09:18:01,257:INFO:                umap: 0.5.7
2026-01-30 09:18:01,257:INFO:     ydata_profiling: 4.18.1
2026-01-30 09:18:01,257:INFO:  explainerdashboard: 0.5.1
2026-01-30 09:18:01,257:INFO:             autoviz: Not installed
2026-01-30 09:18:01,257:INFO:           fairlearn: 0.7.0
2026-01-30 09:18:01,257:INFO:          deepchecks: Not installed
2026-01-30 09:18:01,257:INFO:             xgboost: Not installed
2026-01-30 09:18:01,257:INFO:            catboost: 1.2.8
2026-01-30 09:18:01,257:INFO:              kmodes: 0.12.2
2026-01-30 09:18:01,257:INFO:             mlxtend: 0.23.4
2026-01-30 09:18:01,257:INFO:       statsforecast: 1.5.0
2026-01-30 09:18:01,257:INFO:        tune_sklearn: Not installed
2026-01-30 09:18:01,257:INFO:                 ray: Not installed
2026-01-30 09:18:01,257:INFO:            hyperopt: 0.2.7
2026-01-30 09:18:01,257:INFO:              optuna: 4.6.0
2026-01-30 09:18:01,257:INFO:               skopt: 0.10.2
2026-01-30 09:18:01,257:INFO:              mlflow: 3.8.1
2026-01-30 09:18:01,258:INFO:              gradio: 6.3.0
2026-01-30 09:18:01,259:INFO:             fastapi: 0.128.0
2026-01-30 09:18:01,259:INFO:             uvicorn: 0.40.0
2026-01-30 09:18:01,259:INFO:              m2cgen: 0.10.0
2026-01-30 09:18:01,259:INFO:           evidently: 0.4.40
2026-01-30 09:18:01,259:INFO:               fugue: 0.8.7
2026-01-30 09:18:01,259:INFO:           streamlit: Not installed
2026-01-30 09:18:01,259:INFO:             prophet: Not installed
2026-01-30 09:18:01,259:INFO:None
2026-01-30 09:18:01,260:INFO:Set up data.
2026-01-30 09:18:01,371:INFO:Set up folding strategy.
2026-01-30 09:18:01,371:INFO:Set up train/test split.
2026-01-30 09:18:01,551:INFO:Set up index.
2026-01-30 09:18:01,561:INFO:Assigning column types.
2026-01-30 09:18:01,666:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 09:18:01,696:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:18:01,697:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:18:01,709:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:18:01,709:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:18:01,741:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:18:01,742:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:18:01,758:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:18:01,758:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:18:01,758:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 09:18:01,773:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:18:01,802:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:18:01,802:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:18:01,831:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:18:01,848:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:18:01,848:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:18:01,849:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 09:18:01,893:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:18:01,893:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:18:01,937:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:18:01,937:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:18:01,938:INFO:Preparing preprocessing pipeline...
2026-01-30 09:18:01,957:INFO:Set up simple imputation.
2026-01-30 09:18:01,957:INFO:Set up feature normalization.
2026-01-30 09:18:02,188:INFO:Finished creating preprocessing pipeline.
2026-01-30 09:18:02,190:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SI...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 09:18:02,190:INFO:Creating final display dataframe.
2026-01-30 09:18:02,586:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (429278, 20)
4        Transformed data shape      (429278, 20)
5   Transformed train set shape      (300494, 20)
6    Transformed test set shape      (128784, 20)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              66cd
2026-01-30 09:18:02,630:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:18:02,630:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:18:02,678:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:18:02,678:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:18:02,680:INFO:setup() successfully completed in 1.44s...............
2026-01-30 09:18:02,680:INFO:Initializing compare_models()
2026-01-30 09:18:02,680:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 09:18:02,680:INFO:Checking exceptions
2026-01-30 09:18:02,793:INFO:Preparing display monitor
2026-01-30 09:18:02,793:INFO:Initializing Logistic Regression
2026-01-30 09:18:02,793:INFO:Total runtime is 0.0 minutes
2026-01-30 09:18:02,793:INFO:SubProcess create_model() called ==================================
2026-01-30 09:18:02,793:INFO:Initializing create_model()
2026-01-30 09:18:02,793:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C71A0490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:18:02,793:INFO:Checking exceptions
2026-01-30 09:18:02,793:INFO:Importing libraries
2026-01-30 09:18:02,793:INFO:Copying training dataset
2026-01-30 09:18:03,006:INFO:Defining folds
2026-01-30 09:18:03,006:INFO:Declaring metric variables
2026-01-30 09:18:03,006:INFO:Importing untrained model
2026-01-30 09:18:03,006:INFO:Logistic Regression Imported successfully
2026-01-30 09:18:03,006:INFO:Starting cross validation
2026-01-30 09:18:03,006:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:18:05,182:INFO:Calculating mean and std
2026-01-30 09:18:05,183:INFO:Creating metrics dataframe
2026-01-30 09:18:05,185:INFO:Uploading results into container
2026-01-30 09:18:05,185:INFO:Uploading model into container now
2026-01-30 09:18:05,186:INFO:_master_model_container: 1
2026-01-30 09:18:05,186:INFO:_display_container: 2
2026-01-30 09:18:05,187:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 09:18:05,187:INFO:create_model() successfully completed......................................
2026-01-30 09:18:05,389:INFO:SubProcess create_model() end ==================================
2026-01-30 09:18:05,389:INFO:Creating metrics dataframe
2026-01-30 09:18:05,389:INFO:Initializing Decision Tree Classifier
2026-01-30 09:18:05,389:INFO:Total runtime is 0.043268076578776044 minutes
2026-01-30 09:18:05,389:INFO:SubProcess create_model() called ==================================
2026-01-30 09:18:05,389:INFO:Initializing create_model()
2026-01-30 09:18:05,389:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C71A0490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:18:05,389:INFO:Checking exceptions
2026-01-30 09:18:05,389:INFO:Importing libraries
2026-01-30 09:18:05,389:INFO:Copying training dataset
2026-01-30 09:18:05,590:INFO:Defining folds
2026-01-30 09:18:05,606:INFO:Declaring metric variables
2026-01-30 09:18:05,606:INFO:Importing untrained model
2026-01-30 09:18:05,606:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:18:05,606:INFO:Starting cross validation
2026-01-30 09:18:05,608:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:18:07,669:INFO:Calculating mean and std
2026-01-30 09:18:07,672:INFO:Creating metrics dataframe
2026-01-30 09:18:07,676:INFO:Uploading results into container
2026-01-30 09:18:07,676:INFO:Uploading model into container now
2026-01-30 09:18:07,676:INFO:_master_model_container: 2
2026-01-30 09:18:07,676:INFO:_display_container: 2
2026-01-30 09:18:07,676:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:18:07,676:INFO:create_model() successfully completed......................................
2026-01-30 09:18:07,810:INFO:SubProcess create_model() end ==================================
2026-01-30 09:18:07,810:INFO:Creating metrics dataframe
2026-01-30 09:18:07,812:INFO:Initializing Random Forest Classifier
2026-01-30 09:18:07,812:INFO:Total runtime is 0.08364590803782146 minutes
2026-01-30 09:18:07,813:INFO:SubProcess create_model() called ==================================
2026-01-30 09:18:07,813:INFO:Initializing create_model()
2026-01-30 09:18:07,813:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C71A0490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:18:07,813:INFO:Checking exceptions
2026-01-30 09:18:07,813:INFO:Importing libraries
2026-01-30 09:18:07,813:INFO:Copying training dataset
2026-01-30 09:18:07,960:INFO:Defining folds
2026-01-30 09:18:07,960:INFO:Declaring metric variables
2026-01-30 09:18:07,960:INFO:Importing untrained model
2026-01-30 09:18:07,960:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:18:07,960:INFO:Starting cross validation
2026-01-30 09:18:07,960:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:18:17,590:INFO:Calculating mean and std
2026-01-30 09:18:17,593:INFO:Creating metrics dataframe
2026-01-30 09:18:17,596:INFO:Uploading results into container
2026-01-30 09:18:17,597:INFO:Uploading model into container now
2026-01-30 09:18:17,597:INFO:_master_model_container: 3
2026-01-30 09:18:17,599:INFO:_display_container: 2
2026-01-30 09:18:17,599:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:18:17,600:INFO:create_model() successfully completed......................................
2026-01-30 09:18:17,755:INFO:SubProcess create_model() end ==================================
2026-01-30 09:18:17,756:INFO:Creating metrics dataframe
2026-01-30 09:18:17,758:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 09:18:17,758:INFO:Total runtime is 0.2494127074877421 minutes
2026-01-30 09:18:17,758:INFO:SubProcess create_model() called ==================================
2026-01-30 09:18:17,759:INFO:Initializing create_model()
2026-01-30 09:18:17,759:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C71A0490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:18:17,759:INFO:Checking exceptions
2026-01-30 09:18:17,759:INFO:Importing libraries
2026-01-30 09:18:17,759:INFO:Copying training dataset
2026-01-30 09:18:17,972:INFO:Defining folds
2026-01-30 09:18:17,972:INFO:Declaring metric variables
2026-01-30 09:18:17,972:INFO:Importing untrained model
2026-01-30 09:18:17,972:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:18:17,972:INFO:Starting cross validation
2026-01-30 09:18:17,972:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:18:20,815:INFO:Calculating mean and std
2026-01-30 09:18:20,815:INFO:Creating metrics dataframe
2026-01-30 09:18:20,815:INFO:Uploading results into container
2026-01-30 09:18:20,815:INFO:Uploading model into container now
2026-01-30 09:18:20,815:INFO:_master_model_container: 4
2026-01-30 09:18:20,815:INFO:_display_container: 2
2026-01-30 09:18:20,821:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:18:20,821:INFO:create_model() successfully completed......................................
2026-01-30 09:18:20,973:INFO:SubProcess create_model() end ==================================
2026-01-30 09:18:20,973:INFO:Creating metrics dataframe
2026-01-30 09:18:20,988:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 09:18:20,991:INFO:Initializing create_model()
2026-01-30 09:18:20,991:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:18:20,991:INFO:Checking exceptions
2026-01-30 09:18:20,992:INFO:Importing libraries
2026-01-30 09:18:20,992:INFO:Copying training dataset
2026-01-30 09:18:21,266:INFO:Defining folds
2026-01-30 09:18:21,267:INFO:Declaring metric variables
2026-01-30 09:18:21,267:INFO:Importing untrained model
2026-01-30 09:18:21,267:INFO:Declaring custom model
2026-01-30 09:18:21,268:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:18:21,268:INFO:Cross validation set to False
2026-01-30 09:18:21,268:INFO:Fitting Model
2026-01-30 09:18:25,677:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:18:25,677:INFO:create_model() successfully completed......................................
2026-01-30 09:18:25,809:INFO:Initializing create_model()
2026-01-30 09:18:25,809:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:18:25,809:INFO:Checking exceptions
2026-01-30 09:18:25,809:INFO:Importing libraries
2026-01-30 09:18:25,809:INFO:Copying training dataset
2026-01-30 09:18:25,956:INFO:Defining folds
2026-01-30 09:18:25,956:INFO:Declaring metric variables
2026-01-30 09:18:25,956:INFO:Importing untrained model
2026-01-30 09:18:25,956:INFO:Declaring custom model
2026-01-30 09:18:25,956:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:18:25,956:INFO:Cross validation set to False
2026-01-30 09:18:25,956:INFO:Fitting Model
2026-01-30 09:18:27,181:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:18:27,181:INFO:create_model() successfully completed......................................
2026-01-30 09:18:27,327:INFO:Initializing create_model()
2026-01-30 09:18:27,328:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:18:27,328:INFO:Checking exceptions
2026-01-30 09:18:27,328:INFO:Importing libraries
2026-01-30 09:18:27,328:INFO:Copying training dataset
2026-01-30 09:18:27,503:INFO:Defining folds
2026-01-30 09:18:27,504:INFO:Declaring metric variables
2026-01-30 09:18:27,504:INFO:Importing untrained model
2026-01-30 09:18:27,504:INFO:Declaring custom model
2026-01-30 09:18:27,506:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:18:27,506:INFO:Cross validation set to False
2026-01-30 09:18:27,506:INFO:Fitting Model
2026-01-30 09:18:28,026:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:18:28,059:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005837 seconds.
2026-01-30 09:18:28,059:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:18:28,059:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:18:28,059:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:18:28,060:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:18:28,062:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:18:28,062:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:18:28,556:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:18:28,557:INFO:create_model() successfully completed......................................
2026-01-30 09:18:28,731:INFO:_master_model_container: 4
2026-01-30 09:18:28,732:INFO:_display_container: 2
2026-01-30 09:18:28,733:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2026-01-30 09:18:28,733:INFO:compare_models() successfully completed......................................
2026-01-30 09:18:28,739:INFO:Initializing tune_model()
2026-01-30 09:18:28,739:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:18:28,739:INFO:Checking exceptions
2026-01-30 09:18:28,808:INFO:Copying training dataset
2026-01-30 09:18:28,927:INFO:Checking base model
2026-01-30 09:18:28,927:INFO:Base model : Random Forest Classifier
2026-01-30 09:18:28,928:INFO:Declaring metric variables
2026-01-30 09:18:28,928:INFO:Defining Hyperparameters
2026-01-30 09:18:29,052:INFO:Tuning with n_jobs=-1
2026-01-30 09:18:29,053:INFO:Initializing RandomizedSearchCV
2026-01-30 09:19:42,892:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 09:19:42,893:INFO:Hyperparameter search completed
2026-01-30 09:19:42,893:INFO:SubProcess create_model() called ==================================
2026-01-30 09:19:42,893:INFO:Initializing create_model()
2026-01-30 09:19:42,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03CD1EED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 09:19:42,894:INFO:Checking exceptions
2026-01-30 09:19:42,894:INFO:Importing libraries
2026-01-30 09:19:42,894:INFO:Copying training dataset
2026-01-30 09:19:43,121:INFO:Defining folds
2026-01-30 09:19:43,121:INFO:Declaring metric variables
2026-01-30 09:19:43,121:INFO:Importing untrained model
2026-01-30 09:19:43,121:INFO:Declaring custom model
2026-01-30 09:19:43,121:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:19:43,121:INFO:Starting cross validation
2026-01-30 09:19:43,121:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:19:58,929:INFO:Calculating mean and std
2026-01-30 09:19:58,929:INFO:Creating metrics dataframe
2026-01-30 09:19:58,929:INFO:Finalizing model
2026-01-30 09:20:06,979:INFO:Uploading results into container
2026-01-30 09:20:06,986:INFO:Uploading model into container now
2026-01-30 09:20:06,986:INFO:_master_model_container: 5
2026-01-30 09:20:06,987:INFO:_display_container: 3
2026-01-30 09:20:06,987:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:20:06,988:INFO:create_model() successfully completed......................................
2026-01-30 09:20:07,152:INFO:SubProcess create_model() end ==================================
2026-01-30 09:20:07,152:INFO:choose_better activated
2026-01-30 09:20:07,153:INFO:SubProcess create_model() called ==================================
2026-01-30 09:20:07,154:INFO:Initializing create_model()
2026-01-30 09:20:07,154:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:20:07,154:INFO:Checking exceptions
2026-01-30 09:20:07,155:INFO:Importing libraries
2026-01-30 09:20:07,155:INFO:Copying training dataset
2026-01-30 09:20:07,358:INFO:Defining folds
2026-01-30 09:20:07,358:INFO:Declaring metric variables
2026-01-30 09:20:07,358:INFO:Importing untrained model
2026-01-30 09:20:07,358:INFO:Declaring custom model
2026-01-30 09:20:07,359:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:20:07,359:INFO:Starting cross validation
2026-01-30 09:20:07,360:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:20:17,307:INFO:Calculating mean and std
2026-01-30 09:20:17,308:INFO:Creating metrics dataframe
2026-01-30 09:20:17,310:INFO:Finalizing model
2026-01-30 09:20:22,027:INFO:Uploading results into container
2026-01-30 09:20:22,028:INFO:Uploading model into container now
2026-01-30 09:20:22,029:INFO:_master_model_container: 6
2026-01-30 09:20:22,029:INFO:_display_container: 4
2026-01-30 09:20:22,030:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:20:22,030:INFO:create_model() successfully completed......................................
2026-01-30 09:20:22,157:INFO:SubProcess create_model() end ==================================
2026-01-30 09:20:22,158:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9993
2026-01-30 09:20:22,158:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9934
2026-01-30 09:20:22,158:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 09:20:22,158:INFO:choose_better completed
2026-01-30 09:20:22,159:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:20:22,161:INFO:_master_model_container: 6
2026-01-30 09:20:22,161:INFO:_display_container: 3
2026-01-30 09:20:22,161:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:20:22,161:INFO:tune_model() successfully completed......................................
2026-01-30 09:20:22,282:INFO:Initializing tune_model()
2026-01-30 09:20:22,282:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:20:22,282:INFO:Checking exceptions
2026-01-30 09:20:22,338:INFO:Copying training dataset
2026-01-30 09:20:22,454:INFO:Checking base model
2026-01-30 09:20:22,454:INFO:Base model : Decision Tree Classifier
2026-01-30 09:20:22,454:INFO:Declaring metric variables
2026-01-30 09:20:22,454:INFO:Defining Hyperparameters
2026-01-30 09:20:22,585:INFO:Tuning with n_jobs=-1
2026-01-30 09:20:22,586:INFO:Initializing RandomizedSearchCV
2026-01-30 09:20:26,459:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 09:20:26,460:INFO:Hyperparameter search completed
2026-01-30 09:20:26,461:INFO:SubProcess create_model() called ==================================
2026-01-30 09:20:26,461:INFO:Initializing create_model()
2026-01-30 09:20:26,462:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06E849510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 09:20:26,462:INFO:Checking exceptions
2026-01-30 09:20:26,462:INFO:Importing libraries
2026-01-30 09:20:26,462:INFO:Copying training dataset
2026-01-30 09:20:26,742:INFO:Defining folds
2026-01-30 09:20:26,742:INFO:Declaring metric variables
2026-01-30 09:20:26,742:INFO:Importing untrained model
2026-01-30 09:20:26,742:INFO:Declaring custom model
2026-01-30 09:20:26,743:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:20:26,743:INFO:Starting cross validation
2026-01-30 09:20:26,744:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:20:28,331:INFO:Calculating mean and std
2026-01-30 09:20:28,333:INFO:Creating metrics dataframe
2026-01-30 09:20:28,334:INFO:Finalizing model
2026-01-30 09:20:29,310:INFO:Uploading results into container
2026-01-30 09:20:29,311:INFO:Uploading model into container now
2026-01-30 09:20:29,312:INFO:_master_model_container: 7
2026-01-30 09:20:29,312:INFO:_display_container: 4
2026-01-30 09:20:29,313:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:20:29,313:INFO:create_model() successfully completed......................................
2026-01-30 09:20:29,438:INFO:SubProcess create_model() end ==================================
2026-01-30 09:20:29,438:INFO:choose_better activated
2026-01-30 09:20:29,438:INFO:SubProcess create_model() called ==================================
2026-01-30 09:20:29,438:INFO:Initializing create_model()
2026-01-30 09:20:29,438:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:20:29,438:INFO:Checking exceptions
2026-01-30 09:20:29,438:INFO:Importing libraries
2026-01-30 09:20:29,438:INFO:Copying training dataset
2026-01-30 09:20:29,606:INFO:Defining folds
2026-01-30 09:20:29,606:INFO:Declaring metric variables
2026-01-30 09:20:29,606:INFO:Importing untrained model
2026-01-30 09:20:29,606:INFO:Declaring custom model
2026-01-30 09:20:29,606:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:20:29,606:INFO:Starting cross validation
2026-01-30 09:20:29,606:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:20:31,473:INFO:Calculating mean and std
2026-01-30 09:20:31,474:INFO:Creating metrics dataframe
2026-01-30 09:20:31,475:INFO:Finalizing model
2026-01-30 09:20:33,160:INFO:Uploading results into container
2026-01-30 09:20:33,160:INFO:Uploading model into container now
2026-01-30 09:20:33,160:INFO:_master_model_container: 8
2026-01-30 09:20:33,160:INFO:_display_container: 5
2026-01-30 09:20:33,160:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:20:33,160:INFO:create_model() successfully completed......................................
2026-01-30 09:20:33,298:INFO:SubProcess create_model() end ==================================
2026-01-30 09:20:33,299:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9988
2026-01-30 09:20:33,299:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9876
2026-01-30 09:20:33,299:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 09:20:33,299:INFO:choose_better completed
2026-01-30 09:20:33,299:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:20:33,301:INFO:_master_model_container: 8
2026-01-30 09:20:33,301:INFO:_display_container: 4
2026-01-30 09:20:33,302:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:20:33,302:INFO:tune_model() successfully completed......................................
2026-01-30 09:20:33,434:INFO:Initializing tune_model()
2026-01-30 09:20:33,434:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:20:33,434:INFO:Checking exceptions
2026-01-30 09:20:33,504:INFO:Copying training dataset
2026-01-30 09:20:33,646:INFO:Checking base model
2026-01-30 09:20:33,646:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 09:20:33,648:INFO:Declaring metric variables
2026-01-30 09:20:33,648:INFO:Defining Hyperparameters
2026-01-30 09:20:33,824:INFO:Tuning with n_jobs=-1
2026-01-30 09:20:33,824:INFO:Initializing RandomizedSearchCV
2026-01-30 09:21:00,479:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 09:21:00,480:INFO:Hyperparameter search completed
2026-01-30 09:21:00,481:INFO:SubProcess create_model() called ==================================
2026-01-30 09:21:00,482:INFO:Initializing create_model()
2026-01-30 09:21:00,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06E702410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 09:21:00,482:INFO:Checking exceptions
2026-01-30 09:21:00,482:INFO:Importing libraries
2026-01-30 09:21:00,482:INFO:Copying training dataset
2026-01-30 09:21:00,663:INFO:Defining folds
2026-01-30 09:21:00,663:INFO:Declaring metric variables
2026-01-30 09:21:00,664:INFO:Importing untrained model
2026-01-30 09:21:00,664:INFO:Declaring custom model
2026-01-30 09:21:00,665:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:21:00,665:INFO:Starting cross validation
2026-01-30 09:21:00,666:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:21:08,046:INFO:Calculating mean and std
2026-01-30 09:21:08,046:INFO:Creating metrics dataframe
2026-01-30 09:21:08,051:INFO:Finalizing model
2026-01-30 09:21:08,477:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:21:08,477:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:21:08,477:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:21:08,663:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:21:08,663:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:21:08,663:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:21:08,664:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:21:08,706:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010122 seconds.
2026-01-30 09:21:08,706:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:21:08,706:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:21:08,707:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:21:08,707:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:21:08,711:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:21:08,711:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:21:10,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:21:10,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:21:10,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:21:10,904:INFO:Uploading results into container
2026-01-30 09:21:10,906:INFO:Uploading model into container now
2026-01-30 09:21:10,907:INFO:_master_model_container: 9
2026-01-30 09:21:10,907:INFO:_display_container: 5
2026-01-30 09:21:10,908:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:21:10,908:INFO:create_model() successfully completed......................................
2026-01-30 09:21:11,083:INFO:SubProcess create_model() end ==================================
2026-01-30 09:21:11,083:INFO:choose_better activated
2026-01-30 09:21:11,084:INFO:SubProcess create_model() called ==================================
2026-01-30 09:21:11,085:INFO:Initializing create_model()
2026-01-30 09:21:11,085:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:21:11,085:INFO:Checking exceptions
2026-01-30 09:21:11,087:INFO:Importing libraries
2026-01-30 09:21:11,087:INFO:Copying training dataset
2026-01-30 09:21:11,256:INFO:Defining folds
2026-01-30 09:21:11,256:INFO:Declaring metric variables
2026-01-30 09:21:11,256:INFO:Importing untrained model
2026-01-30 09:21:11,256:INFO:Declaring custom model
2026-01-30 09:21:11,256:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:21:11,256:INFO:Starting cross validation
2026-01-30 09:21:11,256:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:21:14,773:INFO:Calculating mean and std
2026-01-30 09:21:14,773:INFO:Creating metrics dataframe
2026-01-30 09:21:14,773:INFO:Finalizing model
2026-01-30 09:21:15,382:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:21:15,426:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008562 seconds.
2026-01-30 09:21:15,426:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:21:15,426:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:21:15,427:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:21:15,427:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:21:15,430:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:21:15,430:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:21:16,072:INFO:Uploading results into container
2026-01-30 09:21:16,072:INFO:Uploading model into container now
2026-01-30 09:21:16,074:INFO:_master_model_container: 10
2026-01-30 09:21:16,074:INFO:_display_container: 6
2026-01-30 09:21:16,074:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:21:16,074:INFO:create_model() successfully completed......................................
2026-01-30 09:21:16,235:INFO:SubProcess create_model() end ==================================
2026-01-30 09:21:16,236:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9966
2026-01-30 09:21:16,237:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9992
2026-01-30 09:21:16,238:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 09:21:16,238:INFO:choose_better completed
2026-01-30 09:21:16,240:INFO:_master_model_container: 10
2026-01-30 09:21:16,240:INFO:_display_container: 5
2026-01-30 09:21:16,240:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:21:16,240:INFO:tune_model() successfully completed......................................
2026-01-30 09:21:16,381:INFO:Initializing predict_model()
2026-01-30 09:21:16,381:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C3C9E700>)
2026-01-30 09:21:16,381:INFO:Checking exceptions
2026-01-30 09:21:16,381:INFO:Preloading libraries
2026-01-30 09:21:16,381:INFO:Set up data.
2026-01-30 09:21:16,412:INFO:Set up index.
2026-01-30 09:21:17,339:INFO:Initializing predict_model()
2026-01-30 09:21:17,339:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A036A98220>)
2026-01-30 09:21:17,339:INFO:Checking exceptions
2026-01-30 09:21:17,339:INFO:Preloading libraries
2026-01-30 09:21:17,339:INFO:Set up data.
2026-01-30 09:21:17,383:INFO:Set up index.
2026-01-30 09:21:17,944:INFO:Initializing predict_model()
2026-01-30 09:21:17,945:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A036A98220>)
2026-01-30 09:21:17,945:INFO:Checking exceptions
2026-01-30 09:21:17,945:INFO:Preloading libraries
2026-01-30 09:21:17,945:INFO:Set up data.
2026-01-30 09:21:17,979:INFO:Set up index.
2026-01-30 09:21:19,008:INFO:Initializing plot_model()
2026-01-30 09:21:19,008:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:21:19,008:INFO:Checking exceptions
2026-01-30 09:21:19,103:INFO:Preloading libraries
2026-01-30 09:21:19,156:INFO:Copying training dataset
2026-01-30 09:21:19,156:INFO:Plot type: feature
2026-01-30 09:21:19,157:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:21:19,459:INFO:Visual Rendered Successfully
2026-01-30 09:21:19,591:INFO:plot_model() successfully completed......................................
2026-01-30 09:21:19,593:INFO:Initializing plot_model()
2026-01-30 09:21:19,593:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0D02C2990>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:21:19,593:INFO:Checking exceptions
2026-01-30 09:21:19,693:INFO:Preloading libraries
2026-01-30 09:21:19,703:INFO:Copying training dataset
2026-01-30 09:21:19,719:INFO:Plot type: feature_all
2026-01-30 09:21:19,859:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:21:20,203:INFO:Visual Rendered Successfully
2026-01-30 09:21:20,342:INFO:plot_model() successfully completed......................................
2026-01-30 09:21:20,359:INFO:Initializing save_model()
2026-01-30 09:21:20,359:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SI...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 09:21:20,359:INFO:Adding model into prep_pipe
2026-01-30 09:21:20,438:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 09:21:20,443:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges_...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 09:21:20,443:INFO:save_model() successfully completed......................................
2026-01-30 09:28:58,546:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\472289403.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 09:29:00,567:INFO:PyCaret ClassificationExperiment
2026-01-30 09:29:00,567:INFO:Logging name: clf-default-name
2026-01-30 09:29:00,567:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 09:29:00,567:INFO:version 3.3.2
2026-01-30 09:29:00,567:INFO:Initializing setup()
2026-01-30 09:29:00,567:INFO:self.USI: 8d13
2026-01-30 09:29:00,567:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 09:29:00,567:INFO:Checking environment
2026-01-30 09:29:00,567:INFO:python_version: 3.11.11
2026-01-30 09:29:00,567:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 09:29:00,567:INFO:machine: AMD64
2026-01-30 09:29:00,567:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 09:29:00,567:INFO:Memory: svmem(total=34009374720, available=16086147072, percent=52.7, used=17923227648, free=16086147072)
2026-01-30 09:29:00,567:INFO:Physical Core: 12
2026-01-30 09:29:00,567:INFO:Logical Core: 16
2026-01-30 09:29:00,567:INFO:Checking libraries
2026-01-30 09:29:00,567:INFO:System:
2026-01-30 09:29:00,567:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 09:29:00,567:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 09:29:00,567:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 09:29:00,567:INFO:PyCaret required dependencies:
2026-01-30 09:29:00,567:INFO:                 pip: 25.0
2026-01-30 09:29:00,567:INFO:          setuptools: 75.8.0
2026-01-30 09:29:00,567:INFO:             pycaret: 3.3.2
2026-01-30 09:29:00,567:INFO:             IPython: 9.9.0
2026-01-30 09:29:00,567:INFO:          ipywidgets: 8.1.8
2026-01-30 09:29:00,567:INFO:                tqdm: 4.67.1
2026-01-30 09:29:00,567:INFO:               numpy: 1.26.4
2026-01-30 09:29:00,567:INFO:              pandas: 2.1.4
2026-01-30 09:29:00,567:INFO:              jinja2: 3.1.6
2026-01-30 09:29:00,567:INFO:               scipy: 1.11.4
2026-01-30 09:29:00,567:INFO:              joblib: 1.3.2
2026-01-30 09:29:00,567:INFO:             sklearn: 1.4.2
2026-01-30 09:29:00,567:INFO:                pyod: 2.0.6
2026-01-30 09:29:00,567:INFO:            imblearn: 0.14.1
2026-01-30 09:29:00,567:INFO:   category_encoders: 2.7.0
2026-01-30 09:29:00,567:INFO:            lightgbm: 4.6.0
2026-01-30 09:29:00,567:INFO:               numba: 0.62.1
2026-01-30 09:29:00,567:INFO:            requests: 2.32.3
2026-01-30 09:29:00,567:INFO:          matplotlib: 3.7.5
2026-01-30 09:29:00,567:INFO:          scikitplot: 0.3.7
2026-01-30 09:29:00,567:INFO:         yellowbrick: 1.5
2026-01-30 09:29:00,567:INFO:              plotly: 5.24.1
2026-01-30 09:29:00,567:INFO:    plotly-resampler: Not installed
2026-01-30 09:29:00,567:INFO:             kaleido: 1.2.0
2026-01-30 09:29:00,567:INFO:           schemdraw: 0.15
2026-01-30 09:29:00,567:INFO:         statsmodels: 0.14.6
2026-01-30 09:29:00,567:INFO:              sktime: 0.26.0
2026-01-30 09:29:00,567:INFO:               tbats: 1.1.3
2026-01-30 09:29:00,567:INFO:            pmdarima: 2.0.4
2026-01-30 09:29:00,567:INFO:              psutil: 7.2.1
2026-01-30 09:29:00,567:INFO:          markupsafe: 3.0.3
2026-01-30 09:29:00,567:INFO:             pickle5: Not installed
2026-01-30 09:29:00,567:INFO:         cloudpickle: 3.0.0
2026-01-30 09:29:00,567:INFO:         deprecation: 2.1.0
2026-01-30 09:29:00,567:INFO:              xxhash: 3.6.0
2026-01-30 09:29:00,567:INFO:           wurlitzer: Not installed
2026-01-30 09:29:00,567:INFO:PyCaret optional dependencies:
2026-01-30 09:29:00,567:INFO:                shap: 0.44.1
2026-01-30 09:29:00,567:INFO:           interpret: 0.7.3
2026-01-30 09:29:00,567:INFO:                umap: 0.5.7
2026-01-30 09:29:00,567:INFO:     ydata_profiling: 4.18.1
2026-01-30 09:29:00,567:INFO:  explainerdashboard: 0.5.1
2026-01-30 09:29:00,567:INFO:             autoviz: Not installed
2026-01-30 09:29:00,567:INFO:           fairlearn: 0.7.0
2026-01-30 09:29:00,567:INFO:          deepchecks: Not installed
2026-01-30 09:29:00,567:INFO:             xgboost: Not installed
2026-01-30 09:29:00,567:INFO:            catboost: 1.2.8
2026-01-30 09:29:00,567:INFO:              kmodes: 0.12.2
2026-01-30 09:29:00,567:INFO:             mlxtend: 0.23.4
2026-01-30 09:29:00,567:INFO:       statsforecast: 1.5.0
2026-01-30 09:29:00,567:INFO:        tune_sklearn: Not installed
2026-01-30 09:29:00,567:INFO:                 ray: Not installed
2026-01-30 09:29:00,567:INFO:            hyperopt: 0.2.7
2026-01-30 09:29:00,567:INFO:              optuna: 4.6.0
2026-01-30 09:29:00,567:INFO:               skopt: 0.10.2
2026-01-30 09:29:00,567:INFO:              mlflow: 3.8.1
2026-01-30 09:29:00,567:INFO:              gradio: 6.3.0
2026-01-30 09:29:00,567:INFO:             fastapi: 0.128.0
2026-01-30 09:29:00,567:INFO:             uvicorn: 0.40.0
2026-01-30 09:29:00,567:INFO:              m2cgen: 0.10.0
2026-01-30 09:29:00,567:INFO:           evidently: 0.4.40
2026-01-30 09:29:00,567:INFO:               fugue: 0.8.7
2026-01-30 09:29:00,567:INFO:           streamlit: Not installed
2026-01-30 09:29:00,567:INFO:             prophet: Not installed
2026-01-30 09:29:00,567:INFO:None
2026-01-30 09:29:00,567:INFO:Set up data.
2026-01-30 09:29:00,667:INFO:Set up folding strategy.
2026-01-30 09:29:00,667:INFO:Set up train/test split.
2026-01-30 09:29:00,835:INFO:Set up index.
2026-01-30 09:29:00,847:INFO:Assigning column types.
2026-01-30 09:29:00,935:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 09:29:00,963:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:29:00,963:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:29:00,983:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:29:00,983:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:29:01,012:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:29:01,013:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:29:01,029:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:29:01,030:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:29:01,030:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 09:29:01,046:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:29:01,063:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:29:01,063:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:29:01,098:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:29:01,113:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:29:01,113:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:29:01,113:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 09:29:01,147:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:29:01,147:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:29:01,195:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:29:01,196:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:29:01,197:INFO:Preparing preprocessing pipeline...
2026-01-30 09:29:01,213:INFO:Set up simple imputation.
2026-01-30 09:29:01,213:INFO:Set up feature normalization.
2026-01-30 09:29:01,436:INFO:Finished creating preprocessing pipeline.
2026-01-30 09:29:01,436:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SI...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 09:29:01,436:INFO:Creating final display dataframe.
2026-01-30 09:29:01,813:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (429278, 20)
4        Transformed data shape      (429278, 20)
5   Transformed train set shape      (300494, 20)
6    Transformed test set shape      (128784, 20)
7              Numeric features                15
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              8d13
2026-01-30 09:29:01,863:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:29:01,863:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:29:01,897:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:29:01,897:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:29:01,897:INFO:setup() successfully completed in 1.35s...............
2026-01-30 09:29:01,897:INFO:Initializing compare_models()
2026-01-30 09:29:01,897:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 09:29:01,897:INFO:Checking exceptions
2026-01-30 09:29:01,980:INFO:Preparing display monitor
2026-01-30 09:29:01,980:INFO:Initializing Logistic Regression
2026-01-30 09:29:01,980:INFO:Total runtime is 0.0 minutes
2026-01-30 09:29:01,980:INFO:SubProcess create_model() called ==================================
2026-01-30 09:29:01,980:INFO:Initializing create_model()
2026-01-30 09:29:01,980:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C347350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:29:01,980:INFO:Checking exceptions
2026-01-30 09:29:01,980:INFO:Importing libraries
2026-01-30 09:29:01,980:INFO:Copying training dataset
2026-01-30 09:29:02,133:INFO:Defining folds
2026-01-30 09:29:02,134:INFO:Declaring metric variables
2026-01-30 09:29:02,134:INFO:Importing untrained model
2026-01-30 09:29:02,135:INFO:Logistic Regression Imported successfully
2026-01-30 09:29:02,135:INFO:Starting cross validation
2026-01-30 09:29:02,135:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:29:14,299:INFO:Calculating mean and std
2026-01-30 09:29:14,303:INFO:Creating metrics dataframe
2026-01-30 09:29:14,307:INFO:Uploading results into container
2026-01-30 09:29:14,310:INFO:Uploading model into container now
2026-01-30 09:29:14,310:INFO:_master_model_container: 1
2026-01-30 09:29:14,310:INFO:_display_container: 2
2026-01-30 09:29:14,310:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 09:29:14,313:INFO:create_model() successfully completed......................................
2026-01-30 09:29:14,481:INFO:SubProcess create_model() end ==================================
2026-01-30 09:29:14,482:INFO:Creating metrics dataframe
2026-01-30 09:29:14,485:INFO:Initializing Decision Tree Classifier
2026-01-30 09:29:14,485:INFO:Total runtime is 0.20842784643173218 minutes
2026-01-30 09:29:14,485:INFO:SubProcess create_model() called ==================================
2026-01-30 09:29:14,485:INFO:Initializing create_model()
2026-01-30 09:29:14,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C347350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:29:14,485:INFO:Checking exceptions
2026-01-30 09:29:14,485:INFO:Importing libraries
2026-01-30 09:29:14,485:INFO:Copying training dataset
2026-01-30 09:29:14,682:INFO:Defining folds
2026-01-30 09:29:14,682:INFO:Declaring metric variables
2026-01-30 09:29:14,682:INFO:Importing untrained model
2026-01-30 09:29:14,682:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:29:14,682:INFO:Starting cross validation
2026-01-30 09:29:14,682:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:29:20,812:INFO:Calculating mean and std
2026-01-30 09:29:20,813:INFO:Creating metrics dataframe
2026-01-30 09:29:20,815:INFO:Uploading results into container
2026-01-30 09:29:20,815:INFO:Uploading model into container now
2026-01-30 09:29:20,816:INFO:_master_model_container: 2
2026-01-30 09:29:20,816:INFO:_display_container: 2
2026-01-30 09:29:20,817:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:29:20,817:INFO:create_model() successfully completed......................................
2026-01-30 09:29:20,970:INFO:SubProcess create_model() end ==================================
2026-01-30 09:29:20,970:INFO:Creating metrics dataframe
2026-01-30 09:29:20,970:INFO:Initializing Random Forest Classifier
2026-01-30 09:29:20,970:INFO:Total runtime is 0.3165002743403117 minutes
2026-01-30 09:29:20,970:INFO:SubProcess create_model() called ==================================
2026-01-30 09:29:20,970:INFO:Initializing create_model()
2026-01-30 09:29:20,970:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C347350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:29:20,970:INFO:Checking exceptions
2026-01-30 09:29:20,970:INFO:Importing libraries
2026-01-30 09:29:20,970:INFO:Copying training dataset
2026-01-30 09:29:21,143:INFO:Defining folds
2026-01-30 09:29:21,145:INFO:Declaring metric variables
2026-01-30 09:29:21,147:INFO:Importing untrained model
2026-01-30 09:29:21,147:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:29:21,148:INFO:Starting cross validation
2026-01-30 09:29:21,149:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:29:34,164:INFO:Calculating mean and std
2026-01-30 09:29:34,166:INFO:Creating metrics dataframe
2026-01-30 09:29:34,167:INFO:Uploading results into container
2026-01-30 09:29:34,167:INFO:Uploading model into container now
2026-01-30 09:29:34,167:INFO:_master_model_container: 3
2026-01-30 09:29:34,167:INFO:_display_container: 2
2026-01-30 09:29:34,167:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:29:34,167:INFO:create_model() successfully completed......................................
2026-01-30 09:29:34,305:INFO:SubProcess create_model() end ==================================
2026-01-30 09:29:34,305:INFO:Creating metrics dataframe
2026-01-30 09:29:34,306:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 09:29:34,306:INFO:Total runtime is 0.5387771089871725 minutes
2026-01-30 09:29:34,307:INFO:SubProcess create_model() called ==================================
2026-01-30 09:29:34,307:INFO:Initializing create_model()
2026-01-30 09:29:34,307:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C347350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:29:34,307:INFO:Checking exceptions
2026-01-30 09:29:34,307:INFO:Importing libraries
2026-01-30 09:29:34,307:INFO:Copying training dataset
2026-01-30 09:29:34,446:INFO:Defining folds
2026-01-30 09:29:34,446:INFO:Declaring metric variables
2026-01-30 09:29:34,446:INFO:Importing untrained model
2026-01-30 09:29:34,446:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:29:34,446:INFO:Starting cross validation
2026-01-30 09:29:34,446:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:29:41,909:INFO:Calculating mean and std
2026-01-30 09:29:41,915:INFO:Creating metrics dataframe
2026-01-30 09:29:41,917:INFO:Uploading results into container
2026-01-30 09:29:41,917:INFO:Uploading model into container now
2026-01-30 09:29:41,919:INFO:_master_model_container: 4
2026-01-30 09:29:41,919:INFO:_display_container: 2
2026-01-30 09:29:41,919:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:29:41,919:INFO:create_model() successfully completed......................................
2026-01-30 09:29:42,029:INFO:SubProcess create_model() end ==================================
2026-01-30 09:29:42,029:INFO:Creating metrics dataframe
2026-01-30 09:29:42,044:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 09:29:42,046:INFO:Initializing create_model()
2026-01-30 09:29:42,046:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:29:42,046:INFO:Checking exceptions
2026-01-30 09:29:42,046:INFO:Importing libraries
2026-01-30 09:29:42,046:INFO:Copying training dataset
2026-01-30 09:29:42,188:INFO:Defining folds
2026-01-30 09:29:42,188:INFO:Declaring metric variables
2026-01-30 09:29:42,188:INFO:Importing untrained model
2026-01-30 09:29:42,188:INFO:Declaring custom model
2026-01-30 09:29:42,189:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:29:42,189:INFO:Cross validation set to False
2026-01-30 09:29:42,189:INFO:Fitting Model
2026-01-30 09:29:47,099:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:29:47,099:INFO:create_model() successfully completed......................................
2026-01-30 09:29:47,234:INFO:Initializing create_model()
2026-01-30 09:29:47,234:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:29:47,235:INFO:Checking exceptions
2026-01-30 09:29:47,235:INFO:Importing libraries
2026-01-30 09:29:47,235:INFO:Copying training dataset
2026-01-30 09:29:47,374:INFO:Defining folds
2026-01-30 09:29:47,375:INFO:Declaring metric variables
2026-01-30 09:29:47,375:INFO:Importing untrained model
2026-01-30 09:29:47,375:INFO:Declaring custom model
2026-01-30 09:29:47,375:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:29:47,376:INFO:Cross validation set to False
2026-01-30 09:29:47,376:INFO:Fitting Model
2026-01-30 09:29:48,542:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:29:48,542:INFO:create_model() successfully completed......................................
2026-01-30 09:29:48,669:INFO:Initializing create_model()
2026-01-30 09:29:48,669:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:29:48,669:INFO:Checking exceptions
2026-01-30 09:29:48,670:INFO:Importing libraries
2026-01-30 09:29:48,670:INFO:Copying training dataset
2026-01-30 09:29:48,814:INFO:Defining folds
2026-01-30 09:29:48,814:INFO:Declaring metric variables
2026-01-30 09:29:48,814:INFO:Importing untrained model
2026-01-30 09:29:48,815:INFO:Declaring custom model
2026-01-30 09:29:48,815:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:29:48,816:INFO:Cross validation set to False
2026-01-30 09:29:48,816:INFO:Fitting Model
2026-01-30 09:29:49,304:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:29:49,346:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007317 seconds.
2026-01-30 09:29:49,347:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:29:49,347:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:29:49,347:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:29:49,348:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:29:49,350:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:29:49,350:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:29:49,879:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:29:49,879:INFO:create_model() successfully completed......................................
2026-01-30 09:29:50,064:INFO:_master_model_container: 4
2026-01-30 09:29:50,064:INFO:_display_container: 2
2026-01-30 09:29:50,066:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2026-01-30 09:29:50,066:INFO:compare_models() successfully completed......................................
2026-01-30 09:29:50,072:INFO:Initializing tune_model()
2026-01-30 09:29:50,072:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:29:50,073:INFO:Checking exceptions
2026-01-30 09:29:50,136:INFO:Copying training dataset
2026-01-30 09:29:50,228:INFO:Checking base model
2026-01-30 09:29:50,229:INFO:Base model : Random Forest Classifier
2026-01-30 09:29:50,229:INFO:Declaring metric variables
2026-01-30 09:29:50,229:INFO:Defining Hyperparameters
2026-01-30 09:29:50,362:INFO:Tuning with n_jobs=-1
2026-01-30 09:29:50,362:INFO:Initializing RandomizedSearchCV
2026-01-30 09:31:11,486:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 09:31:11,486:INFO:Hyperparameter search completed
2026-01-30 09:31:11,486:INFO:SubProcess create_model() called ==================================
2026-01-30 09:31:11,486:INFO:Initializing create_model()
2026-01-30 09:31:11,486:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C3BF610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 09:31:11,486:INFO:Checking exceptions
2026-01-30 09:31:11,486:INFO:Importing libraries
2026-01-30 09:31:11,486:INFO:Copying training dataset
2026-01-30 09:31:11,714:INFO:Defining folds
2026-01-30 09:31:11,714:INFO:Declaring metric variables
2026-01-30 09:31:11,714:INFO:Importing untrained model
2026-01-30 09:31:11,714:INFO:Declaring custom model
2026-01-30 09:31:11,714:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:31:11,714:INFO:Starting cross validation
2026-01-30 09:31:11,724:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:31:29,109:INFO:Calculating mean and std
2026-01-30 09:31:29,111:INFO:Creating metrics dataframe
2026-01-30 09:31:29,113:INFO:Finalizing model
2026-01-30 09:31:37,745:INFO:Uploading results into container
2026-01-30 09:31:37,746:INFO:Uploading model into container now
2026-01-30 09:31:37,747:INFO:_master_model_container: 5
2026-01-30 09:31:37,747:INFO:_display_container: 3
2026-01-30 09:31:37,748:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:31:37,748:INFO:create_model() successfully completed......................................
2026-01-30 09:31:37,896:INFO:SubProcess create_model() end ==================================
2026-01-30 09:31:37,897:INFO:choose_better activated
2026-01-30 09:31:37,897:INFO:SubProcess create_model() called ==================================
2026-01-30 09:31:37,897:INFO:Initializing create_model()
2026-01-30 09:31:37,897:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:31:37,897:INFO:Checking exceptions
2026-01-30 09:31:37,898:INFO:Importing libraries
2026-01-30 09:31:37,898:INFO:Copying training dataset
2026-01-30 09:31:38,045:INFO:Defining folds
2026-01-30 09:31:38,045:INFO:Declaring metric variables
2026-01-30 09:31:38,045:INFO:Importing untrained model
2026-01-30 09:31:38,045:INFO:Declaring custom model
2026-01-30 09:31:38,045:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:31:38,045:INFO:Starting cross validation
2026-01-30 09:31:38,045:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:31:49,414:INFO:Calculating mean and std
2026-01-30 09:31:49,414:INFO:Creating metrics dataframe
2026-01-30 09:31:49,416:INFO:Finalizing model
2026-01-30 09:31:55,199:INFO:Uploading results into container
2026-01-30 09:31:55,200:INFO:Uploading model into container now
2026-01-30 09:31:55,200:INFO:_master_model_container: 6
2026-01-30 09:31:55,201:INFO:_display_container: 4
2026-01-30 09:31:55,201:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:31:55,201:INFO:create_model() successfully completed......................................
2026-01-30 09:31:55,358:INFO:SubProcess create_model() end ==================================
2026-01-30 09:31:55,359:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9993
2026-01-30 09:31:55,360:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9934
2026-01-30 09:31:55,360:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 09:31:55,360:INFO:choose_better completed
2026-01-30 09:31:55,361:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:31:55,364:INFO:_master_model_container: 6
2026-01-30 09:31:55,365:INFO:_display_container: 3
2026-01-30 09:31:55,365:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:31:55,365:INFO:tune_model() successfully completed......................................
2026-01-30 09:31:55,520:INFO:Initializing tune_model()
2026-01-30 09:31:55,520:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:31:55,520:INFO:Checking exceptions
2026-01-30 09:31:55,603:INFO:Copying training dataset
2026-01-30 09:31:55,799:INFO:Checking base model
2026-01-30 09:31:55,800:INFO:Base model : Decision Tree Classifier
2026-01-30 09:31:55,801:INFO:Declaring metric variables
2026-01-30 09:31:55,801:INFO:Defining Hyperparameters
2026-01-30 09:31:56,002:INFO:Tuning with n_jobs=-1
2026-01-30 09:31:56,003:INFO:Initializing RandomizedSearchCV
2026-01-30 09:32:00,866:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 09:32:00,866:INFO:Hyperparameter search completed
2026-01-30 09:32:00,866:INFO:SubProcess create_model() called ==================================
2026-01-30 09:32:00,866:INFO:Initializing create_model()
2026-01-30 09:32:00,866:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A07F31BF90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 09:32:00,866:INFO:Checking exceptions
2026-01-30 09:32:00,866:INFO:Importing libraries
2026-01-30 09:32:00,866:INFO:Copying training dataset
2026-01-30 09:32:01,028:INFO:Defining folds
2026-01-30 09:32:01,028:INFO:Declaring metric variables
2026-01-30 09:32:01,028:INFO:Importing untrained model
2026-01-30 09:32:01,028:INFO:Declaring custom model
2026-01-30 09:32:01,028:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:32:01,028:INFO:Starting cross validation
2026-01-30 09:32:01,028:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:32:02,673:INFO:Calculating mean and std
2026-01-30 09:32:02,676:INFO:Creating metrics dataframe
2026-01-30 09:32:02,677:INFO:Finalizing model
2026-01-30 09:32:03,544:INFO:Uploading results into container
2026-01-30 09:32:03,545:INFO:Uploading model into container now
2026-01-30 09:32:03,546:INFO:_master_model_container: 7
2026-01-30 09:32:03,546:INFO:_display_container: 4
2026-01-30 09:32:03,547:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:32:03,547:INFO:create_model() successfully completed......................................
2026-01-30 09:32:03,660:INFO:SubProcess create_model() end ==================================
2026-01-30 09:32:03,660:INFO:choose_better activated
2026-01-30 09:32:03,660:INFO:SubProcess create_model() called ==================================
2026-01-30 09:32:03,660:INFO:Initializing create_model()
2026-01-30 09:32:03,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:32:03,660:INFO:Checking exceptions
2026-01-30 09:32:03,660:INFO:Importing libraries
2026-01-30 09:32:03,660:INFO:Copying training dataset
2026-01-30 09:32:03,815:INFO:Defining folds
2026-01-30 09:32:03,815:INFO:Declaring metric variables
2026-01-30 09:32:03,816:INFO:Importing untrained model
2026-01-30 09:32:03,816:INFO:Declaring custom model
2026-01-30 09:32:03,816:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:32:03,816:INFO:Starting cross validation
2026-01-30 09:32:03,817:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:32:05,785:INFO:Calculating mean and std
2026-01-30 09:32:05,785:INFO:Creating metrics dataframe
2026-01-30 09:32:05,785:INFO:Finalizing model
2026-01-30 09:32:07,076:INFO:Uploading results into container
2026-01-30 09:32:07,077:INFO:Uploading model into container now
2026-01-30 09:32:07,077:INFO:_master_model_container: 8
2026-01-30 09:32:07,077:INFO:_display_container: 5
2026-01-30 09:32:07,077:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:32:07,077:INFO:create_model() successfully completed......................................
2026-01-30 09:32:07,200:INFO:SubProcess create_model() end ==================================
2026-01-30 09:32:07,200:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9988
2026-01-30 09:32:07,202:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9876
2026-01-30 09:32:07,202:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 09:32:07,202:INFO:choose_better completed
2026-01-30 09:32:07,202:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:32:07,204:INFO:_master_model_container: 8
2026-01-30 09:32:07,204:INFO:_display_container: 4
2026-01-30 09:32:07,204:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:32:07,204:INFO:tune_model() successfully completed......................................
2026-01-30 09:32:07,327:INFO:Initializing tune_model()
2026-01-30 09:32:07,327:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:32:07,327:INFO:Checking exceptions
2026-01-30 09:32:07,382:INFO:Copying training dataset
2026-01-30 09:32:07,485:INFO:Checking base model
2026-01-30 09:32:07,485:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 09:32:07,486:INFO:Declaring metric variables
2026-01-30 09:32:07,486:INFO:Defining Hyperparameters
2026-01-30 09:32:07,615:INFO:Tuning with n_jobs=-1
2026-01-30 09:32:07,615:INFO:Initializing RandomizedSearchCV
2026-01-30 09:32:38,676:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 09:32:38,677:INFO:Hyperparameter search completed
2026-01-30 09:32:38,677:INFO:SubProcess create_model() called ==================================
2026-01-30 09:32:38,679:INFO:Initializing create_model()
2026-01-30 09:32:38,679:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C3C17F90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 09:32:38,679:INFO:Checking exceptions
2026-01-30 09:32:38,679:INFO:Importing libraries
2026-01-30 09:32:38,680:INFO:Copying training dataset
2026-01-30 09:32:38,860:INFO:Defining folds
2026-01-30 09:32:38,860:INFO:Declaring metric variables
2026-01-30 09:32:38,860:INFO:Importing untrained model
2026-01-30 09:32:38,860:INFO:Declaring custom model
2026-01-30 09:32:38,875:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:32:38,876:INFO:Starting cross validation
2026-01-30 09:32:38,876:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:32:47,399:INFO:Calculating mean and std
2026-01-30 09:32:47,402:INFO:Creating metrics dataframe
2026-01-30 09:32:47,405:INFO:Finalizing model
2026-01-30 09:32:47,786:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:32:47,786:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:32:47,786:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:32:47,932:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:32:47,932:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:32:47,932:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:32:47,934:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:32:47,964:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008276 seconds.
2026-01-30 09:32:47,964:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:32:47,964:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:32:47,967:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:32:47,967:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:32:47,971:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:32:47,971:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:32:50,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:32:50,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:32:50,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-01-30 09:32:50,728:INFO:Uploading results into container
2026-01-30 09:32:50,730:INFO:Uploading model into container now
2026-01-30 09:32:50,730:INFO:_master_model_container: 9
2026-01-30 09:32:50,730:INFO:_display_container: 5
2026-01-30 09:32:50,732:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:32:50,732:INFO:create_model() successfully completed......................................
2026-01-30 09:32:50,926:INFO:SubProcess create_model() end ==================================
2026-01-30 09:32:50,926:INFO:choose_better activated
2026-01-30 09:32:50,926:INFO:SubProcess create_model() called ==================================
2026-01-30 09:32:50,926:INFO:Initializing create_model()
2026-01-30 09:32:50,926:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:32:50,926:INFO:Checking exceptions
2026-01-30 09:32:50,926:INFO:Importing libraries
2026-01-30 09:32:50,926:INFO:Copying training dataset
2026-01-30 09:32:51,093:INFO:Defining folds
2026-01-30 09:32:51,093:INFO:Declaring metric variables
2026-01-30 09:32:51,093:INFO:Importing untrained model
2026-01-30 09:32:51,093:INFO:Declaring custom model
2026-01-30 09:32:51,093:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:32:51,093:INFO:Starting cross validation
2026-01-30 09:32:51,093:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:32:55,122:INFO:Calculating mean and std
2026-01-30 09:32:55,123:INFO:Creating metrics dataframe
2026-01-30 09:32:55,127:INFO:Finalizing model
2026-01-30 09:32:55,712:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:32:55,751:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008999 seconds.
2026-01-30 09:32:55,751:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:32:55,751:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:32:55,751:INFO:[LightGBM] [Info] Total Bins 1967
2026-01-30 09:32:55,752:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 19
2026-01-30 09:32:55,754:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:32:55,755:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:32:56,615:INFO:Uploading results into container
2026-01-30 09:32:56,616:INFO:Uploading model into container now
2026-01-30 09:32:56,617:INFO:_master_model_container: 10
2026-01-30 09:32:56,617:INFO:_display_container: 6
2026-01-30 09:32:56,618:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:32:56,618:INFO:create_model() successfully completed......................................
2026-01-30 09:32:56,806:INFO:SubProcess create_model() end ==================================
2026-01-30 09:32:56,807:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9966
2026-01-30 09:32:56,807:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9992
2026-01-30 09:32:56,808:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 09:32:56,808:INFO:choose_better completed
2026-01-30 09:32:56,812:INFO:_master_model_container: 10
2026-01-30 09:32:56,812:INFO:_display_container: 5
2026-01-30 09:32:56,813:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:32:56,813:INFO:tune_model() successfully completed......................................
2026-01-30 09:32:56,962:INFO:Initializing predict_model()
2026-01-30 09:32:56,963:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A069E44EA0>)
2026-01-30 09:32:56,963:INFO:Checking exceptions
2026-01-30 09:32:56,963:INFO:Preloading libraries
2026-01-30 09:32:56,963:INFO:Set up data.
2026-01-30 09:32:56,998:INFO:Set up index.
2026-01-30 09:32:57,876:INFO:Initializing predict_model()
2026-01-30 09:32:57,876:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A069E44EA0>)
2026-01-30 09:32:57,876:INFO:Checking exceptions
2026-01-30 09:32:57,876:INFO:Preloading libraries
2026-01-30 09:32:57,876:INFO:Set up data.
2026-01-30 09:32:57,895:INFO:Set up index.
2026-01-30 09:32:58,359:INFO:Initializing predict_model()
2026-01-30 09:32:58,359:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A069E44EA0>)
2026-01-30 09:32:58,359:INFO:Checking exceptions
2026-01-30 09:32:58,359:INFO:Preloading libraries
2026-01-30 09:32:58,359:INFO:Set up data.
2026-01-30 09:32:58,396:INFO:Set up index.
2026-01-30 09:32:59,410:INFO:Initializing plot_model()
2026-01-30 09:32:59,410:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:32:59,410:INFO:Checking exceptions
2026-01-30 09:32:59,493:INFO:Preloading libraries
2026-01-30 09:32:59,526:INFO:Copying training dataset
2026-01-30 09:32:59,526:INFO:Plot type: feature
2026-01-30 09:32:59,526:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:32:59,776:INFO:Visual Rendered Successfully
2026-01-30 09:32:59,908:INFO:plot_model() successfully completed......................................
2026-01-30 09:32:59,909:INFO:Initializing plot_model()
2026-01-30 09:32:59,909:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0371E4790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:32:59,910:INFO:Checking exceptions
2026-01-30 09:33:00,004:INFO:Preloading libraries
2026-01-30 09:33:00,026:INFO:Copying training dataset
2026-01-30 09:33:00,026:INFO:Plot type: feature_all
2026-01-30 09:33:00,143:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:33:00,426:INFO:Visual Rendered Successfully
2026-01-30 09:33:00,543:INFO:plot_model() successfully completed......................................
2026-01-30 09:33:00,561:INFO:Initializing save_model()
2026-01-30 09:33:00,561:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SI...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 09:33:00,561:INFO:Adding model into prep_pipe
2026-01-30 09:33:00,627:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 09:33:00,631:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['desmatriculado',
                                             'NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'MINIMUMPAYMENTPAYED',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'YEARPERSONBIRTHDATE',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges_...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 09:33:00,631:INFO:save_model() successfully completed......................................
2026-01-30 09:39:02,378:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\1844400550.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 09:39:04,349:INFO:PyCaret ClassificationExperiment
2026-01-30 09:39:04,349:INFO:Logging name: clf-default-name
2026-01-30 09:39:04,349:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 09:39:04,349:INFO:version 3.3.2
2026-01-30 09:39:04,349:INFO:Initializing setup()
2026-01-30 09:39:04,353:INFO:self.USI: a68c
2026-01-30 09:39:04,353:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 09:39:04,353:INFO:Checking environment
2026-01-30 09:39:04,354:INFO:python_version: 3.11.11
2026-01-30 09:39:04,355:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 09:39:04,355:INFO:machine: AMD64
2026-01-30 09:39:04,355:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 09:39:04,355:INFO:Memory: svmem(total=34009374720, available=15912980480, percent=53.2, used=18096394240, free=15912980480)
2026-01-30 09:39:04,356:INFO:Physical Core: 12
2026-01-30 09:39:04,356:INFO:Logical Core: 16
2026-01-30 09:39:04,356:INFO:Checking libraries
2026-01-30 09:39:04,356:INFO:System:
2026-01-30 09:39:04,356:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 09:39:04,357:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 09:39:04,357:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 09:39:04,357:INFO:PyCaret required dependencies:
2026-01-30 09:39:04,357:INFO:                 pip: 25.0
2026-01-30 09:39:04,357:INFO:          setuptools: 75.8.0
2026-01-30 09:39:04,357:INFO:             pycaret: 3.3.2
2026-01-30 09:39:04,357:INFO:             IPython: 9.9.0
2026-01-30 09:39:04,357:INFO:          ipywidgets: 8.1.8
2026-01-30 09:39:04,357:INFO:                tqdm: 4.67.1
2026-01-30 09:39:04,357:INFO:               numpy: 1.26.4
2026-01-30 09:39:04,357:INFO:              pandas: 2.1.4
2026-01-30 09:39:04,357:INFO:              jinja2: 3.1.6
2026-01-30 09:39:04,357:INFO:               scipy: 1.11.4
2026-01-30 09:39:04,357:INFO:              joblib: 1.3.2
2026-01-30 09:39:04,357:INFO:             sklearn: 1.4.2
2026-01-30 09:39:04,357:INFO:                pyod: 2.0.6
2026-01-30 09:39:04,358:INFO:            imblearn: 0.14.1
2026-01-30 09:39:04,358:INFO:   category_encoders: 2.7.0
2026-01-30 09:39:04,358:INFO:            lightgbm: 4.6.0
2026-01-30 09:39:04,358:INFO:               numba: 0.62.1
2026-01-30 09:39:04,358:INFO:            requests: 2.32.3
2026-01-30 09:39:04,358:INFO:          matplotlib: 3.7.5
2026-01-30 09:39:04,358:INFO:          scikitplot: 0.3.7
2026-01-30 09:39:04,358:INFO:         yellowbrick: 1.5
2026-01-30 09:39:04,358:INFO:              plotly: 5.24.1
2026-01-30 09:39:04,358:INFO:    plotly-resampler: Not installed
2026-01-30 09:39:04,358:INFO:             kaleido: 1.2.0
2026-01-30 09:39:04,358:INFO:           schemdraw: 0.15
2026-01-30 09:39:04,358:INFO:         statsmodels: 0.14.6
2026-01-30 09:39:04,358:INFO:              sktime: 0.26.0
2026-01-30 09:39:04,358:INFO:               tbats: 1.1.3
2026-01-30 09:39:04,358:INFO:            pmdarima: 2.0.4
2026-01-30 09:39:04,358:INFO:              psutil: 7.2.1
2026-01-30 09:39:04,358:INFO:          markupsafe: 3.0.3
2026-01-30 09:39:04,358:INFO:             pickle5: Not installed
2026-01-30 09:39:04,358:INFO:         cloudpickle: 3.0.0
2026-01-30 09:39:04,358:INFO:         deprecation: 2.1.0
2026-01-30 09:39:04,358:INFO:              xxhash: 3.6.0
2026-01-30 09:39:04,358:INFO:           wurlitzer: Not installed
2026-01-30 09:39:04,358:INFO:PyCaret optional dependencies:
2026-01-30 09:39:04,358:INFO:                shap: 0.44.1
2026-01-30 09:39:04,358:INFO:           interpret: 0.7.3
2026-01-30 09:39:04,358:INFO:                umap: 0.5.7
2026-01-30 09:39:04,358:INFO:     ydata_profiling: 4.18.1
2026-01-30 09:39:04,358:INFO:  explainerdashboard: 0.5.1
2026-01-30 09:39:04,358:INFO:             autoviz: Not installed
2026-01-30 09:39:04,358:INFO:           fairlearn: 0.7.0
2026-01-30 09:39:04,358:INFO:          deepchecks: Not installed
2026-01-30 09:39:04,358:INFO:             xgboost: Not installed
2026-01-30 09:39:04,358:INFO:            catboost: 1.2.8
2026-01-30 09:39:04,358:INFO:              kmodes: 0.12.2
2026-01-30 09:39:04,358:INFO:             mlxtend: 0.23.4
2026-01-30 09:39:04,358:INFO:       statsforecast: 1.5.0
2026-01-30 09:39:04,358:INFO:        tune_sklearn: Not installed
2026-01-30 09:39:04,358:INFO:                 ray: Not installed
2026-01-30 09:39:04,358:INFO:            hyperopt: 0.2.7
2026-01-30 09:39:04,358:INFO:              optuna: 4.6.0
2026-01-30 09:39:04,358:INFO:               skopt: 0.10.2
2026-01-30 09:39:04,358:INFO:              mlflow: 3.8.1
2026-01-30 09:39:04,358:INFO:              gradio: 6.3.0
2026-01-30 09:39:04,358:INFO:             fastapi: 0.128.0
2026-01-30 09:39:04,358:INFO:             uvicorn: 0.40.0
2026-01-30 09:39:04,358:INFO:              m2cgen: 0.10.0
2026-01-30 09:39:04,358:INFO:           evidently: 0.4.40
2026-01-30 09:39:04,358:INFO:               fugue: 0.8.7
2026-01-30 09:39:04,358:INFO:           streamlit: Not installed
2026-01-30 09:39:04,358:INFO:             prophet: Not installed
2026-01-30 09:39:04,358:INFO:None
2026-01-30 09:39:04,358:INFO:Set up data.
2026-01-30 09:39:04,474:INFO:Set up folding strategy.
2026-01-30 09:39:04,474:INFO:Set up train/test split.
2026-01-30 09:39:04,661:INFO:Set up index.
2026-01-30 09:39:04,673:INFO:Assigning column types.
2026-01-30 09:39:04,778:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 09:39:04,811:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:39:04,811:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:39:04,827:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:39:04,827:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:39:04,839:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:39:04,839:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:39:04,861:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:39:04,861:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:39:04,870:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 09:39:04,894:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:39:04,911:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:39:04,911:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:39:04,937:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:39:04,954:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:39:04,954:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:39:04,955:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 09:39:04,994:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:39:04,994:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:39:05,037:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:39:05,037:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:39:05,039:INFO:Preparing preprocessing pipeline...
2026-01-30 09:39:05,058:INFO:Set up simple imputation.
2026-01-30 09:39:05,059:INFO:Set up feature normalization.
2026-01-30 09:39:05,507:INFO:Finished creating preprocessing pipeline.
2026-01-30 09:39:05,507:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 09:39:05,507:INFO:Creating final display dataframe.
2026-01-30 09:39:06,610:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (429278, 22)
4        Transformed data shape      (429278, 22)
5   Transformed train set shape      (300494, 22)
6    Transformed test set shape      (128784, 22)
7              Numeric features                18
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              a68c
2026-01-30 09:39:06,638:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:39:06,638:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:39:06,690:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:39:06,690:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:39:06,690:INFO:setup() successfully completed in 2.35s...............
2026-01-30 09:39:06,690:INFO:Initializing compare_models()
2026-01-30 09:39:06,690:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 09:39:06,690:INFO:Checking exceptions
2026-01-30 09:39:06,791:INFO:Preparing display monitor
2026-01-30 09:39:06,791:INFO:Initializing Logistic Regression
2026-01-30 09:39:06,791:INFO:Total runtime is 0.0 minutes
2026-01-30 09:39:06,791:INFO:SubProcess create_model() called ==================================
2026-01-30 09:39:06,791:INFO:Initializing create_model()
2026-01-30 09:39:06,791:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C9F29550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:39:06,791:INFO:Checking exceptions
2026-01-30 09:39:06,791:INFO:Importing libraries
2026-01-30 09:39:06,791:INFO:Copying training dataset
2026-01-30 09:39:06,925:INFO:Defining folds
2026-01-30 09:39:06,925:INFO:Declaring metric variables
2026-01-30 09:39:06,925:INFO:Importing untrained model
2026-01-30 09:39:06,925:INFO:Logistic Regression Imported successfully
2026-01-30 09:39:06,925:INFO:Starting cross validation
2026-01-30 09:39:06,925:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:39:14,419:INFO:Calculating mean and std
2026-01-30 09:39:14,420:INFO:Creating metrics dataframe
2026-01-30 09:39:14,421:INFO:Uploading results into container
2026-01-30 09:39:14,421:INFO:Uploading model into container now
2026-01-30 09:39:14,421:INFO:_master_model_container: 1
2026-01-30 09:39:14,421:INFO:_display_container: 2
2026-01-30 09:39:14,425:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 09:39:14,425:INFO:create_model() successfully completed......................................
2026-01-30 09:39:14,541:INFO:SubProcess create_model() end ==================================
2026-01-30 09:39:14,541:INFO:Creating metrics dataframe
2026-01-30 09:39:14,541:INFO:Initializing Decision Tree Classifier
2026-01-30 09:39:14,541:INFO:Total runtime is 0.12916266918182373 minutes
2026-01-30 09:39:14,541:INFO:SubProcess create_model() called ==================================
2026-01-30 09:39:14,541:INFO:Initializing create_model()
2026-01-30 09:39:14,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C9F29550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:39:14,541:INFO:Checking exceptions
2026-01-30 09:39:14,541:INFO:Importing libraries
2026-01-30 09:39:14,541:INFO:Copying training dataset
2026-01-30 09:39:14,671:INFO:Defining folds
2026-01-30 09:39:14,671:INFO:Declaring metric variables
2026-01-30 09:39:14,671:INFO:Importing untrained model
2026-01-30 09:39:14,671:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:39:14,671:INFO:Starting cross validation
2026-01-30 09:39:14,671:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:39:20,653:INFO:Calculating mean and std
2026-01-30 09:39:20,655:INFO:Creating metrics dataframe
2026-01-30 09:39:20,659:INFO:Uploading results into container
2026-01-30 09:39:20,659:INFO:Uploading model into container now
2026-01-30 09:39:20,661:INFO:_master_model_container: 2
2026-01-30 09:39:20,661:INFO:_display_container: 2
2026-01-30 09:39:20,661:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:39:20,661:INFO:create_model() successfully completed......................................
2026-01-30 09:39:20,771:INFO:SubProcess create_model() end ==================================
2026-01-30 09:39:20,771:INFO:Creating metrics dataframe
2026-01-30 09:39:20,771:INFO:Initializing Random Forest Classifier
2026-01-30 09:39:20,771:INFO:Total runtime is 0.23299130996068318 minutes
2026-01-30 09:39:20,771:INFO:SubProcess create_model() called ==================================
2026-01-30 09:39:20,771:INFO:Initializing create_model()
2026-01-30 09:39:20,771:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C9F29550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:39:20,771:INFO:Checking exceptions
2026-01-30 09:39:20,771:INFO:Importing libraries
2026-01-30 09:39:20,771:INFO:Copying training dataset
2026-01-30 09:39:20,904:INFO:Defining folds
2026-01-30 09:39:20,904:INFO:Declaring metric variables
2026-01-30 09:39:20,904:INFO:Importing untrained model
2026-01-30 09:39:20,904:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:39:20,904:INFO:Starting cross validation
2026-01-30 09:39:20,904:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:39:37,903:INFO:Calculating mean and std
2026-01-30 09:39:37,904:INFO:Creating metrics dataframe
2026-01-30 09:39:37,904:INFO:Uploading results into container
2026-01-30 09:39:37,904:INFO:Uploading model into container now
2026-01-30 09:39:37,904:INFO:_master_model_container: 3
2026-01-30 09:39:37,904:INFO:_display_container: 2
2026-01-30 09:39:37,904:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:39:37,904:INFO:create_model() successfully completed......................................
2026-01-30 09:39:38,021:INFO:SubProcess create_model() end ==================================
2026-01-30 09:39:38,021:INFO:Creating metrics dataframe
2026-01-30 09:39:38,021:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 09:39:38,021:INFO:Total runtime is 0.5204903920491537 minutes
2026-01-30 09:39:38,021:INFO:SubProcess create_model() called ==================================
2026-01-30 09:39:38,021:INFO:Initializing create_model()
2026-01-30 09:39:38,021:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C9F29550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:39:38,021:INFO:Checking exceptions
2026-01-30 09:39:38,021:INFO:Importing libraries
2026-01-30 09:39:38,021:INFO:Copying training dataset
2026-01-30 09:39:38,171:INFO:Defining folds
2026-01-30 09:39:38,171:INFO:Declaring metric variables
2026-01-30 09:39:38,171:INFO:Importing untrained model
2026-01-30 09:39:38,171:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:39:38,171:INFO:Starting cross validation
2026-01-30 09:39:38,171:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:39:45,503:INFO:Calculating mean and std
2026-01-30 09:39:45,504:INFO:Creating metrics dataframe
2026-01-30 09:39:45,504:INFO:Uploading results into container
2026-01-30 09:39:45,504:INFO:Uploading model into container now
2026-01-30 09:39:45,504:INFO:_master_model_container: 4
2026-01-30 09:39:45,504:INFO:_display_container: 2
2026-01-30 09:39:45,504:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:39:45,504:INFO:create_model() successfully completed......................................
2026-01-30 09:39:45,624:INFO:SubProcess create_model() end ==================================
2026-01-30 09:39:45,624:INFO:Creating metrics dataframe
2026-01-30 09:39:45,624:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 09:39:45,624:INFO:Initializing create_model()
2026-01-30 09:39:45,624:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:39:45,624:INFO:Checking exceptions
2026-01-30 09:39:45,624:INFO:Importing libraries
2026-01-30 09:39:45,624:INFO:Copying training dataset
2026-01-30 09:39:45,771:INFO:Defining folds
2026-01-30 09:39:45,771:INFO:Declaring metric variables
2026-01-30 09:39:45,771:INFO:Importing untrained model
2026-01-30 09:39:45,771:INFO:Declaring custom model
2026-01-30 09:39:45,771:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:39:45,771:INFO:Cross validation set to False
2026-01-30 09:39:45,771:INFO:Fitting Model
2026-01-30 09:39:52,757:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:39:52,757:INFO:create_model() successfully completed......................................
2026-01-30 09:39:52,871:INFO:Initializing create_model()
2026-01-30 09:39:52,871:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:39:52,871:INFO:Checking exceptions
2026-01-30 09:39:52,871:INFO:Importing libraries
2026-01-30 09:39:52,871:INFO:Copying training dataset
2026-01-30 09:39:53,023:INFO:Defining folds
2026-01-30 09:39:53,023:INFO:Declaring metric variables
2026-01-30 09:39:53,023:INFO:Importing untrained model
2026-01-30 09:39:53,023:INFO:Declaring custom model
2026-01-30 09:39:53,023:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:39:53,023:INFO:Cross validation set to False
2026-01-30 09:39:53,023:INFO:Fitting Model
2026-01-30 09:39:53,651:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:39:53,699:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008173 seconds.
2026-01-30 09:39:53,699:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:39:53,699:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:39:53,701:INFO:[LightGBM] [Info] Total Bins 3112
2026-01-30 09:39:53,701:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 21
2026-01-30 09:39:53,704:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:39:53,704:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:39:54,537:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:39:54,537:INFO:create_model() successfully completed......................................
2026-01-30 09:39:54,704:INFO:Initializing create_model()
2026-01-30 09:39:54,719:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:39:54,720:INFO:Checking exceptions
2026-01-30 09:39:54,720:INFO:Importing libraries
2026-01-30 09:39:54,720:INFO:Copying training dataset
2026-01-30 09:39:54,889:INFO:Defining folds
2026-01-30 09:39:54,889:INFO:Declaring metric variables
2026-01-30 09:39:54,889:INFO:Importing untrained model
2026-01-30 09:39:54,889:INFO:Declaring custom model
2026-01-30 09:39:54,889:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:39:54,889:INFO:Cross validation set to False
2026-01-30 09:39:54,889:INFO:Fitting Model
2026-01-30 09:39:57,713:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:39:57,713:INFO:create_model() successfully completed......................................
2026-01-30 09:39:57,843:INFO:_master_model_container: 4
2026-01-30 09:39:57,844:INFO:_display_container: 2
2026-01-30 09:39:57,844:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 09:39:57,844:INFO:compare_models() successfully completed......................................
2026-01-30 09:39:57,861:INFO:Initializing tune_model()
2026-01-30 09:39:57,861:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:39:57,861:INFO:Checking exceptions
2026-01-30 09:39:57,916:INFO:Copying training dataset
2026-01-30 09:39:58,022:INFO:Checking base model
2026-01-30 09:39:58,023:INFO:Base model : Random Forest Classifier
2026-01-30 09:39:58,023:INFO:Declaring metric variables
2026-01-30 09:39:58,024:INFO:Defining Hyperparameters
2026-01-30 09:39:58,141:INFO:Tuning with n_jobs=-1
2026-01-30 09:39:58,142:INFO:Initializing RandomizedSearchCV
2026-01-30 09:42:15,983:INFO:best_params: {'actual_estimator__n_estimators': 120, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 09:42:15,986:INFO:Hyperparameter search completed
2026-01-30 09:42:15,986:INFO:SubProcess create_model() called ==================================
2026-01-30 09:42:15,986:INFO:Initializing create_model()
2026-01-30 09:42:15,986:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06A301290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': True})
2026-01-30 09:42:15,986:INFO:Checking exceptions
2026-01-30 09:42:15,986:INFO:Importing libraries
2026-01-30 09:42:15,986:INFO:Copying training dataset
2026-01-30 09:42:16,220:INFO:Defining folds
2026-01-30 09:42:16,220:INFO:Declaring metric variables
2026-01-30 09:42:16,220:INFO:Importing untrained model
2026-01-30 09:42:16,220:INFO:Declaring custom model
2026-01-30 09:42:16,220:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:42:16,220:INFO:Starting cross validation
2026-01-30 09:42:16,220:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:42:30,870:INFO:Calculating mean and std
2026-01-30 09:42:30,870:INFO:Creating metrics dataframe
2026-01-30 09:42:30,874:INFO:Finalizing model
2026-01-30 09:42:37,952:INFO:Uploading results into container
2026-01-30 09:42:37,952:INFO:Uploading model into container now
2026-01-30 09:42:37,952:INFO:_master_model_container: 5
2026-01-30 09:42:37,966:INFO:_display_container: 3
2026-01-30 09:42:37,966:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:42:37,966:INFO:create_model() successfully completed......................................
2026-01-30 09:42:38,118:INFO:SubProcess create_model() end ==================================
2026-01-30 09:42:38,118:INFO:choose_better activated
2026-01-30 09:42:38,118:INFO:SubProcess create_model() called ==================================
2026-01-30 09:42:38,118:INFO:Initializing create_model()
2026-01-30 09:42:38,118:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:42:38,118:INFO:Checking exceptions
2026-01-30 09:42:38,118:INFO:Importing libraries
2026-01-30 09:42:38,118:INFO:Copying training dataset
2026-01-30 09:42:38,285:INFO:Defining folds
2026-01-30 09:42:38,285:INFO:Declaring metric variables
2026-01-30 09:42:38,285:INFO:Importing untrained model
2026-01-30 09:42:38,285:INFO:Declaring custom model
2026-01-30 09:42:38,285:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:42:38,285:INFO:Starting cross validation
2026-01-30 09:42:38,285:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:42:56,385:INFO:Calculating mean and std
2026-01-30 09:42:56,385:INFO:Creating metrics dataframe
2026-01-30 09:42:56,385:INFO:Finalizing model
2026-01-30 09:43:04,851:INFO:Uploading results into container
2026-01-30 09:43:04,852:INFO:Uploading model into container now
2026-01-30 09:43:04,853:INFO:_master_model_container: 6
2026-01-30 09:43:04,853:INFO:_display_container: 4
2026-01-30 09:43:04,853:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:43:04,853:INFO:create_model() successfully completed......................................
2026-01-30 09:43:04,970:INFO:SubProcess create_model() end ==================================
2026-01-30 09:43:04,970:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.998
2026-01-30 09:43:04,970:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9879
2026-01-30 09:43:04,970:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 09:43:04,970:INFO:choose_better completed
2026-01-30 09:43:04,970:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:43:04,985:INFO:_master_model_container: 6
2026-01-30 09:43:04,985:INFO:_display_container: 3
2026-01-30 09:43:04,985:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:43:04,985:INFO:tune_model() successfully completed......................................
2026-01-30 09:43:05,107:INFO:Initializing tune_model()
2026-01-30 09:43:05,107:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:43:05,107:INFO:Checking exceptions
2026-01-30 09:43:05,151:INFO:Copying training dataset
2026-01-30 09:43:05,262:INFO:Checking base model
2026-01-30 09:43:05,262:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 09:43:05,262:INFO:Declaring metric variables
2026-01-30 09:43:05,262:INFO:Defining Hyperparameters
2026-01-30 09:43:05,368:INFO:Tuning with n_jobs=-1
2026-01-30 09:43:05,368:INFO:Initializing RandomizedSearchCV
2026-01-30 09:43:43,827:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 09:43:43,828:INFO:Hyperparameter search completed
2026-01-30 09:43:43,830:INFO:SubProcess create_model() called ==================================
2026-01-30 09:43:43,832:INFO:Initializing create_model()
2026-01-30 09:43:43,833:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C28A16D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 09:43:43,833:INFO:Checking exceptions
2026-01-30 09:43:43,833:INFO:Importing libraries
2026-01-30 09:43:43,833:INFO:Copying training dataset
2026-01-30 09:43:44,105:INFO:Defining folds
2026-01-30 09:43:44,105:INFO:Declaring metric variables
2026-01-30 09:43:44,105:INFO:Importing untrained model
2026-01-30 09:43:44,105:INFO:Declaring custom model
2026-01-30 09:43:44,107:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:43:44,108:INFO:Starting cross validation
2026-01-30 09:43:44,109:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:43:53,442:INFO:Calculating mean and std
2026-01-30 09:43:53,442:INFO:Creating metrics dataframe
2026-01-30 09:43:53,442:INFO:Finalizing model
2026-01-30 09:43:54,017:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:43:54,017:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:43:54,017:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:43:54,196:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 09:43:54,196:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 09:43:54,196:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 09:43:54,197:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:43:54,255:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009683 seconds.
2026-01-30 09:43:54,255:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:43:54,255:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:43:54,256:INFO:[LightGBM] [Info] Total Bins 3112
2026-01-30 09:43:54,257:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 21
2026-01-30 09:43:54,261:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:43:54,261:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:43:57,221:INFO:Uploading results into container
2026-01-30 09:43:57,223:INFO:Uploading model into container now
2026-01-30 09:43:57,223:INFO:_master_model_container: 7
2026-01-30 09:43:57,223:INFO:_display_container: 4
2026-01-30 09:43:57,225:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:43:57,225:INFO:create_model() successfully completed......................................
2026-01-30 09:43:57,411:INFO:SubProcess create_model() end ==================================
2026-01-30 09:43:57,411:INFO:choose_better activated
2026-01-30 09:43:57,411:INFO:SubProcess create_model() called ==================================
2026-01-30 09:43:57,412:INFO:Initializing create_model()
2026-01-30 09:43:57,412:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:43:57,412:INFO:Checking exceptions
2026-01-30 09:43:57,413:INFO:Importing libraries
2026-01-30 09:43:57,413:INFO:Copying training dataset
2026-01-30 09:43:57,600:INFO:Defining folds
2026-01-30 09:43:57,600:INFO:Declaring metric variables
2026-01-30 09:43:57,600:INFO:Importing untrained model
2026-01-30 09:43:57,600:INFO:Declaring custom model
2026-01-30 09:43:57,600:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:43:57,600:INFO:Starting cross validation
2026-01-30 09:43:57,600:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:44:01,906:INFO:Calculating mean and std
2026-01-30 09:44:01,906:INFO:Creating metrics dataframe
2026-01-30 09:44:01,906:INFO:Finalizing model
2026-01-30 09:44:02,554:INFO:[LightGBM] [Info] Number of positive: 116896, number of negative: 183598
2026-01-30 09:44:02,607:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008181 seconds.
2026-01-30 09:44:02,607:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:44:02,607:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:44:02,607:INFO:[LightGBM] [Info] Total Bins 3112
2026-01-30 09:44:02,609:INFO:[LightGBM] [Info] Number of data points in the train set: 300494, number of used features: 21
2026-01-30 09:44:02,612:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.389013 -> initscore=-0.451464
2026-01-30 09:44:02,612:INFO:[LightGBM] [Info] Start training from score -0.451464
2026-01-30 09:44:03,503:INFO:Uploading results into container
2026-01-30 09:44:03,504:INFO:Uploading model into container now
2026-01-30 09:44:03,505:INFO:_master_model_container: 8
2026-01-30 09:44:03,505:INFO:_display_container: 5
2026-01-30 09:44:03,506:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:44:03,506:INFO:create_model() successfully completed......................................
2026-01-30 09:44:03,700:INFO:SubProcess create_model() end ==================================
2026-01-30 09:44:03,700:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9936
2026-01-30 09:44:03,700:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9981
2026-01-30 09:44:03,700:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 09:44:03,700:INFO:choose_better completed
2026-01-30 09:44:03,700:INFO:_master_model_container: 8
2026-01-30 09:44:03,700:INFO:_display_container: 4
2026-01-30 09:44:03,700:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:44:03,700:INFO:tune_model() successfully completed......................................
2026-01-30 09:44:03,833:INFO:Initializing tune_model()
2026-01-30 09:44:03,833:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:44:03,833:INFO:Checking exceptions
2026-01-30 09:44:03,907:INFO:Copying training dataset
2026-01-30 09:44:04,017:INFO:Checking base model
2026-01-30 09:44:04,017:INFO:Base model : Decision Tree Classifier
2026-01-30 09:44:04,017:INFO:Declaring metric variables
2026-01-30 09:44:04,017:INFO:Defining Hyperparameters
2026-01-30 09:44:04,134:INFO:Tuning with n_jobs=-1
2026-01-30 09:44:04,134:INFO:Initializing RandomizedSearchCV
2026-01-30 09:44:10,874:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 09:44:10,874:INFO:Hyperparameter search completed
2026-01-30 09:44:10,874:INFO:SubProcess create_model() called ==================================
2026-01-30 09:44:10,874:INFO:Initializing create_model()
2026-01-30 09:44:10,874:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06E646BD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 09:44:10,874:INFO:Checking exceptions
2026-01-30 09:44:10,874:INFO:Importing libraries
2026-01-30 09:44:10,874:INFO:Copying training dataset
2026-01-30 09:44:11,083:INFO:Defining folds
2026-01-30 09:44:11,083:INFO:Declaring metric variables
2026-01-30 09:44:11,083:INFO:Importing untrained model
2026-01-30 09:44:11,083:INFO:Declaring custom model
2026-01-30 09:44:11,083:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:44:11,083:INFO:Starting cross validation
2026-01-30 09:44:11,083:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:44:13,487:INFO:Calculating mean and std
2026-01-30 09:44:13,489:INFO:Creating metrics dataframe
2026-01-30 09:44:13,494:INFO:Finalizing model
2026-01-30 09:44:15,064:INFO:Uploading results into container
2026-01-30 09:44:15,065:INFO:Uploading model into container now
2026-01-30 09:44:15,065:INFO:_master_model_container: 9
2026-01-30 09:44:15,065:INFO:_display_container: 5
2026-01-30 09:44:15,065:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:44:15,066:INFO:create_model() successfully completed......................................
2026-01-30 09:44:15,192:INFO:SubProcess create_model() end ==================================
2026-01-30 09:44:15,192:INFO:choose_better activated
2026-01-30 09:44:15,192:INFO:SubProcess create_model() called ==================================
2026-01-30 09:44:15,193:INFO:Initializing create_model()
2026-01-30 09:44:15,193:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:44:15,193:INFO:Checking exceptions
2026-01-30 09:44:15,193:INFO:Importing libraries
2026-01-30 09:44:15,194:INFO:Copying training dataset
2026-01-30 09:44:15,367:INFO:Defining folds
2026-01-30 09:44:15,367:INFO:Declaring metric variables
2026-01-30 09:44:15,367:INFO:Importing untrained model
2026-01-30 09:44:15,367:INFO:Declaring custom model
2026-01-30 09:44:15,367:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:44:15,367:INFO:Starting cross validation
2026-01-30 09:44:15,367:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:44:18,821:INFO:Calculating mean and std
2026-01-30 09:44:18,821:INFO:Creating metrics dataframe
2026-01-30 09:44:18,822:INFO:Finalizing model
2026-01-30 09:44:21,655:INFO:Uploading results into container
2026-01-30 09:44:21,655:INFO:Uploading model into container now
2026-01-30 09:44:21,666:INFO:_master_model_container: 10
2026-01-30 09:44:21,666:INFO:_display_container: 6
2026-01-30 09:44:21,666:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:44:21,666:INFO:create_model() successfully completed......................................
2026-01-30 09:44:21,807:INFO:SubProcess create_model() end ==================================
2026-01-30 09:44:21,807:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9856
2026-01-30 09:44:21,808:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9831
2026-01-30 09:44:21,808:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 09:44:21,808:INFO:choose_better completed
2026-01-30 09:44:21,808:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:44:21,810:INFO:_master_model_container: 10
2026-01-30 09:44:21,811:INFO:_display_container: 5
2026-01-30 09:44:21,811:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:44:21,811:INFO:tune_model() successfully completed......................................
2026-01-30 09:44:21,967:INFO:Initializing predict_model()
2026-01-30 09:44:21,967:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03C5465C0>)
2026-01-30 09:44:21,968:INFO:Checking exceptions
2026-01-30 09:44:21,968:INFO:Preloading libraries
2026-01-30 09:44:21,968:INFO:Set up data.
2026-01-30 09:44:22,007:INFO:Set up index.
2026-01-30 09:44:23,289:INFO:Initializing predict_model()
2026-01-30 09:44:23,289:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C3C9E700>)
2026-01-30 09:44:23,289:INFO:Checking exceptions
2026-01-30 09:44:23,289:INFO:Preloading libraries
2026-01-30 09:44:23,289:INFO:Set up data.
2026-01-30 09:44:23,333:INFO:Set up index.
2026-01-30 09:44:24,423:INFO:Initializing predict_model()
2026-01-30 09:44:24,423:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03C5465C0>)
2026-01-30 09:44:24,423:INFO:Checking exceptions
2026-01-30 09:44:24,423:INFO:Preloading libraries
2026-01-30 09:44:24,423:INFO:Set up data.
2026-01-30 09:44:24,460:INFO:Set up index.
2026-01-30 09:44:24,955:INFO:Initializing plot_model()
2026-01-30 09:44:24,961:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:44:24,961:INFO:Checking exceptions
2026-01-30 09:44:25,057:INFO:Preloading libraries
2026-01-30 09:44:25,136:INFO:Copying training dataset
2026-01-30 09:44:25,136:INFO:Plot type: feature
2026-01-30 09:44:25,136:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:44:25,418:INFO:Visual Rendered Successfully
2026-01-30 09:44:25,538:INFO:plot_model() successfully completed......................................
2026-01-30 09:44:25,549:INFO:Initializing plot_model()
2026-01-30 09:44:25,549:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C387150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 09:44:25,549:INFO:Checking exceptions
2026-01-30 09:44:25,633:INFO:Preloading libraries
2026-01-30 09:44:25,700:INFO:Copying training dataset
2026-01-30 09:44:25,700:INFO:Plot type: feature_all
2026-01-30 09:44:25,850:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 09:44:26,189:INFO:Visual Rendered Successfully
2026-01-30 09:44:26,307:INFO:plot_model() successfully completed......................................
2026-01-30 09:44:26,323:INFO:Initializing save_model()
2026-01-30 09:44:26,323:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 09:44:26,323:INFO:Adding model into prep_pipe
2026-01-30 09:44:26,426:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 09:44:26,429:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PO...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 09:44:26,429:INFO:save_model() successfully completed......................................
2026-01-30 09:54:19,796:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\3560701889.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 09:54:21,875:INFO:PyCaret ClassificationExperiment
2026-01-30 09:54:21,890:INFO:Logging name: clf-default-name
2026-01-30 09:54:21,890:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 09:54:21,890:INFO:version 3.3.2
2026-01-30 09:54:21,891:INFO:Initializing setup()
2026-01-30 09:54:21,891:INFO:self.USI: 0e4f
2026-01-30 09:54:21,891:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 09:54:21,891:INFO:Checking environment
2026-01-30 09:54:21,892:INFO:python_version: 3.11.11
2026-01-30 09:54:21,892:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 09:54:21,892:INFO:machine: AMD64
2026-01-30 09:54:21,893:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 09:54:21,893:INFO:Memory: svmem(total=34009374720, available=15314989056, percent=55.0, used=18694385664, free=15314989056)
2026-01-30 09:54:21,893:INFO:Physical Core: 12
2026-01-30 09:54:21,893:INFO:Logical Core: 16
2026-01-30 09:54:21,894:INFO:Checking libraries
2026-01-30 09:54:21,894:INFO:System:
2026-01-30 09:54:21,894:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 09:54:21,894:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 09:54:21,894:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 09:54:21,894:INFO:PyCaret required dependencies:
2026-01-30 09:54:21,894:INFO:                 pip: 25.0
2026-01-30 09:54:21,894:INFO:          setuptools: 75.8.0
2026-01-30 09:54:21,894:INFO:             pycaret: 3.3.2
2026-01-30 09:54:21,894:INFO:             IPython: 9.9.0
2026-01-30 09:54:21,894:INFO:          ipywidgets: 8.1.8
2026-01-30 09:54:21,894:INFO:                tqdm: 4.67.1
2026-01-30 09:54:21,894:INFO:               numpy: 1.26.4
2026-01-30 09:54:21,894:INFO:              pandas: 2.1.4
2026-01-30 09:54:21,894:INFO:              jinja2: 3.1.6
2026-01-30 09:54:21,894:INFO:               scipy: 1.11.4
2026-01-30 09:54:21,894:INFO:              joblib: 1.3.2
2026-01-30 09:54:21,894:INFO:             sklearn: 1.4.2
2026-01-30 09:54:21,894:INFO:                pyod: 2.0.6
2026-01-30 09:54:21,894:INFO:            imblearn: 0.14.1
2026-01-30 09:54:21,894:INFO:   category_encoders: 2.7.0
2026-01-30 09:54:21,894:INFO:            lightgbm: 4.6.0
2026-01-30 09:54:21,894:INFO:               numba: 0.62.1
2026-01-30 09:54:21,894:INFO:            requests: 2.32.3
2026-01-30 09:54:21,894:INFO:          matplotlib: 3.7.5
2026-01-30 09:54:21,894:INFO:          scikitplot: 0.3.7
2026-01-30 09:54:21,894:INFO:         yellowbrick: 1.5
2026-01-30 09:54:21,894:INFO:              plotly: 5.24.1
2026-01-30 09:54:21,894:INFO:    plotly-resampler: Not installed
2026-01-30 09:54:21,894:INFO:             kaleido: 1.2.0
2026-01-30 09:54:21,894:INFO:           schemdraw: 0.15
2026-01-30 09:54:21,894:INFO:         statsmodels: 0.14.6
2026-01-30 09:54:21,894:INFO:              sktime: 0.26.0
2026-01-30 09:54:21,894:INFO:               tbats: 1.1.3
2026-01-30 09:54:21,894:INFO:            pmdarima: 2.0.4
2026-01-30 09:54:21,894:INFO:              psutil: 7.2.1
2026-01-30 09:54:21,894:INFO:          markupsafe: 3.0.3
2026-01-30 09:54:21,894:INFO:             pickle5: Not installed
2026-01-30 09:54:21,894:INFO:         cloudpickle: 3.0.0
2026-01-30 09:54:21,894:INFO:         deprecation: 2.1.0
2026-01-30 09:54:21,894:INFO:              xxhash: 3.6.0
2026-01-30 09:54:21,894:INFO:           wurlitzer: Not installed
2026-01-30 09:54:21,894:INFO:PyCaret optional dependencies:
2026-01-30 09:54:21,894:INFO:                shap: 0.44.1
2026-01-30 09:54:21,894:INFO:           interpret: 0.7.3
2026-01-30 09:54:21,894:INFO:                umap: 0.5.7
2026-01-30 09:54:21,894:INFO:     ydata_profiling: 4.18.1
2026-01-30 09:54:21,894:INFO:  explainerdashboard: 0.5.1
2026-01-30 09:54:21,894:INFO:             autoviz: Not installed
2026-01-30 09:54:21,894:INFO:           fairlearn: 0.7.0
2026-01-30 09:54:21,894:INFO:          deepchecks: Not installed
2026-01-30 09:54:21,894:INFO:             xgboost: Not installed
2026-01-30 09:54:21,894:INFO:            catboost: 1.2.8
2026-01-30 09:54:21,894:INFO:              kmodes: 0.12.2
2026-01-30 09:54:21,894:INFO:             mlxtend: 0.23.4
2026-01-30 09:54:21,894:INFO:       statsforecast: 1.5.0
2026-01-30 09:54:21,894:INFO:        tune_sklearn: Not installed
2026-01-30 09:54:21,894:INFO:                 ray: Not installed
2026-01-30 09:54:21,894:INFO:            hyperopt: 0.2.7
2026-01-30 09:54:21,894:INFO:              optuna: 4.6.0
2026-01-30 09:54:21,894:INFO:               skopt: 0.10.2
2026-01-30 09:54:21,894:INFO:              mlflow: 3.8.1
2026-01-30 09:54:21,894:INFO:              gradio: 6.3.0
2026-01-30 09:54:21,894:INFO:             fastapi: 0.128.0
2026-01-30 09:54:21,894:INFO:             uvicorn: 0.40.0
2026-01-30 09:54:21,894:INFO:              m2cgen: 0.10.0
2026-01-30 09:54:21,894:INFO:           evidently: 0.4.40
2026-01-30 09:54:21,894:INFO:               fugue: 0.8.7
2026-01-30 09:54:21,894:INFO:           streamlit: Not installed
2026-01-30 09:54:21,894:INFO:             prophet: Not installed
2026-01-30 09:54:21,894:INFO:None
2026-01-30 09:54:21,894:INFO:Set up data.
2026-01-30 09:54:22,054:INFO:Set up folding strategy.
2026-01-30 09:54:22,054:INFO:Set up train/test split.
2026-01-30 09:54:22,275:INFO:Set up index.
2026-01-30 09:54:22,291:INFO:Assigning column types.
2026-01-30 09:54:22,456:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 09:54:22,486:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:54:22,487:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:54:22,506:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:54:22,506:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:54:22,536:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 09:54:22,537:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:54:22,556:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:54:22,557:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:54:22,558:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 09:54:22,584:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:54:22,610:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:54:22,610:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:54:22,644:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 09:54:22,658:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:54:22,658:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:54:22,658:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 09:54:22,708:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:54:22,708:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:54:22,758:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:54:22,758:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:54:22,758:INFO:Preparing preprocessing pipeline...
2026-01-30 09:54:22,791:INFO:Set up simple imputation.
2026-01-30 09:54:22,791:INFO:Set up feature normalization.
2026-01-30 09:54:23,696:INFO:Finished creating preprocessing pipeline.
2026-01-30 09:54:23,707:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 09:54:23,708:INFO:Creating final display dataframe.
2026-01-30 09:54:25,695:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 28)
4        Transformed data shape      (482669, 28)
5   Transformed train set shape      (337868, 28)
6    Transformed test set shape      (144801, 28)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              0e4f
2026-01-30 09:54:25,761:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:54:25,761:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:54:25,828:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 09:54:25,828:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 09:54:25,828:INFO:setup() successfully completed in 3.95s...............
2026-01-30 09:54:25,828:INFO:Initializing compare_models()
2026-01-30 09:54:25,828:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 09:54:25,828:INFO:Checking exceptions
2026-01-30 09:54:25,975:INFO:Preparing display monitor
2026-01-30 09:54:25,991:INFO:Initializing Logistic Regression
2026-01-30 09:54:25,991:INFO:Total runtime is 0.0 minutes
2026-01-30 09:54:25,991:INFO:SubProcess create_model() called ==================================
2026-01-30 09:54:25,991:INFO:Initializing create_model()
2026-01-30 09:54:25,991:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0D02B07D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:54:25,991:INFO:Checking exceptions
2026-01-30 09:54:25,991:INFO:Importing libraries
2026-01-30 09:54:25,991:INFO:Copying training dataset
2026-01-30 09:54:26,227:INFO:Defining folds
2026-01-30 09:54:26,227:INFO:Declaring metric variables
2026-01-30 09:54:26,227:INFO:Importing untrained model
2026-01-30 09:54:26,227:INFO:Logistic Regression Imported successfully
2026-01-30 09:54:26,227:INFO:Starting cross validation
2026-01-30 09:54:26,227:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:54:36,429:INFO:Calculating mean and std
2026-01-30 09:54:36,429:INFO:Creating metrics dataframe
2026-01-30 09:54:36,429:INFO:Uploading results into container
2026-01-30 09:54:36,429:INFO:Uploading model into container now
2026-01-30 09:54:36,429:INFO:_master_model_container: 1
2026-01-30 09:54:36,429:INFO:_display_container: 2
2026-01-30 09:54:36,429:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 09:54:36,429:INFO:create_model() successfully completed......................................
2026-01-30 09:54:36,558:INFO:SubProcess create_model() end ==================================
2026-01-30 09:54:36,573:INFO:Creating metrics dataframe
2026-01-30 09:54:36,575:INFO:Initializing Decision Tree Classifier
2026-01-30 09:54:36,575:INFO:Total runtime is 0.17639289697011312 minutes
2026-01-30 09:54:36,575:INFO:SubProcess create_model() called ==================================
2026-01-30 09:54:36,575:INFO:Initializing create_model()
2026-01-30 09:54:36,575:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0D02B07D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:54:36,575:INFO:Checking exceptions
2026-01-30 09:54:36,575:INFO:Importing libraries
2026-01-30 09:54:36,575:INFO:Copying training dataset
2026-01-30 09:54:36,774:INFO:Defining folds
2026-01-30 09:54:36,774:INFO:Declaring metric variables
2026-01-30 09:54:36,774:INFO:Importing untrained model
2026-01-30 09:54:36,774:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:54:36,774:INFO:Starting cross validation
2026-01-30 09:54:36,774:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:54:45,395:INFO:Calculating mean and std
2026-01-30 09:54:45,395:INFO:Creating metrics dataframe
2026-01-30 09:54:45,395:INFO:Uploading results into container
2026-01-30 09:54:45,395:INFO:Uploading model into container now
2026-01-30 09:54:45,395:INFO:_master_model_container: 2
2026-01-30 09:54:45,395:INFO:_display_container: 2
2026-01-30 09:54:45,395:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:54:45,395:INFO:create_model() successfully completed......................................
2026-01-30 09:54:45,524:INFO:SubProcess create_model() end ==================================
2026-01-30 09:54:45,524:INFO:Creating metrics dataframe
2026-01-30 09:54:45,524:INFO:Initializing Random Forest Classifier
2026-01-30 09:54:45,524:INFO:Total runtime is 0.3255495190620422 minutes
2026-01-30 09:54:45,524:INFO:SubProcess create_model() called ==================================
2026-01-30 09:54:45,524:INFO:Initializing create_model()
2026-01-30 09:54:45,524:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0D02B07D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:54:45,524:INFO:Checking exceptions
2026-01-30 09:54:45,524:INFO:Importing libraries
2026-01-30 09:54:45,524:INFO:Copying training dataset
2026-01-30 09:54:45,724:INFO:Defining folds
2026-01-30 09:54:45,724:INFO:Declaring metric variables
2026-01-30 09:54:45,725:INFO:Importing untrained model
2026-01-30 09:54:45,725:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:54:45,725:INFO:Starting cross validation
2026-01-30 09:54:45,725:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:55:10,669:INFO:Calculating mean and std
2026-01-30 09:55:10,669:INFO:Creating metrics dataframe
2026-01-30 09:55:10,673:INFO:Uploading results into container
2026-01-30 09:55:10,674:INFO:Uploading model into container now
2026-01-30 09:55:10,674:INFO:_master_model_container: 3
2026-01-30 09:55:10,674:INFO:_display_container: 2
2026-01-30 09:55:10,674:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:55:10,674:INFO:create_model() successfully completed......................................
2026-01-30 09:55:10,807:INFO:SubProcess create_model() end ==================================
2026-01-30 09:55:10,807:INFO:Creating metrics dataframe
2026-01-30 09:55:10,807:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 09:55:10,807:INFO:Total runtime is 0.7469383398691813 minutes
2026-01-30 09:55:10,807:INFO:SubProcess create_model() called ==================================
2026-01-30 09:55:10,807:INFO:Initializing create_model()
2026-01-30 09:55:10,807:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0D02B07D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:55:10,807:INFO:Checking exceptions
2026-01-30 09:55:10,807:INFO:Importing libraries
2026-01-30 09:55:10,807:INFO:Copying training dataset
2026-01-30 09:55:11,091:INFO:Defining folds
2026-01-30 09:55:11,091:INFO:Declaring metric variables
2026-01-30 09:55:11,091:INFO:Importing untrained model
2026-01-30 09:55:11,091:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:55:11,091:INFO:Starting cross validation
2026-01-30 09:55:11,091:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:55:20,519:INFO:Calculating mean and std
2026-01-30 09:55:20,523:INFO:Creating metrics dataframe
2026-01-30 09:55:20,524:INFO:Uploading results into container
2026-01-30 09:55:20,524:INFO:Uploading model into container now
2026-01-30 09:55:20,524:INFO:_master_model_container: 4
2026-01-30 09:55:20,524:INFO:_display_container: 2
2026-01-30 09:55:20,524:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:55:20,524:INFO:create_model() successfully completed......................................
2026-01-30 09:55:20,652:INFO:SubProcess create_model() end ==================================
2026-01-30 09:55:20,652:INFO:Creating metrics dataframe
2026-01-30 09:55:20,656:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 09:55:20,657:INFO:Initializing create_model()
2026-01-30 09:55:20,657:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:55:20,658:INFO:Checking exceptions
2026-01-30 09:55:20,658:INFO:Importing libraries
2026-01-30 09:55:20,658:INFO:Copying training dataset
2026-01-30 09:55:20,841:INFO:Defining folds
2026-01-30 09:55:20,841:INFO:Declaring metric variables
2026-01-30 09:55:20,841:INFO:Importing untrained model
2026-01-30 09:55:20,841:INFO:Declaring custom model
2026-01-30 09:55:20,841:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:55:20,841:INFO:Cross validation set to False
2026-01-30 09:55:20,841:INFO:Fitting Model
2026-01-30 09:55:30,974:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:55:30,974:INFO:create_model() successfully completed......................................
2026-01-30 09:55:31,112:INFO:Initializing create_model()
2026-01-30 09:55:31,112:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:55:31,112:INFO:Checking exceptions
2026-01-30 09:55:31,112:INFO:Importing libraries
2026-01-30 09:55:31,112:INFO:Copying training dataset
2026-01-30 09:55:31,307:INFO:Defining folds
2026-01-30 09:55:31,307:INFO:Declaring metric variables
2026-01-30 09:55:31,307:INFO:Importing untrained model
2026-01-30 09:55:31,307:INFO:Declaring custom model
2026-01-30 09:55:31,323:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 09:55:31,323:INFO:Cross validation set to False
2026-01-30 09:55:31,323:INFO:Fitting Model
2026-01-30 09:55:32,242:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 09:55:32,304:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014178 seconds.
2026-01-30 09:55:32,304:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 09:55:32,304:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 09:55:32,306:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 09:55:32,307:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 09:55:32,309:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 09:55:32,309:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 09:55:33,474:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 09:55:33,474:INFO:create_model() successfully completed......................................
2026-01-30 09:55:33,674:INFO:Initializing create_model()
2026-01-30 09:55:33,674:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:55:33,674:INFO:Checking exceptions
2026-01-30 09:55:33,674:INFO:Importing libraries
2026-01-30 09:55:33,674:INFO:Copying training dataset
2026-01-30 09:55:33,974:INFO:Defining folds
2026-01-30 09:55:33,989:INFO:Declaring metric variables
2026-01-30 09:55:33,989:INFO:Importing untrained model
2026-01-30 09:55:33,989:INFO:Declaring custom model
2026-01-30 09:55:33,989:INFO:Decision Tree Classifier Imported successfully
2026-01-30 09:55:33,990:INFO:Cross validation set to False
2026-01-30 09:55:33,990:INFO:Fitting Model
2026-01-30 09:55:36,774:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 09:55:36,774:INFO:create_model() successfully completed......................................
2026-01-30 09:55:36,909:INFO:_master_model_container: 4
2026-01-30 09:55:36,909:INFO:_display_container: 2
2026-01-30 09:55:36,909:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 09:55:36,909:INFO:compare_models() successfully completed......................................
2026-01-30 09:55:36,909:INFO:Initializing tune_model()
2026-01-30 09:55:36,909:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:55:36,909:INFO:Checking exceptions
2026-01-30 09:55:36,990:INFO:Copying training dataset
2026-01-30 09:55:37,109:INFO:Checking base model
2026-01-30 09:55:37,109:INFO:Base model : Random Forest Classifier
2026-01-30 09:55:37,109:INFO:Declaring metric variables
2026-01-30 09:55:37,109:INFO:Defining Hyperparameters
2026-01-30 09:55:37,226:INFO:Tuning with n_jobs=-1
2026-01-30 09:55:37,226:INFO:Initializing RandomizedSearchCV
2026-01-30 09:58:27,555:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 09:58:27,558:INFO:Hyperparameter search completed
2026-01-30 09:58:27,558:INFO:SubProcess create_model() called ==================================
2026-01-30 09:58:27,560:INFO:Initializing create_model()
2026-01-30 09:58:27,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A06E675E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 09:58:27,560:INFO:Checking exceptions
2026-01-30 09:58:27,560:INFO:Importing libraries
2026-01-30 09:58:27,561:INFO:Copying training dataset
2026-01-30 09:58:27,988:INFO:Defining folds
2026-01-30 09:58:27,988:INFO:Declaring metric variables
2026-01-30 09:58:27,988:INFO:Importing untrained model
2026-01-30 09:58:27,988:INFO:Declaring custom model
2026-01-30 09:58:27,988:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:58:27,988:INFO:Starting cross validation
2026-01-30 09:58:27,988:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:59:01,021:INFO:Calculating mean and std
2026-01-30 09:59:01,024:INFO:Creating metrics dataframe
2026-01-30 09:59:01,027:INFO:Finalizing model
2026-01-30 09:59:17,913:INFO:Uploading results into container
2026-01-30 09:59:17,914:INFO:Uploading model into container now
2026-01-30 09:59:17,915:INFO:_master_model_container: 5
2026-01-30 09:59:17,916:INFO:_display_container: 3
2026-01-30 09:59:17,917:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:59:17,917:INFO:create_model() successfully completed......................................
2026-01-30 09:59:18,082:INFO:SubProcess create_model() end ==================================
2026-01-30 09:59:18,082:INFO:choose_better activated
2026-01-30 09:59:18,082:INFO:SubProcess create_model() called ==================================
2026-01-30 09:59:18,084:INFO:Initializing create_model()
2026-01-30 09:59:18,084:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 09:59:18,084:INFO:Checking exceptions
2026-01-30 09:59:18,085:INFO:Importing libraries
2026-01-30 09:59:18,085:INFO:Copying training dataset
2026-01-30 09:59:18,469:INFO:Defining folds
2026-01-30 09:59:18,469:INFO:Declaring metric variables
2026-01-30 09:59:18,469:INFO:Importing untrained model
2026-01-30 09:59:18,469:INFO:Declaring custom model
2026-01-30 09:59:18,470:INFO:Random Forest Classifier Imported successfully
2026-01-30 09:59:18,470:INFO:Starting cross validation
2026-01-30 09:59:18,472:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 09:59:40,639:INFO:Calculating mean and std
2026-01-30 09:59:40,640:INFO:Creating metrics dataframe
2026-01-30 09:59:40,642:INFO:Finalizing model
2026-01-30 09:59:51,731:INFO:Uploading results into container
2026-01-30 09:59:51,732:INFO:Uploading model into container now
2026-01-30 09:59:51,732:INFO:_master_model_container: 6
2026-01-30 09:59:51,733:INFO:_display_container: 4
2026-01-30 09:59:51,733:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:59:51,733:INFO:create_model() successfully completed......................................
2026-01-30 09:59:51,899:INFO:SubProcess create_model() end ==================================
2026-01-30 09:59:51,900:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9985
2026-01-30 09:59:51,901:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9894
2026-01-30 09:59:51,901:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 09:59:51,901:INFO:choose_better completed
2026-01-30 09:59:51,902:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 09:59:51,905:INFO:_master_model_container: 6
2026-01-30 09:59:51,906:INFO:_display_container: 3
2026-01-30 09:59:51,906:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 09:59:51,907:INFO:tune_model() successfully completed......................................
2026-01-30 09:59:52,068:INFO:Initializing tune_model()
2026-01-30 09:59:52,068:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 09:59:52,068:INFO:Checking exceptions
2026-01-30 09:59:52,189:INFO:Copying training dataset
2026-01-30 09:59:52,385:INFO:Checking base model
2026-01-30 09:59:52,385:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 09:59:52,386:INFO:Declaring metric variables
2026-01-30 09:59:52,387:INFO:Defining Hyperparameters
2026-01-30 09:59:52,564:INFO:Tuning with n_jobs=-1
2026-01-30 09:59:52,565:INFO:Initializing RandomizedSearchCV
2026-01-30 10:00:39,556:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 10:00:39,558:INFO:Hyperparameter search completed
2026-01-30 10:00:39,558:INFO:SubProcess create_model() called ==================================
2026-01-30 10:00:39,559:INFO:Initializing create_model()
2026-01-30 10:00:39,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A074FCD0D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 10:00:39,560:INFO:Checking exceptions
2026-01-30 10:00:39,560:INFO:Importing libraries
2026-01-30 10:00:39,560:INFO:Copying training dataset
2026-01-30 10:00:39,882:INFO:Defining folds
2026-01-30 10:00:39,883:INFO:Declaring metric variables
2026-01-30 10:00:39,883:INFO:Importing untrained model
2026-01-30 10:00:39,883:INFO:Declaring custom model
2026-01-30 10:00:39,885:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:00:39,885:INFO:Starting cross validation
2026-01-30 10:00:39,886:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:00:51,413:INFO:Calculating mean and std
2026-01-30 10:00:51,414:INFO:Creating metrics dataframe
2026-01-30 10:00:51,416:INFO:Finalizing model
2026-01-30 10:00:52,223:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:00:52,223:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:00:52,223:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:00:52,437:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:00:52,438:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:00:52,438:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:00:52,438:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:00:52,502:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014042 seconds.
2026-01-30 10:00:52,502:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:00:52,502:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:00:52,502:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:00:52,504:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:00:52,509:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:00:52,510:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:00:56,832:INFO:Uploading results into container
2026-01-30 10:00:56,833:INFO:Uploading model into container now
2026-01-30 10:00:56,834:INFO:_master_model_container: 7
2026-01-30 10:00:56,835:INFO:_display_container: 4
2026-01-30 10:00:56,836:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:00:56,836:INFO:create_model() successfully completed......................................
2026-01-30 10:00:57,036:INFO:SubProcess create_model() end ==================================
2026-01-30 10:00:57,036:INFO:choose_better activated
2026-01-30 10:00:57,036:INFO:SubProcess create_model() called ==================================
2026-01-30 10:00:57,038:INFO:Initializing create_model()
2026-01-30 10:00:57,038:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:00:57,038:INFO:Checking exceptions
2026-01-30 10:00:57,039:INFO:Importing libraries
2026-01-30 10:00:57,040:INFO:Copying training dataset
2026-01-30 10:00:57,289:INFO:Defining folds
2026-01-30 10:00:57,289:INFO:Declaring metric variables
2026-01-30 10:00:57,289:INFO:Importing untrained model
2026-01-30 10:00:57,289:INFO:Declaring custom model
2026-01-30 10:00:57,290:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:00:57,290:INFO:Starting cross validation
2026-01-30 10:00:57,291:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:01:02,991:INFO:Calculating mean and std
2026-01-30 10:01:02,991:INFO:Creating metrics dataframe
2026-01-30 10:01:02,992:INFO:Finalizing model
2026-01-30 10:01:03,951:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:01:04,011:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016510 seconds.
2026-01-30 10:01:04,011:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:01:04,011:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:01:04,012:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:01:04,012:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:01:04,015:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:01:04,015:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:01:05,325:INFO:Uploading results into container
2026-01-30 10:01:05,326:INFO:Uploading model into container now
2026-01-30 10:01:05,327:INFO:_master_model_container: 8
2026-01-30 10:01:05,327:INFO:_display_container: 5
2026-01-30 10:01:05,328:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:01:05,328:INFO:create_model() successfully completed......................................
2026-01-30 10:01:05,528:INFO:SubProcess create_model() end ==================================
2026-01-30 10:01:05,529:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9945
2026-01-30 10:01:05,530:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9987
2026-01-30 10:01:05,531:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 10:01:05,531:INFO:choose_better completed
2026-01-30 10:01:05,534:INFO:_master_model_container: 8
2026-01-30 10:01:05,534:INFO:_display_container: 4
2026-01-30 10:01:05,535:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:01:05,535:INFO:tune_model() successfully completed......................................
2026-01-30 10:01:05,691:INFO:Initializing tune_model()
2026-01-30 10:01:05,692:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:01:05,692:INFO:Checking exceptions
2026-01-30 10:01:05,781:INFO:Copying training dataset
2026-01-30 10:01:05,953:INFO:Checking base model
2026-01-30 10:01:05,953:INFO:Base model : Decision Tree Classifier
2026-01-30 10:01:05,954:INFO:Declaring metric variables
2026-01-30 10:01:05,955:INFO:Defining Hyperparameters
2026-01-30 10:01:06,090:INFO:Tuning with n_jobs=-1
2026-01-30 10:01:06,090:INFO:Initializing RandomizedSearchCV
2026-01-30 10:01:15,556:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 10:01:15,557:INFO:Hyperparameter search completed
2026-01-30 10:01:15,558:INFO:SubProcess create_model() called ==================================
2026-01-30 10:01:15,559:INFO:Initializing create_model()
2026-01-30 10:01:15,559:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C295D9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 10:01:15,559:INFO:Checking exceptions
2026-01-30 10:01:15,559:INFO:Importing libraries
2026-01-30 10:01:15,560:INFO:Copying training dataset
2026-01-30 10:01:15,784:INFO:Defining folds
2026-01-30 10:01:15,785:INFO:Declaring metric variables
2026-01-30 10:01:15,785:INFO:Importing untrained model
2026-01-30 10:01:15,785:INFO:Declaring custom model
2026-01-30 10:01:15,786:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:01:15,786:INFO:Starting cross validation
2026-01-30 10:01:15,788:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:01:18,941:INFO:Calculating mean and std
2026-01-30 10:01:18,943:INFO:Creating metrics dataframe
2026-01-30 10:01:18,945:INFO:Finalizing model
2026-01-30 10:01:21,310:INFO:Uploading results into container
2026-01-30 10:01:21,312:INFO:Uploading model into container now
2026-01-30 10:01:21,312:INFO:_master_model_container: 9
2026-01-30 10:01:21,312:INFO:_display_container: 5
2026-01-30 10:01:21,313:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:01:21,313:INFO:create_model() successfully completed......................................
2026-01-30 10:01:21,461:INFO:SubProcess create_model() end ==================================
2026-01-30 10:01:21,461:INFO:choose_better activated
2026-01-30 10:01:21,461:INFO:SubProcess create_model() called ==================================
2026-01-30 10:01:21,462:INFO:Initializing create_model()
2026-01-30 10:01:21,462:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:01:21,462:INFO:Checking exceptions
2026-01-30 10:01:21,462:INFO:Importing libraries
2026-01-30 10:01:21,462:INFO:Copying training dataset
2026-01-30 10:01:21,673:INFO:Defining folds
2026-01-30 10:01:21,673:INFO:Declaring metric variables
2026-01-30 10:01:21,673:INFO:Importing untrained model
2026-01-30 10:01:21,674:INFO:Declaring custom model
2026-01-30 10:01:21,674:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:01:21,674:INFO:Starting cross validation
2026-01-30 10:01:21,675:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:01:26,057:INFO:Calculating mean and std
2026-01-30 10:01:26,058:INFO:Creating metrics dataframe
2026-01-30 10:01:26,061:INFO:Finalizing model
2026-01-30 10:01:29,406:INFO:Uploading results into container
2026-01-30 10:01:29,406:INFO:Uploading model into container now
2026-01-30 10:01:29,407:INFO:_master_model_container: 10
2026-01-30 10:01:29,407:INFO:_display_container: 6
2026-01-30 10:01:29,407:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:01:29,407:INFO:create_model() successfully completed......................................
2026-01-30 10:01:29,553:INFO:SubProcess create_model() end ==================================
2026-01-30 10:01:29,553:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.989
2026-01-30 10:01:29,554:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9826
2026-01-30 10:01:29,554:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 10:01:29,554:INFO:choose_better completed
2026-01-30 10:01:29,554:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:01:29,556:INFO:_master_model_container: 10
2026-01-30 10:01:29,557:INFO:_display_container: 5
2026-01-30 10:01:29,557:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:01:29,557:INFO:tune_model() successfully completed......................................
2026-01-30 10:01:29,709:INFO:Initializing predict_model()
2026-01-30 10:01:29,710:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCB7A590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A069E44EA0>)
2026-01-30 10:01:29,710:INFO:Checking exceptions
2026-01-30 10:01:29,710:INFO:Preloading libraries
2026-01-30 10:01:29,710:INFO:Set up data.
2026-01-30 10:01:29,736:INFO:Set up index.
2026-01-30 10:01:30,158:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 640, in roc_auc_score
    return _average_binary_score(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 382, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 640, in roc_auc_score
    return _average_binary_score(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 382, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.


2026-01-30 10:01:30,181:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.

2026-01-30 10:02:49,008:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\1960418326.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 10:02:51,284:INFO:PyCaret ClassificationExperiment
2026-01-30 10:02:51,284:INFO:Logging name: clf-default-name
2026-01-30 10:02:51,284:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 10:02:51,284:INFO:version 3.3.2
2026-01-30 10:02:51,284:INFO:Initializing setup()
2026-01-30 10:02:51,284:INFO:self.USI: 60e5
2026-01-30 10:02:51,284:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 10:02:51,284:INFO:Checking environment
2026-01-30 10:02:51,284:INFO:python_version: 3.11.11
2026-01-30 10:02:51,284:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 10:02:51,284:INFO:machine: AMD64
2026-01-30 10:02:51,284:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 10:02:51,285:INFO:Memory: svmem(total=34009374720, available=12001648640, percent=64.7, used=22007726080, free=12001648640)
2026-01-30 10:02:51,286:INFO:Physical Core: 12
2026-01-30 10:02:51,286:INFO:Logical Core: 16
2026-01-30 10:02:51,286:INFO:Checking libraries
2026-01-30 10:02:51,286:INFO:System:
2026-01-30 10:02:51,286:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 10:02:51,286:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 10:02:51,286:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 10:02:51,286:INFO:PyCaret required dependencies:
2026-01-30 10:02:51,286:INFO:                 pip: 25.0
2026-01-30 10:02:51,286:INFO:          setuptools: 75.8.0
2026-01-30 10:02:51,286:INFO:             pycaret: 3.3.2
2026-01-30 10:02:51,286:INFO:             IPython: 9.9.0
2026-01-30 10:02:51,286:INFO:          ipywidgets: 8.1.8
2026-01-30 10:02:51,286:INFO:                tqdm: 4.67.1
2026-01-30 10:02:51,286:INFO:               numpy: 1.26.4
2026-01-30 10:02:51,286:INFO:              pandas: 2.1.4
2026-01-30 10:02:51,286:INFO:              jinja2: 3.1.6
2026-01-30 10:02:51,286:INFO:               scipy: 1.11.4
2026-01-30 10:02:51,286:INFO:              joblib: 1.3.2
2026-01-30 10:02:51,286:INFO:             sklearn: 1.4.2
2026-01-30 10:02:51,286:INFO:                pyod: 2.0.6
2026-01-30 10:02:51,286:INFO:            imblearn: 0.14.1
2026-01-30 10:02:51,286:INFO:   category_encoders: 2.7.0
2026-01-30 10:02:51,286:INFO:            lightgbm: 4.6.0
2026-01-30 10:02:51,286:INFO:               numba: 0.62.1
2026-01-30 10:02:51,287:INFO:            requests: 2.32.3
2026-01-30 10:02:51,287:INFO:          matplotlib: 3.7.5
2026-01-30 10:02:51,287:INFO:          scikitplot: 0.3.7
2026-01-30 10:02:51,287:INFO:         yellowbrick: 1.5
2026-01-30 10:02:51,287:INFO:              plotly: 5.24.1
2026-01-30 10:02:51,287:INFO:    plotly-resampler: Not installed
2026-01-30 10:02:51,287:INFO:             kaleido: 1.2.0
2026-01-30 10:02:51,287:INFO:           schemdraw: 0.15
2026-01-30 10:02:51,287:INFO:         statsmodels: 0.14.6
2026-01-30 10:02:51,287:INFO:              sktime: 0.26.0
2026-01-30 10:02:51,287:INFO:               tbats: 1.1.3
2026-01-30 10:02:51,287:INFO:            pmdarima: 2.0.4
2026-01-30 10:02:51,287:INFO:              psutil: 7.2.1
2026-01-30 10:02:51,287:INFO:          markupsafe: 3.0.3
2026-01-30 10:02:51,287:INFO:             pickle5: Not installed
2026-01-30 10:02:51,287:INFO:         cloudpickle: 3.0.0
2026-01-30 10:02:51,287:INFO:         deprecation: 2.1.0
2026-01-30 10:02:51,287:INFO:              xxhash: 3.6.0
2026-01-30 10:02:51,287:INFO:           wurlitzer: Not installed
2026-01-30 10:02:51,287:INFO:PyCaret optional dependencies:
2026-01-30 10:02:51,287:INFO:                shap: 0.44.1
2026-01-30 10:02:51,287:INFO:           interpret: 0.7.3
2026-01-30 10:02:51,287:INFO:                umap: 0.5.7
2026-01-30 10:02:51,289:INFO:     ydata_profiling: 4.18.1
2026-01-30 10:02:51,289:INFO:  explainerdashboard: 0.5.1
2026-01-30 10:02:51,289:INFO:             autoviz: Not installed
2026-01-30 10:02:51,289:INFO:           fairlearn: 0.7.0
2026-01-30 10:02:51,289:INFO:          deepchecks: Not installed
2026-01-30 10:02:51,290:INFO:             xgboost: Not installed
2026-01-30 10:02:51,291:INFO:            catboost: 1.2.8
2026-01-30 10:02:51,291:INFO:              kmodes: 0.12.2
2026-01-30 10:02:51,292:INFO:             mlxtend: 0.23.4
2026-01-30 10:02:51,292:INFO:       statsforecast: 1.5.0
2026-01-30 10:02:51,293:INFO:        tune_sklearn: Not installed
2026-01-30 10:02:51,293:INFO:                 ray: Not installed
2026-01-30 10:02:51,293:INFO:            hyperopt: 0.2.7
2026-01-30 10:02:51,294:INFO:              optuna: 4.6.0
2026-01-30 10:02:51,294:INFO:               skopt: 0.10.2
2026-01-30 10:02:51,294:INFO:              mlflow: 3.8.1
2026-01-30 10:02:51,294:INFO:              gradio: 6.3.0
2026-01-30 10:02:51,294:INFO:             fastapi: 0.128.0
2026-01-30 10:02:51,294:INFO:             uvicorn: 0.40.0
2026-01-30 10:02:51,294:INFO:              m2cgen: 0.10.0
2026-01-30 10:02:51,294:INFO:           evidently: 0.4.40
2026-01-30 10:02:51,294:INFO:               fugue: 0.8.7
2026-01-30 10:02:51,294:INFO:           streamlit: Not installed
2026-01-30 10:02:51,294:INFO:             prophet: Not installed
2026-01-30 10:02:51,294:INFO:None
2026-01-30 10:02:51,294:INFO:Set up data.
2026-01-30 10:02:51,447:INFO:Set up folding strategy.
2026-01-30 10:02:51,447:INFO:Set up train/test split.
2026-01-30 10:02:51,692:INFO:Set up index.
2026-01-30 10:02:51,703:INFO:Assigning column types.
2026-01-30 10:02:51,857:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 10:02:51,885:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 10:02:51,885:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:02:51,903:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:02:51,904:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:02:51,934:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 10:02:51,934:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:02:51,954:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:02:51,954:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:02:51,955:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 10:02:51,985:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:02:52,002:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:02:52,002:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:02:52,032:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:02:52,050:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:02:52,050:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:02:52,051:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 10:02:52,097:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:02:52,098:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:02:52,145:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:02:52,145:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:02:52,147:INFO:Preparing preprocessing pipeline...
2026-01-30 10:02:52,177:INFO:Set up simple imputation.
2026-01-30 10:02:52,177:INFO:Set up feature normalization.
2026-01-30 10:02:52,502:INFO:Finished creating preprocessing pipeline.
2026-01-30 10:02:52,505:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 10:02:52,505:INFO:Creating final display dataframe.
2026-01-30 10:02:53,362:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 28)
4        Transformed data shape      (482669, 28)
5   Transformed train set shape      (337868, 28)
6    Transformed test set shape      (144801, 28)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              60e5
2026-01-30 10:02:53,434:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:02:53,434:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:02:53,506:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:02:53,508:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:02:53,510:INFO:setup() successfully completed in 2.23s...............
2026-01-30 10:02:53,510:INFO:Initializing compare_models()
2026-01-30 10:02:53,510:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 10:02:53,510:INFO:Checking exceptions
2026-01-30 10:02:53,680:INFO:Preparing display monitor
2026-01-30 10:02:53,685:INFO:Initializing Logistic Regression
2026-01-30 10:02:53,685:INFO:Total runtime is 0.0 minutes
2026-01-30 10:02:53,685:INFO:SubProcess create_model() called ==================================
2026-01-30 10:02:53,686:INFO:Initializing create_model()
2026-01-30 10:02:53,686:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C57F050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:02:53,686:INFO:Checking exceptions
2026-01-30 10:02:53,686:INFO:Importing libraries
2026-01-30 10:02:53,686:INFO:Copying training dataset
2026-01-30 10:02:53,946:INFO:Defining folds
2026-01-30 10:02:53,946:INFO:Declaring metric variables
2026-01-30 10:02:53,946:INFO:Importing untrained model
2026-01-30 10:02:53,947:INFO:Logistic Regression Imported successfully
2026-01-30 10:02:53,947:INFO:Starting cross validation
2026-01-30 10:02:53,947:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:02:58,109:INFO:Calculating mean and std
2026-01-30 10:02:58,114:INFO:Creating metrics dataframe
2026-01-30 10:02:58,118:INFO:Uploading results into container
2026-01-30 10:02:58,120:INFO:Uploading model into container now
2026-01-30 10:02:58,123:INFO:_master_model_container: 1
2026-01-30 10:02:58,124:INFO:_display_container: 2
2026-01-30 10:02:58,126:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 10:02:58,127:INFO:create_model() successfully completed......................................
2026-01-30 10:02:58,286:INFO:SubProcess create_model() end ==================================
2026-01-30 10:02:58,286:INFO:Creating metrics dataframe
2026-01-30 10:02:58,289:INFO:Initializing Decision Tree Classifier
2026-01-30 10:02:58,290:INFO:Total runtime is 0.07675575415293376 minutes
2026-01-30 10:02:58,291:INFO:SubProcess create_model() called ==================================
2026-01-30 10:02:58,291:INFO:Initializing create_model()
2026-01-30 10:02:58,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C57F050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:02:58,292:INFO:Checking exceptions
2026-01-30 10:02:58,292:INFO:Importing libraries
2026-01-30 10:02:58,292:INFO:Copying training dataset
2026-01-30 10:02:58,630:INFO:Defining folds
2026-01-30 10:02:58,630:INFO:Declaring metric variables
2026-01-30 10:02:58,631:INFO:Importing untrained model
2026-01-30 10:02:58,631:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:02:58,632:INFO:Starting cross validation
2026-01-30 10:02:58,633:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:03:02,845:INFO:Calculating mean and std
2026-01-30 10:03:02,846:INFO:Creating metrics dataframe
2026-01-30 10:03:02,848:INFO:Uploading results into container
2026-01-30 10:03:02,848:INFO:Uploading model into container now
2026-01-30 10:03:02,849:INFO:_master_model_container: 2
2026-01-30 10:03:02,849:INFO:_display_container: 2
2026-01-30 10:03:02,850:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:03:02,850:INFO:create_model() successfully completed......................................
2026-01-30 10:03:03,004:INFO:SubProcess create_model() end ==================================
2026-01-30 10:03:03,004:INFO:Creating metrics dataframe
2026-01-30 10:03:03,006:INFO:Initializing Random Forest Classifier
2026-01-30 10:03:03,006:INFO:Total runtime is 0.15534850756327312 minutes
2026-01-30 10:03:03,006:INFO:SubProcess create_model() called ==================================
2026-01-30 10:03:03,006:INFO:Initializing create_model()
2026-01-30 10:03:03,007:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C57F050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:03:03,007:INFO:Checking exceptions
2026-01-30 10:03:03,007:INFO:Importing libraries
2026-01-30 10:03:03,007:INFO:Copying training dataset
2026-01-30 10:03:03,213:INFO:Defining folds
2026-01-30 10:03:03,213:INFO:Declaring metric variables
2026-01-30 10:03:03,213:INFO:Importing untrained model
2026-01-30 10:03:03,215:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:03:03,215:INFO:Starting cross validation
2026-01-30 10:03:03,216:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:03:23,840:INFO:Calculating mean and std
2026-01-30 10:03:23,842:INFO:Creating metrics dataframe
2026-01-30 10:03:23,844:INFO:Uploading results into container
2026-01-30 10:03:23,844:INFO:Uploading model into container now
2026-01-30 10:03:23,845:INFO:_master_model_container: 3
2026-01-30 10:03:23,845:INFO:_display_container: 2
2026-01-30 10:03:23,846:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:03:23,846:INFO:create_model() successfully completed......................................
2026-01-30 10:03:23,990:INFO:SubProcess create_model() end ==================================
2026-01-30 10:03:23,991:INFO:Creating metrics dataframe
2026-01-30 10:03:23,993:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 10:03:23,993:INFO:Total runtime is 0.5051378766695658 minutes
2026-01-30 10:03:23,993:INFO:SubProcess create_model() called ==================================
2026-01-30 10:03:23,993:INFO:Initializing create_model()
2026-01-30 10:03:23,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C57F050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:03:23,993:INFO:Checking exceptions
2026-01-30 10:03:23,993:INFO:Importing libraries
2026-01-30 10:03:23,993:INFO:Copying training dataset
2026-01-30 10:03:24,208:INFO:Defining folds
2026-01-30 10:03:24,209:INFO:Declaring metric variables
2026-01-30 10:03:24,209:INFO:Importing untrained model
2026-01-30 10:03:24,209:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:03:24,210:INFO:Starting cross validation
2026-01-30 10:03:24,210:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:03:29,704:INFO:Calculating mean and std
2026-01-30 10:03:29,705:INFO:Creating metrics dataframe
2026-01-30 10:03:29,707:INFO:Uploading results into container
2026-01-30 10:03:29,707:INFO:Uploading model into container now
2026-01-30 10:03:29,708:INFO:_master_model_container: 4
2026-01-30 10:03:29,708:INFO:_display_container: 2
2026-01-30 10:03:29,708:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:03:29,708:INFO:create_model() successfully completed......................................
2026-01-30 10:03:29,849:INFO:SubProcess create_model() end ==================================
2026-01-30 10:03:29,849:INFO:Creating metrics dataframe
2026-01-30 10:03:29,852:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 10:03:29,853:INFO:Initializing create_model()
2026-01-30 10:03:29,853:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:03:29,853:INFO:Checking exceptions
2026-01-30 10:03:29,854:INFO:Importing libraries
2026-01-30 10:03:29,854:INFO:Copying training dataset
2026-01-30 10:03:30,051:INFO:Defining folds
2026-01-30 10:03:30,051:INFO:Declaring metric variables
2026-01-30 10:03:30,051:INFO:Importing untrained model
2026-01-30 10:03:30,051:INFO:Declaring custom model
2026-01-30 10:03:30,052:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:03:30,052:INFO:Cross validation set to False
2026-01-30 10:03:30,052:INFO:Fitting Model
2026-01-30 10:03:40,854:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:03:40,855:INFO:create_model() successfully completed......................................
2026-01-30 10:03:41,013:INFO:Initializing create_model()
2026-01-30 10:03:41,014:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:03:41,014:INFO:Checking exceptions
2026-01-30 10:03:41,014:INFO:Importing libraries
2026-01-30 10:03:41,014:INFO:Copying training dataset
2026-01-30 10:03:41,243:INFO:Defining folds
2026-01-30 10:03:41,243:INFO:Declaring metric variables
2026-01-30 10:03:41,243:INFO:Importing untrained model
2026-01-30 10:03:41,243:INFO:Declaring custom model
2026-01-30 10:03:41,245:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:03:41,246:INFO:Cross validation set to False
2026-01-30 10:03:41,246:INFO:Fitting Model
2026-01-30 10:03:42,175:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:03:42,237:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014972 seconds.
2026-01-30 10:03:42,237:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:03:42,237:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:03:42,238:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:03:42,239:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:03:42,241:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:03:42,242:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:03:43,349:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:03:43,350:INFO:create_model() successfully completed......................................
2026-01-30 10:03:43,577:INFO:Initializing create_model()
2026-01-30 10:03:43,578:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:03:43,578:INFO:Checking exceptions
2026-01-30 10:03:43,579:INFO:Importing libraries
2026-01-30 10:03:43,579:INFO:Copying training dataset
2026-01-30 10:03:43,889:INFO:Defining folds
2026-01-30 10:03:43,889:INFO:Declaring metric variables
2026-01-30 10:03:43,890:INFO:Importing untrained model
2026-01-30 10:03:43,890:INFO:Declaring custom model
2026-01-30 10:03:43,890:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:03:43,891:INFO:Cross validation set to False
2026-01-30 10:03:43,891:INFO:Fitting Model
2026-01-30 10:03:46,873:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:03:46,874:INFO:create_model() successfully completed......................................
2026-01-30 10:03:47,018:INFO:_master_model_container: 4
2026-01-30 10:03:47,019:INFO:_display_container: 2
2026-01-30 10:03:47,020:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 10:03:47,020:INFO:compare_models() successfully completed......................................
2026-01-30 10:03:47,026:INFO:Initializing tune_model()
2026-01-30 10:03:47,026:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:03:47,026:INFO:Checking exceptions
2026-01-30 10:03:47,105:INFO:Copying training dataset
2026-01-30 10:03:47,268:INFO:Checking base model
2026-01-30 10:03:47,269:INFO:Base model : Random Forest Classifier
2026-01-30 10:03:47,269:INFO:Declaring metric variables
2026-01-30 10:03:47,269:INFO:Defining Hyperparameters
2026-01-30 10:03:47,414:INFO:Tuning with n_jobs=-1
2026-01-30 10:03:47,414:INFO:Initializing RandomizedSearchCV
2026-01-30 10:06:42,417:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 10:06:42,418:INFO:Hyperparameter search completed
2026-01-30 10:06:42,419:INFO:SubProcess create_model() called ==================================
2026-01-30 10:06:42,420:INFO:Initializing create_model()
2026-01-30 10:06:42,420:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0CCB75350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 10:06:42,420:INFO:Checking exceptions
2026-01-30 10:06:42,420:INFO:Importing libraries
2026-01-30 10:06:42,420:INFO:Copying training dataset
2026-01-30 10:06:42,684:INFO:Defining folds
2026-01-30 10:06:42,685:INFO:Declaring metric variables
2026-01-30 10:06:42,685:INFO:Importing untrained model
2026-01-30 10:06:42,685:INFO:Declaring custom model
2026-01-30 10:06:42,686:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:06:42,686:INFO:Starting cross validation
2026-01-30 10:06:42,687:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:07:16,513:INFO:Calculating mean and std
2026-01-30 10:07:16,515:INFO:Creating metrics dataframe
2026-01-30 10:07:16,516:INFO:Finalizing model
2026-01-30 10:07:34,329:INFO:Uploading results into container
2026-01-30 10:07:34,330:INFO:Uploading model into container now
2026-01-30 10:07:34,332:INFO:_master_model_container: 5
2026-01-30 10:07:34,332:INFO:_display_container: 3
2026-01-30 10:07:34,332:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:07:34,332:INFO:create_model() successfully completed......................................
2026-01-30 10:07:34,497:INFO:SubProcess create_model() end ==================================
2026-01-30 10:07:34,497:INFO:choose_better activated
2026-01-30 10:07:34,498:INFO:SubProcess create_model() called ==================================
2026-01-30 10:07:34,498:INFO:Initializing create_model()
2026-01-30 10:07:34,498:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:07:34,498:INFO:Checking exceptions
2026-01-30 10:07:34,499:INFO:Importing libraries
2026-01-30 10:07:34,499:INFO:Copying training dataset
2026-01-30 10:07:34,748:INFO:Defining folds
2026-01-30 10:07:34,749:INFO:Declaring metric variables
2026-01-30 10:07:34,749:INFO:Importing untrained model
2026-01-30 10:07:34,749:INFO:Declaring custom model
2026-01-30 10:07:34,749:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:07:34,750:INFO:Starting cross validation
2026-01-30 10:07:34,750:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:07:59,024:INFO:Calculating mean and std
2026-01-30 10:07:59,025:INFO:Creating metrics dataframe
2026-01-30 10:07:59,027:INFO:Finalizing model
2026-01-30 10:08:10,681:INFO:Uploading results into container
2026-01-30 10:08:10,683:INFO:Uploading model into container now
2026-01-30 10:08:10,684:INFO:_master_model_container: 6
2026-01-30 10:08:10,684:INFO:_display_container: 4
2026-01-30 10:08:10,685:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:08:10,685:INFO:create_model() successfully completed......................................
2026-01-30 10:08:10,888:INFO:SubProcess create_model() end ==================================
2026-01-30 10:08:10,889:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9985
2026-01-30 10:08:10,890:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9894
2026-01-30 10:08:10,890:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 10:08:10,890:INFO:choose_better completed
2026-01-30 10:08:10,891:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:08:10,893:INFO:_master_model_container: 6
2026-01-30 10:08:10,893:INFO:_display_container: 3
2026-01-30 10:08:10,894:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:08:10,894:INFO:tune_model() successfully completed......................................
2026-01-30 10:08:11,076:INFO:Initializing tune_model()
2026-01-30 10:08:11,076:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:08:11,076:INFO:Checking exceptions
2026-01-30 10:08:11,199:INFO:Copying training dataset
2026-01-30 10:08:11,358:INFO:Checking base model
2026-01-30 10:08:11,359:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 10:08:11,359:INFO:Declaring metric variables
2026-01-30 10:08:11,360:INFO:Defining Hyperparameters
2026-01-30 10:08:11,531:INFO:Tuning with n_jobs=-1
2026-01-30 10:08:11,531:INFO:Initializing RandomizedSearchCV
2026-01-30 10:09:01,031:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 10:09:01,033:INFO:Hyperparameter search completed
2026-01-30 10:09:01,033:INFO:SubProcess create_model() called ==================================
2026-01-30 10:09:01,035:INFO:Initializing create_model()
2026-01-30 10:09:01,035:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03CF7E090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 10:09:01,035:INFO:Checking exceptions
2026-01-30 10:09:01,035:INFO:Importing libraries
2026-01-30 10:09:01,036:INFO:Copying training dataset
2026-01-30 10:09:01,345:INFO:Defining folds
2026-01-30 10:09:01,345:INFO:Declaring metric variables
2026-01-30 10:09:01,346:INFO:Importing untrained model
2026-01-30 10:09:01,346:INFO:Declaring custom model
2026-01-30 10:09:01,347:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:09:01,348:INFO:Starting cross validation
2026-01-30 10:09:01,349:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:09:13,730:INFO:Calculating mean and std
2026-01-30 10:09:13,733:INFO:Creating metrics dataframe
2026-01-30 10:09:13,740:INFO:Finalizing model
2026-01-30 10:09:14,645:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:09:14,645:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:09:14,645:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:09:14,877:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:09:14,877:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:09:14,877:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:09:14,878:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:09:14,940:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014003 seconds.
2026-01-30 10:09:14,940:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:09:14,940:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:09:14,941:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:09:14,942:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:09:14,948:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:09:14,948:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:09:19,410:INFO:Uploading results into container
2026-01-30 10:09:19,413:INFO:Uploading model into container now
2026-01-30 10:09:19,414:INFO:_master_model_container: 7
2026-01-30 10:09:19,414:INFO:_display_container: 4
2026-01-30 10:09:19,415:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:09:19,415:INFO:create_model() successfully completed......................................
2026-01-30 10:09:19,632:INFO:SubProcess create_model() end ==================================
2026-01-30 10:09:19,633:INFO:choose_better activated
2026-01-30 10:09:19,634:INFO:SubProcess create_model() called ==================================
2026-01-30 10:09:19,635:INFO:Initializing create_model()
2026-01-30 10:09:19,636:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:09:19,636:INFO:Checking exceptions
2026-01-30 10:09:19,637:INFO:Importing libraries
2026-01-30 10:09:19,637:INFO:Copying training dataset
2026-01-30 10:09:19,909:INFO:Defining folds
2026-01-30 10:09:19,910:INFO:Declaring metric variables
2026-01-30 10:09:19,910:INFO:Importing untrained model
2026-01-30 10:09:19,910:INFO:Declaring custom model
2026-01-30 10:09:19,911:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:09:19,911:INFO:Starting cross validation
2026-01-30 10:09:19,912:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:09:26,041:INFO:Calculating mean and std
2026-01-30 10:09:26,043:INFO:Creating metrics dataframe
2026-01-30 10:09:26,046:INFO:Finalizing model
2026-01-30 10:09:27,142:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:09:27,225:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016146 seconds.
2026-01-30 10:09:27,225:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:09:27,225:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:09:27,226:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:09:27,227:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:09:27,230:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:09:27,230:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:09:28,396:INFO:Uploading results into container
2026-01-30 10:09:28,398:INFO:Uploading model into container now
2026-01-30 10:09:28,399:INFO:_master_model_container: 8
2026-01-30 10:09:28,399:INFO:_display_container: 5
2026-01-30 10:09:28,400:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:09:28,401:INFO:create_model() successfully completed......................................
2026-01-30 10:09:28,617:INFO:SubProcess create_model() end ==================================
2026-01-30 10:09:28,619:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9945
2026-01-30 10:09:28,620:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9987
2026-01-30 10:09:28,621:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 10:09:28,621:INFO:choose_better completed
2026-01-30 10:09:28,623:INFO:_master_model_container: 8
2026-01-30 10:09:28,624:INFO:_display_container: 4
2026-01-30 10:09:28,624:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:09:28,624:INFO:tune_model() successfully completed......................................
2026-01-30 10:09:28,801:INFO:Initializing tune_model()
2026-01-30 10:09:28,801:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:09:28,801:INFO:Checking exceptions
2026-01-30 10:09:28,898:INFO:Copying training dataset
2026-01-30 10:09:29,164:INFO:Checking base model
2026-01-30 10:09:29,164:INFO:Base model : Decision Tree Classifier
2026-01-30 10:09:29,165:INFO:Declaring metric variables
2026-01-30 10:09:29,165:INFO:Defining Hyperparameters
2026-01-30 10:09:29,365:INFO:Tuning with n_jobs=-1
2026-01-30 10:09:29,365:INFO:Initializing RandomizedSearchCV
2026-01-30 10:09:39,406:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 10:09:39,408:INFO:Hyperparameter search completed
2026-01-30 10:09:39,409:INFO:SubProcess create_model() called ==================================
2026-01-30 10:09:39,411:INFO:Initializing create_model()
2026-01-30 10:09:39,412:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C3AF350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 10:09:39,412:INFO:Checking exceptions
2026-01-30 10:09:39,412:INFO:Importing libraries
2026-01-30 10:09:39,412:INFO:Copying training dataset
2026-01-30 10:09:39,656:INFO:Defining folds
2026-01-30 10:09:39,657:INFO:Declaring metric variables
2026-01-30 10:09:39,657:INFO:Importing untrained model
2026-01-30 10:09:39,657:INFO:Declaring custom model
2026-01-30 10:09:39,658:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:09:39,658:INFO:Starting cross validation
2026-01-30 10:09:39,659:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:09:42,904:INFO:Calculating mean and std
2026-01-30 10:09:42,905:INFO:Creating metrics dataframe
2026-01-30 10:09:42,906:INFO:Finalizing model
2026-01-30 10:09:45,241:INFO:Uploading results into container
2026-01-30 10:09:45,242:INFO:Uploading model into container now
2026-01-30 10:09:45,243:INFO:_master_model_container: 9
2026-01-30 10:09:45,243:INFO:_display_container: 5
2026-01-30 10:09:45,243:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:09:45,244:INFO:create_model() successfully completed......................................
2026-01-30 10:09:45,454:INFO:SubProcess create_model() end ==================================
2026-01-30 10:09:45,455:INFO:choose_better activated
2026-01-30 10:09:45,455:INFO:SubProcess create_model() called ==================================
2026-01-30 10:09:45,455:INFO:Initializing create_model()
2026-01-30 10:09:45,455:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:09:45,455:INFO:Checking exceptions
2026-01-30 10:09:45,456:INFO:Importing libraries
2026-01-30 10:09:45,456:INFO:Copying training dataset
2026-01-30 10:09:45,705:INFO:Defining folds
2026-01-30 10:09:45,706:INFO:Declaring metric variables
2026-01-30 10:09:45,706:INFO:Importing untrained model
2026-01-30 10:09:45,706:INFO:Declaring custom model
2026-01-30 10:09:45,706:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:09:45,706:INFO:Starting cross validation
2026-01-30 10:09:45,707:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:09:50,230:INFO:Calculating mean and std
2026-01-30 10:09:50,231:INFO:Creating metrics dataframe
2026-01-30 10:09:50,234:INFO:Finalizing model
2026-01-30 10:09:53,817:INFO:Uploading results into container
2026-01-30 10:09:53,818:INFO:Uploading model into container now
2026-01-30 10:09:53,818:INFO:_master_model_container: 10
2026-01-30 10:09:53,818:INFO:_display_container: 6
2026-01-30 10:09:53,819:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:09:53,819:INFO:create_model() successfully completed......................................
2026-01-30 10:09:53,984:INFO:SubProcess create_model() end ==================================
2026-01-30 10:09:53,985:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.989
2026-01-30 10:09:53,985:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9826
2026-01-30 10:09:53,985:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 10:09:53,985:INFO:choose_better completed
2026-01-30 10:09:53,986:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:09:53,988:INFO:_master_model_container: 10
2026-01-30 10:09:53,988:INFO:_display_container: 5
2026-01-30 10:09:53,988:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:09:53,988:INFO:tune_model() successfully completed......................................
2026-01-30 10:09:54,185:INFO:Initializing predict_model()
2026-01-30 10:09:54,185:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C9F74040>)
2026-01-30 10:09:54,185:INFO:Checking exceptions
2026-01-30 10:09:54,185:INFO:Preloading libraries
2026-01-30 10:09:54,186:INFO:Set up data.
2026-01-30 10:09:54,221:INFO:Set up index.
2026-01-30 10:09:54,433:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 640, in roc_auc_score
    return _average_binary_score(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 382, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 640, in roc_auc_score
    return _average_binary_score(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 382, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.


2026-01-30 10:09:54,448:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.

2026-01-30 10:09:54,720:INFO:Initializing predict_model()
2026-01-30 10:09:54,720:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C9F747C0>)
2026-01-30 10:09:54,720:INFO:Checking exceptions
2026-01-30 10:09:54,721:INFO:Preloading libraries
2026-01-30 10:09:54,721:INFO:Set up data.
2026-01-30 10:09:54,747:INFO:Set up index.
2026-01-30 10:09:54,968:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 640, in roc_auc_score
    return _average_binary_score(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 382, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 640, in roc_auc_score
    return _average_binary_score(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 382, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.


2026-01-30 10:09:54,994:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.

2026-01-30 10:09:55,330:INFO:Initializing predict_model()
2026-01-30 10:09:55,331:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A06E646BD0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C9F74860>)
2026-01-30 10:09:55,331:INFO:Checking exceptions
2026-01-30 10:09:55,331:INFO:Preloading libraries
2026-01-30 10:09:55,331:INFO:Set up data.
2026-01-30 10:09:55,375:INFO:Set up index.
2026-01-30 10:09:55,458:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py:585: UserWarning: Traceback (most recent call last):
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py", line 580, in _calculate_metric
    calculated_metric = score_func(y_test, target, sample_weight=weights, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 640, in roc_auc_score
    return _average_binary_score(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 382, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\utils\generic.py", line 583, in _calculate_metric
    calculated_metric = score_func(y_test, target, **kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 640, in roc_auc_score
    return _average_binary_score(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_base.py", line 75, in _average_binary_score
    return binary_metric(y_true, y_score, sample_weight=sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_ranking.py", line 382, in _binary_roc_auc_score
    raise ValueError(
ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.


2026-01-30 10:09:55,491:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.

2026-01-30 10:11:37,314:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\3053220979.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 10:13:01,137:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\3424666871.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 10:13:03,486:INFO:PyCaret ClassificationExperiment
2026-01-30 10:13:03,486:INFO:Logging name: clf-default-name
2026-01-30 10:13:03,486:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 10:13:03,486:INFO:version 3.3.2
2026-01-30 10:13:03,486:INFO:Initializing setup()
2026-01-30 10:13:03,486:INFO:self.USI: e91e
2026-01-30 10:13:03,486:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 10:13:03,487:INFO:Checking environment
2026-01-30 10:13:03,487:INFO:python_version: 3.11.11
2026-01-30 10:13:03,487:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 10:13:03,487:INFO:machine: AMD64
2026-01-30 10:13:03,487:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 10:13:03,487:INFO:Memory: svmem(total=34009374720, available=11281739776, percent=66.8, used=22727634944, free=11281739776)
2026-01-30 10:13:03,487:INFO:Physical Core: 12
2026-01-30 10:13:03,487:INFO:Logical Core: 16
2026-01-30 10:13:03,487:INFO:Checking libraries
2026-01-30 10:13:03,487:INFO:System:
2026-01-30 10:13:03,487:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 10:13:03,487:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 10:13:03,487:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 10:13:03,487:INFO:PyCaret required dependencies:
2026-01-30 10:13:03,487:INFO:                 pip: 25.0
2026-01-30 10:13:03,487:INFO:          setuptools: 75.8.0
2026-01-30 10:13:03,488:INFO:             pycaret: 3.3.2
2026-01-30 10:13:03,488:INFO:             IPython: 9.9.0
2026-01-30 10:13:03,489:INFO:          ipywidgets: 8.1.8
2026-01-30 10:13:03,489:INFO:                tqdm: 4.67.1
2026-01-30 10:13:03,489:INFO:               numpy: 1.26.4
2026-01-30 10:13:03,489:INFO:              pandas: 2.1.4
2026-01-30 10:13:03,489:INFO:              jinja2: 3.1.6
2026-01-30 10:13:03,489:INFO:               scipy: 1.11.4
2026-01-30 10:13:03,489:INFO:              joblib: 1.3.2
2026-01-30 10:13:03,489:INFO:             sklearn: 1.4.2
2026-01-30 10:13:03,489:INFO:                pyod: 2.0.6
2026-01-30 10:13:03,489:INFO:            imblearn: 0.14.1
2026-01-30 10:13:03,490:INFO:   category_encoders: 2.7.0
2026-01-30 10:13:03,490:INFO:            lightgbm: 4.6.0
2026-01-30 10:13:03,490:INFO:               numba: 0.62.1
2026-01-30 10:13:03,490:INFO:            requests: 2.32.3
2026-01-30 10:13:03,490:INFO:          matplotlib: 3.7.5
2026-01-30 10:13:03,490:INFO:          scikitplot: 0.3.7
2026-01-30 10:13:03,490:INFO:         yellowbrick: 1.5
2026-01-30 10:13:03,490:INFO:              plotly: 5.24.1
2026-01-30 10:13:03,490:INFO:    plotly-resampler: Not installed
2026-01-30 10:13:03,490:INFO:             kaleido: 1.2.0
2026-01-30 10:13:03,490:INFO:           schemdraw: 0.15
2026-01-30 10:13:03,490:INFO:         statsmodels: 0.14.6
2026-01-30 10:13:03,491:INFO:              sktime: 0.26.0
2026-01-30 10:13:03,491:INFO:               tbats: 1.1.3
2026-01-30 10:13:03,491:INFO:            pmdarima: 2.0.4
2026-01-30 10:13:03,491:INFO:              psutil: 7.2.1
2026-01-30 10:13:03,491:INFO:          markupsafe: 3.0.3
2026-01-30 10:13:03,491:INFO:             pickle5: Not installed
2026-01-30 10:13:03,491:INFO:         cloudpickle: 3.0.0
2026-01-30 10:13:03,491:INFO:         deprecation: 2.1.0
2026-01-30 10:13:03,491:INFO:              xxhash: 3.6.0
2026-01-30 10:13:03,491:INFO:           wurlitzer: Not installed
2026-01-30 10:13:03,492:INFO:PyCaret optional dependencies:
2026-01-30 10:13:03,492:INFO:                shap: 0.44.1
2026-01-30 10:13:03,492:INFO:           interpret: 0.7.3
2026-01-30 10:13:03,492:INFO:                umap: 0.5.7
2026-01-30 10:13:03,492:INFO:     ydata_profiling: 4.18.1
2026-01-30 10:13:03,492:INFO:  explainerdashboard: 0.5.1
2026-01-30 10:13:03,492:INFO:             autoviz: Not installed
2026-01-30 10:13:03,492:INFO:           fairlearn: 0.7.0
2026-01-30 10:13:03,493:INFO:          deepchecks: Not installed
2026-01-30 10:13:03,493:INFO:             xgboost: Not installed
2026-01-30 10:13:03,493:INFO:            catboost: 1.2.8
2026-01-30 10:13:03,493:INFO:              kmodes: 0.12.2
2026-01-30 10:13:03,493:INFO:             mlxtend: 0.23.4
2026-01-30 10:13:03,493:INFO:       statsforecast: 1.5.0
2026-01-30 10:13:03,493:INFO:        tune_sklearn: Not installed
2026-01-30 10:13:03,493:INFO:                 ray: Not installed
2026-01-30 10:13:03,493:INFO:            hyperopt: 0.2.7
2026-01-30 10:13:03,493:INFO:              optuna: 4.6.0
2026-01-30 10:13:03,493:INFO:               skopt: 0.10.2
2026-01-30 10:13:03,495:INFO:              mlflow: 3.8.1
2026-01-30 10:13:03,495:INFO:              gradio: 6.3.0
2026-01-30 10:13:03,495:INFO:             fastapi: 0.128.0
2026-01-30 10:13:03,495:INFO:             uvicorn: 0.40.0
2026-01-30 10:13:03,495:INFO:              m2cgen: 0.10.0
2026-01-30 10:13:03,495:INFO:           evidently: 0.4.40
2026-01-30 10:13:03,495:INFO:               fugue: 0.8.7
2026-01-30 10:13:03,496:INFO:           streamlit: Not installed
2026-01-30 10:13:03,496:INFO:             prophet: Not installed
2026-01-30 10:13:03,496:INFO:None
2026-01-30 10:13:03,496:INFO:Set up data.
2026-01-30 10:13:03,641:INFO:Set up folding strategy.
2026-01-30 10:13:03,641:INFO:Set up train/test split.
2026-01-30 10:13:03,878:INFO:Set up index.
2026-01-30 10:13:03,889:INFO:Assigning column types.
2026-01-30 10:13:04,052:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 10:13:04,083:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 10:13:04,084:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:13:04,104:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:13:04,104:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:13:04,137:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 10:13:04,137:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:13:04,158:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:13:04,159:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:13:04,160:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 10:13:04,192:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:13:04,211:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:13:04,212:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:13:04,245:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:13:04,265:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:13:04,267:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:13:04,267:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 10:13:04,317:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:13:04,318:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:13:04,370:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:13:04,370:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:13:04,372:INFO:Preparing preprocessing pipeline...
2026-01-30 10:13:04,402:INFO:Set up simple imputation.
2026-01-30 10:13:04,403:INFO:Set up feature normalization.
2026-01-30 10:13:04,721:INFO:Finished creating preprocessing pipeline.
2026-01-30 10:13:04,724:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 10:13:04,725:INFO:Creating final display dataframe.
2026-01-30 10:13:05,444:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 28)
4        Transformed data shape      (482669, 28)
5   Transformed train set shape      (337868, 28)
6    Transformed test set shape      (144801, 28)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              e91e
2026-01-30 10:13:05,496:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:13:05,496:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:13:05,551:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:13:05,551:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:13:05,552:INFO:setup() successfully completed in 2.08s...............
2026-01-30 10:13:05,552:INFO:Initializing compare_models()
2026-01-30 10:13:05,553:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 10:13:05,553:INFO:Checking exceptions
2026-01-30 10:13:05,678:INFO:Preparing display monitor
2026-01-30 10:13:05,681:INFO:Initializing Logistic Regression
2026-01-30 10:13:05,682:INFO:Total runtime is 1.659393310546875e-05 minutes
2026-01-30 10:13:05,682:INFO:SubProcess create_model() called ==================================
2026-01-30 10:13:05,682:INFO:Initializing create_model()
2026-01-30 10:13:05,682:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04EFD5490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:13:05,682:INFO:Checking exceptions
2026-01-30 10:13:05,682:INFO:Importing libraries
2026-01-30 10:13:05,682:INFO:Copying training dataset
2026-01-30 10:13:05,889:INFO:Defining folds
2026-01-30 10:13:05,889:INFO:Declaring metric variables
2026-01-30 10:13:05,889:INFO:Importing untrained model
2026-01-30 10:13:05,889:INFO:Logistic Regression Imported successfully
2026-01-30 10:13:05,889:INFO:Starting cross validation
2026-01-30 10:13:05,890:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:13:09,628:INFO:Calculating mean and std
2026-01-30 10:13:09,630:INFO:Creating metrics dataframe
2026-01-30 10:13:09,633:INFO:Uploading results into container
2026-01-30 10:13:09,634:INFO:Uploading model into container now
2026-01-30 10:13:09,635:INFO:_master_model_container: 1
2026-01-30 10:13:09,635:INFO:_display_container: 2
2026-01-30 10:13:09,636:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 10:13:09,636:INFO:create_model() successfully completed......................................
2026-01-30 10:13:09,814:INFO:SubProcess create_model() end ==================================
2026-01-30 10:13:09,814:INFO:Creating metrics dataframe
2026-01-30 10:13:09,816:INFO:Initializing Decision Tree Classifier
2026-01-30 10:13:09,816:INFO:Total runtime is 0.06890580654144286 minutes
2026-01-30 10:13:09,817:INFO:SubProcess create_model() called ==================================
2026-01-30 10:13:09,817:INFO:Initializing create_model()
2026-01-30 10:13:09,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04EFD5490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:13:09,817:INFO:Checking exceptions
2026-01-30 10:13:09,817:INFO:Importing libraries
2026-01-30 10:13:09,817:INFO:Copying training dataset
2026-01-30 10:13:10,037:INFO:Defining folds
2026-01-30 10:13:10,037:INFO:Declaring metric variables
2026-01-30 10:13:10,038:INFO:Importing untrained model
2026-01-30 10:13:10,038:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:13:10,039:INFO:Starting cross validation
2026-01-30 10:13:10,039:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:13:14,147:INFO:Calculating mean and std
2026-01-30 10:13:14,148:INFO:Creating metrics dataframe
2026-01-30 10:13:14,150:INFO:Uploading results into container
2026-01-30 10:13:14,150:INFO:Uploading model into container now
2026-01-30 10:13:14,150:INFO:_master_model_container: 2
2026-01-30 10:13:14,150:INFO:_display_container: 2
2026-01-30 10:13:14,151:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:13:14,151:INFO:create_model() successfully completed......................................
2026-01-30 10:13:14,327:INFO:SubProcess create_model() end ==================================
2026-01-30 10:13:14,327:INFO:Creating metrics dataframe
2026-01-30 10:13:14,330:INFO:Initializing Random Forest Classifier
2026-01-30 10:13:14,331:INFO:Total runtime is 0.1441625396410624 minutes
2026-01-30 10:13:14,331:INFO:SubProcess create_model() called ==================================
2026-01-30 10:13:14,331:INFO:Initializing create_model()
2026-01-30 10:13:14,331:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04EFD5490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:13:14,331:INFO:Checking exceptions
2026-01-30 10:13:14,331:INFO:Importing libraries
2026-01-30 10:13:14,331:INFO:Copying training dataset
2026-01-30 10:13:14,538:INFO:Defining folds
2026-01-30 10:13:14,538:INFO:Declaring metric variables
2026-01-30 10:13:14,539:INFO:Importing untrained model
2026-01-30 10:13:14,539:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:13:14,539:INFO:Starting cross validation
2026-01-30 10:13:14,540:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:13:36,189:INFO:Calculating mean and std
2026-01-30 10:13:36,192:INFO:Creating metrics dataframe
2026-01-30 10:13:36,193:INFO:Uploading results into container
2026-01-30 10:13:36,193:INFO:Uploading model into container now
2026-01-30 10:13:36,193:INFO:_master_model_container: 3
2026-01-30 10:13:36,193:INFO:_display_container: 2
2026-01-30 10:13:36,198:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:13:36,198:INFO:create_model() successfully completed......................................
2026-01-30 10:13:36,437:INFO:SubProcess create_model() end ==================================
2026-01-30 10:13:36,437:INFO:Creating metrics dataframe
2026-01-30 10:13:36,440:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 10:13:36,440:INFO:Total runtime is 0.5126409689585367 minutes
2026-01-30 10:13:36,440:INFO:SubProcess create_model() called ==================================
2026-01-30 10:13:36,440:INFO:Initializing create_model()
2026-01-30 10:13:36,441:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04EFD5490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:13:36,441:INFO:Checking exceptions
2026-01-30 10:13:36,441:INFO:Importing libraries
2026-01-30 10:13:36,441:INFO:Copying training dataset
2026-01-30 10:13:36,705:INFO:Defining folds
2026-01-30 10:13:36,706:INFO:Declaring metric variables
2026-01-30 10:13:36,706:INFO:Importing untrained model
2026-01-30 10:13:36,707:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:13:36,707:INFO:Starting cross validation
2026-01-30 10:13:36,708:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:13:42,306:INFO:Calculating mean and std
2026-01-30 10:13:42,309:INFO:Creating metrics dataframe
2026-01-30 10:13:42,312:INFO:Uploading results into container
2026-01-30 10:13:42,313:INFO:Uploading model into container now
2026-01-30 10:13:42,314:INFO:_master_model_container: 4
2026-01-30 10:13:42,314:INFO:_display_container: 2
2026-01-30 10:13:42,315:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:13:42,315:INFO:create_model() successfully completed......................................
2026-01-30 10:13:42,578:INFO:SubProcess create_model() end ==================================
2026-01-30 10:13:42,578:INFO:Creating metrics dataframe
2026-01-30 10:13:42,582:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 10:13:42,584:INFO:Initializing create_model()
2026-01-30 10:13:42,584:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:13:42,584:INFO:Checking exceptions
2026-01-30 10:13:42,585:INFO:Importing libraries
2026-01-30 10:13:42,585:INFO:Copying training dataset
2026-01-30 10:13:42,890:INFO:Defining folds
2026-01-30 10:13:42,890:INFO:Declaring metric variables
2026-01-30 10:13:42,890:INFO:Importing untrained model
2026-01-30 10:13:42,890:INFO:Declaring custom model
2026-01-30 10:13:42,891:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:13:42,892:INFO:Cross validation set to False
2026-01-30 10:13:42,893:INFO:Fitting Model
2026-01-30 10:13:53,868:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:13:53,869:INFO:create_model() successfully completed......................................
2026-01-30 10:13:54,076:INFO:Initializing create_model()
2026-01-30 10:13:54,076:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:13:54,076:INFO:Checking exceptions
2026-01-30 10:13:54,078:INFO:Importing libraries
2026-01-30 10:13:54,078:INFO:Copying training dataset
2026-01-30 10:13:54,308:INFO:Defining folds
2026-01-30 10:13:54,308:INFO:Declaring metric variables
2026-01-30 10:13:54,309:INFO:Importing untrained model
2026-01-30 10:13:54,309:INFO:Declaring custom model
2026-01-30 10:13:54,311:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:13:54,311:INFO:Cross validation set to False
2026-01-30 10:13:54,311:INFO:Fitting Model
2026-01-30 10:13:55,388:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:13:55,454:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015382 seconds.
2026-01-30 10:13:55,454:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:13:55,455:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:13:55,455:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:13:55,456:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:13:55,460:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:13:55,460:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:13:56,695:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:13:56,696:INFO:create_model() successfully completed......................................
2026-01-30 10:13:56,955:INFO:Initializing create_model()
2026-01-30 10:13:56,955:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:13:56,955:INFO:Checking exceptions
2026-01-30 10:13:56,957:INFO:Importing libraries
2026-01-30 10:13:56,957:INFO:Copying training dataset
2026-01-30 10:13:57,220:INFO:Defining folds
2026-01-30 10:13:57,220:INFO:Declaring metric variables
2026-01-30 10:13:57,220:INFO:Importing untrained model
2026-01-30 10:13:57,220:INFO:Declaring custom model
2026-01-30 10:13:57,221:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:13:57,221:INFO:Cross validation set to False
2026-01-30 10:13:57,222:INFO:Fitting Model
2026-01-30 10:14:00,450:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:14:00,450:INFO:create_model() successfully completed......................................
2026-01-30 10:14:00,609:INFO:_master_model_container: 4
2026-01-30 10:14:00,609:INFO:_display_container: 2
2026-01-30 10:14:00,611:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 10:14:00,611:INFO:compare_models() successfully completed......................................
2026-01-30 10:14:00,628:INFO:Initializing tune_model()
2026-01-30 10:14:00,628:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:14:00,628:INFO:Checking exceptions
2026-01-30 10:14:00,717:INFO:Copying training dataset
2026-01-30 10:14:00,887:INFO:Checking base model
2026-01-30 10:14:00,888:INFO:Base model : Random Forest Classifier
2026-01-30 10:14:00,888:INFO:Declaring metric variables
2026-01-30 10:14:00,889:INFO:Defining Hyperparameters
2026-01-30 10:14:01,045:INFO:Tuning with n_jobs=-1
2026-01-30 10:14:01,045:INFO:Initializing RandomizedSearchCV
2026-01-30 10:16:54,746:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 10:16:54,746:INFO:Hyperparameter search completed
2026-01-30 10:16:54,746:INFO:SubProcess create_model() called ==================================
2026-01-30 10:16:54,746:INFO:Initializing create_model()
2026-01-30 10:16:54,746:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C86B010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 10:16:54,746:INFO:Checking exceptions
2026-01-30 10:16:54,746:INFO:Importing libraries
2026-01-30 10:16:54,746:INFO:Copying training dataset
2026-01-30 10:16:55,039:INFO:Defining folds
2026-01-30 10:16:55,039:INFO:Declaring metric variables
2026-01-30 10:16:55,039:INFO:Importing untrained model
2026-01-30 10:16:55,039:INFO:Declaring custom model
2026-01-30 10:16:55,039:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:16:55,039:INFO:Starting cross validation
2026-01-30 10:16:55,039:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:17:27,418:INFO:Calculating mean and std
2026-01-30 10:17:27,420:INFO:Creating metrics dataframe
2026-01-30 10:17:27,422:INFO:Finalizing model
2026-01-30 10:17:43,154:INFO:Uploading results into container
2026-01-30 10:17:43,155:INFO:Uploading model into container now
2026-01-30 10:17:43,156:INFO:_master_model_container: 5
2026-01-30 10:17:43,156:INFO:_display_container: 3
2026-01-30 10:17:43,158:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:17:43,158:INFO:create_model() successfully completed......................................
2026-01-30 10:17:43,328:INFO:SubProcess create_model() end ==================================
2026-01-30 10:17:43,328:INFO:choose_better activated
2026-01-30 10:17:43,328:INFO:SubProcess create_model() called ==================================
2026-01-30 10:17:43,329:INFO:Initializing create_model()
2026-01-30 10:17:43,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:17:43,329:INFO:Checking exceptions
2026-01-30 10:17:43,329:INFO:Importing libraries
2026-01-30 10:17:43,330:INFO:Copying training dataset
2026-01-30 10:17:43,543:INFO:Defining folds
2026-01-30 10:17:43,544:INFO:Declaring metric variables
2026-01-30 10:17:43,544:INFO:Importing untrained model
2026-01-30 10:17:43,544:INFO:Declaring custom model
2026-01-30 10:17:43,544:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:17:43,545:INFO:Starting cross validation
2026-01-30 10:17:43,545:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:18:06,409:INFO:Calculating mean and std
2026-01-30 10:18:06,410:INFO:Creating metrics dataframe
2026-01-30 10:18:06,411:INFO:Finalizing model
2026-01-30 10:18:17,025:INFO:Uploading results into container
2026-01-30 10:18:17,027:INFO:Uploading model into container now
2026-01-30 10:18:17,027:INFO:_master_model_container: 6
2026-01-30 10:18:17,028:INFO:_display_container: 4
2026-01-30 10:18:17,029:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:18:17,029:INFO:create_model() successfully completed......................................
2026-01-30 10:18:17,231:INFO:SubProcess create_model() end ==================================
2026-01-30 10:18:17,232:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9985
2026-01-30 10:18:17,232:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9894
2026-01-30 10:18:17,232:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 10:18:17,233:INFO:choose_better completed
2026-01-30 10:18:17,233:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:18:17,236:INFO:_master_model_container: 6
2026-01-30 10:18:17,236:INFO:_display_container: 3
2026-01-30 10:18:17,237:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:18:17,237:INFO:tune_model() successfully completed......................................
2026-01-30 10:18:17,407:INFO:Initializing tune_model()
2026-01-30 10:18:17,407:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:18:17,408:INFO:Checking exceptions
2026-01-30 10:18:17,487:INFO:Copying training dataset
2026-01-30 10:18:17,645:INFO:Checking base model
2026-01-30 10:18:17,646:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 10:18:17,647:INFO:Declaring metric variables
2026-01-30 10:18:17,647:INFO:Defining Hyperparameters
2026-01-30 10:18:17,823:INFO:Tuning with n_jobs=-1
2026-01-30 10:18:17,823:INFO:Initializing RandomizedSearchCV
2026-01-30 10:19:02,500:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 10:19:02,501:INFO:Hyperparameter search completed
2026-01-30 10:19:02,502:INFO:SubProcess create_model() called ==================================
2026-01-30 10:19:02,505:INFO:Initializing create_model()
2026-01-30 10:19:02,505:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0D02B0E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 10:19:02,506:INFO:Checking exceptions
2026-01-30 10:19:02,506:INFO:Importing libraries
2026-01-30 10:19:02,506:INFO:Copying training dataset
2026-01-30 10:19:02,908:INFO:Defining folds
2026-01-30 10:19:02,908:INFO:Declaring metric variables
2026-01-30 10:19:02,908:INFO:Importing untrained model
2026-01-30 10:19:02,909:INFO:Declaring custom model
2026-01-30 10:19:02,911:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:19:02,911:INFO:Starting cross validation
2026-01-30 10:19:02,912:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:19:14,173:INFO:Calculating mean and std
2026-01-30 10:19:14,174:INFO:Creating metrics dataframe
2026-01-30 10:19:14,174:INFO:Finalizing model
2026-01-30 10:19:14,938:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:19:14,938:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:19:14,938:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:19:15,152:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:19:15,154:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:19:15,154:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:19:15,154:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:19:15,207:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013022 seconds.
2026-01-30 10:19:15,207:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:19:15,207:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:19:15,209:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:19:15,210:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:19:15,215:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:19:15,215:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:19:18,895:INFO:Uploading results into container
2026-01-30 10:19:18,897:INFO:Uploading model into container now
2026-01-30 10:19:18,897:INFO:_master_model_container: 7
2026-01-30 10:19:18,899:INFO:_display_container: 4
2026-01-30 10:19:18,899:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:19:18,901:INFO:create_model() successfully completed......................................
2026-01-30 10:19:19,104:INFO:SubProcess create_model() end ==================================
2026-01-30 10:19:19,104:INFO:choose_better activated
2026-01-30 10:19:19,104:INFO:SubProcess create_model() called ==================================
2026-01-30 10:19:19,104:INFO:Initializing create_model()
2026-01-30 10:19:19,104:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:19:19,104:INFO:Checking exceptions
2026-01-30 10:19:19,104:INFO:Importing libraries
2026-01-30 10:19:19,104:INFO:Copying training dataset
2026-01-30 10:19:19,336:INFO:Defining folds
2026-01-30 10:19:19,336:INFO:Declaring metric variables
2026-01-30 10:19:19,336:INFO:Importing untrained model
2026-01-30 10:19:19,336:INFO:Declaring custom model
2026-01-30 10:19:19,336:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:19:19,336:INFO:Starting cross validation
2026-01-30 10:19:19,336:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:19:24,241:INFO:Calculating mean and std
2026-01-30 10:19:24,241:INFO:Creating metrics dataframe
2026-01-30 10:19:24,241:INFO:Finalizing model
2026-01-30 10:19:25,128:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:19:25,194:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012585 seconds.
2026-01-30 10:19:25,194:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:19:25,194:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:19:25,194:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:19:25,194:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:19:25,198:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:19:25,198:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:19:26,108:INFO:Uploading results into container
2026-01-30 10:19:26,108:INFO:Uploading model into container now
2026-01-30 10:19:26,110:INFO:_master_model_container: 8
2026-01-30 10:19:26,110:INFO:_display_container: 5
2026-01-30 10:19:26,110:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:19:26,110:INFO:create_model() successfully completed......................................
2026-01-30 10:19:26,330:INFO:SubProcess create_model() end ==================================
2026-01-30 10:19:26,330:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9945
2026-01-30 10:19:26,330:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9987
2026-01-30 10:19:26,330:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 10:19:26,330:INFO:choose_better completed
2026-01-30 10:19:26,330:INFO:_master_model_container: 8
2026-01-30 10:19:26,330:INFO:_display_container: 4
2026-01-30 10:19:26,330:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:19:26,330:INFO:tune_model() successfully completed......................................
2026-01-30 10:19:26,486:INFO:Initializing tune_model()
2026-01-30 10:19:26,486:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:19:26,486:INFO:Checking exceptions
2026-01-30 10:19:26,574:INFO:Copying training dataset
2026-01-30 10:19:26,721:INFO:Checking base model
2026-01-30 10:19:26,721:INFO:Base model : Decision Tree Classifier
2026-01-30 10:19:26,721:INFO:Declaring metric variables
2026-01-30 10:19:26,721:INFO:Defining Hyperparameters
2026-01-30 10:19:26,871:INFO:Tuning with n_jobs=-1
2026-01-30 10:19:26,871:INFO:Initializing RandomizedSearchCV
2026-01-30 10:19:35,453:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 10:19:35,454:INFO:Hyperparameter search completed
2026-01-30 10:19:35,454:INFO:SubProcess create_model() called ==================================
2026-01-30 10:19:35,456:INFO:Initializing create_model()
2026-01-30 10:19:35,456:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C71DB810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 10:19:35,456:INFO:Checking exceptions
2026-01-30 10:19:35,456:INFO:Importing libraries
2026-01-30 10:19:35,456:INFO:Copying training dataset
2026-01-30 10:19:35,687:INFO:Defining folds
2026-01-30 10:19:35,687:INFO:Declaring metric variables
2026-01-30 10:19:35,687:INFO:Importing untrained model
2026-01-30 10:19:35,687:INFO:Declaring custom model
2026-01-30 10:19:35,687:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:19:35,687:INFO:Starting cross validation
2026-01-30 10:19:35,687:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:19:38,688:INFO:Calculating mean and std
2026-01-30 10:19:38,691:INFO:Creating metrics dataframe
2026-01-30 10:19:38,693:INFO:Finalizing model
2026-01-30 10:19:40,703:INFO:Uploading results into container
2026-01-30 10:19:40,703:INFO:Uploading model into container now
2026-01-30 10:19:40,703:INFO:_master_model_container: 9
2026-01-30 10:19:40,703:INFO:_display_container: 5
2026-01-30 10:19:40,703:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:19:40,703:INFO:create_model() successfully completed......................................
2026-01-30 10:19:40,853:INFO:SubProcess create_model() end ==================================
2026-01-30 10:19:40,853:INFO:choose_better activated
2026-01-30 10:19:40,853:INFO:SubProcess create_model() called ==================================
2026-01-30 10:19:40,853:INFO:Initializing create_model()
2026-01-30 10:19:40,853:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:19:40,853:INFO:Checking exceptions
2026-01-30 10:19:40,853:INFO:Importing libraries
2026-01-30 10:19:40,853:INFO:Copying training dataset
2026-01-30 10:19:41,053:INFO:Defining folds
2026-01-30 10:19:41,053:INFO:Declaring metric variables
2026-01-30 10:19:41,053:INFO:Importing untrained model
2026-01-30 10:19:41,053:INFO:Declaring custom model
2026-01-30 10:19:41,053:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:19:41,053:INFO:Starting cross validation
2026-01-30 10:19:41,053:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:19:44,805:INFO:Calculating mean and std
2026-01-30 10:19:44,808:INFO:Creating metrics dataframe
2026-01-30 10:19:44,810:INFO:Finalizing model
2026-01-30 10:19:47,703:INFO:Uploading results into container
2026-01-30 10:19:47,703:INFO:Uploading model into container now
2026-01-30 10:19:47,703:INFO:_master_model_container: 10
2026-01-30 10:19:47,703:INFO:_display_container: 6
2026-01-30 10:19:47,703:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:19:47,703:INFO:create_model() successfully completed......................................
2026-01-30 10:19:47,853:INFO:SubProcess create_model() end ==================================
2026-01-30 10:19:47,853:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.989
2026-01-30 10:19:47,853:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9826
2026-01-30 10:19:47,853:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 10:19:47,853:INFO:choose_better completed
2026-01-30 10:19:47,853:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:19:47,853:INFO:_master_model_container: 10
2026-01-30 10:19:47,853:INFO:_display_container: 5
2026-01-30 10:19:47,853:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:19:47,853:INFO:tune_model() successfully completed......................................
2026-01-30 10:19:48,047:INFO:Initializing predict_model()
2026-01-30 10:19:48,047:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0482014D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A06E850F40>)
2026-01-30 10:19:48,047:INFO:Checking exceptions
2026-01-30 10:19:48,047:INFO:Preloading libraries
2026-01-30 10:19:48,047:INFO:Set up data.
2026-01-30 10:21:10,305:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\3860568932.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 10:21:12,595:INFO:PyCaret ClassificationExperiment
2026-01-30 10:21:12,596:INFO:Logging name: clf-default-name
2026-01-30 10:21:12,596:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 10:21:12,596:INFO:version 3.3.2
2026-01-30 10:21:12,596:INFO:Initializing setup()
2026-01-30 10:21:12,596:INFO:self.USI: 145c
2026-01-30 10:21:12,598:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 10:21:12,598:INFO:Checking environment
2026-01-30 10:21:12,598:INFO:python_version: 3.11.11
2026-01-30 10:21:12,599:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 10:21:12,599:INFO:machine: AMD64
2026-01-30 10:21:12,599:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 10:21:12,599:INFO:Memory: svmem(total=34009374720, available=10333499392, percent=69.6, used=23675875328, free=10333499392)
2026-01-30 10:21:12,600:INFO:Physical Core: 12
2026-01-30 10:21:12,600:INFO:Logical Core: 16
2026-01-30 10:21:12,600:INFO:Checking libraries
2026-01-30 10:21:12,600:INFO:System:
2026-01-30 10:21:12,600:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 10:21:12,600:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 10:21:12,601:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 10:21:12,602:INFO:PyCaret required dependencies:
2026-01-30 10:21:12,602:INFO:                 pip: 25.0
2026-01-30 10:21:12,602:INFO:          setuptools: 75.8.0
2026-01-30 10:21:12,602:INFO:             pycaret: 3.3.2
2026-01-30 10:21:12,602:INFO:             IPython: 9.9.0
2026-01-30 10:21:12,602:INFO:          ipywidgets: 8.1.8
2026-01-30 10:21:12,603:INFO:                tqdm: 4.67.1
2026-01-30 10:21:12,603:INFO:               numpy: 1.26.4
2026-01-30 10:21:12,603:INFO:              pandas: 2.1.4
2026-01-30 10:21:12,603:INFO:              jinja2: 3.1.6
2026-01-30 10:21:12,603:INFO:               scipy: 1.11.4
2026-01-30 10:21:12,603:INFO:              joblib: 1.3.2
2026-01-30 10:21:12,603:INFO:             sklearn: 1.4.2
2026-01-30 10:21:12,604:INFO:                pyod: 2.0.6
2026-01-30 10:21:12,604:INFO:            imblearn: 0.14.1
2026-01-30 10:21:12,604:INFO:   category_encoders: 2.7.0
2026-01-30 10:21:12,605:INFO:            lightgbm: 4.6.0
2026-01-30 10:21:12,606:INFO:               numba: 0.62.1
2026-01-30 10:21:12,606:INFO:            requests: 2.32.3
2026-01-30 10:21:12,606:INFO:          matplotlib: 3.7.5
2026-01-30 10:21:12,606:INFO:          scikitplot: 0.3.7
2026-01-30 10:21:12,606:INFO:         yellowbrick: 1.5
2026-01-30 10:21:12,606:INFO:              plotly: 5.24.1
2026-01-30 10:21:12,607:INFO:    plotly-resampler: Not installed
2026-01-30 10:21:12,607:INFO:             kaleido: 1.2.0
2026-01-30 10:21:12,607:INFO:           schemdraw: 0.15
2026-01-30 10:21:12,607:INFO:         statsmodels: 0.14.6
2026-01-30 10:21:12,607:INFO:              sktime: 0.26.0
2026-01-30 10:21:12,607:INFO:               tbats: 1.1.3
2026-01-30 10:21:12,607:INFO:            pmdarima: 2.0.4
2026-01-30 10:21:12,608:INFO:              psutil: 7.2.1
2026-01-30 10:21:12,608:INFO:          markupsafe: 3.0.3
2026-01-30 10:21:12,608:INFO:             pickle5: Not installed
2026-01-30 10:21:12,608:INFO:         cloudpickle: 3.0.0
2026-01-30 10:21:12,609:INFO:         deprecation: 2.1.0
2026-01-30 10:21:12,609:INFO:              xxhash: 3.6.0
2026-01-30 10:21:12,609:INFO:           wurlitzer: Not installed
2026-01-30 10:21:12,609:INFO:PyCaret optional dependencies:
2026-01-30 10:21:12,609:INFO:                shap: 0.44.1
2026-01-30 10:21:12,610:INFO:           interpret: 0.7.3
2026-01-30 10:21:12,610:INFO:                umap: 0.5.7
2026-01-30 10:21:12,610:INFO:     ydata_profiling: 4.18.1
2026-01-30 10:21:12,610:INFO:  explainerdashboard: 0.5.1
2026-01-30 10:21:12,610:INFO:             autoviz: Not installed
2026-01-30 10:21:12,610:INFO:           fairlearn: 0.7.0
2026-01-30 10:21:12,611:INFO:          deepchecks: Not installed
2026-01-30 10:21:12,611:INFO:             xgboost: Not installed
2026-01-30 10:21:12,611:INFO:            catboost: 1.2.8
2026-01-30 10:21:12,611:INFO:              kmodes: 0.12.2
2026-01-30 10:21:12,611:INFO:             mlxtend: 0.23.4
2026-01-30 10:21:12,611:INFO:       statsforecast: 1.5.0
2026-01-30 10:21:12,611:INFO:        tune_sklearn: Not installed
2026-01-30 10:21:12,611:INFO:                 ray: Not installed
2026-01-30 10:21:12,612:INFO:            hyperopt: 0.2.7
2026-01-30 10:21:12,612:INFO:              optuna: 4.6.0
2026-01-30 10:21:12,613:INFO:               skopt: 0.10.2
2026-01-30 10:21:12,613:INFO:              mlflow: 3.8.1
2026-01-30 10:21:12,613:INFO:              gradio: 6.3.0
2026-01-30 10:21:12,613:INFO:             fastapi: 0.128.0
2026-01-30 10:21:12,613:INFO:             uvicorn: 0.40.0
2026-01-30 10:21:12,613:INFO:              m2cgen: 0.10.0
2026-01-30 10:21:12,614:INFO:           evidently: 0.4.40
2026-01-30 10:21:12,614:INFO:               fugue: 0.8.7
2026-01-30 10:21:12,614:INFO:           streamlit: Not installed
2026-01-30 10:21:12,614:INFO:             prophet: Not installed
2026-01-30 10:21:12,615:INFO:None
2026-01-30 10:21:12,615:INFO:Set up data.
2026-01-30 10:21:12,766:INFO:Set up folding strategy.
2026-01-30 10:21:12,766:INFO:Set up train/test split.
2026-01-30 10:21:13,022:INFO:Set up index.
2026-01-30 10:21:13,033:INFO:Assigning column types.
2026-01-30 10:21:13,185:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 10:21:13,202:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 10:21:13,202:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:21:13,219:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:21:13,219:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:21:13,252:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 10:21:13,252:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:21:13,272:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:21:13,285:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:21:13,285:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 10:21:13,302:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:21:13,335:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:21:13,335:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:21:13,352:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:21:13,385:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:21:13,385:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:21:13,386:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 10:21:13,435:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:21:13,435:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:21:13,468:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:21:13,484:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:21:13,486:INFO:Preparing preprocessing pipeline...
2026-01-30 10:21:13,502:INFO:Set up simple imputation.
2026-01-30 10:21:13,502:INFO:Set up feature normalization.
2026-01-30 10:21:13,801:INFO:Finished creating preprocessing pipeline.
2026-01-30 10:21:13,806:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 10:21:13,806:INFO:Creating final display dataframe.
2026-01-30 10:21:14,469:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 28)
4        Transformed data shape      (482669, 28)
5   Transformed train set shape      (337868, 28)
6    Transformed test set shape      (144801, 28)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              145c
2026-01-30 10:21:14,519:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:21:14,519:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:21:14,575:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:21:14,576:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:21:14,577:INFO:setup() successfully completed in 1.99s...............
2026-01-30 10:21:14,577:INFO:Initializing compare_models()
2026-01-30 10:21:14,577:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 10:21:14,577:INFO:Checking exceptions
2026-01-30 10:21:14,752:INFO:Preparing display monitor
2026-01-30 10:21:14,756:INFO:Initializing Logistic Regression
2026-01-30 10:21:14,758:INFO:Total runtime is 3.84370485941569e-05 minutes
2026-01-30 10:21:14,758:INFO:SubProcess create_model() called ==================================
2026-01-30 10:21:14,758:INFO:Initializing create_model()
2026-01-30 10:21:14,759:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04D4BB5D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:21:14,759:INFO:Checking exceptions
2026-01-30 10:21:14,759:INFO:Importing libraries
2026-01-30 10:21:14,759:INFO:Copying training dataset
2026-01-30 10:21:15,018:INFO:Defining folds
2026-01-30 10:21:15,035:INFO:Declaring metric variables
2026-01-30 10:21:15,035:INFO:Importing untrained model
2026-01-30 10:21:15,035:INFO:Logistic Regression Imported successfully
2026-01-30 10:21:15,035:INFO:Starting cross validation
2026-01-30 10:21:15,035:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:21:18,459:INFO:Calculating mean and std
2026-01-30 10:21:18,459:INFO:Creating metrics dataframe
2026-01-30 10:21:18,459:INFO:Uploading results into container
2026-01-30 10:21:18,459:INFO:Uploading model into container now
2026-01-30 10:21:18,459:INFO:_master_model_container: 1
2026-01-30 10:21:18,459:INFO:_display_container: 2
2026-01-30 10:21:18,459:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 10:21:18,459:INFO:create_model() successfully completed......................................
2026-01-30 10:21:18,684:INFO:SubProcess create_model() end ==================================
2026-01-30 10:21:18,689:INFO:Creating metrics dataframe
2026-01-30 10:21:18,693:INFO:Initializing Decision Tree Classifier
2026-01-30 10:21:18,693:INFO:Total runtime is 0.06562891801198324 minutes
2026-01-30 10:21:18,693:INFO:SubProcess create_model() called ==================================
2026-01-30 10:21:18,694:INFO:Initializing create_model()
2026-01-30 10:21:18,694:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04D4BB5D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:21:18,694:INFO:Checking exceptions
2026-01-30 10:21:18,694:INFO:Importing libraries
2026-01-30 10:21:18,694:INFO:Copying training dataset
2026-01-30 10:21:18,918:INFO:Defining folds
2026-01-30 10:21:18,918:INFO:Declaring metric variables
2026-01-30 10:21:18,918:INFO:Importing untrained model
2026-01-30 10:21:18,918:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:21:18,918:INFO:Starting cross validation
2026-01-30 10:21:18,918:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:21:22,798:INFO:Calculating mean and std
2026-01-30 10:21:22,802:INFO:Creating metrics dataframe
2026-01-30 10:21:22,802:INFO:Uploading results into container
2026-01-30 10:21:22,802:INFO:Uploading model into container now
2026-01-30 10:21:22,802:INFO:_master_model_container: 2
2026-01-30 10:21:22,802:INFO:_display_container: 2
2026-01-30 10:21:22,802:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:21:22,802:INFO:create_model() successfully completed......................................
2026-01-30 10:21:22,968:INFO:SubProcess create_model() end ==================================
2026-01-30 10:21:22,968:INFO:Creating metrics dataframe
2026-01-30 10:21:22,984:INFO:Initializing Random Forest Classifier
2026-01-30 10:21:22,985:INFO:Total runtime is 0.13715004920959473 minutes
2026-01-30 10:21:22,985:INFO:SubProcess create_model() called ==================================
2026-01-30 10:21:22,985:INFO:Initializing create_model()
2026-01-30 10:21:22,985:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04D4BB5D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:21:22,985:INFO:Checking exceptions
2026-01-30 10:21:22,985:INFO:Importing libraries
2026-01-30 10:21:22,985:INFO:Copying training dataset
2026-01-30 10:21:23,168:INFO:Defining folds
2026-01-30 10:21:23,184:INFO:Declaring metric variables
2026-01-30 10:21:23,184:INFO:Importing untrained model
2026-01-30 10:21:23,185:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:21:23,185:INFO:Starting cross validation
2026-01-30 10:21:23,185:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:21:42,090:INFO:Calculating mean and std
2026-01-30 10:21:42,093:INFO:Creating metrics dataframe
2026-01-30 10:21:42,096:INFO:Uploading results into container
2026-01-30 10:21:42,097:INFO:Uploading model into container now
2026-01-30 10:21:42,098:INFO:_master_model_container: 3
2026-01-30 10:21:42,098:INFO:_display_container: 2
2026-01-30 10:21:42,099:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:21:42,099:INFO:create_model() successfully completed......................................
2026-01-30 10:21:42,299:INFO:SubProcess create_model() end ==================================
2026-01-30 10:21:42,299:INFO:Creating metrics dataframe
2026-01-30 10:21:42,302:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 10:21:42,302:INFO:Total runtime is 0.4591042995452881 minutes
2026-01-30 10:21:42,302:INFO:SubProcess create_model() called ==================================
2026-01-30 10:21:42,302:INFO:Initializing create_model()
2026-01-30 10:21:42,302:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04D4BB5D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:21:42,302:INFO:Checking exceptions
2026-01-30 10:21:42,302:INFO:Importing libraries
2026-01-30 10:21:42,302:INFO:Copying training dataset
2026-01-30 10:21:42,659:INFO:Defining folds
2026-01-30 10:21:42,659:INFO:Declaring metric variables
2026-01-30 10:21:42,660:INFO:Importing untrained model
2026-01-30 10:21:42,660:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:21:42,661:INFO:Starting cross validation
2026-01-30 10:21:42,662:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:21:47,834:INFO:Calculating mean and std
2026-01-30 10:21:47,834:INFO:Creating metrics dataframe
2026-01-30 10:21:47,834:INFO:Uploading results into container
2026-01-30 10:21:47,834:INFO:Uploading model into container now
2026-01-30 10:21:47,834:INFO:_master_model_container: 4
2026-01-30 10:21:47,834:INFO:_display_container: 2
2026-01-30 10:21:47,834:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:21:47,834:INFO:create_model() successfully completed......................................
2026-01-30 10:21:48,002:INFO:SubProcess create_model() end ==================================
2026-01-30 10:21:48,002:INFO:Creating metrics dataframe
2026-01-30 10:21:48,002:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 10:21:48,002:INFO:Initializing create_model()
2026-01-30 10:21:48,002:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:21:48,002:INFO:Checking exceptions
2026-01-30 10:21:48,002:INFO:Importing libraries
2026-01-30 10:21:48,002:INFO:Copying training dataset
2026-01-30 10:21:48,204:INFO:Defining folds
2026-01-30 10:21:48,204:INFO:Declaring metric variables
2026-01-30 10:21:48,204:INFO:Importing untrained model
2026-01-30 10:21:48,204:INFO:Declaring custom model
2026-01-30 10:21:48,204:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:21:48,204:INFO:Cross validation set to False
2026-01-30 10:21:48,204:INFO:Fitting Model
2026-01-30 10:21:57,854:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:21:57,854:INFO:create_model() successfully completed......................................
2026-01-30 10:21:58,035:INFO:Initializing create_model()
2026-01-30 10:21:58,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:21:58,037:INFO:Checking exceptions
2026-01-30 10:21:58,037:INFO:Importing libraries
2026-01-30 10:21:58,037:INFO:Copying training dataset
2026-01-30 10:21:58,220:INFO:Defining folds
2026-01-30 10:21:58,220:INFO:Declaring metric variables
2026-01-30 10:21:58,220:INFO:Importing untrained model
2026-01-30 10:21:58,220:INFO:Declaring custom model
2026-01-30 10:21:58,220:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:21:58,220:INFO:Cross validation set to False
2026-01-30 10:21:58,220:INFO:Fitting Model
2026-01-30 10:21:59,128:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:21:59,195:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012663 seconds.
2026-01-30 10:21:59,197:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:21:59,197:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:21:59,197:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:21:59,197:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:21:59,199:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:21:59,199:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:22:00,091:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:22:00,093:INFO:create_model() successfully completed......................................
2026-01-30 10:22:00,323:INFO:Initializing create_model()
2026-01-30 10:22:00,323:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:22:00,323:INFO:Checking exceptions
2026-01-30 10:22:00,323:INFO:Importing libraries
2026-01-30 10:22:00,323:INFO:Copying training dataset
2026-01-30 10:22:00,541:INFO:Defining folds
2026-01-30 10:22:00,541:INFO:Declaring metric variables
2026-01-30 10:22:00,541:INFO:Importing untrained model
2026-01-30 10:22:00,541:INFO:Declaring custom model
2026-01-30 10:22:00,541:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:22:00,541:INFO:Cross validation set to False
2026-01-30 10:22:00,541:INFO:Fitting Model
2026-01-30 10:22:03,792:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:22:03,792:INFO:create_model() successfully completed......................................
2026-01-30 10:22:03,961:INFO:_master_model_container: 4
2026-01-30 10:22:03,961:INFO:_display_container: 2
2026-01-30 10:22:03,962:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 10:22:03,962:INFO:compare_models() successfully completed......................................
2026-01-30 10:22:03,981:INFO:Initializing tune_model()
2026-01-30 10:22:03,981:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:22:03,981:INFO:Checking exceptions
2026-01-30 10:22:04,054:INFO:Copying training dataset
2026-01-30 10:22:04,216:INFO:Checking base model
2026-01-30 10:22:04,216:INFO:Base model : Random Forest Classifier
2026-01-30 10:22:04,217:INFO:Declaring metric variables
2026-01-30 10:22:04,218:INFO:Defining Hyperparameters
2026-01-30 10:22:04,367:INFO:Tuning with n_jobs=-1
2026-01-30 10:22:04,367:INFO:Initializing RandomizedSearchCV
2026-01-30 10:24:50,387:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 10:24:50,387:INFO:Hyperparameter search completed
2026-01-30 10:24:50,387:INFO:SubProcess create_model() called ==================================
2026-01-30 10:24:50,387:INFO:Initializing create_model()
2026-01-30 10:24:50,387:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03CE1AC90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 10:24:50,387:INFO:Checking exceptions
2026-01-30 10:24:50,387:INFO:Importing libraries
2026-01-30 10:24:50,387:INFO:Copying training dataset
2026-01-30 10:24:50,615:INFO:Defining folds
2026-01-30 10:24:50,615:INFO:Declaring metric variables
2026-01-30 10:24:50,615:INFO:Importing untrained model
2026-01-30 10:24:50,615:INFO:Declaring custom model
2026-01-30 10:24:50,615:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:24:50,615:INFO:Starting cross validation
2026-01-30 10:24:50,615:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:25:23,142:INFO:Calculating mean and std
2026-01-30 10:25:23,143:INFO:Creating metrics dataframe
2026-01-30 10:25:23,145:INFO:Finalizing model
2026-01-30 10:25:39,015:INFO:Uploading results into container
2026-01-30 10:25:39,015:INFO:Uploading model into container now
2026-01-30 10:25:39,015:INFO:_master_model_container: 5
2026-01-30 10:25:39,025:INFO:_display_container: 3
2026-01-30 10:25:39,025:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:25:39,025:INFO:create_model() successfully completed......................................
2026-01-30 10:25:39,198:INFO:SubProcess create_model() end ==================================
2026-01-30 10:25:39,198:INFO:choose_better activated
2026-01-30 10:25:39,198:INFO:SubProcess create_model() called ==================================
2026-01-30 10:25:39,198:INFO:Initializing create_model()
2026-01-30 10:25:39,198:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:25:39,198:INFO:Checking exceptions
2026-01-30 10:25:39,214:INFO:Importing libraries
2026-01-30 10:25:39,214:INFO:Copying training dataset
2026-01-30 10:25:39,465:INFO:Defining folds
2026-01-30 10:25:39,465:INFO:Declaring metric variables
2026-01-30 10:25:39,465:INFO:Importing untrained model
2026-01-30 10:25:39,465:INFO:Declaring custom model
2026-01-30 10:25:39,465:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:25:39,465:INFO:Starting cross validation
2026-01-30 10:25:39,465:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:26:00,508:INFO:Calculating mean and std
2026-01-30 10:26:00,508:INFO:Creating metrics dataframe
2026-01-30 10:26:00,508:INFO:Finalizing model
2026-01-30 10:26:10,631:INFO:Uploading results into container
2026-01-30 10:26:10,631:INFO:Uploading model into container now
2026-01-30 10:26:10,631:INFO:_master_model_container: 6
2026-01-30 10:26:10,631:INFO:_display_container: 4
2026-01-30 10:26:10,631:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:26:10,631:INFO:create_model() successfully completed......................................
2026-01-30 10:26:10,814:INFO:SubProcess create_model() end ==================================
2026-01-30 10:26:10,814:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9985
2026-01-30 10:26:10,814:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9894
2026-01-30 10:26:10,814:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 10:26:10,814:INFO:choose_better completed
2026-01-30 10:26:10,814:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:26:10,814:INFO:_master_model_container: 6
2026-01-30 10:26:10,814:INFO:_display_container: 3
2026-01-30 10:26:10,814:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:26:10,814:INFO:tune_model() successfully completed......................................
2026-01-30 10:26:10,981:INFO:Initializing tune_model()
2026-01-30 10:26:10,981:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:26:10,981:INFO:Checking exceptions
2026-01-30 10:26:11,074:INFO:Copying training dataset
2026-01-30 10:26:11,198:INFO:Checking base model
2026-01-30 10:26:11,198:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 10:26:11,213:INFO:Declaring metric variables
2026-01-30 10:26:11,213:INFO:Defining Hyperparameters
2026-01-30 10:26:11,364:INFO:Tuning with n_jobs=-1
2026-01-30 10:26:11,364:INFO:Initializing RandomizedSearchCV
2026-01-30 10:26:56,428:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 10:26:56,428:INFO:Hyperparameter search completed
2026-01-30 10:26:56,428:INFO:SubProcess create_model() called ==================================
2026-01-30 10:26:56,428:INFO:Initializing create_model()
2026-01-30 10:26:56,428:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04825E010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 10:26:56,428:INFO:Checking exceptions
2026-01-30 10:26:56,428:INFO:Importing libraries
2026-01-30 10:26:56,428:INFO:Copying training dataset
2026-01-30 10:26:56,682:INFO:Defining folds
2026-01-30 10:26:56,682:INFO:Declaring metric variables
2026-01-30 10:26:56,682:INFO:Importing untrained model
2026-01-30 10:26:56,682:INFO:Declaring custom model
2026-01-30 10:26:56,686:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:26:56,686:INFO:Starting cross validation
2026-01-30 10:26:56,686:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:27:07,423:INFO:Calculating mean and std
2026-01-30 10:27:07,423:INFO:Creating metrics dataframe
2026-01-30 10:27:07,431:INFO:Finalizing model
2026-01-30 10:27:08,197:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:27:08,197:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:27:08,197:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:27:08,445:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:27:08,445:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:27:08,445:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:27:08,447:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:27:08,513:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016019 seconds.
2026-01-30 10:27:08,513:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:27:08,513:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:27:08,515:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:27:08,516:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:27:08,520:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:27:08,522:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:27:12,001:INFO:Uploading results into container
2026-01-30 10:27:12,003:INFO:Uploading model into container now
2026-01-30 10:27:12,003:INFO:_master_model_container: 7
2026-01-30 10:27:12,005:INFO:_display_container: 4
2026-01-30 10:27:12,005:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:27:12,007:INFO:create_model() successfully completed......................................
2026-01-30 10:27:12,236:INFO:SubProcess create_model() end ==================================
2026-01-30 10:27:12,238:INFO:choose_better activated
2026-01-30 10:27:12,238:INFO:SubProcess create_model() called ==================================
2026-01-30 10:27:12,238:INFO:Initializing create_model()
2026-01-30 10:27:12,238:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:27:12,240:INFO:Checking exceptions
2026-01-30 10:27:12,240:INFO:Importing libraries
2026-01-30 10:27:12,240:INFO:Copying training dataset
2026-01-30 10:27:12,447:INFO:Defining folds
2026-01-30 10:27:12,447:INFO:Declaring metric variables
2026-01-30 10:27:12,447:INFO:Importing untrained model
2026-01-30 10:27:12,447:INFO:Declaring custom model
2026-01-30 10:27:12,447:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:27:12,447:INFO:Starting cross validation
2026-01-30 10:27:12,447:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:27:17,530:INFO:Calculating mean and std
2026-01-30 10:27:17,530:INFO:Creating metrics dataframe
2026-01-30 10:27:17,530:INFO:Finalizing model
2026-01-30 10:27:18,457:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:27:18,505:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014214 seconds.
2026-01-30 10:27:18,505:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:27:18,505:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:27:18,505:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:27:18,505:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:27:18,509:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:27:18,509:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:27:19,411:INFO:Uploading results into container
2026-01-30 10:27:19,411:INFO:Uploading model into container now
2026-01-30 10:27:19,413:INFO:_master_model_container: 8
2026-01-30 10:27:19,413:INFO:_display_container: 5
2026-01-30 10:27:19,414:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:27:19,414:INFO:create_model() successfully completed......................................
2026-01-30 10:27:19,630:INFO:SubProcess create_model() end ==================================
2026-01-30 10:27:19,630:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9945
2026-01-30 10:27:19,630:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9987
2026-01-30 10:27:19,630:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 10:27:19,630:INFO:choose_better completed
2026-01-30 10:27:19,646:INFO:_master_model_container: 8
2026-01-30 10:27:19,646:INFO:_display_container: 4
2026-01-30 10:27:19,646:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:27:19,646:INFO:tune_model() successfully completed......................................
2026-01-30 10:27:19,815:INFO:Initializing tune_model()
2026-01-30 10:27:19,815:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:27:19,815:INFO:Checking exceptions
2026-01-30 10:27:19,904:INFO:Copying training dataset
2026-01-30 10:27:20,064:INFO:Checking base model
2026-01-30 10:27:20,064:INFO:Base model : Decision Tree Classifier
2026-01-30 10:27:20,064:INFO:Declaring metric variables
2026-01-30 10:27:20,064:INFO:Defining Hyperparameters
2026-01-30 10:27:20,234:INFO:Tuning with n_jobs=-1
2026-01-30 10:27:20,234:INFO:Initializing RandomizedSearchCV
2026-01-30 10:27:29,011:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 10:27:29,011:INFO:Hyperparameter search completed
2026-01-30 10:27:29,011:INFO:SubProcess create_model() called ==================================
2026-01-30 10:27:29,011:INFO:Initializing create_model()
2026-01-30 10:27:29,011:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C4F3150>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 10:27:29,011:INFO:Checking exceptions
2026-01-30 10:27:29,011:INFO:Importing libraries
2026-01-30 10:27:29,011:INFO:Copying training dataset
2026-01-30 10:27:29,231:INFO:Defining folds
2026-01-30 10:27:29,231:INFO:Declaring metric variables
2026-01-30 10:27:29,231:INFO:Importing untrained model
2026-01-30 10:27:29,231:INFO:Declaring custom model
2026-01-30 10:27:29,233:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:27:29,233:INFO:Starting cross validation
2026-01-30 10:27:29,233:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:27:32,303:INFO:Calculating mean and std
2026-01-30 10:27:32,303:INFO:Creating metrics dataframe
2026-01-30 10:27:32,309:INFO:Finalizing model
2026-01-30 10:27:34,346:INFO:Uploading results into container
2026-01-30 10:27:34,346:INFO:Uploading model into container now
2026-01-30 10:27:34,346:INFO:_master_model_container: 9
2026-01-30 10:27:34,346:INFO:_display_container: 5
2026-01-30 10:27:34,346:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:27:34,346:INFO:create_model() successfully completed......................................
2026-01-30 10:27:34,498:INFO:SubProcess create_model() end ==================================
2026-01-30 10:27:34,498:INFO:choose_better activated
2026-01-30 10:27:34,498:INFO:SubProcess create_model() called ==================================
2026-01-30 10:27:34,498:INFO:Initializing create_model()
2026-01-30 10:27:34,498:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:27:34,498:INFO:Checking exceptions
2026-01-30 10:27:34,498:INFO:Importing libraries
2026-01-30 10:27:34,498:INFO:Copying training dataset
2026-01-30 10:27:34,732:INFO:Defining folds
2026-01-30 10:27:34,732:INFO:Declaring metric variables
2026-01-30 10:27:34,732:INFO:Importing untrained model
2026-01-30 10:27:34,732:INFO:Declaring custom model
2026-01-30 10:27:34,733:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:27:34,733:INFO:Starting cross validation
2026-01-30 10:27:34,734:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:27:38,519:INFO:Calculating mean and std
2026-01-30 10:27:38,520:INFO:Creating metrics dataframe
2026-01-30 10:27:38,520:INFO:Finalizing model
2026-01-30 10:27:41,735:INFO:Uploading results into container
2026-01-30 10:27:41,736:INFO:Uploading model into container now
2026-01-30 10:27:41,736:INFO:_master_model_container: 10
2026-01-30 10:27:41,736:INFO:_display_container: 6
2026-01-30 10:27:41,736:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:27:41,736:INFO:create_model() successfully completed......................................
2026-01-30 10:27:41,897:INFO:SubProcess create_model() end ==================================
2026-01-30 10:27:41,898:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.989
2026-01-30 10:27:41,898:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9826
2026-01-30 10:27:41,898:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 10:27:41,898:INFO:choose_better completed
2026-01-30 10:27:41,898:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:27:41,900:INFO:_master_model_container: 10
2026-01-30 10:27:41,901:INFO:_display_container: 5
2026-01-30 10:27:41,901:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:27:41,901:INFO:tune_model() successfully completed......................................
2026-01-30 10:27:42,069:INFO:Initializing predict_model()
2026-01-30 10:27:42,069:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C299D1D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C9F760C0>)
2026-01-30 10:27:42,069:INFO:Checking exceptions
2026-01-30 10:27:42,069:INFO:Preloading libraries
2026-01-30 10:27:42,069:INFO:Set up data.
2026-01-30 10:27:42,079:INFO:Set up index.
2026-01-30 10:27:42,613:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_array_api.py:290: RuntimeWarning: invalid value encountered in cast

2026-01-30 10:29:40,062:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\503664258.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 10:29:42,260:INFO:PyCaret ClassificationExperiment
2026-01-30 10:29:42,260:INFO:Logging name: clf-default-name
2026-01-30 10:29:42,260:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 10:29:42,260:INFO:version 3.3.2
2026-01-30 10:29:42,260:INFO:Initializing setup()
2026-01-30 10:29:42,260:INFO:self.USI: 16e5
2026-01-30 10:29:42,260:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 10:29:42,260:INFO:Checking environment
2026-01-30 10:29:42,260:INFO:python_version: 3.11.11
2026-01-30 10:29:42,260:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 10:29:42,260:INFO:machine: AMD64
2026-01-30 10:29:42,260:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 10:29:42,261:INFO:Memory: svmem(total=34009374720, available=9681879040, percent=71.5, used=24327495680, free=9681879040)
2026-01-30 10:29:42,261:INFO:Physical Core: 12
2026-01-30 10:29:42,261:INFO:Logical Core: 16
2026-01-30 10:29:42,261:INFO:Checking libraries
2026-01-30 10:29:42,261:INFO:System:
2026-01-30 10:29:42,261:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 10:29:42,261:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 10:29:42,261:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 10:29:42,261:INFO:PyCaret required dependencies:
2026-01-30 10:29:42,261:INFO:                 pip: 25.0
2026-01-30 10:29:42,261:INFO:          setuptools: 75.8.0
2026-01-30 10:29:42,261:INFO:             pycaret: 3.3.2
2026-01-30 10:29:42,261:INFO:             IPython: 9.9.0
2026-01-30 10:29:42,261:INFO:          ipywidgets: 8.1.8
2026-01-30 10:29:42,261:INFO:                tqdm: 4.67.1
2026-01-30 10:29:42,261:INFO:               numpy: 1.26.4
2026-01-30 10:29:42,261:INFO:              pandas: 2.1.4
2026-01-30 10:29:42,261:INFO:              jinja2: 3.1.6
2026-01-30 10:29:42,261:INFO:               scipy: 1.11.4
2026-01-30 10:29:42,261:INFO:              joblib: 1.3.2
2026-01-30 10:29:42,261:INFO:             sklearn: 1.4.2
2026-01-30 10:29:42,262:INFO:                pyod: 2.0.6
2026-01-30 10:29:42,262:INFO:            imblearn: 0.14.1
2026-01-30 10:29:42,262:INFO:   category_encoders: 2.7.0
2026-01-30 10:29:42,262:INFO:            lightgbm: 4.6.0
2026-01-30 10:29:42,262:INFO:               numba: 0.62.1
2026-01-30 10:29:42,262:INFO:            requests: 2.32.3
2026-01-30 10:29:42,262:INFO:          matplotlib: 3.7.5
2026-01-30 10:29:42,263:INFO:          scikitplot: 0.3.7
2026-01-30 10:29:42,263:INFO:         yellowbrick: 1.5
2026-01-30 10:29:42,264:INFO:              plotly: 5.24.1
2026-01-30 10:29:42,264:INFO:    plotly-resampler: Not installed
2026-01-30 10:29:42,264:INFO:             kaleido: 1.2.0
2026-01-30 10:29:42,264:INFO:           schemdraw: 0.15
2026-01-30 10:29:42,264:INFO:         statsmodels: 0.14.6
2026-01-30 10:29:42,264:INFO:              sktime: 0.26.0
2026-01-30 10:29:42,264:INFO:               tbats: 1.1.3
2026-01-30 10:29:42,264:INFO:            pmdarima: 2.0.4
2026-01-30 10:29:42,264:INFO:              psutil: 7.2.1
2026-01-30 10:29:42,264:INFO:          markupsafe: 3.0.3
2026-01-30 10:29:42,264:INFO:             pickle5: Not installed
2026-01-30 10:29:42,264:INFO:         cloudpickle: 3.0.0
2026-01-30 10:29:42,264:INFO:         deprecation: 2.1.0
2026-01-30 10:29:42,264:INFO:              xxhash: 3.6.0
2026-01-30 10:29:42,264:INFO:           wurlitzer: Not installed
2026-01-30 10:29:42,264:INFO:PyCaret optional dependencies:
2026-01-30 10:29:42,264:INFO:                shap: 0.44.1
2026-01-30 10:29:42,264:INFO:           interpret: 0.7.3
2026-01-30 10:29:42,264:INFO:                umap: 0.5.7
2026-01-30 10:29:42,264:INFO:     ydata_profiling: 4.18.1
2026-01-30 10:29:42,265:INFO:  explainerdashboard: 0.5.1
2026-01-30 10:29:42,265:INFO:             autoviz: Not installed
2026-01-30 10:29:42,265:INFO:           fairlearn: 0.7.0
2026-01-30 10:29:42,265:INFO:          deepchecks: Not installed
2026-01-30 10:29:42,265:INFO:             xgboost: Not installed
2026-01-30 10:29:42,265:INFO:            catboost: 1.2.8
2026-01-30 10:29:42,265:INFO:              kmodes: 0.12.2
2026-01-30 10:29:42,265:INFO:             mlxtend: 0.23.4
2026-01-30 10:29:42,265:INFO:       statsforecast: 1.5.0
2026-01-30 10:29:42,265:INFO:        tune_sklearn: Not installed
2026-01-30 10:29:42,265:INFO:                 ray: Not installed
2026-01-30 10:29:42,265:INFO:            hyperopt: 0.2.7
2026-01-30 10:29:42,265:INFO:              optuna: 4.6.0
2026-01-30 10:29:42,265:INFO:               skopt: 0.10.2
2026-01-30 10:29:42,265:INFO:              mlflow: 3.8.1
2026-01-30 10:29:42,266:INFO:              gradio: 6.3.0
2026-01-30 10:29:42,266:INFO:             fastapi: 0.128.0
2026-01-30 10:29:42,266:INFO:             uvicorn: 0.40.0
2026-01-30 10:29:42,266:INFO:              m2cgen: 0.10.0
2026-01-30 10:29:42,266:INFO:           evidently: 0.4.40
2026-01-30 10:29:42,266:INFO:               fugue: 0.8.7
2026-01-30 10:29:42,266:INFO:           streamlit: Not installed
2026-01-30 10:29:42,266:INFO:             prophet: Not installed
2026-01-30 10:29:42,266:INFO:None
2026-01-30 10:29:42,266:INFO:Set up data.
2026-01-30 10:29:42,413:INFO:Set up folding strategy.
2026-01-30 10:29:42,413:INFO:Set up train/test split.
2026-01-30 10:29:42,631:INFO:Set up index.
2026-01-30 10:29:42,645:INFO:Assigning column types.
2026-01-30 10:29:42,800:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 10:29:42,830:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 10:29:42,830:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:29:42,849:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:29:42,849:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:29:42,882:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 10:29:42,882:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:29:42,906:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:29:42,906:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:29:42,906:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 10:29:42,944:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:29:42,961:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:29:42,961:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:29:43,000:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 10:29:43,022:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:29:43,022:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:29:43,024:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 10:29:43,078:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:29:43,078:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:29:43,144:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:29:43,144:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:29:43,145:INFO:Preparing preprocessing pipeline...
2026-01-30 10:29:43,177:INFO:Set up simple imputation.
2026-01-30 10:29:43,177:INFO:Set up feature normalization.
2026-01-30 10:29:43,473:INFO:Finished creating preprocessing pipeline.
2026-01-30 10:29:43,476:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 10:29:43,477:INFO:Creating final display dataframe.
2026-01-30 10:29:44,132:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 28)
4        Transformed data shape      (482669, 28)
5   Transformed train set shape      (337868, 28)
6    Transformed test set shape      (144801, 28)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              16e5
2026-01-30 10:29:44,178:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:29:44,178:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:29:44,232:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 10:29:44,232:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 10:29:44,232:INFO:setup() successfully completed in 1.99s...............
2026-01-30 10:29:44,232:INFO:Initializing compare_models()
2026-01-30 10:29:44,232:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 10:29:44,232:INFO:Checking exceptions
2026-01-30 10:29:44,365:INFO:Preparing display monitor
2026-01-30 10:29:44,378:INFO:Initializing Logistic Regression
2026-01-30 10:29:44,378:INFO:Total runtime is 0.0 minutes
2026-01-30 10:29:44,378:INFO:SubProcess create_model() called ==================================
2026-01-30 10:29:44,378:INFO:Initializing create_model()
2026-01-30 10:29:44,378:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A053B26C50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:29:44,378:INFO:Checking exceptions
2026-01-30 10:29:44,378:INFO:Importing libraries
2026-01-30 10:29:44,378:INFO:Copying training dataset
2026-01-30 10:29:44,595:INFO:Defining folds
2026-01-30 10:29:44,607:INFO:Declaring metric variables
2026-01-30 10:29:44,607:INFO:Importing untrained model
2026-01-30 10:29:44,610:INFO:Logistic Regression Imported successfully
2026-01-30 10:29:44,610:INFO:Starting cross validation
2026-01-30 10:29:44,611:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:29:48,600:INFO:Calculating mean and std
2026-01-30 10:29:48,602:INFO:Creating metrics dataframe
2026-01-30 10:29:48,606:INFO:Uploading results into container
2026-01-30 10:29:48,607:INFO:Uploading model into container now
2026-01-30 10:29:48,608:INFO:_master_model_container: 1
2026-01-30 10:29:48,608:INFO:_display_container: 2
2026-01-30 10:29:48,609:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 10:29:48,610:INFO:create_model() successfully completed......................................
2026-01-30 10:29:48,839:INFO:SubProcess create_model() end ==================================
2026-01-30 10:29:48,839:INFO:Creating metrics dataframe
2026-01-30 10:29:48,841:INFO:Initializing Decision Tree Classifier
2026-01-30 10:29:48,841:INFO:Total runtime is 0.07438475290934245 minutes
2026-01-30 10:29:48,841:INFO:SubProcess create_model() called ==================================
2026-01-30 10:29:48,841:INFO:Initializing create_model()
2026-01-30 10:29:48,843:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A053B26C50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:29:48,843:INFO:Checking exceptions
2026-01-30 10:29:48,843:INFO:Importing libraries
2026-01-30 10:29:48,843:INFO:Copying training dataset
2026-01-30 10:29:49,049:INFO:Defining folds
2026-01-30 10:29:49,050:INFO:Declaring metric variables
2026-01-30 10:29:49,050:INFO:Importing untrained model
2026-01-30 10:29:49,051:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:29:49,051:INFO:Starting cross validation
2026-01-30 10:29:49,052:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:29:53,220:INFO:Calculating mean and std
2026-01-30 10:29:53,220:INFO:Creating metrics dataframe
2026-01-30 10:29:53,220:INFO:Uploading results into container
2026-01-30 10:29:53,230:INFO:Uploading model into container now
2026-01-30 10:29:53,230:INFO:_master_model_container: 2
2026-01-30 10:29:53,230:INFO:_display_container: 2
2026-01-30 10:29:53,230:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:29:53,230:INFO:create_model() successfully completed......................................
2026-01-30 10:29:53,397:INFO:SubProcess create_model() end ==================================
2026-01-30 10:29:53,397:INFO:Creating metrics dataframe
2026-01-30 10:29:53,397:INFO:Initializing Random Forest Classifier
2026-01-30 10:29:53,397:INFO:Total runtime is 0.15030624866485595 minutes
2026-01-30 10:29:53,397:INFO:SubProcess create_model() called ==================================
2026-01-30 10:29:53,412:INFO:Initializing create_model()
2026-01-30 10:29:53,412:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A053B26C50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:29:53,412:INFO:Checking exceptions
2026-01-30 10:29:53,412:INFO:Importing libraries
2026-01-30 10:29:53,412:INFO:Copying training dataset
2026-01-30 10:29:53,640:INFO:Defining folds
2026-01-30 10:29:53,641:INFO:Declaring metric variables
2026-01-30 10:29:53,641:INFO:Importing untrained model
2026-01-30 10:29:53,642:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:29:53,642:INFO:Starting cross validation
2026-01-30 10:29:53,643:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:30:15,330:INFO:Calculating mean and std
2026-01-30 10:30:15,332:INFO:Creating metrics dataframe
2026-01-30 10:30:15,333:INFO:Uploading results into container
2026-01-30 10:30:15,334:INFO:Uploading model into container now
2026-01-30 10:30:15,334:INFO:_master_model_container: 3
2026-01-30 10:30:15,334:INFO:_display_container: 2
2026-01-30 10:30:15,335:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:30:15,335:INFO:create_model() successfully completed......................................
2026-01-30 10:30:15,528:INFO:SubProcess create_model() end ==================================
2026-01-30 10:30:15,528:INFO:Creating metrics dataframe
2026-01-30 10:30:15,530:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 10:30:15,531:INFO:Total runtime is 0.5191914717356364 minutes
2026-01-30 10:30:15,531:INFO:SubProcess create_model() called ==================================
2026-01-30 10:30:15,531:INFO:Initializing create_model()
2026-01-30 10:30:15,531:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A053B26C50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:30:15,531:INFO:Checking exceptions
2026-01-30 10:30:15,531:INFO:Importing libraries
2026-01-30 10:30:15,531:INFO:Copying training dataset
2026-01-30 10:30:15,731:INFO:Defining folds
2026-01-30 10:30:15,731:INFO:Declaring metric variables
2026-01-30 10:30:15,731:INFO:Importing untrained model
2026-01-30 10:30:15,731:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:30:15,731:INFO:Starting cross validation
2026-01-30 10:30:15,731:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:30:20,756:INFO:Calculating mean and std
2026-01-30 10:30:20,760:INFO:Creating metrics dataframe
2026-01-30 10:30:20,762:INFO:Uploading results into container
2026-01-30 10:30:20,762:INFO:Uploading model into container now
2026-01-30 10:30:20,764:INFO:_master_model_container: 4
2026-01-30 10:30:20,764:INFO:_display_container: 2
2026-01-30 10:30:20,764:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:30:20,764:INFO:create_model() successfully completed......................................
2026-01-30 10:30:20,931:INFO:SubProcess create_model() end ==================================
2026-01-30 10:30:20,931:INFO:Creating metrics dataframe
2026-01-30 10:30:20,931:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 10:30:20,931:INFO:Initializing create_model()
2026-01-30 10:30:20,931:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:30:20,931:INFO:Checking exceptions
2026-01-30 10:30:20,931:INFO:Importing libraries
2026-01-30 10:30:20,931:INFO:Copying training dataset
2026-01-30 10:30:21,158:INFO:Defining folds
2026-01-30 10:30:21,158:INFO:Declaring metric variables
2026-01-30 10:30:21,158:INFO:Importing untrained model
2026-01-30 10:30:21,159:INFO:Declaring custom model
2026-01-30 10:30:21,159:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:30:21,160:INFO:Cross validation set to False
2026-01-30 10:30:21,160:INFO:Fitting Model
2026-01-30 10:30:31,944:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:30:31,944:INFO:create_model() successfully completed......................................
2026-01-30 10:30:32,131:INFO:Initializing create_model()
2026-01-30 10:30:32,131:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:30:32,131:INFO:Checking exceptions
2026-01-30 10:30:32,131:INFO:Importing libraries
2026-01-30 10:30:32,131:INFO:Copying training dataset
2026-01-30 10:30:32,327:INFO:Defining folds
2026-01-30 10:30:32,327:INFO:Declaring metric variables
2026-01-30 10:30:32,327:INFO:Importing untrained model
2026-01-30 10:30:32,327:INFO:Declaring custom model
2026-01-30 10:30:32,327:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:30:32,327:INFO:Cross validation set to False
2026-01-30 10:30:32,327:INFO:Fitting Model
2026-01-30 10:30:33,228:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:30:33,294:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016880 seconds.
2026-01-30 10:30:33,294:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:30:33,294:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:30:33,295:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:30:33,296:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:30:33,298:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:30:33,299:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:30:34,233:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:30:34,233:INFO:create_model() successfully completed......................................
2026-01-30 10:30:34,476:INFO:Initializing create_model()
2026-01-30 10:30:34,476:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:30:34,476:INFO:Checking exceptions
2026-01-30 10:30:34,477:INFO:Importing libraries
2026-01-30 10:30:34,477:INFO:Copying training dataset
2026-01-30 10:30:34,677:INFO:Defining folds
2026-01-30 10:30:34,677:INFO:Declaring metric variables
2026-01-30 10:30:34,677:INFO:Importing untrained model
2026-01-30 10:30:34,677:INFO:Declaring custom model
2026-01-30 10:30:34,693:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:30:34,693:INFO:Cross validation set to False
2026-01-30 10:30:34,693:INFO:Fitting Model
2026-01-30 10:30:37,611:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:30:37,611:INFO:create_model() successfully completed......................................
2026-01-30 10:30:37,814:INFO:_master_model_container: 4
2026-01-30 10:30:37,814:INFO:_display_container: 2
2026-01-30 10:30:37,815:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 10:30:37,815:INFO:compare_models() successfully completed......................................
2026-01-30 10:30:37,836:INFO:Initializing tune_model()
2026-01-30 10:30:37,836:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:30:37,836:INFO:Checking exceptions
2026-01-30 10:30:37,932:INFO:Copying training dataset
2026-01-30 10:30:38,088:INFO:Checking base model
2026-01-30 10:30:38,088:INFO:Base model : Random Forest Classifier
2026-01-30 10:30:38,089:INFO:Declaring metric variables
2026-01-30 10:30:38,089:INFO:Defining Hyperparameters
2026-01-30 10:30:38,295:INFO:Tuning with n_jobs=-1
2026-01-30 10:30:38,295:INFO:Initializing RandomizedSearchCV
2026-01-30 10:33:26,929:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 10:33:26,930:INFO:Hyperparameter search completed
2026-01-30 10:33:26,931:INFO:SubProcess create_model() called ==================================
2026-01-30 10:33:26,932:INFO:Initializing create_model()
2026-01-30 10:33:26,932:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04D500ED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 10:33:26,932:INFO:Checking exceptions
2026-01-30 10:33:26,932:INFO:Importing libraries
2026-01-30 10:33:26,932:INFO:Copying training dataset
2026-01-30 10:33:27,310:INFO:Defining folds
2026-01-30 10:33:27,311:INFO:Declaring metric variables
2026-01-30 10:33:27,311:INFO:Importing untrained model
2026-01-30 10:33:27,311:INFO:Declaring custom model
2026-01-30 10:33:27,313:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:33:27,313:INFO:Starting cross validation
2026-01-30 10:33:27,314:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:33:59,581:INFO:Calculating mean and std
2026-01-30 10:33:59,582:INFO:Creating metrics dataframe
2026-01-30 10:33:59,585:INFO:Finalizing model
2026-01-30 10:34:16,224:INFO:Uploading results into container
2026-01-30 10:34:16,225:INFO:Uploading model into container now
2026-01-30 10:34:16,225:INFO:_master_model_container: 5
2026-01-30 10:34:16,226:INFO:_display_container: 3
2026-01-30 10:34:16,227:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:34:16,227:INFO:create_model() successfully completed......................................
2026-01-30 10:34:16,435:INFO:SubProcess create_model() end ==================================
2026-01-30 10:34:16,435:INFO:choose_better activated
2026-01-30 10:34:16,436:INFO:SubProcess create_model() called ==================================
2026-01-30 10:34:16,436:INFO:Initializing create_model()
2026-01-30 10:34:16,436:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:34:16,436:INFO:Checking exceptions
2026-01-30 10:34:16,437:INFO:Importing libraries
2026-01-30 10:34:16,437:INFO:Copying training dataset
2026-01-30 10:34:16,707:INFO:Defining folds
2026-01-30 10:34:16,707:INFO:Declaring metric variables
2026-01-30 10:34:16,707:INFO:Importing untrained model
2026-01-30 10:34:16,707:INFO:Declaring custom model
2026-01-30 10:34:16,708:INFO:Random Forest Classifier Imported successfully
2026-01-30 10:34:16,708:INFO:Starting cross validation
2026-01-30 10:34:16,709:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:34:37,749:INFO:Calculating mean and std
2026-01-30 10:34:37,750:INFO:Creating metrics dataframe
2026-01-30 10:34:37,752:INFO:Finalizing model
2026-01-30 10:34:47,818:INFO:Uploading results into container
2026-01-30 10:34:47,819:INFO:Uploading model into container now
2026-01-30 10:34:47,819:INFO:_master_model_container: 6
2026-01-30 10:34:47,819:INFO:_display_container: 4
2026-01-30 10:34:47,820:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:34:47,820:INFO:create_model() successfully completed......................................
2026-01-30 10:34:48,011:INFO:SubProcess create_model() end ==================================
2026-01-30 10:34:48,012:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9985
2026-01-30 10:34:48,012:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9894
2026-01-30 10:34:48,013:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 10:34:48,013:INFO:choose_better completed
2026-01-30 10:34:48,013:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:34:48,015:INFO:_master_model_container: 6
2026-01-30 10:34:48,015:INFO:_display_container: 3
2026-01-30 10:34:48,016:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 10:34:48,016:INFO:tune_model() successfully completed......................................
2026-01-30 10:34:48,194:INFO:Initializing tune_model()
2026-01-30 10:34:48,194:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:34:48,194:INFO:Checking exceptions
2026-01-30 10:34:48,280:INFO:Copying training dataset
2026-01-30 10:34:48,422:INFO:Checking base model
2026-01-30 10:34:48,423:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 10:34:48,424:INFO:Declaring metric variables
2026-01-30 10:34:48,424:INFO:Defining Hyperparameters
2026-01-30 10:34:48,595:INFO:Tuning with n_jobs=-1
2026-01-30 10:34:48,595:INFO:Initializing RandomizedSearchCV
2026-01-30 10:35:31,506:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 10:35:31,508:INFO:Hyperparameter search completed
2026-01-30 10:35:31,509:INFO:SubProcess create_model() called ==================================
2026-01-30 10:35:31,511:INFO:Initializing create_model()
2026-01-30 10:35:31,511:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03D5BDD90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 10:35:31,511:INFO:Checking exceptions
2026-01-30 10:35:31,512:INFO:Importing libraries
2026-01-30 10:35:31,512:INFO:Copying training dataset
2026-01-30 10:35:31,895:INFO:Defining folds
2026-01-30 10:35:31,895:INFO:Declaring metric variables
2026-01-30 10:35:31,896:INFO:Importing untrained model
2026-01-30 10:35:31,897:INFO:Declaring custom model
2026-01-30 10:35:31,898:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:35:31,898:INFO:Starting cross validation
2026-01-30 10:35:31,899:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:35:42,866:INFO:Calculating mean and std
2026-01-30 10:35:42,867:INFO:Creating metrics dataframe
2026-01-30 10:35:42,869:INFO:Finalizing model
2026-01-30 10:35:43,596:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:35:43,596:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:35:43,596:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:35:43,809:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 10:35:43,809:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 10:35:43,809:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 10:35:43,810:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:35:43,858:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012089 seconds.
2026-01-30 10:35:43,859:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:35:43,859:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:35:43,859:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:35:43,860:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:35:43,865:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:35:43,865:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:35:47,739:INFO:Uploading results into container
2026-01-30 10:35:47,741:INFO:Uploading model into container now
2026-01-30 10:35:47,742:INFO:_master_model_container: 7
2026-01-30 10:35:47,742:INFO:_display_container: 4
2026-01-30 10:35:47,744:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:35:47,744:INFO:create_model() successfully completed......................................
2026-01-30 10:35:47,984:INFO:SubProcess create_model() end ==================================
2026-01-30 10:35:47,984:INFO:choose_better activated
2026-01-30 10:35:47,985:INFO:SubProcess create_model() called ==================================
2026-01-30 10:35:47,987:INFO:Initializing create_model()
2026-01-30 10:35:47,987:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:35:47,987:INFO:Checking exceptions
2026-01-30 10:35:47,988:INFO:Importing libraries
2026-01-30 10:35:47,988:INFO:Copying training dataset
2026-01-30 10:35:48,217:INFO:Defining folds
2026-01-30 10:35:48,217:INFO:Declaring metric variables
2026-01-30 10:35:48,218:INFO:Importing untrained model
2026-01-30 10:35:48,218:INFO:Declaring custom model
2026-01-30 10:35:48,218:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 10:35:48,219:INFO:Starting cross validation
2026-01-30 10:35:48,219:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:35:53,108:INFO:Calculating mean and std
2026-01-30 10:35:53,109:INFO:Creating metrics dataframe
2026-01-30 10:35:53,113:INFO:Finalizing model
2026-01-30 10:35:54,065:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 10:35:54,127:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011123 seconds.
2026-01-30 10:35:54,128:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 10:35:54,128:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 10:35:54,128:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 10:35:54,129:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 10:35:54,131:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 10:35:54,131:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 10:35:55,217:INFO:Uploading results into container
2026-01-30 10:35:55,218:INFO:Uploading model into container now
2026-01-30 10:35:55,218:INFO:_master_model_container: 8
2026-01-30 10:35:55,218:INFO:_display_container: 5
2026-01-30 10:35:55,220:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:35:55,220:INFO:create_model() successfully completed......................................
2026-01-30 10:35:55,461:INFO:SubProcess create_model() end ==================================
2026-01-30 10:35:55,462:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9945
2026-01-30 10:35:55,462:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9987
2026-01-30 10:35:55,463:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 10:35:55,463:INFO:choose_better completed
2026-01-30 10:35:55,465:INFO:_master_model_container: 8
2026-01-30 10:35:55,466:INFO:_display_container: 4
2026-01-30 10:35:55,467:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 10:35:55,467:INFO:tune_model() successfully completed......................................
2026-01-30 10:35:55,648:INFO:Initializing tune_model()
2026-01-30 10:35:55,649:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 10:35:55,649:INFO:Checking exceptions
2026-01-30 10:35:55,731:INFO:Copying training dataset
2026-01-30 10:35:55,861:INFO:Checking base model
2026-01-30 10:35:55,861:INFO:Base model : Decision Tree Classifier
2026-01-30 10:35:55,861:INFO:Declaring metric variables
2026-01-30 10:35:55,861:INFO:Defining Hyperparameters
2026-01-30 10:35:56,031:INFO:Tuning with n_jobs=-1
2026-01-30 10:35:56,032:INFO:Initializing RandomizedSearchCV
2026-01-30 10:36:04,537:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 10:36:04,538:INFO:Hyperparameter search completed
2026-01-30 10:36:04,539:INFO:SubProcess create_model() called ==================================
2026-01-30 10:36:04,540:INFO:Initializing create_model()
2026-01-30 10:36:04,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03CFFBBD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 10:36:04,541:INFO:Checking exceptions
2026-01-30 10:36:04,541:INFO:Importing libraries
2026-01-30 10:36:04,541:INFO:Copying training dataset
2026-01-30 10:36:04,768:INFO:Defining folds
2026-01-30 10:36:04,768:INFO:Declaring metric variables
2026-01-30 10:36:04,768:INFO:Importing untrained model
2026-01-30 10:36:04,769:INFO:Declaring custom model
2026-01-30 10:36:04,769:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:36:04,769:INFO:Starting cross validation
2026-01-30 10:36:04,770:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:36:07,925:INFO:Calculating mean and std
2026-01-30 10:36:07,926:INFO:Creating metrics dataframe
2026-01-30 10:36:07,927:INFO:Finalizing model
2026-01-30 10:36:09,906:INFO:Uploading results into container
2026-01-30 10:36:09,907:INFO:Uploading model into container now
2026-01-30 10:36:09,908:INFO:_master_model_container: 9
2026-01-30 10:36:09,908:INFO:_display_container: 5
2026-01-30 10:36:09,909:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:36:09,909:INFO:create_model() successfully completed......................................
2026-01-30 10:36:10,078:INFO:SubProcess create_model() end ==================================
2026-01-30 10:36:10,079:INFO:choose_better activated
2026-01-30 10:36:10,079:INFO:SubProcess create_model() called ==================================
2026-01-30 10:36:10,079:INFO:Initializing create_model()
2026-01-30 10:36:10,079:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 10:36:10,079:INFO:Checking exceptions
2026-01-30 10:36:10,080:INFO:Importing libraries
2026-01-30 10:36:10,080:INFO:Copying training dataset
2026-01-30 10:36:10,320:INFO:Defining folds
2026-01-30 10:36:10,320:INFO:Declaring metric variables
2026-01-30 10:36:10,320:INFO:Importing untrained model
2026-01-30 10:36:10,320:INFO:Declaring custom model
2026-01-30 10:36:10,321:INFO:Decision Tree Classifier Imported successfully
2026-01-30 10:36:10,321:INFO:Starting cross validation
2026-01-30 10:36:10,322:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 10:36:14,511:INFO:Calculating mean and std
2026-01-30 10:36:14,513:INFO:Creating metrics dataframe
2026-01-30 10:36:14,515:INFO:Finalizing model
2026-01-30 10:36:17,699:INFO:Uploading results into container
2026-01-30 10:36:17,699:INFO:Uploading model into container now
2026-01-30 10:36:17,700:INFO:_master_model_container: 10
2026-01-30 10:36:17,700:INFO:_display_container: 6
2026-01-30 10:36:17,700:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:36:17,700:INFO:create_model() successfully completed......................................
2026-01-30 10:36:17,869:INFO:SubProcess create_model() end ==================================
2026-01-30 10:36:17,869:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.989
2026-01-30 10:36:17,870:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9826
2026-01-30 10:36:17,870:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 10:36:17,870:INFO:choose_better completed
2026-01-30 10:36:17,870:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 10:36:17,873:INFO:_master_model_container: 10
2026-01-30 10:36:17,873:INFO:_display_container: 5
2026-01-30 10:36:17,873:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 10:36:17,873:INFO:tune_model() successfully completed......................................
2026-01-30 10:36:18,059:INFO:Initializing predict_model()
2026-01-30 10:36:18,060:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03C59F240>)
2026-01-30 10:36:18,060:INFO:Checking exceptions
2026-01-30 10:36:18,060:INFO:Preloading libraries
2026-01-30 10:36:18,060:INFO:Set up data.
2026-01-30 10:36:18,072:INFO:Set up index.
2026-01-30 10:36:18,475:INFO:Initializing predict_model()
2026-01-30 10:36:18,475:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A06E850F40>)
2026-01-30 10:36:18,475:INFO:Checking exceptions
2026-01-30 10:36:18,476:INFO:Preloading libraries
2026-01-30 10:36:18,476:INFO:Set up data.
2026-01-30 10:36:18,489:INFO:Set up index.
2026-01-30 10:36:18,913:INFO:Initializing predict_model()
2026-01-30 10:36:18,915:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03D1FF1A0>)
2026-01-30 10:36:18,915:INFO:Checking exceptions
2026-01-30 10:36:18,915:INFO:Preloading libraries
2026-01-30 10:36:18,915:INFO:Set up data.
2026-01-30 10:36:18,925:INFO:Set up index.
2026-01-30 10:36:19,330:INFO:Initializing plot_model()
2026-01-30 10:36:19,330:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 10:36:19,330:INFO:Checking exceptions
2026-01-30 10:36:19,448:INFO:Preloading libraries
2026-01-30 10:36:19,528:INFO:Copying training dataset
2026-01-30 10:36:19,528:INFO:Plot type: feature
2026-01-30 10:36:19,529:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 10:36:19,907:INFO:Visual Rendered Successfully
2026-01-30 10:36:20,081:INFO:plot_model() successfully completed......................................
2026-01-30 10:36:20,092:INFO:Initializing plot_model()
2026-01-30 10:36:20,093:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03C48F790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 10:36:20,093:INFO:Checking exceptions
2026-01-30 10:36:20,220:INFO:Preloading libraries
2026-01-30 10:36:20,282:INFO:Copying training dataset
2026-01-30 10:36:20,282:INFO:Plot type: feature_all
2026-01-30 10:36:20,502:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 10:36:20,982:INFO:Visual Rendered Successfully
2026-01-30 10:36:21,156:INFO:plot_model() successfully completed......................................
2026-01-30 10:36:21,171:INFO:Initializing save_model()
2026-01-30 10:36:21,171:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 10:36:21,171:INFO:Adding model into prep_pipe
2026-01-30 10:36:21,288:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 10:36:21,295:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PO...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 10:36:21,296:INFO:save_model() successfully completed......................................
2026-01-30 11:55:06,857:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-30 11:55:06,857:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-30 11:55:06,857:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-30 11:55:06,857:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-01-30 11:55:10,035:INFO:Initializing load_model()
2026-01-30 11:55:10,035:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 11:57:07,233:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\503664258.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 11:57:09,574:INFO:PyCaret ClassificationExperiment
2026-01-30 11:57:09,574:INFO:Logging name: clf-default-name
2026-01-30 11:57:09,574:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 11:57:09,574:INFO:version 3.3.2
2026-01-30 11:57:09,574:INFO:Initializing setup()
2026-01-30 11:57:09,574:INFO:self.USI: b17d
2026-01-30 11:57:09,574:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 11:57:09,574:INFO:Checking environment
2026-01-30 11:57:09,574:INFO:python_version: 3.11.11
2026-01-30 11:57:09,574:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 11:57:09,574:INFO:machine: AMD64
2026-01-30 11:57:09,574:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 11:57:09,574:INFO:Memory: svmem(total=34009374720, available=14170279936, percent=58.3, used=19839094784, free=14170279936)
2026-01-30 11:57:09,574:INFO:Physical Core: 12
2026-01-30 11:57:09,574:INFO:Logical Core: 16
2026-01-30 11:57:09,574:INFO:Checking libraries
2026-01-30 11:57:09,574:INFO:System:
2026-01-30 11:57:09,574:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 11:57:09,574:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 11:57:09,574:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 11:57:09,574:INFO:PyCaret required dependencies:
2026-01-30 11:57:09,574:INFO:                 pip: 25.0
2026-01-30 11:57:09,574:INFO:          setuptools: 75.8.0
2026-01-30 11:57:09,574:INFO:             pycaret: 3.3.2
2026-01-30 11:57:09,574:INFO:             IPython: 9.9.0
2026-01-30 11:57:09,574:INFO:          ipywidgets: 8.1.8
2026-01-30 11:57:09,574:INFO:                tqdm: 4.67.1
2026-01-30 11:57:09,574:INFO:               numpy: 1.26.4
2026-01-30 11:57:09,574:INFO:              pandas: 2.1.4
2026-01-30 11:57:09,574:INFO:              jinja2: 3.1.6
2026-01-30 11:57:09,574:INFO:               scipy: 1.11.4
2026-01-30 11:57:09,574:INFO:              joblib: 1.3.2
2026-01-30 11:57:09,574:INFO:             sklearn: 1.4.2
2026-01-30 11:57:09,574:INFO:                pyod: 2.0.6
2026-01-30 11:57:09,574:INFO:            imblearn: 0.14.1
2026-01-30 11:57:09,574:INFO:   category_encoders: 2.7.0
2026-01-30 11:57:09,574:INFO:            lightgbm: 4.6.0
2026-01-30 11:57:09,574:INFO:               numba: 0.62.1
2026-01-30 11:57:09,574:INFO:            requests: 2.32.3
2026-01-30 11:57:09,574:INFO:          matplotlib: 3.7.5
2026-01-30 11:57:09,574:INFO:          scikitplot: 0.3.7
2026-01-30 11:57:09,574:INFO:         yellowbrick: 1.5
2026-01-30 11:57:09,574:INFO:              plotly: 5.24.1
2026-01-30 11:57:09,574:INFO:    plotly-resampler: Not installed
2026-01-30 11:57:09,574:INFO:             kaleido: 1.2.0
2026-01-30 11:57:09,574:INFO:           schemdraw: 0.15
2026-01-30 11:57:09,574:INFO:         statsmodels: 0.14.6
2026-01-30 11:57:09,574:INFO:              sktime: 0.26.0
2026-01-30 11:57:09,574:INFO:               tbats: 1.1.3
2026-01-30 11:57:09,574:INFO:            pmdarima: 2.0.4
2026-01-30 11:57:09,574:INFO:              psutil: 7.2.1
2026-01-30 11:57:09,574:INFO:          markupsafe: 3.0.3
2026-01-30 11:57:09,574:INFO:             pickle5: Not installed
2026-01-30 11:57:09,574:INFO:         cloudpickle: 3.0.0
2026-01-30 11:57:09,574:INFO:         deprecation: 2.1.0
2026-01-30 11:57:09,574:INFO:              xxhash: 3.6.0
2026-01-30 11:57:09,574:INFO:           wurlitzer: Not installed
2026-01-30 11:57:09,574:INFO:PyCaret optional dependencies:
2026-01-30 11:57:09,574:INFO:                shap: 0.44.1
2026-01-30 11:57:09,574:INFO:           interpret: 0.7.3
2026-01-30 11:57:09,574:INFO:                umap: 0.5.7
2026-01-30 11:57:09,574:INFO:     ydata_profiling: 4.18.1
2026-01-30 11:57:09,574:INFO:  explainerdashboard: 0.5.1
2026-01-30 11:57:09,574:INFO:             autoviz: Not installed
2026-01-30 11:57:09,574:INFO:           fairlearn: 0.7.0
2026-01-30 11:57:09,574:INFO:          deepchecks: Not installed
2026-01-30 11:57:09,574:INFO:             xgboost: Not installed
2026-01-30 11:57:09,574:INFO:            catboost: 1.2.8
2026-01-30 11:57:09,574:INFO:              kmodes: 0.12.2
2026-01-30 11:57:09,574:INFO:             mlxtend: 0.23.4
2026-01-30 11:57:09,574:INFO:       statsforecast: 1.5.0
2026-01-30 11:57:09,574:INFO:        tune_sklearn: Not installed
2026-01-30 11:57:09,574:INFO:                 ray: Not installed
2026-01-30 11:57:09,574:INFO:            hyperopt: 0.2.7
2026-01-30 11:57:09,574:INFO:              optuna: 4.6.0
2026-01-30 11:57:09,574:INFO:               skopt: 0.10.2
2026-01-30 11:57:09,574:INFO:              mlflow: 3.8.1
2026-01-30 11:57:09,574:INFO:              gradio: 6.3.0
2026-01-30 11:57:09,574:INFO:             fastapi: 0.128.0
2026-01-30 11:57:09,574:INFO:             uvicorn: 0.40.0
2026-01-30 11:57:09,574:INFO:              m2cgen: 0.10.0
2026-01-30 11:57:09,574:INFO:           evidently: 0.4.40
2026-01-30 11:57:09,574:INFO:               fugue: 0.8.7
2026-01-30 11:57:09,574:INFO:           streamlit: Not installed
2026-01-30 11:57:09,574:INFO:             prophet: Not installed
2026-01-30 11:57:09,574:INFO:None
2026-01-30 11:57:09,574:INFO:Set up data.
2026-01-30 11:57:09,755:INFO:Set up folding strategy.
2026-01-30 11:57:09,755:INFO:Set up train/test split.
2026-01-30 11:57:10,230:INFO:Set up index.
2026-01-30 11:57:10,250:INFO:Assigning column types.
2026-01-30 11:57:10,494:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 11:57:10,536:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 11:57:10,536:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 11:57:10,553:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 11:57:10,566:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 11:57:10,606:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 11:57:10,606:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 11:57:10,633:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 11:57:10,634:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 11:57:10,635:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 11:57:10,670:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 11:57:10,703:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 11:57:10,703:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 11:57:10,738:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 11:57:10,772:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 11:57:10,773:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 11:57:10,773:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 11:57:10,836:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 11:57:10,836:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 11:57:10,903:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 11:57:10,903:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 11:57:10,903:INFO:Preparing preprocessing pipeline...
2026-01-30 11:57:10,953:INFO:Set up simple imputation.
2026-01-30 11:57:10,953:INFO:Set up feature normalization.
2026-01-30 11:57:11,333:INFO:Finished creating preprocessing pipeline.
2026-01-30 11:57:11,333:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 11:57:11,333:INFO:Creating final display dataframe.
2026-01-30 11:57:12,198:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 28)
4        Transformed data shape      (482669, 28)
5   Transformed train set shape      (337868, 28)
6    Transformed test set shape      (144801, 28)
7              Numeric features                24
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              b17d
2026-01-30 11:57:12,250:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 11:57:12,250:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 11:57:12,316:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 11:57:12,316:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 11:57:12,316:INFO:setup() successfully completed in 2.76s...............
2026-01-30 11:57:12,316:INFO:Initializing compare_models()
2026-01-30 11:57:12,316:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 11:57:12,316:INFO:Checking exceptions
2026-01-30 11:57:12,466:INFO:Preparing display monitor
2026-01-30 11:57:12,466:INFO:Initializing Logistic Regression
2026-01-30 11:57:12,466:INFO:Total runtime is 0.0 minutes
2026-01-30 11:57:12,466:INFO:SubProcess create_model() called ==================================
2026-01-30 11:57:12,466:INFO:Initializing create_model()
2026-01-30 11:57:12,466:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0536012D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 11:57:12,466:INFO:Checking exceptions
2026-01-30 11:57:12,466:INFO:Importing libraries
2026-01-30 11:57:12,466:INFO:Copying training dataset
2026-01-30 11:57:12,729:INFO:Defining folds
2026-01-30 11:57:12,729:INFO:Declaring metric variables
2026-01-30 11:57:12,730:INFO:Importing untrained model
2026-01-30 11:57:12,730:INFO:Logistic Regression Imported successfully
2026-01-30 11:57:12,730:INFO:Starting cross validation
2026-01-30 11:57:12,731:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 11:57:25,659:INFO:Calculating mean and std
2026-01-30 11:57:25,660:INFO:Creating metrics dataframe
2026-01-30 11:57:25,664:INFO:Uploading results into container
2026-01-30 11:57:25,664:INFO:Uploading model into container now
2026-01-30 11:57:25,665:INFO:_master_model_container: 1
2026-01-30 11:57:25,665:INFO:_display_container: 2
2026-01-30 11:57:25,666:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 11:57:25,666:INFO:create_model() successfully completed......................................
2026-01-30 11:57:25,849:INFO:SubProcess create_model() end ==================================
2026-01-30 11:57:25,850:INFO:Creating metrics dataframe
2026-01-30 11:57:25,850:INFO:Initializing Decision Tree Classifier
2026-01-30 11:57:25,850:INFO:Total runtime is 0.22305553754170734 minutes
2026-01-30 11:57:25,850:INFO:SubProcess create_model() called ==================================
2026-01-30 11:57:25,850:INFO:Initializing create_model()
2026-01-30 11:57:25,850:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0536012D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 11:57:25,850:INFO:Checking exceptions
2026-01-30 11:57:25,850:INFO:Importing libraries
2026-01-30 11:57:25,850:INFO:Copying training dataset
2026-01-30 11:57:26,018:INFO:Defining folds
2026-01-30 11:57:26,018:INFO:Declaring metric variables
2026-01-30 11:57:26,019:INFO:Importing untrained model
2026-01-30 11:57:26,019:INFO:Decision Tree Classifier Imported successfully
2026-01-30 11:57:26,019:INFO:Starting cross validation
2026-01-30 11:57:26,020:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 11:57:33,554:INFO:Calculating mean and std
2026-01-30 11:57:33,554:INFO:Creating metrics dataframe
2026-01-30 11:57:33,554:INFO:Uploading results into container
2026-01-30 11:57:33,554:INFO:Uploading model into container now
2026-01-30 11:57:33,554:INFO:_master_model_container: 2
2026-01-30 11:57:33,554:INFO:_display_container: 2
2026-01-30 11:57:33,554:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 11:57:33,554:INFO:create_model() successfully completed......................................
2026-01-30 11:57:33,716:INFO:SubProcess create_model() end ==================================
2026-01-30 11:57:33,716:INFO:Creating metrics dataframe
2026-01-30 11:57:33,716:INFO:Initializing Random Forest Classifier
2026-01-30 11:57:33,716:INFO:Total runtime is 0.35416378180185953 minutes
2026-01-30 11:57:33,716:INFO:SubProcess create_model() called ==================================
2026-01-30 11:57:33,716:INFO:Initializing create_model()
2026-01-30 11:57:33,716:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0536012D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 11:57:33,716:INFO:Checking exceptions
2026-01-30 11:57:33,716:INFO:Importing libraries
2026-01-30 11:57:33,716:INFO:Copying training dataset
2026-01-30 11:57:33,925:INFO:Defining folds
2026-01-30 11:57:33,926:INFO:Declaring metric variables
2026-01-30 11:57:33,926:INFO:Importing untrained model
2026-01-30 11:57:33,926:INFO:Random Forest Classifier Imported successfully
2026-01-30 11:57:33,926:INFO:Starting cross validation
2026-01-30 11:57:33,927:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 11:57:58,036:INFO:Calculating mean and std
2026-01-30 11:57:58,038:INFO:Creating metrics dataframe
2026-01-30 11:57:58,038:INFO:Uploading results into container
2026-01-30 11:57:58,038:INFO:Uploading model into container now
2026-01-30 11:57:58,038:INFO:_master_model_container: 3
2026-01-30 11:57:58,038:INFO:_display_container: 2
2026-01-30 11:57:58,038:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 11:57:58,038:INFO:create_model() successfully completed......................................
2026-01-30 11:57:58,290:INFO:SubProcess create_model() end ==================================
2026-01-30 11:57:58,290:INFO:Creating metrics dataframe
2026-01-30 11:57:58,293:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 11:57:58,293:INFO:Total runtime is 0.7637775659561157 minutes
2026-01-30 11:57:58,293:INFO:SubProcess create_model() called ==================================
2026-01-30 11:57:58,293:INFO:Initializing create_model()
2026-01-30 11:57:58,293:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0536012D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 11:57:58,293:INFO:Checking exceptions
2026-01-30 11:57:58,293:INFO:Importing libraries
2026-01-30 11:57:58,293:INFO:Copying training dataset
2026-01-30 11:57:58,575:INFO:Defining folds
2026-01-30 11:57:58,575:INFO:Declaring metric variables
2026-01-30 11:57:58,576:INFO:Importing untrained model
2026-01-30 11:57:58,576:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 11:57:58,577:INFO:Starting cross validation
2026-01-30 11:57:58,578:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 11:58:08,826:INFO:Calculating mean and std
2026-01-30 11:58:08,827:INFO:Creating metrics dataframe
2026-01-30 11:58:08,828:INFO:Uploading results into container
2026-01-30 11:58:08,829:INFO:Uploading model into container now
2026-01-30 11:58:08,829:INFO:_master_model_container: 4
2026-01-30 11:58:08,829:INFO:_display_container: 2
2026-01-30 11:58:08,830:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 11:58:08,830:INFO:create_model() successfully completed......................................
2026-01-30 11:58:08,999:INFO:SubProcess create_model() end ==================================
2026-01-30 11:58:08,999:INFO:Creating metrics dataframe
2026-01-30 11:58:08,999:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 11:58:08,999:INFO:Initializing create_model()
2026-01-30 11:58:08,999:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 11:58:08,999:INFO:Checking exceptions
2026-01-30 11:58:08,999:INFO:Importing libraries
2026-01-30 11:58:08,999:INFO:Copying training dataset
2026-01-30 11:58:09,199:INFO:Defining folds
2026-01-30 11:58:09,199:INFO:Declaring metric variables
2026-01-30 11:58:09,199:INFO:Importing untrained model
2026-01-30 11:58:09,199:INFO:Declaring custom model
2026-01-30 11:58:09,199:INFO:Random Forest Classifier Imported successfully
2026-01-30 11:58:09,201:INFO:Cross validation set to False
2026-01-30 11:58:09,201:INFO:Fitting Model
2026-01-30 11:58:18,771:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 11:58:18,771:INFO:create_model() successfully completed......................................
2026-01-30 11:58:18,949:INFO:Initializing create_model()
2026-01-30 11:58:18,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 11:58:18,949:INFO:Checking exceptions
2026-01-30 11:58:18,949:INFO:Importing libraries
2026-01-30 11:58:18,949:INFO:Copying training dataset
2026-01-30 11:58:19,132:INFO:Defining folds
2026-01-30 11:58:19,148:INFO:Declaring metric variables
2026-01-30 11:58:19,148:INFO:Importing untrained model
2026-01-30 11:58:19,148:INFO:Declaring custom model
2026-01-30 11:58:19,148:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 11:58:19,149:INFO:Cross validation set to False
2026-01-30 11:58:19,149:INFO:Fitting Model
2026-01-30 11:58:20,005:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 11:58:20,081:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016610 seconds.
2026-01-30 11:58:20,081:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 11:58:20,081:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 11:58:20,082:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 11:58:20,082:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 11:58:20,085:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 11:58:20,085:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 11:58:21,009:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 11:58:21,009:INFO:create_model() successfully completed......................................
2026-01-30 11:58:21,247:INFO:Initializing create_model()
2026-01-30 11:58:21,247:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 11:58:21,249:INFO:Checking exceptions
2026-01-30 11:58:21,249:INFO:Importing libraries
2026-01-30 11:58:21,249:INFO:Copying training dataset
2026-01-30 11:58:21,449:INFO:Defining folds
2026-01-30 11:58:21,449:INFO:Declaring metric variables
2026-01-30 11:58:21,449:INFO:Importing untrained model
2026-01-30 11:58:21,449:INFO:Declaring custom model
2026-01-30 11:58:21,449:INFO:Decision Tree Classifier Imported successfully
2026-01-30 11:58:21,464:INFO:Cross validation set to False
2026-01-30 11:58:21,464:INFO:Fitting Model
2026-01-30 11:58:24,432:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 11:58:24,432:INFO:create_model() successfully completed......................................
2026-01-30 11:58:24,632:INFO:_master_model_container: 4
2026-01-30 11:58:24,632:INFO:_display_container: 2
2026-01-30 11:58:24,632:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 11:58:24,632:INFO:compare_models() successfully completed......................................
2026-01-30 11:58:24,632:INFO:Initializing tune_model()
2026-01-30 11:58:24,632:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 11:58:24,632:INFO:Checking exceptions
2026-01-30 11:58:24,719:INFO:Copying training dataset
2026-01-30 11:58:24,865:INFO:Checking base model
2026-01-30 11:58:24,865:INFO:Base model : Random Forest Classifier
2026-01-30 11:58:24,865:INFO:Declaring metric variables
2026-01-30 11:58:24,866:INFO:Defining Hyperparameters
2026-01-30 11:58:25,032:INFO:Tuning with n_jobs=-1
2026-01-30 11:58:25,032:INFO:Initializing RandomizedSearchCV
2026-01-30 12:01:15,391:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 12:01:15,393:INFO:Hyperparameter search completed
2026-01-30 12:01:15,393:INFO:SubProcess create_model() called ==================================
2026-01-30 12:01:15,395:INFO:Initializing create_model()
2026-01-30 12:01:15,396:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A048BAE990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 12:01:15,396:INFO:Checking exceptions
2026-01-30 12:01:15,396:INFO:Importing libraries
2026-01-30 12:01:15,397:INFO:Copying training dataset
2026-01-30 12:01:15,713:INFO:Defining folds
2026-01-30 12:01:15,713:INFO:Declaring metric variables
2026-01-30 12:01:15,713:INFO:Importing untrained model
2026-01-30 12:01:15,713:INFO:Declaring custom model
2026-01-30 12:01:15,713:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:01:15,713:INFO:Starting cross validation
2026-01-30 12:01:15,713:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:01:49,729:INFO:Calculating mean and std
2026-01-30 12:01:49,730:INFO:Creating metrics dataframe
2026-01-30 12:01:49,730:INFO:Finalizing model
2026-01-30 12:02:06,077:INFO:Uploading results into container
2026-01-30 12:02:06,079:INFO:Uploading model into container now
2026-01-30 12:02:06,080:INFO:_master_model_container: 5
2026-01-30 12:02:06,080:INFO:_display_container: 3
2026-01-30 12:02:06,081:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:02:06,081:INFO:create_model() successfully completed......................................
2026-01-30 12:02:06,298:INFO:SubProcess create_model() end ==================================
2026-01-30 12:02:06,298:INFO:choose_better activated
2026-01-30 12:02:06,299:INFO:SubProcess create_model() called ==================================
2026-01-30 12:02:06,300:INFO:Initializing create_model()
2026-01-30 12:02:06,300:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:02:06,300:INFO:Checking exceptions
2026-01-30 12:02:06,300:INFO:Importing libraries
2026-01-30 12:02:06,300:INFO:Copying training dataset
2026-01-30 12:02:06,513:INFO:Defining folds
2026-01-30 12:02:06,513:INFO:Declaring metric variables
2026-01-30 12:02:06,513:INFO:Importing untrained model
2026-01-30 12:02:06,513:INFO:Declaring custom model
2026-01-30 12:02:06,514:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:02:06,514:INFO:Starting cross validation
2026-01-30 12:02:06,515:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:02:28,045:INFO:Calculating mean and std
2026-01-30 12:02:28,045:INFO:Creating metrics dataframe
2026-01-30 12:02:28,045:INFO:Finalizing model
2026-01-30 12:02:38,145:INFO:Uploading results into container
2026-01-30 12:02:38,145:INFO:Uploading model into container now
2026-01-30 12:02:38,145:INFO:_master_model_container: 6
2026-01-30 12:02:38,145:INFO:_display_container: 4
2026-01-30 12:02:38,145:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:02:38,145:INFO:create_model() successfully completed......................................
2026-01-30 12:02:38,331:INFO:SubProcess create_model() end ==================================
2026-01-30 12:02:38,331:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9985
2026-01-30 12:02:38,331:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9894
2026-01-30 12:02:38,331:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 12:02:38,331:INFO:choose_better completed
2026-01-30 12:02:38,331:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 12:02:38,345:INFO:_master_model_container: 6
2026-01-30 12:02:38,345:INFO:_display_container: 3
2026-01-30 12:02:38,345:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:02:38,345:INFO:tune_model() successfully completed......................................
2026-01-30 12:02:38,512:INFO:Initializing tune_model()
2026-01-30 12:02:38,512:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:02:38,512:INFO:Checking exceptions
2026-01-30 12:02:38,598:INFO:Copying training dataset
2026-01-30 12:02:38,731:INFO:Checking base model
2026-01-30 12:02:38,731:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 12:02:38,731:INFO:Declaring metric variables
2026-01-30 12:02:38,731:INFO:Defining Hyperparameters
2026-01-30 12:02:38,895:INFO:Tuning with n_jobs=-1
2026-01-30 12:02:38,895:INFO:Initializing RandomizedSearchCV
2026-01-30 12:03:24,744:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 12:03:24,745:INFO:Hyperparameter search completed
2026-01-30 12:03:24,745:INFO:SubProcess create_model() called ==================================
2026-01-30 12:03:24,745:INFO:Initializing create_model()
2026-01-30 12:03:24,745:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03CE100D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 12:03:24,745:INFO:Checking exceptions
2026-01-30 12:03:24,745:INFO:Importing libraries
2026-01-30 12:03:24,745:INFO:Copying training dataset
2026-01-30 12:03:25,011:INFO:Defining folds
2026-01-30 12:03:25,011:INFO:Declaring metric variables
2026-01-30 12:03:25,011:INFO:Importing untrained model
2026-01-30 12:03:25,011:INFO:Declaring custom model
2026-01-30 12:03:25,011:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:03:25,011:INFO:Starting cross validation
2026-01-30 12:03:25,025:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:03:35,832:INFO:Calculating mean and std
2026-01-30 12:03:35,835:INFO:Creating metrics dataframe
2026-01-30 12:03:35,837:INFO:Finalizing model
2026-01-30 12:03:36,658:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 12:03:36,659:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 12:03:36,659:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 12:03:36,882:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 12:03:36,882:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 12:03:36,882:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 12:03:36,883:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:03:36,944:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014886 seconds.
2026-01-30 12:03:36,944:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:03:36,944:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:03:36,945:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 12:03:36,946:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 12:03:36,951:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:03:36,951:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:03:40,600:INFO:Uploading results into container
2026-01-30 12:03:40,601:INFO:Uploading model into container now
2026-01-30 12:03:40,602:INFO:_master_model_container: 7
2026-01-30 12:03:40,602:INFO:_display_container: 4
2026-01-30 12:03:40,603:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:03:40,604:INFO:create_model() successfully completed......................................
2026-01-30 12:03:40,844:INFO:SubProcess create_model() end ==================================
2026-01-30 12:03:40,845:INFO:choose_better activated
2026-01-30 12:03:40,846:INFO:SubProcess create_model() called ==================================
2026-01-30 12:03:40,847:INFO:Initializing create_model()
2026-01-30 12:03:40,847:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:03:40,847:INFO:Checking exceptions
2026-01-30 12:03:40,848:INFO:Importing libraries
2026-01-30 12:03:40,848:INFO:Copying training dataset
2026-01-30 12:03:41,077:INFO:Defining folds
2026-01-30 12:03:41,077:INFO:Declaring metric variables
2026-01-30 12:03:41,078:INFO:Importing untrained model
2026-01-30 12:03:41,078:INFO:Declaring custom model
2026-01-30 12:03:41,078:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:03:41,078:INFO:Starting cross validation
2026-01-30 12:03:41,078:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:03:46,051:INFO:Calculating mean and std
2026-01-30 12:03:46,052:INFO:Creating metrics dataframe
2026-01-30 12:03:46,054:INFO:Finalizing model
2026-01-30 12:03:46,960:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:03:47,016:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016709 seconds.
2026-01-30 12:03:47,016:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:03:47,016:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:03:47,016:INFO:[LightGBM] [Info] Total Bins 3123
2026-01-30 12:03:47,016:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 27
2026-01-30 12:03:47,020:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:03:47,020:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:03:47,953:INFO:Uploading results into container
2026-01-30 12:03:47,953:INFO:Uploading model into container now
2026-01-30 12:03:47,955:INFO:_master_model_container: 8
2026-01-30 12:03:47,955:INFO:_display_container: 5
2026-01-30 12:03:47,955:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:03:47,955:INFO:create_model() successfully completed......................................
2026-01-30 12:03:48,233:INFO:SubProcess create_model() end ==================================
2026-01-30 12:03:48,233:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9945
2026-01-30 12:03:48,233:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9987
2026-01-30 12:03:48,233:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 12:03:48,233:INFO:choose_better completed
2026-01-30 12:03:48,233:INFO:_master_model_container: 8
2026-01-30 12:03:48,233:INFO:_display_container: 4
2026-01-30 12:03:48,233:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:03:48,233:INFO:tune_model() successfully completed......................................
2026-01-30 12:03:48,411:INFO:Initializing tune_model()
2026-01-30 12:03:48,411:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:03:48,411:INFO:Checking exceptions
2026-01-30 12:03:48,503:INFO:Copying training dataset
2026-01-30 12:03:48,628:INFO:Checking base model
2026-01-30 12:03:48,628:INFO:Base model : Decision Tree Classifier
2026-01-30 12:03:48,628:INFO:Declaring metric variables
2026-01-30 12:03:48,628:INFO:Defining Hyperparameters
2026-01-30 12:03:48,811:INFO:Tuning with n_jobs=-1
2026-01-30 12:03:48,811:INFO:Initializing RandomizedSearchCV
2026-01-30 12:03:58,461:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 12:03:58,462:INFO:Hyperparameter search completed
2026-01-30 12:03:58,462:INFO:SubProcess create_model() called ==================================
2026-01-30 12:03:58,463:INFO:Initializing create_model()
2026-01-30 12:03:58,463:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03D06B890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 12:03:58,464:INFO:Checking exceptions
2026-01-30 12:03:58,464:INFO:Importing libraries
2026-01-30 12:03:58,464:INFO:Copying training dataset
2026-01-30 12:03:58,733:INFO:Defining folds
2026-01-30 12:03:58,733:INFO:Declaring metric variables
2026-01-30 12:03:58,733:INFO:Importing untrained model
2026-01-30 12:03:58,734:INFO:Declaring custom model
2026-01-30 12:03:58,735:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:03:58,735:INFO:Starting cross validation
2026-01-30 12:03:58,736:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:04:02,006:INFO:Calculating mean and std
2026-01-30 12:04:02,009:INFO:Creating metrics dataframe
2026-01-30 12:04:02,013:INFO:Finalizing model
2026-01-30 12:04:04,688:INFO:Uploading results into container
2026-01-30 12:04:04,688:INFO:Uploading model into container now
2026-01-30 12:04:04,694:INFO:_master_model_container: 9
2026-01-30 12:04:04,694:INFO:_display_container: 5
2026-01-30 12:04:04,694:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:04:04,694:INFO:create_model() successfully completed......................................
2026-01-30 12:04:04,948:INFO:SubProcess create_model() end ==================================
2026-01-30 12:04:04,948:INFO:choose_better activated
2026-01-30 12:04:04,949:INFO:SubProcess create_model() called ==================================
2026-01-30 12:04:04,949:INFO:Initializing create_model()
2026-01-30 12:04:04,950:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:04:04,950:INFO:Checking exceptions
2026-01-30 12:04:04,951:INFO:Importing libraries
2026-01-30 12:04:04,951:INFO:Copying training dataset
2026-01-30 12:04:05,262:INFO:Defining folds
2026-01-30 12:04:05,262:INFO:Declaring metric variables
2026-01-30 12:04:05,262:INFO:Importing untrained model
2026-01-30 12:04:05,262:INFO:Declaring custom model
2026-01-30 12:04:05,263:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:04:05,264:INFO:Starting cross validation
2026-01-30 12:04:05,266:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:04:10,352:INFO:Calculating mean and std
2026-01-30 12:04:10,352:INFO:Creating metrics dataframe
2026-01-30 12:04:10,354:INFO:Finalizing model
2026-01-30 12:04:13,846:INFO:Uploading results into container
2026-01-30 12:04:13,846:INFO:Uploading model into container now
2026-01-30 12:04:13,847:INFO:_master_model_container: 10
2026-01-30 12:04:13,847:INFO:_display_container: 6
2026-01-30 12:04:13,847:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:04:13,847:INFO:create_model() successfully completed......................................
2026-01-30 12:04:14,027:INFO:SubProcess create_model() end ==================================
2026-01-30 12:04:14,027:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.989
2026-01-30 12:04:14,027:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9826
2026-01-30 12:04:14,027:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 12:04:14,027:INFO:choose_better completed
2026-01-30 12:04:14,027:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 12:04:14,027:INFO:_master_model_container: 10
2026-01-30 12:04:14,027:INFO:_display_container: 5
2026-01-30 12:04:14,027:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:04:14,027:INFO:tune_model() successfully completed......................................
2026-01-30 12:04:14,226:INFO:Initializing predict_model()
2026-01-30 12:04:14,226:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A06E731760>)
2026-01-30 12:04:14,226:INFO:Checking exceptions
2026-01-30 12:04:14,226:INFO:Preloading libraries
2026-01-30 12:04:14,226:INFO:Set up data.
2026-01-30 12:04:14,232:INFO:Set up index.
2026-01-30 12:04:14,697:INFO:Initializing predict_model()
2026-01-30 12:04:14,698:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03D1FF1A0>)
2026-01-30 12:04:14,698:INFO:Checking exceptions
2026-01-30 12:04:14,698:INFO:Preloading libraries
2026-01-30 12:04:14,698:INFO:Set up data.
2026-01-30 12:04:14,711:INFO:Set up index.
2026-01-30 12:04:15,228:INFO:Initializing predict_model()
2026-01-30 12:04:15,228:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03D1FE700>)
2026-01-30 12:04:15,228:INFO:Checking exceptions
2026-01-30 12:04:15,229:INFO:Preloading libraries
2026-01-30 12:04:15,229:INFO:Set up data.
2026-01-30 12:04:15,243:INFO:Set up index.
2026-01-30 12:04:15,655:INFO:Initializing plot_model()
2026-01-30 12:04:15,655:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 12:04:15,655:INFO:Checking exceptions
2026-01-30 12:04:15,793:INFO:Preloading libraries
2026-01-30 12:04:15,924:INFO:Copying training dataset
2026-01-30 12:04:15,924:INFO:Plot type: feature
2026-01-30 12:04:15,925:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 12:04:16,360:INFO:Visual Rendered Successfully
2026-01-30 12:04:16,570:INFO:plot_model() successfully completed......................................
2026-01-30 12:04:16,586:INFO:Initializing plot_model()
2026-01-30 12:04:16,586:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 12:04:16,586:INFO:Checking exceptions
2026-01-30 12:04:16,714:INFO:Preloading libraries
2026-01-30 12:04:16,794:INFO:Copying training dataset
2026-01-30 12:04:16,794:INFO:Plot type: feature_all
2026-01-30 12:04:17,010:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 12:04:17,477:INFO:Visual Rendered Successfully
2026-01-30 12:04:17,664:INFO:plot_model() successfully completed......................................
2026-01-30 12:04:17,689:INFO:Initializing save_model()
2026-01-30 12:04:17,689:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdina...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 12:04:17,689:INFO:Adding model into prep_pipe
2026-01-30 12:04:17,831:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 12:04:17,831:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_AMOUNT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PO...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 12:04:17,831:INFO:save_model() successfully completed......................................
2026-01-30 12:08:02,808:INFO:Initializing load_model()
2026-01-30 12:08:02,809:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:08:05,357:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\770966573.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:09:13,859:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\2705027463.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 12:09:16,837:INFO:PyCaret ClassificationExperiment
2026-01-30 12:09:16,838:INFO:Logging name: clf-default-name
2026-01-30 12:09:16,839:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 12:09:16,839:INFO:version 3.3.2
2026-01-30 12:09:16,839:INFO:Initializing setup()
2026-01-30 12:09:16,839:INFO:self.USI: 662d
2026-01-30 12:09:16,839:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 12:09:16,839:INFO:Checking environment
2026-01-30 12:09:16,839:INFO:python_version: 3.11.11
2026-01-30 12:09:16,839:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 12:09:16,839:INFO:machine: AMD64
2026-01-30 12:09:16,839:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 12:09:16,839:INFO:Memory: svmem(total=34009374720, available=14299074560, percent=58.0, used=19710300160, free=14299074560)
2026-01-30 12:09:16,839:INFO:Physical Core: 12
2026-01-30 12:09:16,841:INFO:Logical Core: 16
2026-01-30 12:09:16,841:INFO:Checking libraries
2026-01-30 12:09:16,841:INFO:System:
2026-01-30 12:09:16,841:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 12:09:16,842:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 12:09:16,842:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 12:09:16,842:INFO:PyCaret required dependencies:
2026-01-30 12:09:16,842:INFO:                 pip: 25.0
2026-01-30 12:09:16,842:INFO:          setuptools: 75.8.0
2026-01-30 12:09:16,842:INFO:             pycaret: 3.3.2
2026-01-30 12:09:16,842:INFO:             IPython: 9.9.0
2026-01-30 12:09:16,843:INFO:          ipywidgets: 8.1.8
2026-01-30 12:09:16,843:INFO:                tqdm: 4.67.1
2026-01-30 12:09:16,843:INFO:               numpy: 1.26.4
2026-01-30 12:09:16,844:INFO:              pandas: 2.1.4
2026-01-30 12:09:16,844:INFO:              jinja2: 3.1.6
2026-01-30 12:09:16,845:INFO:               scipy: 1.11.4
2026-01-30 12:09:16,845:INFO:              joblib: 1.3.2
2026-01-30 12:09:16,845:INFO:             sklearn: 1.4.2
2026-01-30 12:09:16,845:INFO:                pyod: 2.0.6
2026-01-30 12:09:16,845:INFO:            imblearn: 0.14.1
2026-01-30 12:09:16,845:INFO:   category_encoders: 2.7.0
2026-01-30 12:09:16,846:INFO:            lightgbm: 4.6.0
2026-01-30 12:09:16,846:INFO:               numba: 0.62.1
2026-01-30 12:09:16,846:INFO:            requests: 2.32.3
2026-01-30 12:09:16,846:INFO:          matplotlib: 3.7.5
2026-01-30 12:09:16,846:INFO:          scikitplot: 0.3.7
2026-01-30 12:09:16,846:INFO:         yellowbrick: 1.5
2026-01-30 12:09:16,846:INFO:              plotly: 5.24.1
2026-01-30 12:09:16,847:INFO:    plotly-resampler: Not installed
2026-01-30 12:09:16,847:INFO:             kaleido: 1.2.0
2026-01-30 12:09:16,847:INFO:           schemdraw: 0.15
2026-01-30 12:09:16,847:INFO:         statsmodels: 0.14.6
2026-01-30 12:09:16,847:INFO:              sktime: 0.26.0
2026-01-30 12:09:16,847:INFO:               tbats: 1.1.3
2026-01-30 12:09:16,847:INFO:            pmdarima: 2.0.4
2026-01-30 12:09:16,847:INFO:              psutil: 7.2.1
2026-01-30 12:09:16,847:INFO:          markupsafe: 3.0.3
2026-01-30 12:09:16,847:INFO:             pickle5: Not installed
2026-01-30 12:09:16,847:INFO:         cloudpickle: 3.0.0
2026-01-30 12:09:16,848:INFO:         deprecation: 2.1.0
2026-01-30 12:09:16,848:INFO:              xxhash: 3.6.0
2026-01-30 12:09:16,848:INFO:           wurlitzer: Not installed
2026-01-30 12:09:16,848:INFO:PyCaret optional dependencies:
2026-01-30 12:09:16,848:INFO:                shap: 0.44.1
2026-01-30 12:09:16,848:INFO:           interpret: 0.7.3
2026-01-30 12:09:16,848:INFO:                umap: 0.5.7
2026-01-30 12:09:16,848:INFO:     ydata_profiling: 4.18.1
2026-01-30 12:09:16,848:INFO:  explainerdashboard: 0.5.1
2026-01-30 12:09:16,849:INFO:             autoviz: Not installed
2026-01-30 12:09:16,849:INFO:           fairlearn: 0.7.0
2026-01-30 12:09:16,849:INFO:          deepchecks: Not installed
2026-01-30 12:09:16,849:INFO:             xgboost: Not installed
2026-01-30 12:09:16,849:INFO:            catboost: 1.2.8
2026-01-30 12:09:16,849:INFO:              kmodes: 0.12.2
2026-01-30 12:09:16,850:INFO:             mlxtend: 0.23.4
2026-01-30 12:09:16,850:INFO:       statsforecast: 1.5.0
2026-01-30 12:09:16,850:INFO:        tune_sklearn: Not installed
2026-01-30 12:09:16,850:INFO:                 ray: Not installed
2026-01-30 12:09:16,850:INFO:            hyperopt: 0.2.7
2026-01-30 12:09:16,850:INFO:              optuna: 4.6.0
2026-01-30 12:09:16,850:INFO:               skopt: 0.10.2
2026-01-30 12:09:16,850:INFO:              mlflow: 3.8.1
2026-01-30 12:09:16,851:INFO:              gradio: 6.3.0
2026-01-30 12:09:16,851:INFO:             fastapi: 0.128.0
2026-01-30 12:09:16,851:INFO:             uvicorn: 0.40.0
2026-01-30 12:09:16,851:INFO:              m2cgen: 0.10.0
2026-01-30 12:09:16,851:INFO:           evidently: 0.4.40
2026-01-30 12:09:16,851:INFO:               fugue: 0.8.7
2026-01-30 12:09:16,851:INFO:           streamlit: Not installed
2026-01-30 12:09:16,851:INFO:             prophet: Not installed
2026-01-30 12:09:16,851:INFO:None
2026-01-30 12:09:16,851:INFO:Set up data.
2026-01-30 12:09:17,004:INFO:Set up folding strategy.
2026-01-30 12:09:17,005:INFO:Set up train/test split.
2026-01-30 12:09:17,257:INFO:Set up index.
2026-01-30 12:09:17,276:INFO:Assigning column types.
2026-01-30 12:09:17,455:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 12:09:17,492:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 12:09:17,493:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:09:17,517:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:09:17,517:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:09:17,557:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 12:09:17,558:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:09:17,582:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:09:17,583:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:09:17,584:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 12:09:17,624:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:09:17,649:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:09:17,649:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:09:17,690:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:09:17,715:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:09:17,715:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:09:17,715:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 12:09:17,780:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:09:17,780:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:09:17,842:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:09:17,843:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:09:17,844:INFO:Preparing preprocessing pipeline...
2026-01-30 12:09:17,881:INFO:Set up simple imputation.
2026-01-30 12:09:17,881:INFO:Set up feature normalization.
2026-01-30 12:09:18,935:INFO:Finished creating preprocessing pipeline.
2026-01-30 12:09:18,941:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 12:09:18,941:INFO:Creating final display dataframe.
2026-01-30 12:09:21,931:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 27)
4        Transformed data shape      (482669, 27)
5   Transformed train set shape      (337868, 27)
6    Transformed test set shape      (144801, 27)
7              Numeric features                23
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              662d
2026-01-30 12:09:22,017:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:09:22,017:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:09:22,098:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:09:22,098:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:09:22,101:INFO:setup() successfully completed in 5.27s...............
2026-01-30 12:09:22,101:INFO:Initializing compare_models()
2026-01-30 12:09:22,101:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 12:09:22,101:INFO:Checking exceptions
2026-01-30 12:09:22,278:INFO:Preparing display monitor
2026-01-30 12:09:22,280:INFO:Initializing Logistic Regression
2026-01-30 12:09:22,280:INFO:Total runtime is 0.0 minutes
2026-01-30 12:09:22,280:INFO:SubProcess create_model() called ==================================
2026-01-30 12:09:22,280:INFO:Initializing create_model()
2026-01-30 12:09:22,280:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03F649690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:09:22,280:INFO:Checking exceptions
2026-01-30 12:09:22,280:INFO:Importing libraries
2026-01-30 12:09:22,280:INFO:Copying training dataset
2026-01-30 12:09:22,473:INFO:Defining folds
2026-01-30 12:09:22,473:INFO:Declaring metric variables
2026-01-30 12:09:22,473:INFO:Importing untrained model
2026-01-30 12:09:22,474:INFO:Logistic Regression Imported successfully
2026-01-30 12:09:22,474:INFO:Starting cross validation
2026-01-30 12:09:22,474:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:09:31,244:INFO:Calculating mean and std
2026-01-30 12:09:31,245:INFO:Creating metrics dataframe
2026-01-30 12:09:31,247:INFO:Uploading results into container
2026-01-30 12:09:31,248:INFO:Uploading model into container now
2026-01-30 12:09:31,248:INFO:_master_model_container: 1
2026-01-30 12:09:31,248:INFO:_display_container: 2
2026-01-30 12:09:31,249:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 12:09:31,249:INFO:create_model() successfully completed......................................
2026-01-30 12:09:31,406:INFO:SubProcess create_model() end ==================================
2026-01-30 12:09:31,406:INFO:Creating metrics dataframe
2026-01-30 12:09:31,406:INFO:Initializing Decision Tree Classifier
2026-01-30 12:09:31,406:INFO:Total runtime is 0.15210139751434326 minutes
2026-01-30 12:09:31,406:INFO:SubProcess create_model() called ==================================
2026-01-30 12:09:31,406:INFO:Initializing create_model()
2026-01-30 12:09:31,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03F649690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:09:31,406:INFO:Checking exceptions
2026-01-30 12:09:31,406:INFO:Importing libraries
2026-01-30 12:09:31,406:INFO:Copying training dataset
2026-01-30 12:09:31,584:INFO:Defining folds
2026-01-30 12:09:31,585:INFO:Declaring metric variables
2026-01-30 12:09:31,585:INFO:Importing untrained model
2026-01-30 12:09:31,585:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:09:31,586:INFO:Starting cross validation
2026-01-30 12:09:31,586:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:09:40,700:INFO:Calculating mean and std
2026-01-30 12:09:40,702:INFO:Creating metrics dataframe
2026-01-30 12:09:40,705:INFO:Uploading results into container
2026-01-30 12:09:40,706:INFO:Uploading model into container now
2026-01-30 12:09:40,706:INFO:_master_model_container: 2
2026-01-30 12:09:40,706:INFO:_display_container: 2
2026-01-30 12:09:40,706:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:09:40,707:INFO:create_model() successfully completed......................................
2026-01-30 12:09:40,855:INFO:SubProcess create_model() end ==================================
2026-01-30 12:09:40,855:INFO:Creating metrics dataframe
2026-01-30 12:09:40,855:INFO:Initializing Random Forest Classifier
2026-01-30 12:09:40,855:INFO:Total runtime is 0.3095914800961812 minutes
2026-01-30 12:09:40,855:INFO:SubProcess create_model() called ==================================
2026-01-30 12:09:40,855:INFO:Initializing create_model()
2026-01-30 12:09:40,855:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03F649690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:09:40,855:INFO:Checking exceptions
2026-01-30 12:09:40,855:INFO:Importing libraries
2026-01-30 12:09:40,855:INFO:Copying training dataset
2026-01-30 12:09:41,034:INFO:Defining folds
2026-01-30 12:09:41,034:INFO:Declaring metric variables
2026-01-30 12:09:41,035:INFO:Importing untrained model
2026-01-30 12:09:41,035:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:09:41,036:INFO:Starting cross validation
2026-01-30 12:09:41,036:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:10:04,178:INFO:Calculating mean and std
2026-01-30 12:10:04,182:INFO:Creating metrics dataframe
2026-01-30 12:10:04,183:INFO:Uploading results into container
2026-01-30 12:10:04,183:INFO:Uploading model into container now
2026-01-30 12:10:04,185:INFO:_master_model_container: 3
2026-01-30 12:10:04,185:INFO:_display_container: 2
2026-01-30 12:10:04,185:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:10:04,185:INFO:create_model() successfully completed......................................
2026-01-30 12:10:04,381:INFO:SubProcess create_model() end ==================================
2026-01-30 12:10:04,381:INFO:Creating metrics dataframe
2026-01-30 12:10:04,383:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 12:10:04,383:INFO:Total runtime is 0.7017121752103169 minutes
2026-01-30 12:10:04,383:INFO:SubProcess create_model() called ==================================
2026-01-30 12:10:04,384:INFO:Initializing create_model()
2026-01-30 12:10:04,384:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03F649690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:10:04,384:INFO:Checking exceptions
2026-01-30 12:10:04,384:INFO:Importing libraries
2026-01-30 12:10:04,384:INFO:Copying training dataset
2026-01-30 12:10:04,649:INFO:Defining folds
2026-01-30 12:10:04,649:INFO:Declaring metric variables
2026-01-30 12:10:04,649:INFO:Importing untrained model
2026-01-30 12:10:04,650:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:10:04,650:INFO:Starting cross validation
2026-01-30 12:10:04,651:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:10:13,362:INFO:Calculating mean and std
2026-01-30 12:10:13,362:INFO:Creating metrics dataframe
2026-01-30 12:10:13,362:INFO:Uploading results into container
2026-01-30 12:10:13,362:INFO:Uploading model into container now
2026-01-30 12:10:13,362:INFO:_master_model_container: 4
2026-01-30 12:10:13,362:INFO:_display_container: 2
2026-01-30 12:10:13,362:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:10:13,362:INFO:create_model() successfully completed......................................
2026-01-30 12:10:13,541:INFO:SubProcess create_model() end ==================================
2026-01-30 12:10:13,541:INFO:Creating metrics dataframe
2026-01-30 12:10:13,543:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 12:10:13,544:INFO:Initializing create_model()
2026-01-30 12:10:13,544:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:10:13,545:INFO:Checking exceptions
2026-01-30 12:10:13,545:INFO:Importing libraries
2026-01-30 12:10:13,545:INFO:Copying training dataset
2026-01-30 12:10:13,730:INFO:Defining folds
2026-01-30 12:10:13,730:INFO:Declaring metric variables
2026-01-30 12:10:13,731:INFO:Importing untrained model
2026-01-30 12:10:13,731:INFO:Declaring custom model
2026-01-30 12:10:13,731:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:10:13,732:INFO:Cross validation set to False
2026-01-30 12:10:13,732:INFO:Fitting Model
2026-01-30 12:10:23,491:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:10:23,491:INFO:create_model() successfully completed......................................
2026-01-30 12:10:23,660:INFO:Initializing create_model()
2026-01-30 12:10:23,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:10:23,661:INFO:Checking exceptions
2026-01-30 12:10:23,662:INFO:Importing libraries
2026-01-30 12:10:23,662:INFO:Copying training dataset
2026-01-30 12:10:23,864:INFO:Defining folds
2026-01-30 12:10:23,864:INFO:Declaring metric variables
2026-01-30 12:10:23,864:INFO:Importing untrained model
2026-01-30 12:10:23,864:INFO:Declaring custom model
2026-01-30 12:10:23,865:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:10:23,865:INFO:Cross validation set to False
2026-01-30 12:10:23,865:INFO:Fitting Model
2026-01-30 12:10:24,917:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:10:24,979:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015172 seconds.
2026-01-30 12:10:24,980:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:10:24,980:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:10:24,980:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:10:24,981:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:10:24,984:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:10:24,984:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:10:25,919:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:10:25,919:INFO:create_model() successfully completed......................................
2026-01-30 12:10:26,183:INFO:Initializing create_model()
2026-01-30 12:10:26,183:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:10:26,184:INFO:Checking exceptions
2026-01-30 12:10:26,185:INFO:Importing libraries
2026-01-30 12:10:26,185:INFO:Copying training dataset
2026-01-30 12:10:26,517:INFO:Defining folds
2026-01-30 12:10:26,518:INFO:Declaring metric variables
2026-01-30 12:10:26,518:INFO:Importing untrained model
2026-01-30 12:10:26,518:INFO:Declaring custom model
2026-01-30 12:10:26,518:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:10:26,519:INFO:Cross validation set to False
2026-01-30 12:10:26,519:INFO:Fitting Model
2026-01-30 12:10:29,755:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:10:29,756:INFO:create_model() successfully completed......................................
2026-01-30 12:10:30,023:INFO:_master_model_container: 4
2026-01-30 12:10:30,024:INFO:_display_container: 2
2026-01-30 12:10:30,025:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 12:10:30,025:INFO:compare_models() successfully completed......................................
2026-01-30 12:10:30,034:INFO:Initializing tune_model()
2026-01-30 12:10:30,034:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:10:30,034:INFO:Checking exceptions
2026-01-30 12:10:30,135:INFO:Copying training dataset
2026-01-30 12:10:30,271:INFO:Checking base model
2026-01-30 12:10:30,273:INFO:Base model : Random Forest Classifier
2026-01-30 12:10:30,274:INFO:Declaring metric variables
2026-01-30 12:10:30,274:INFO:Defining Hyperparameters
2026-01-30 12:10:30,441:INFO:Tuning with n_jobs=-1
2026-01-30 12:10:30,442:INFO:Initializing RandomizedSearchCV
2026-01-30 12:13:13,943:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 12:13:13,943:INFO:Hyperparameter search completed
2026-01-30 12:13:13,943:INFO:SubProcess create_model() called ==================================
2026-01-30 12:13:13,943:INFO:Initializing create_model()
2026-01-30 12:13:13,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C7D9690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 12:13:13,943:INFO:Checking exceptions
2026-01-30 12:13:13,943:INFO:Importing libraries
2026-01-30 12:13:13,943:INFO:Copying training dataset
2026-01-30 12:13:14,186:INFO:Defining folds
2026-01-30 12:13:14,186:INFO:Declaring metric variables
2026-01-30 12:13:14,186:INFO:Importing untrained model
2026-01-30 12:13:14,186:INFO:Declaring custom model
2026-01-30 12:13:14,186:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:13:14,186:INFO:Starting cross validation
2026-01-30 12:13:14,186:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:13:46,239:INFO:Calculating mean and std
2026-01-30 12:13:46,239:INFO:Creating metrics dataframe
2026-01-30 12:13:46,239:INFO:Finalizing model
2026-01-30 12:14:02,009:INFO:Uploading results into container
2026-01-30 12:14:02,009:INFO:Uploading model into container now
2026-01-30 12:14:02,009:INFO:_master_model_container: 5
2026-01-30 12:14:02,009:INFO:_display_container: 3
2026-01-30 12:14:02,009:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:14:02,009:INFO:create_model() successfully completed......................................
2026-01-30 12:14:02,202:INFO:SubProcess create_model() end ==================================
2026-01-30 12:14:02,202:INFO:choose_better activated
2026-01-30 12:14:02,202:INFO:SubProcess create_model() called ==================================
2026-01-30 12:14:02,202:INFO:Initializing create_model()
2026-01-30 12:14:02,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:14:02,202:INFO:Checking exceptions
2026-01-30 12:14:02,202:INFO:Importing libraries
2026-01-30 12:14:02,202:INFO:Copying training dataset
2026-01-30 12:14:02,386:INFO:Defining folds
2026-01-30 12:14:02,386:INFO:Declaring metric variables
2026-01-30 12:14:02,386:INFO:Importing untrained model
2026-01-30 12:14:02,386:INFO:Declaring custom model
2026-01-30 12:14:02,386:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:14:02,386:INFO:Starting cross validation
2026-01-30 12:14:02,386:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:14:25,135:INFO:Calculating mean and std
2026-01-30 12:14:25,135:INFO:Creating metrics dataframe
2026-01-30 12:14:25,135:INFO:Finalizing model
2026-01-30 12:14:35,792:INFO:Uploading results into container
2026-01-30 12:14:35,800:INFO:Uploading model into container now
2026-01-30 12:14:35,800:INFO:_master_model_container: 6
2026-01-30 12:14:35,800:INFO:_display_container: 4
2026-01-30 12:14:35,801:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:14:35,801:INFO:create_model() successfully completed......................................
2026-01-30 12:14:35,985:INFO:SubProcess create_model() end ==================================
2026-01-30 12:14:35,985:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9962
2026-01-30 12:14:35,985:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9658
2026-01-30 12:14:35,985:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 12:14:35,985:INFO:choose_better completed
2026-01-30 12:14:35,985:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 12:14:35,985:INFO:_master_model_container: 6
2026-01-30 12:14:35,985:INFO:_display_container: 3
2026-01-30 12:14:35,985:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:14:35,985:INFO:tune_model() successfully completed......................................
2026-01-30 12:14:36,170:INFO:Initializing tune_model()
2026-01-30 12:14:36,170:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:14:36,170:INFO:Checking exceptions
2026-01-30 12:14:36,240:INFO:Copying training dataset
2026-01-30 12:14:36,362:INFO:Checking base model
2026-01-30 12:14:36,362:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 12:14:36,366:INFO:Declaring metric variables
2026-01-30 12:14:36,366:INFO:Defining Hyperparameters
2026-01-30 12:14:36,519:INFO:Tuning with n_jobs=-1
2026-01-30 12:14:36,519:INFO:Initializing RandomizedSearchCV
2026-01-30 12:15:19,258:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 12:15:19,258:INFO:Hyperparameter search completed
2026-01-30 12:15:19,258:INFO:SubProcess create_model() called ==================================
2026-01-30 12:15:19,258:INFO:Initializing create_model()
2026-01-30 12:15:19,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04CA73550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 12:15:19,258:INFO:Checking exceptions
2026-01-30 12:15:19,258:INFO:Importing libraries
2026-01-30 12:15:19,258:INFO:Copying training dataset
2026-01-30 12:15:19,523:INFO:Defining folds
2026-01-30 12:15:19,523:INFO:Declaring metric variables
2026-01-30 12:15:19,524:INFO:Importing untrained model
2026-01-30 12:15:19,524:INFO:Declaring custom model
2026-01-30 12:15:19,526:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:15:19,526:INFO:Starting cross validation
2026-01-30 12:15:19,527:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:15:30,660:INFO:Calculating mean and std
2026-01-30 12:15:30,662:INFO:Creating metrics dataframe
2026-01-30 12:15:30,664:INFO:Finalizing model
2026-01-30 12:15:31,568:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 12:15:31,569:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 12:15:31,569:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 12:15:31,786:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 12:15:31,786:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 12:15:31,786:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 12:15:31,788:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:15:31,853:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013389 seconds.
2026-01-30 12:15:31,853:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:15:31,853:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:15:31,853:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:15:31,855:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:15:31,859:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:15:31,861:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:15:36,027:INFO:Uploading results into container
2026-01-30 12:15:36,028:INFO:Uploading model into container now
2026-01-30 12:15:36,030:INFO:_master_model_container: 7
2026-01-30 12:15:36,030:INFO:_display_container: 4
2026-01-30 12:15:36,032:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:15:36,032:INFO:create_model() successfully completed......................................
2026-01-30 12:15:36,288:INFO:SubProcess create_model() end ==================================
2026-01-30 12:15:36,289:INFO:choose_better activated
2026-01-30 12:15:36,289:INFO:SubProcess create_model() called ==================================
2026-01-30 12:15:36,290:INFO:Initializing create_model()
2026-01-30 12:15:36,290:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:15:36,290:INFO:Checking exceptions
2026-01-30 12:15:36,292:INFO:Importing libraries
2026-01-30 12:15:36,292:INFO:Copying training dataset
2026-01-30 12:15:36,606:INFO:Defining folds
2026-01-30 12:15:36,606:INFO:Declaring metric variables
2026-01-30 12:15:36,607:INFO:Importing untrained model
2026-01-30 12:15:36,607:INFO:Declaring custom model
2026-01-30 12:15:36,608:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:15:36,608:INFO:Starting cross validation
2026-01-30 12:15:36,609:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:15:42,054:INFO:Calculating mean and std
2026-01-30 12:15:42,055:INFO:Creating metrics dataframe
2026-01-30 12:15:42,056:INFO:Finalizing model
2026-01-30 12:15:43,075:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:15:43,135:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013812 seconds.
2026-01-30 12:15:43,136:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:15:43,136:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:15:43,136:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:15:43,137:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:15:43,140:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:15:43,140:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:15:44,105:INFO:Uploading results into container
2026-01-30 12:15:44,107:INFO:Uploading model into container now
2026-01-30 12:15:44,107:INFO:_master_model_container: 8
2026-01-30 12:15:44,107:INFO:_display_container: 5
2026-01-30 12:15:44,107:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:15:44,109:INFO:create_model() successfully completed......................................
2026-01-30 12:15:44,387:INFO:SubProcess create_model() end ==================================
2026-01-30 12:15:44,389:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9766
2026-01-30 12:15:44,390:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9936
2026-01-30 12:15:44,391:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 12:15:44,391:INFO:choose_better completed
2026-01-30 12:15:44,393:INFO:_master_model_container: 8
2026-01-30 12:15:44,393:INFO:_display_container: 4
2026-01-30 12:15:44,395:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:15:44,395:INFO:tune_model() successfully completed......................................
2026-01-30 12:15:44,604:INFO:Initializing tune_model()
2026-01-30 12:15:44,604:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:15:44,605:INFO:Checking exceptions
2026-01-30 12:15:44,708:INFO:Copying training dataset
2026-01-30 12:15:44,838:INFO:Checking base model
2026-01-30 12:15:44,838:INFO:Base model : Decision Tree Classifier
2026-01-30 12:15:44,838:INFO:Declaring metric variables
2026-01-30 12:15:44,839:INFO:Defining Hyperparameters
2026-01-30 12:15:45,030:INFO:Tuning with n_jobs=-1
2026-01-30 12:15:45,030:INFO:Initializing RandomizedSearchCV
2026-01-30 12:15:53,301:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 12:15:53,301:INFO:Hyperparameter search completed
2026-01-30 12:15:53,301:INFO:SubProcess create_model() called ==================================
2026-01-30 12:15:53,301:INFO:Initializing create_model()
2026-01-30 12:15:53,301:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C38F990>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 12:15:53,301:INFO:Checking exceptions
2026-01-30 12:15:53,301:INFO:Importing libraries
2026-01-30 12:15:53,301:INFO:Copying training dataset
2026-01-30 12:15:53,535:INFO:Defining folds
2026-01-30 12:15:53,535:INFO:Declaring metric variables
2026-01-30 12:15:53,535:INFO:Importing untrained model
2026-01-30 12:15:53,535:INFO:Declaring custom model
2026-01-30 12:15:53,536:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:15:53,536:INFO:Starting cross validation
2026-01-30 12:15:53,537:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:15:56,442:INFO:Calculating mean and std
2026-01-30 12:15:56,442:INFO:Creating metrics dataframe
2026-01-30 12:15:56,451:INFO:Finalizing model
2026-01-30 12:15:58,200:INFO:Uploading results into container
2026-01-30 12:15:58,200:INFO:Uploading model into container now
2026-01-30 12:15:58,200:INFO:_master_model_container: 9
2026-01-30 12:15:58,200:INFO:_display_container: 5
2026-01-30 12:15:58,200:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:15:58,200:INFO:create_model() successfully completed......................................
2026-01-30 12:15:58,367:INFO:SubProcess create_model() end ==================================
2026-01-30 12:15:58,367:INFO:choose_better activated
2026-01-30 12:15:58,367:INFO:SubProcess create_model() called ==================================
2026-01-30 12:15:58,367:INFO:Initializing create_model()
2026-01-30 12:15:58,367:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:15:58,367:INFO:Checking exceptions
2026-01-30 12:15:58,367:INFO:Importing libraries
2026-01-30 12:15:58,367:INFO:Copying training dataset
2026-01-30 12:15:58,567:INFO:Defining folds
2026-01-30 12:15:58,567:INFO:Declaring metric variables
2026-01-30 12:15:58,567:INFO:Importing untrained model
2026-01-30 12:15:58,567:INFO:Declaring custom model
2026-01-30 12:15:58,567:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:15:58,567:INFO:Starting cross validation
2026-01-30 12:15:58,567:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:16:02,453:INFO:Calculating mean and std
2026-01-30 12:16:02,455:INFO:Creating metrics dataframe
2026-01-30 12:16:02,456:INFO:Finalizing model
2026-01-30 12:16:05,534:INFO:Uploading results into container
2026-01-30 12:16:05,534:INFO:Uploading model into container now
2026-01-30 12:16:05,534:INFO:_master_model_container: 10
2026-01-30 12:16:05,534:INFO:_display_container: 6
2026-01-30 12:16:05,534:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:16:05,534:INFO:create_model() successfully completed......................................
2026-01-30 12:16:05,700:INFO:SubProcess create_model() end ==================================
2026-01-30 12:16:05,700:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9701
2026-01-30 12:16:05,700:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9544
2026-01-30 12:16:05,700:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 12:16:05,700:INFO:choose_better completed
2026-01-30 12:16:05,700:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 12:16:05,700:INFO:_master_model_container: 10
2026-01-30 12:16:05,700:INFO:_display_container: 5
2026-01-30 12:16:05,700:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:16:05,700:INFO:tune_model() successfully completed......................................
2026-01-30 12:16:05,884:INFO:Initializing predict_model()
2026-01-30 12:16:05,884:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C9E37600>)
2026-01-30 12:16:05,884:INFO:Checking exceptions
2026-01-30 12:16:05,884:INFO:Preloading libraries
2026-01-30 12:16:05,884:INFO:Set up data.
2026-01-30 12:16:05,884:INFO:Set up index.
2026-01-30 12:16:06,400:INFO:Initializing predict_model()
2026-01-30 12:16:06,400:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C9E365C0>)
2026-01-30 12:16:06,400:INFO:Checking exceptions
2026-01-30 12:16:06,400:INFO:Preloading libraries
2026-01-30 12:16:06,400:INFO:Set up data.
2026-01-30 12:16:06,417:INFO:Set up index.
2026-01-30 12:16:06,810:INFO:Initializing predict_model()
2026-01-30 12:16:06,810:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0C9E349A0>)
2026-01-30 12:16:06,810:INFO:Checking exceptions
2026-01-30 12:16:06,810:INFO:Preloading libraries
2026-01-30 12:16:06,810:INFO:Set up data.
2026-01-30 12:16:06,817:INFO:Set up index.
2026-01-30 12:16:07,151:INFO:Initializing plot_model()
2026-01-30 12:16:07,151:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 12:16:07,151:INFO:Checking exceptions
2026-01-30 12:16:07,250:INFO:Preloading libraries
2026-01-30 12:16:07,369:INFO:Copying training dataset
2026-01-30 12:16:07,369:INFO:Plot type: feature
2026-01-30 12:16:07,369:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 12:16:07,700:INFO:Visual Rendered Successfully
2026-01-30 12:16:07,877:INFO:plot_model() successfully completed......................................
2026-01-30 12:16:07,884:INFO:Initializing plot_model()
2026-01-30 12:16:07,884:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C27B3A10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 12:16:07,884:INFO:Checking exceptions
2026-01-30 12:16:07,983:INFO:Preloading libraries
2026-01-30 12:16:08,083:INFO:Copying training dataset
2026-01-30 12:16:08,083:INFO:Plot type: feature_all
2026-01-30 12:16:08,284:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 12:16:08,684:INFO:Visual Rendered Successfully
2026-01-30 12:16:08,850:INFO:plot_model() successfully completed......................................
2026-01-30 12:16:08,873:INFO:Initializing save_model()
2026-01-30 12:16:08,873:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 12:16:08,873:INFO:Adding model into prep_pipe
2026-01-30 12:16:09,017:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 12:16:09,017:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGAD...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 12:16:09,017:INFO:save_model() successfully completed......................................
2026-01-30 12:16:43,042:INFO:Initializing load_model()
2026-01-30 12:16:43,042:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:16:45,200:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\3530218875.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:16:45,700:INFO:Initializing predict_model()
2026-01-30 12:16:45,700:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C4C357590>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C524F4AE0>)
2026-01-30 12:16:45,700:INFO:Checking exceptions
2026-01-30 12:16:45,700:INFO:Preloading libraries
2026-01-30 12:16:45,700:INFO:Set up data.
2026-01-30 12:16:45,800:INFO:Set up index.
2026-01-30 12:19:38,795:INFO:Initializing load_model()
2026-01-30 12:19:38,795:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:19:40,892:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\1468113168.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:21:14,101:INFO:Initializing load_model()
2026-01-30 12:21:14,101:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:21:16,330:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\3896292828.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:22:11,901:INFO:Initializing load_model()
2026-01-30 12:22:11,901:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:22:14,179:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\3420052680.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:23:35,373:INFO:Initializing load_model()
2026-01-30 12:23:35,374:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:23:37,477:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2096248682.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:24:36,048:INFO:Initializing load_model()
2026-01-30 12:24:36,048:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:24:38,299:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2914251258.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:25:49,909:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\3289443975.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 12:25:52,059:INFO:PyCaret ClassificationExperiment
2026-01-30 12:25:52,059:INFO:Logging name: clf-default-name
2026-01-30 12:25:52,059:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 12:25:52,059:INFO:version 3.3.2
2026-01-30 12:25:52,059:INFO:Initializing setup()
2026-01-30 12:25:52,059:INFO:self.USI: 85dd
2026-01-30 12:25:52,059:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 12:25:52,059:INFO:Checking environment
2026-01-30 12:25:52,059:INFO:python_version: 3.11.11
2026-01-30 12:25:52,059:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 12:25:52,059:INFO:machine: AMD64
2026-01-30 12:25:52,059:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 12:25:52,062:INFO:Memory: svmem(total=34009374720, available=12823748608, percent=62.3, used=21185626112, free=12823748608)
2026-01-30 12:25:52,062:INFO:Physical Core: 12
2026-01-30 12:25:52,062:INFO:Logical Core: 16
2026-01-30 12:25:52,062:INFO:Checking libraries
2026-01-30 12:25:52,062:INFO:System:
2026-01-30 12:25:52,062:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 12:25:52,062:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 12:25:52,062:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 12:25:52,062:INFO:PyCaret required dependencies:
2026-01-30 12:25:52,062:INFO:                 pip: 25.0
2026-01-30 12:25:52,062:INFO:          setuptools: 75.8.0
2026-01-30 12:25:52,062:INFO:             pycaret: 3.3.2
2026-01-30 12:25:52,062:INFO:             IPython: 9.9.0
2026-01-30 12:25:52,064:INFO:          ipywidgets: 8.1.8
2026-01-30 12:25:52,064:INFO:                tqdm: 4.67.1
2026-01-30 12:25:52,064:INFO:               numpy: 1.26.4
2026-01-30 12:25:52,064:INFO:              pandas: 2.1.4
2026-01-30 12:25:52,064:INFO:              jinja2: 3.1.6
2026-01-30 12:25:52,064:INFO:               scipy: 1.11.4
2026-01-30 12:25:52,064:INFO:              joblib: 1.3.2
2026-01-30 12:25:52,064:INFO:             sklearn: 1.4.2
2026-01-30 12:25:52,064:INFO:                pyod: 2.0.6
2026-01-30 12:25:52,064:INFO:            imblearn: 0.14.1
2026-01-30 12:25:52,064:INFO:   category_encoders: 2.7.0
2026-01-30 12:25:52,064:INFO:            lightgbm: 4.6.0
2026-01-30 12:25:52,064:INFO:               numba: 0.62.1
2026-01-30 12:25:52,064:INFO:            requests: 2.32.3
2026-01-30 12:25:52,064:INFO:          matplotlib: 3.7.5
2026-01-30 12:25:52,064:INFO:          scikitplot: 0.3.7
2026-01-30 12:25:52,064:INFO:         yellowbrick: 1.5
2026-01-30 12:25:52,064:INFO:              plotly: 5.24.1
2026-01-30 12:25:52,064:INFO:    plotly-resampler: Not installed
2026-01-30 12:25:52,064:INFO:             kaleido: 1.2.0
2026-01-30 12:25:52,064:INFO:           schemdraw: 0.15
2026-01-30 12:25:52,064:INFO:         statsmodels: 0.14.6
2026-01-30 12:25:52,064:INFO:              sktime: 0.26.0
2026-01-30 12:25:52,064:INFO:               tbats: 1.1.3
2026-01-30 12:25:52,064:INFO:            pmdarima: 2.0.4
2026-01-30 12:25:52,064:INFO:              psutil: 7.2.1
2026-01-30 12:25:52,064:INFO:          markupsafe: 3.0.3
2026-01-30 12:25:52,064:INFO:             pickle5: Not installed
2026-01-30 12:25:52,064:INFO:         cloudpickle: 3.0.0
2026-01-30 12:25:52,064:INFO:         deprecation: 2.1.0
2026-01-30 12:25:52,064:INFO:              xxhash: 3.6.0
2026-01-30 12:25:52,064:INFO:           wurlitzer: Not installed
2026-01-30 12:25:52,064:INFO:PyCaret optional dependencies:
2026-01-30 12:25:52,064:INFO:                shap: 0.44.1
2026-01-30 12:25:52,064:INFO:           interpret: 0.7.3
2026-01-30 12:25:52,064:INFO:                umap: 0.5.7
2026-01-30 12:25:52,064:INFO:     ydata_profiling: 4.18.1
2026-01-30 12:25:52,064:INFO:  explainerdashboard: 0.5.1
2026-01-30 12:25:52,064:INFO:             autoviz: Not installed
2026-01-30 12:25:52,064:INFO:           fairlearn: 0.7.0
2026-01-30 12:25:52,064:INFO:          deepchecks: Not installed
2026-01-30 12:25:52,064:INFO:             xgboost: Not installed
2026-01-30 12:25:52,064:INFO:            catboost: 1.2.8
2026-01-30 12:25:52,064:INFO:              kmodes: 0.12.2
2026-01-30 12:25:52,064:INFO:             mlxtend: 0.23.4
2026-01-30 12:25:52,064:INFO:       statsforecast: 1.5.0
2026-01-30 12:25:52,064:INFO:        tune_sklearn: Not installed
2026-01-30 12:25:52,064:INFO:                 ray: Not installed
2026-01-30 12:25:52,064:INFO:            hyperopt: 0.2.7
2026-01-30 12:25:52,064:INFO:              optuna: 4.6.0
2026-01-30 12:25:52,064:INFO:               skopt: 0.10.2
2026-01-30 12:25:52,064:INFO:              mlflow: 3.8.1
2026-01-30 12:25:52,064:INFO:              gradio: 6.3.0
2026-01-30 12:25:52,064:INFO:             fastapi: 0.128.0
2026-01-30 12:25:52,064:INFO:             uvicorn: 0.40.0
2026-01-30 12:25:52,064:INFO:              m2cgen: 0.10.0
2026-01-30 12:25:52,064:INFO:           evidently: 0.4.40
2026-01-30 12:25:52,064:INFO:               fugue: 0.8.7
2026-01-30 12:25:52,064:INFO:           streamlit: Not installed
2026-01-30 12:25:52,064:INFO:             prophet: Not installed
2026-01-30 12:25:52,064:INFO:None
2026-01-30 12:25:52,064:INFO:Set up data.
2026-01-30 12:25:52,192:INFO:Set up folding strategy.
2026-01-30 12:25:52,192:INFO:Set up train/test split.
2026-01-30 12:25:52,392:INFO:Set up index.
2026-01-30 12:25:52,412:INFO:Assigning column types.
2026-01-30 12:25:52,559:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 12:25:52,587:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 12:25:52,587:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:25:52,592:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:25:52,592:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:25:52,626:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 12:25:52,626:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:25:52,645:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:25:52,645:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:25:52,645:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 12:25:52,675:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:25:52,692:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:25:52,692:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:25:52,725:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:25:52,742:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:25:52,742:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:25:52,742:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 12:25:52,799:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:25:52,799:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:25:52,842:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:25:52,842:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:25:52,842:INFO:Preparing preprocessing pipeline...
2026-01-30 12:25:52,875:INFO:Set up simple imputation.
2026-01-30 12:25:52,875:INFO:Set up feature normalization.
2026-01-30 12:25:53,179:INFO:Finished creating preprocessing pipeline.
2026-01-30 12:25:53,179:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 12:25:53,179:INFO:Creating final display dataframe.
2026-01-30 12:25:53,795:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 27)
4        Transformed data shape      (482669, 27)
5   Transformed train set shape      (337868, 27)
6    Transformed test set shape      (144801, 27)
7              Numeric features                23
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              85dd
2026-01-30 12:25:53,841:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:25:53,841:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:25:53,876:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:25:53,876:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:25:53,876:INFO:setup() successfully completed in 1.83s...............
2026-01-30 12:25:53,876:INFO:Initializing compare_models()
2026-01-30 12:25:53,890:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 12:25:53,890:INFO:Checking exceptions
2026-01-30 12:25:53,992:INFO:Preparing display monitor
2026-01-30 12:25:53,992:INFO:Initializing Logistic Regression
2026-01-30 12:25:53,992:INFO:Total runtime is 0.0 minutes
2026-01-30 12:25:53,992:INFO:SubProcess create_model() called ==================================
2026-01-30 12:25:53,992:INFO:Initializing create_model()
2026-01-30 12:25:53,992:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C76F610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:25:53,992:INFO:Checking exceptions
2026-01-30 12:25:53,992:INFO:Importing libraries
2026-01-30 12:25:53,992:INFO:Copying training dataset
2026-01-30 12:25:54,159:INFO:Defining folds
2026-01-30 12:25:54,159:INFO:Declaring metric variables
2026-01-30 12:25:54,159:INFO:Importing untrained model
2026-01-30 12:25:54,159:INFO:Logistic Regression Imported successfully
2026-01-30 12:25:54,159:INFO:Starting cross validation
2026-01-30 12:25:54,159:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:26:02,949:INFO:Calculating mean and std
2026-01-30 12:26:02,951:INFO:Creating metrics dataframe
2026-01-30 12:26:02,954:INFO:Uploading results into container
2026-01-30 12:26:02,954:INFO:Uploading model into container now
2026-01-30 12:26:02,956:INFO:_master_model_container: 1
2026-01-30 12:26:02,956:INFO:_display_container: 2
2026-01-30 12:26:02,956:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 12:26:02,956:INFO:create_model() successfully completed......................................
2026-01-30 12:26:03,125:INFO:SubProcess create_model() end ==================================
2026-01-30 12:26:03,125:INFO:Creating metrics dataframe
2026-01-30 12:26:03,125:INFO:Initializing Decision Tree Classifier
2026-01-30 12:26:03,125:INFO:Total runtime is 0.1522191842397054 minutes
2026-01-30 12:26:03,125:INFO:SubProcess create_model() called ==================================
2026-01-30 12:26:03,125:INFO:Initializing create_model()
2026-01-30 12:26:03,125:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C76F610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:26:03,125:INFO:Checking exceptions
2026-01-30 12:26:03,125:INFO:Importing libraries
2026-01-30 12:26:03,125:INFO:Copying training dataset
2026-01-30 12:26:03,308:INFO:Defining folds
2026-01-30 12:26:03,308:INFO:Declaring metric variables
2026-01-30 12:26:03,308:INFO:Importing untrained model
2026-01-30 12:26:03,308:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:26:03,308:INFO:Starting cross validation
2026-01-30 12:26:03,308:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:26:12,309:INFO:Calculating mean and std
2026-01-30 12:26:12,309:INFO:Creating metrics dataframe
2026-01-30 12:26:12,309:INFO:Uploading results into container
2026-01-30 12:26:12,309:INFO:Uploading model into container now
2026-01-30 12:26:12,309:INFO:_master_model_container: 2
2026-01-30 12:26:12,309:INFO:_display_container: 2
2026-01-30 12:26:12,309:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:26:12,309:INFO:create_model() successfully completed......................................
2026-01-30 12:26:12,476:INFO:SubProcess create_model() end ==================================
2026-01-30 12:26:12,476:INFO:Creating metrics dataframe
2026-01-30 12:26:12,476:INFO:Initializing Random Forest Classifier
2026-01-30 12:26:12,476:INFO:Total runtime is 0.3080564459164937 minutes
2026-01-30 12:26:12,476:INFO:SubProcess create_model() called ==================================
2026-01-30 12:26:12,476:INFO:Initializing create_model()
2026-01-30 12:26:12,476:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C76F610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:26:12,476:INFO:Checking exceptions
2026-01-30 12:26:12,476:INFO:Importing libraries
2026-01-30 12:26:12,476:INFO:Copying training dataset
2026-01-30 12:26:12,662:INFO:Defining folds
2026-01-30 12:26:12,662:INFO:Declaring metric variables
2026-01-30 12:26:12,662:INFO:Importing untrained model
2026-01-30 12:26:12,662:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:26:12,662:INFO:Starting cross validation
2026-01-30 12:26:12,662:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:26:39,226:INFO:Calculating mean and std
2026-01-30 12:26:39,228:INFO:Creating metrics dataframe
2026-01-30 12:26:39,231:INFO:Uploading results into container
2026-01-30 12:26:39,231:INFO:Uploading model into container now
2026-01-30 12:26:39,231:INFO:_master_model_container: 3
2026-01-30 12:26:39,231:INFO:_display_container: 2
2026-01-30 12:26:39,231:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:26:39,231:INFO:create_model() successfully completed......................................
2026-01-30 12:26:39,425:INFO:SubProcess create_model() end ==================================
2026-01-30 12:26:39,425:INFO:Creating metrics dataframe
2026-01-30 12:26:39,425:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 12:26:39,425:INFO:Total runtime is 0.7572128812472025 minutes
2026-01-30 12:26:39,425:INFO:SubProcess create_model() called ==================================
2026-01-30 12:26:39,425:INFO:Initializing create_model()
2026-01-30 12:26:39,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C76F610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:26:39,425:INFO:Checking exceptions
2026-01-30 12:26:39,425:INFO:Importing libraries
2026-01-30 12:26:39,425:INFO:Copying training dataset
2026-01-30 12:26:39,642:INFO:Defining folds
2026-01-30 12:26:39,642:INFO:Declaring metric variables
2026-01-30 12:26:39,642:INFO:Importing untrained model
2026-01-30 12:26:39,642:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:26:39,642:INFO:Starting cross validation
2026-01-30 12:26:39,642:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:26:50,092:INFO:Calculating mean and std
2026-01-30 12:26:50,094:INFO:Creating metrics dataframe
2026-01-30 12:26:50,099:INFO:Uploading results into container
2026-01-30 12:26:50,099:INFO:Uploading model into container now
2026-01-30 12:26:50,100:INFO:_master_model_container: 4
2026-01-30 12:26:50,100:INFO:_display_container: 2
2026-01-30 12:26:50,101:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:26:50,101:INFO:create_model() successfully completed......................................
2026-01-30 12:26:50,273:INFO:SubProcess create_model() end ==================================
2026-01-30 12:26:50,273:INFO:Creating metrics dataframe
2026-01-30 12:26:50,273:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 12:26:50,288:INFO:Initializing create_model()
2026-01-30 12:26:50,289:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:26:50,289:INFO:Checking exceptions
2026-01-30 12:26:50,289:INFO:Importing libraries
2026-01-30 12:26:50,289:INFO:Copying training dataset
2026-01-30 12:26:50,457:INFO:Defining folds
2026-01-30 12:26:50,457:INFO:Declaring metric variables
2026-01-30 12:26:50,457:INFO:Importing untrained model
2026-01-30 12:26:50,457:INFO:Declaring custom model
2026-01-30 12:26:50,457:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:26:50,457:INFO:Cross validation set to False
2026-01-30 12:26:50,457:INFO:Fitting Model
2026-01-30 12:27:01,198:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:27:01,198:INFO:create_model() successfully completed......................................
2026-01-30 12:27:01,379:INFO:Initializing create_model()
2026-01-30 12:27:01,379:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:27:01,379:INFO:Checking exceptions
2026-01-30 12:27:01,379:INFO:Importing libraries
2026-01-30 12:27:01,379:INFO:Copying training dataset
2026-01-30 12:27:01,560:INFO:Defining folds
2026-01-30 12:27:01,560:INFO:Declaring metric variables
2026-01-30 12:27:01,560:INFO:Importing untrained model
2026-01-30 12:27:01,560:INFO:Declaring custom model
2026-01-30 12:27:01,560:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:27:01,560:INFO:Cross validation set to False
2026-01-30 12:27:01,560:INFO:Fitting Model
2026-01-30 12:27:02,383:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:27:02,446:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015792 seconds.
2026-01-30 12:27:02,446:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:27:02,446:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:27:02,446:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:27:02,448:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:27:02,450:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:27:02,450:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:27:03,268:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:27:03,270:INFO:create_model() successfully completed......................................
2026-01-30 12:27:03,510:INFO:Initializing create_model()
2026-01-30 12:27:03,510:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:27:03,511:INFO:Checking exceptions
2026-01-30 12:27:03,511:INFO:Importing libraries
2026-01-30 12:27:03,511:INFO:Copying training dataset
2026-01-30 12:27:03,796:INFO:Defining folds
2026-01-30 12:27:03,796:INFO:Declaring metric variables
2026-01-30 12:27:03,796:INFO:Importing untrained model
2026-01-30 12:27:03,796:INFO:Declaring custom model
2026-01-30 12:27:03,796:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:27:03,798:INFO:Cross validation set to False
2026-01-30 12:27:03,798:INFO:Fitting Model
2026-01-30 12:27:06,623:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:27:06,623:INFO:create_model() successfully completed......................................
2026-01-30 12:27:06,799:INFO:_master_model_container: 4
2026-01-30 12:27:06,799:INFO:_display_container: 2
2026-01-30 12:27:06,801:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 12:27:06,801:INFO:compare_models() successfully completed......................................
2026-01-30 12:27:06,812:INFO:Initializing tune_model()
2026-01-30 12:27:06,812:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:27:06,813:INFO:Checking exceptions
2026-01-30 12:27:06,877:INFO:Copying training dataset
2026-01-30 12:27:06,989:INFO:Checking base model
2026-01-30 12:27:06,989:INFO:Base model : Random Forest Classifier
2026-01-30 12:27:06,989:INFO:Declaring metric variables
2026-01-30 12:27:06,989:INFO:Defining Hyperparameters
2026-01-30 12:27:07,141:INFO:Tuning with n_jobs=-1
2026-01-30 12:27:07,141:INFO:Initializing RandomizedSearchCV
2026-01-30 12:30:01,424:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 12:30:01,425:INFO:Hyperparameter search completed
2026-01-30 12:30:01,426:INFO:SubProcess create_model() called ==================================
2026-01-30 12:30:01,427:INFO:Initializing create_model()
2026-01-30 12:30:01,428:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03CFFB5D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 12:30:01,428:INFO:Checking exceptions
2026-01-30 12:30:01,428:INFO:Importing libraries
2026-01-30 12:30:01,428:INFO:Copying training dataset
2026-01-30 12:30:01,717:INFO:Defining folds
2026-01-30 12:30:01,718:INFO:Declaring metric variables
2026-01-30 12:30:01,718:INFO:Importing untrained model
2026-01-30 12:30:01,718:INFO:Declaring custom model
2026-01-30 12:30:01,719:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:30:01,719:INFO:Starting cross validation
2026-01-30 12:30:01,720:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:30:34,356:INFO:Calculating mean and std
2026-01-30 12:30:34,356:INFO:Creating metrics dataframe
2026-01-30 12:30:34,356:INFO:Finalizing model
2026-01-30 12:30:50,379:INFO:Uploading results into container
2026-01-30 12:30:50,380:INFO:Uploading model into container now
2026-01-30 12:30:50,381:INFO:_master_model_container: 5
2026-01-30 12:30:50,381:INFO:_display_container: 3
2026-01-30 12:30:50,381:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:30:50,381:INFO:create_model() successfully completed......................................
2026-01-30 12:30:50,571:INFO:SubProcess create_model() end ==================================
2026-01-30 12:30:50,571:INFO:choose_better activated
2026-01-30 12:30:50,571:INFO:SubProcess create_model() called ==================================
2026-01-30 12:30:50,572:INFO:Initializing create_model()
2026-01-30 12:30:50,572:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:30:50,572:INFO:Checking exceptions
2026-01-30 12:30:50,573:INFO:Importing libraries
2026-01-30 12:30:50,573:INFO:Copying training dataset
2026-01-30 12:30:50,821:INFO:Defining folds
2026-01-30 12:30:50,821:INFO:Declaring metric variables
2026-01-30 12:30:50,821:INFO:Importing untrained model
2026-01-30 12:30:50,821:INFO:Declaring custom model
2026-01-30 12:30:50,821:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:30:50,821:INFO:Starting cross validation
2026-01-30 12:30:50,821:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:31:14,374:INFO:Calculating mean and std
2026-01-30 12:31:14,374:INFO:Creating metrics dataframe
2026-01-30 12:31:14,374:INFO:Finalizing model
2026-01-30 12:31:26,214:INFO:Uploading results into container
2026-01-30 12:31:26,215:INFO:Uploading model into container now
2026-01-30 12:31:26,215:INFO:_master_model_container: 6
2026-01-30 12:31:26,216:INFO:_display_container: 4
2026-01-30 12:31:26,216:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:31:26,216:INFO:create_model() successfully completed......................................
2026-01-30 12:31:26,410:INFO:SubProcess create_model() end ==================================
2026-01-30 12:31:26,410:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9962
2026-01-30 12:31:26,411:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9658
2026-01-30 12:31:26,411:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 12:31:26,411:INFO:choose_better completed
2026-01-30 12:31:26,411:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 12:31:26,413:INFO:_master_model_container: 6
2026-01-30 12:31:26,414:INFO:_display_container: 3
2026-01-30 12:31:26,414:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:31:26,414:INFO:tune_model() successfully completed......................................
2026-01-30 12:31:26,590:INFO:Initializing tune_model()
2026-01-30 12:31:26,590:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:31:26,590:INFO:Checking exceptions
2026-01-30 12:31:26,664:INFO:Copying training dataset
2026-01-30 12:31:26,800:INFO:Checking base model
2026-01-30 12:31:26,800:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 12:31:26,801:INFO:Declaring metric variables
2026-01-30 12:31:26,801:INFO:Defining Hyperparameters
2026-01-30 12:31:26,977:INFO:Tuning with n_jobs=-1
2026-01-30 12:31:26,977:INFO:Initializing RandomizedSearchCV
2026-01-30 12:32:09,587:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 12:32:09,589:INFO:Hyperparameter search completed
2026-01-30 12:32:09,589:INFO:SubProcess create_model() called ==================================
2026-01-30 12:32:09,591:INFO:Initializing create_model()
2026-01-30 12:32:09,591:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0482018D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 12:32:09,591:INFO:Checking exceptions
2026-01-30 12:32:09,591:INFO:Importing libraries
2026-01-30 12:32:09,591:INFO:Copying training dataset
2026-01-30 12:32:09,853:INFO:Defining folds
2026-01-30 12:32:09,853:INFO:Declaring metric variables
2026-01-30 12:32:09,853:INFO:Importing untrained model
2026-01-30 12:32:09,853:INFO:Declaring custom model
2026-01-30 12:32:09,853:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:32:09,853:INFO:Starting cross validation
2026-01-30 12:32:09,853:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:32:19,890:INFO:Calculating mean and std
2026-01-30 12:32:19,890:INFO:Creating metrics dataframe
2026-01-30 12:32:19,890:INFO:Finalizing model
2026-01-30 12:32:20,637:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 12:32:20,637:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 12:32:20,637:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 12:32:20,834:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 12:32:20,834:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 12:32:20,834:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 12:32:20,835:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:32:20,893:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013462 seconds.
2026-01-30 12:32:20,894:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:32:20,894:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:32:20,894:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:32:20,895:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:32:20,901:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:32:20,901:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:32:24,666:INFO:Uploading results into container
2026-01-30 12:32:24,668:INFO:Uploading model into container now
2026-01-30 12:32:24,668:INFO:_master_model_container: 7
2026-01-30 12:32:24,669:INFO:_display_container: 4
2026-01-30 12:32:24,670:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:32:24,671:INFO:create_model() successfully completed......................................
2026-01-30 12:32:24,919:INFO:SubProcess create_model() end ==================================
2026-01-30 12:32:24,921:INFO:choose_better activated
2026-01-30 12:32:24,921:INFO:SubProcess create_model() called ==================================
2026-01-30 12:32:24,922:INFO:Initializing create_model()
2026-01-30 12:32:24,922:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:32:24,923:INFO:Checking exceptions
2026-01-30 12:32:24,924:INFO:Importing libraries
2026-01-30 12:32:24,924:INFO:Copying training dataset
2026-01-30 12:32:25,203:INFO:Defining folds
2026-01-30 12:32:25,203:INFO:Declaring metric variables
2026-01-30 12:32:25,203:INFO:Importing untrained model
2026-01-30 12:32:25,203:INFO:Declaring custom model
2026-01-30 12:32:25,203:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:32:25,203:INFO:Starting cross validation
2026-01-30 12:32:25,203:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:32:30,797:INFO:Calculating mean and std
2026-01-30 12:32:30,798:INFO:Creating metrics dataframe
2026-01-30 12:32:30,801:INFO:Finalizing model
2026-01-30 12:32:31,883:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:32:31,943:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.017205 seconds.
2026-01-30 12:32:31,943:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:32:31,945:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:32:31,945:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:32:31,945:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:32:31,950:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:32:31,950:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:32:33,287:INFO:Uploading results into container
2026-01-30 12:32:33,288:INFO:Uploading model into container now
2026-01-30 12:32:33,289:INFO:_master_model_container: 8
2026-01-30 12:32:33,289:INFO:_display_container: 5
2026-01-30 12:32:33,290:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:32:33,291:INFO:create_model() successfully completed......................................
2026-01-30 12:32:33,538:INFO:SubProcess create_model() end ==================================
2026-01-30 12:32:33,538:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9766
2026-01-30 12:32:33,554:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9936
2026-01-30 12:32:33,555:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 12:32:33,555:INFO:choose_better completed
2026-01-30 12:32:33,555:INFO:_master_model_container: 8
2026-01-30 12:32:33,555:INFO:_display_container: 4
2026-01-30 12:32:33,555:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:32:33,555:INFO:tune_model() successfully completed......................................
2026-01-30 12:32:33,802:INFO:Initializing tune_model()
2026-01-30 12:32:33,802:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:32:33,802:INFO:Checking exceptions
2026-01-30 12:32:33,956:INFO:Copying training dataset
2026-01-30 12:32:34,103:INFO:Checking base model
2026-01-30 12:32:34,103:INFO:Base model : Decision Tree Classifier
2026-01-30 12:32:34,103:INFO:Declaring metric variables
2026-01-30 12:32:34,103:INFO:Defining Hyperparameters
2026-01-30 12:32:34,290:INFO:Tuning with n_jobs=-1
2026-01-30 12:32:34,290:INFO:Initializing RandomizedSearchCV
2026-01-30 12:32:43,006:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 12:32:43,008:INFO:Hyperparameter search completed
2026-01-30 12:32:43,008:INFO:SubProcess create_model() called ==================================
2026-01-30 12:32:43,009:INFO:Initializing create_model()
2026-01-30 12:32:43,009:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C9FF2490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 12:32:43,009:INFO:Checking exceptions
2026-01-30 12:32:43,009:INFO:Importing libraries
2026-01-30 12:32:43,009:INFO:Copying training dataset
2026-01-30 12:32:43,237:INFO:Defining folds
2026-01-30 12:32:43,238:INFO:Declaring metric variables
2026-01-30 12:32:43,238:INFO:Importing untrained model
2026-01-30 12:32:43,238:INFO:Declaring custom model
2026-01-30 12:32:43,239:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:32:43,239:INFO:Starting cross validation
2026-01-30 12:32:43,240:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:32:46,062:INFO:Calculating mean and std
2026-01-30 12:32:46,062:INFO:Creating metrics dataframe
2026-01-30 12:32:46,070:INFO:Finalizing model
2026-01-30 12:32:47,919:INFO:Uploading results into container
2026-01-30 12:32:47,919:INFO:Uploading model into container now
2026-01-30 12:32:47,919:INFO:_master_model_container: 9
2026-01-30 12:32:47,919:INFO:_display_container: 5
2026-01-30 12:32:47,919:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:32:47,919:INFO:create_model() successfully completed......................................
2026-01-30 12:32:48,103:INFO:SubProcess create_model() end ==================================
2026-01-30 12:32:48,103:INFO:choose_better activated
2026-01-30 12:32:48,103:INFO:SubProcess create_model() called ==================================
2026-01-30 12:32:48,103:INFO:Initializing create_model()
2026-01-30 12:32:48,103:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:32:48,103:INFO:Checking exceptions
2026-01-30 12:32:48,103:INFO:Importing libraries
2026-01-30 12:32:48,103:INFO:Copying training dataset
2026-01-30 12:32:48,303:INFO:Defining folds
2026-01-30 12:32:48,303:INFO:Declaring metric variables
2026-01-30 12:32:48,303:INFO:Importing untrained model
2026-01-30 12:32:48,303:INFO:Declaring custom model
2026-01-30 12:32:48,303:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:32:48,303:INFO:Starting cross validation
2026-01-30 12:32:48,303:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:32:52,440:INFO:Calculating mean and std
2026-01-30 12:32:52,441:INFO:Creating metrics dataframe
2026-01-30 12:32:52,442:INFO:Finalizing model
2026-01-30 12:32:55,552:INFO:Uploading results into container
2026-01-30 12:32:55,552:INFO:Uploading model into container now
2026-01-30 12:32:55,552:INFO:_master_model_container: 10
2026-01-30 12:32:55,552:INFO:_display_container: 6
2026-01-30 12:32:55,552:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:32:55,552:INFO:create_model() successfully completed......................................
2026-01-30 12:32:55,719:INFO:SubProcess create_model() end ==================================
2026-01-30 12:32:55,719:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9701
2026-01-30 12:32:55,719:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9544
2026-01-30 12:32:55,719:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 12:32:55,719:INFO:choose_better completed
2026-01-30 12:32:55,719:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 12:32:55,719:INFO:_master_model_container: 10
2026-01-30 12:32:55,719:INFO:_display_container: 5
2026-01-30 12:32:55,719:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:32:55,719:INFO:tune_model() successfully completed......................................
2026-01-30 12:32:55,902:INFO:Initializing predict_model()
2026-01-30 12:32:55,902:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A07498FC40>)
2026-01-30 12:32:55,902:INFO:Checking exceptions
2026-01-30 12:32:55,902:INFO:Preloading libraries
2026-01-30 12:32:55,903:INFO:Set up data.
2026-01-30 12:32:55,903:INFO:Set up index.
2026-01-30 12:32:56,291:INFO:Initializing predict_model()
2026-01-30 12:32:56,291:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0530D3BA0>)
2026-01-30 12:32:56,291:INFO:Checking exceptions
2026-01-30 12:32:56,291:INFO:Preloading libraries
2026-01-30 12:32:56,291:INFO:Set up data.
2026-01-30 12:32:56,301:INFO:Set up index.
2026-01-30 12:32:56,685:INFO:Initializing predict_model()
2026-01-30 12:32:56,685:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04D030E90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0530D3920>)
2026-01-30 12:32:56,685:INFO:Checking exceptions
2026-01-30 12:32:56,685:INFO:Preloading libraries
2026-01-30 12:32:56,685:INFO:Set up data.
2026-01-30 12:32:56,706:INFO:Set up index.
2026-01-30 12:36:25,958:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\1851865221.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 12:36:28,250:INFO:PyCaret ClassificationExperiment
2026-01-30 12:36:28,250:INFO:Logging name: clf-default-name
2026-01-30 12:36:28,250:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 12:36:28,250:INFO:version 3.3.2
2026-01-30 12:36:28,250:INFO:Initializing setup()
2026-01-30 12:36:28,250:INFO:self.USI: 0c31
2026-01-30 12:36:28,250:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 12:36:28,250:INFO:Checking environment
2026-01-30 12:36:28,250:INFO:python_version: 3.11.11
2026-01-30 12:36:28,250:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 12:36:28,250:INFO:machine: AMD64
2026-01-30 12:36:28,250:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 12:36:28,250:INFO:Memory: svmem(total=34009374720, available=9214246912, percent=72.9, used=24795127808, free=9214246912)
2026-01-30 12:36:28,250:INFO:Physical Core: 12
2026-01-30 12:36:28,250:INFO:Logical Core: 16
2026-01-30 12:36:28,250:INFO:Checking libraries
2026-01-30 12:36:28,250:INFO:System:
2026-01-30 12:36:28,250:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 12:36:28,250:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 12:36:28,250:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 12:36:28,250:INFO:PyCaret required dependencies:
2026-01-30 12:36:28,250:INFO:                 pip: 25.0
2026-01-30 12:36:28,250:INFO:          setuptools: 75.8.0
2026-01-30 12:36:28,250:INFO:             pycaret: 3.3.2
2026-01-30 12:36:28,250:INFO:             IPython: 9.9.0
2026-01-30 12:36:28,250:INFO:          ipywidgets: 8.1.8
2026-01-30 12:36:28,250:INFO:                tqdm: 4.67.1
2026-01-30 12:36:28,250:INFO:               numpy: 1.26.4
2026-01-30 12:36:28,250:INFO:              pandas: 2.1.4
2026-01-30 12:36:28,250:INFO:              jinja2: 3.1.6
2026-01-30 12:36:28,250:INFO:               scipy: 1.11.4
2026-01-30 12:36:28,250:INFO:              joblib: 1.3.2
2026-01-30 12:36:28,250:INFO:             sklearn: 1.4.2
2026-01-30 12:36:28,250:INFO:                pyod: 2.0.6
2026-01-30 12:36:28,250:INFO:            imblearn: 0.14.1
2026-01-30 12:36:28,250:INFO:   category_encoders: 2.7.0
2026-01-30 12:36:28,250:INFO:            lightgbm: 4.6.0
2026-01-30 12:36:28,250:INFO:               numba: 0.62.1
2026-01-30 12:36:28,250:INFO:            requests: 2.32.3
2026-01-30 12:36:28,250:INFO:          matplotlib: 3.7.5
2026-01-30 12:36:28,250:INFO:          scikitplot: 0.3.7
2026-01-30 12:36:28,250:INFO:         yellowbrick: 1.5
2026-01-30 12:36:28,250:INFO:              plotly: 5.24.1
2026-01-30 12:36:28,250:INFO:    plotly-resampler: Not installed
2026-01-30 12:36:28,250:INFO:             kaleido: 1.2.0
2026-01-30 12:36:28,250:INFO:           schemdraw: 0.15
2026-01-30 12:36:28,250:INFO:         statsmodels: 0.14.6
2026-01-30 12:36:28,250:INFO:              sktime: 0.26.0
2026-01-30 12:36:28,250:INFO:               tbats: 1.1.3
2026-01-30 12:36:28,250:INFO:            pmdarima: 2.0.4
2026-01-30 12:36:28,250:INFO:              psutil: 7.2.1
2026-01-30 12:36:28,250:INFO:          markupsafe: 3.0.3
2026-01-30 12:36:28,250:INFO:             pickle5: Not installed
2026-01-30 12:36:28,250:INFO:         cloudpickle: 3.0.0
2026-01-30 12:36:28,250:INFO:         deprecation: 2.1.0
2026-01-30 12:36:28,250:INFO:              xxhash: 3.6.0
2026-01-30 12:36:28,250:INFO:           wurlitzer: Not installed
2026-01-30 12:36:28,250:INFO:PyCaret optional dependencies:
2026-01-30 12:36:28,250:INFO:                shap: 0.44.1
2026-01-30 12:36:28,250:INFO:           interpret: 0.7.3
2026-01-30 12:36:28,250:INFO:                umap: 0.5.7
2026-01-30 12:36:28,250:INFO:     ydata_profiling: 4.18.1
2026-01-30 12:36:28,250:INFO:  explainerdashboard: 0.5.1
2026-01-30 12:36:28,250:INFO:             autoviz: Not installed
2026-01-30 12:36:28,250:INFO:           fairlearn: 0.7.0
2026-01-30 12:36:28,250:INFO:          deepchecks: Not installed
2026-01-30 12:36:28,250:INFO:             xgboost: Not installed
2026-01-30 12:36:28,250:INFO:            catboost: 1.2.8
2026-01-30 12:36:28,250:INFO:              kmodes: 0.12.2
2026-01-30 12:36:28,250:INFO:             mlxtend: 0.23.4
2026-01-30 12:36:28,250:INFO:       statsforecast: 1.5.0
2026-01-30 12:36:28,250:INFO:        tune_sklearn: Not installed
2026-01-30 12:36:28,250:INFO:                 ray: Not installed
2026-01-30 12:36:28,250:INFO:            hyperopt: 0.2.7
2026-01-30 12:36:28,250:INFO:              optuna: 4.6.0
2026-01-30 12:36:28,250:INFO:               skopt: 0.10.2
2026-01-30 12:36:28,250:INFO:              mlflow: 3.8.1
2026-01-30 12:36:28,250:INFO:              gradio: 6.3.0
2026-01-30 12:36:28,250:INFO:             fastapi: 0.128.0
2026-01-30 12:36:28,250:INFO:             uvicorn: 0.40.0
2026-01-30 12:36:28,250:INFO:              m2cgen: 0.10.0
2026-01-30 12:36:28,250:INFO:           evidently: 0.4.40
2026-01-30 12:36:28,250:INFO:               fugue: 0.8.7
2026-01-30 12:36:28,250:INFO:           streamlit: Not installed
2026-01-30 12:36:28,250:INFO:             prophet: Not installed
2026-01-30 12:36:28,250:INFO:None
2026-01-30 12:36:28,250:INFO:Set up data.
2026-01-30 12:36:28,381:INFO:Set up folding strategy.
2026-01-30 12:36:28,381:INFO:Set up train/test split.
2026-01-30 12:36:28,606:INFO:Set up index.
2026-01-30 12:36:28,615:INFO:Assigning column types.
2026-01-30 12:36:28,833:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 12:36:28,873:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 12:36:28,874:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:36:28,896:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:36:28,896:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:36:28,928:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 12:36:28,929:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:36:28,952:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:36:28,952:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:36:28,953:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 12:36:28,985:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:36:29,003:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:36:29,004:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:36:29,036:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:36:29,055:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:36:29,056:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:36:29,056:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 12:36:29,104:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:36:29,104:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:36:29,154:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:36:29,154:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:36:29,155:INFO:Preparing preprocessing pipeline...
2026-01-30 12:36:29,185:INFO:Set up simple imputation.
2026-01-30 12:36:29,185:INFO:Set up feature normalization.
2026-01-30 12:36:29,473:INFO:Finished creating preprocessing pipeline.
2026-01-30 12:36:29,476:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 12:36:29,476:INFO:Creating final display dataframe.
2026-01-30 12:36:30,161:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 27)
4        Transformed data shape      (482669, 27)
5   Transformed train set shape      (337868, 27)
6    Transformed test set shape      (144801, 27)
7              Numeric features                23
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              0c31
2026-01-30 12:36:30,242:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:36:30,243:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:36:30,315:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:36:30,316:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:36:30,318:INFO:setup() successfully completed in 2.1s...............
2026-01-30 12:36:30,318:INFO:Initializing compare_models()
2026-01-30 12:36:30,319:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 12:36:30,319:INFO:Checking exceptions
2026-01-30 12:36:30,517:INFO:Preparing display monitor
2026-01-30 12:36:30,521:INFO:Initializing Logistic Regression
2026-01-30 12:36:30,521:INFO:Total runtime is 0.0 minutes
2026-01-30 12:36:30,522:INFO:SubProcess create_model() called ==================================
2026-01-30 12:36:30,522:INFO:Initializing create_model()
2026-01-30 12:36:30,522:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0482EB790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:36:30,522:INFO:Checking exceptions
2026-01-30 12:36:30,522:INFO:Importing libraries
2026-01-30 12:36:30,522:INFO:Copying training dataset
2026-01-30 12:36:30,851:INFO:Defining folds
2026-01-30 12:36:30,851:INFO:Declaring metric variables
2026-01-30 12:36:30,852:INFO:Importing untrained model
2026-01-30 12:36:30,852:INFO:Logistic Regression Imported successfully
2026-01-30 12:36:30,853:INFO:Starting cross validation
2026-01-30 12:36:30,854:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:36:34,589:INFO:Calculating mean and std
2026-01-30 12:36:34,589:INFO:Creating metrics dataframe
2026-01-30 12:36:34,594:INFO:Uploading results into container
2026-01-30 12:36:34,594:INFO:Uploading model into container now
2026-01-30 12:36:34,594:INFO:_master_model_container: 1
2026-01-30 12:36:34,594:INFO:_display_container: 2
2026-01-30 12:36:34,594:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 12:36:34,594:INFO:create_model() successfully completed......................................
2026-01-30 12:36:34,822:INFO:SubProcess create_model() end ==================================
2026-01-30 12:36:34,823:INFO:Creating metrics dataframe
2026-01-30 12:36:34,825:INFO:Initializing Decision Tree Classifier
2026-01-30 12:36:34,825:INFO:Total runtime is 0.07174501021703085 minutes
2026-01-30 12:36:34,825:INFO:SubProcess create_model() called ==================================
2026-01-30 12:36:34,825:INFO:Initializing create_model()
2026-01-30 12:36:34,825:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0482EB790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:36:34,825:INFO:Checking exceptions
2026-01-30 12:36:34,825:INFO:Importing libraries
2026-01-30 12:36:34,825:INFO:Copying training dataset
2026-01-30 12:36:35,034:INFO:Defining folds
2026-01-30 12:36:35,035:INFO:Declaring metric variables
2026-01-30 12:36:35,035:INFO:Importing untrained model
2026-01-30 12:36:35,035:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:36:35,036:INFO:Starting cross validation
2026-01-30 12:36:35,036:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:36:38,300:INFO:Calculating mean and std
2026-01-30 12:36:38,305:INFO:Creating metrics dataframe
2026-01-30 12:36:38,306:INFO:Uploading results into container
2026-01-30 12:36:38,307:INFO:Uploading model into container now
2026-01-30 12:36:38,307:INFO:_master_model_container: 2
2026-01-30 12:36:38,307:INFO:_display_container: 2
2026-01-30 12:36:38,308:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:36:38,308:INFO:create_model() successfully completed......................................
2026-01-30 12:36:38,464:INFO:SubProcess create_model() end ==================================
2026-01-30 12:36:38,464:INFO:Creating metrics dataframe
2026-01-30 12:36:38,464:INFO:Initializing Random Forest Classifier
2026-01-30 12:36:38,464:INFO:Total runtime is 0.13239903052647908 minutes
2026-01-30 12:36:38,464:INFO:SubProcess create_model() called ==================================
2026-01-30 12:36:38,464:INFO:Initializing create_model()
2026-01-30 12:36:38,464:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0482EB790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:36:38,464:INFO:Checking exceptions
2026-01-30 12:36:38,464:INFO:Importing libraries
2026-01-30 12:36:38,464:INFO:Copying training dataset
2026-01-30 12:36:38,640:INFO:Defining folds
2026-01-30 12:36:38,641:INFO:Declaring metric variables
2026-01-30 12:36:38,641:INFO:Importing untrained model
2026-01-30 12:36:38,642:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:36:38,642:INFO:Starting cross validation
2026-01-30 12:36:38,643:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:36:58,226:INFO:Calculating mean and std
2026-01-30 12:36:58,226:INFO:Creating metrics dataframe
2026-01-30 12:36:58,226:INFO:Uploading results into container
2026-01-30 12:36:58,226:INFO:Uploading model into container now
2026-01-30 12:36:58,232:INFO:_master_model_container: 3
2026-01-30 12:36:58,232:INFO:_display_container: 2
2026-01-30 12:36:58,233:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:36:58,233:INFO:create_model() successfully completed......................................
2026-01-30 12:36:58,434:INFO:SubProcess create_model() end ==================================
2026-01-30 12:36:58,434:INFO:Creating metrics dataframe
2026-01-30 12:36:58,437:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 12:36:58,437:INFO:Total runtime is 0.46527887980143234 minutes
2026-01-30 12:36:58,437:INFO:SubProcess create_model() called ==================================
2026-01-30 12:36:58,438:INFO:Initializing create_model()
2026-01-30 12:36:58,438:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0482EB790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:36:58,438:INFO:Checking exceptions
2026-01-30 12:36:58,438:INFO:Importing libraries
2026-01-30 12:36:58,438:INFO:Copying training dataset
2026-01-30 12:36:58,631:INFO:Defining folds
2026-01-30 12:36:58,632:INFO:Declaring metric variables
2026-01-30 12:36:58,632:INFO:Importing untrained model
2026-01-30 12:36:58,633:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:36:58,633:INFO:Starting cross validation
2026-01-30 12:36:58,634:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:37:03,379:INFO:Calculating mean and std
2026-01-30 12:37:03,381:INFO:Creating metrics dataframe
2026-01-30 12:37:03,383:INFO:Uploading results into container
2026-01-30 12:37:03,383:INFO:Uploading model into container now
2026-01-30 12:37:03,385:INFO:_master_model_container: 4
2026-01-30 12:37:03,385:INFO:_display_container: 2
2026-01-30 12:37:03,385:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:37:03,386:INFO:create_model() successfully completed......................................
2026-01-30 12:37:03,549:INFO:SubProcess create_model() end ==================================
2026-01-30 12:37:03,549:INFO:Creating metrics dataframe
2026-01-30 12:37:03,552:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 12:37:03,553:INFO:Initializing create_model()
2026-01-30 12:37:03,554:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:37:03,554:INFO:Checking exceptions
2026-01-30 12:37:03,554:INFO:Importing libraries
2026-01-30 12:37:03,554:INFO:Copying training dataset
2026-01-30 12:37:03,687:INFO:Initializing load_model()
2026-01-30 12:37:03,687:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:37:03,749:INFO:Defining folds
2026-01-30 12:37:03,750:INFO:Declaring metric variables
2026-01-30 12:37:03,750:INFO:Importing untrained model
2026-01-30 12:37:03,750:INFO:Declaring custom model
2026-01-30 12:37:03,750:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:37:03,750:INFO:Cross validation set to False
2026-01-30 12:37:03,751:INFO:Fitting Model
2026-01-30 12:37:11,801:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2096248682.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:37:15,476:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:37:15,477:INFO:create_model() successfully completed......................................
2026-01-30 12:37:15,665:INFO:Initializing create_model()
2026-01-30 12:37:15,665:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:37:15,665:INFO:Checking exceptions
2026-01-30 12:37:15,665:INFO:Importing libraries
2026-01-30 12:37:15,665:INFO:Copying training dataset
2026-01-30 12:37:15,899:INFO:Defining folds
2026-01-30 12:37:15,899:INFO:Declaring metric variables
2026-01-30 12:37:15,899:INFO:Importing untrained model
2026-01-30 12:37:15,899:INFO:Declaring custom model
2026-01-30 12:37:15,899:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:37:15,899:INFO:Cross validation set to False
2026-01-30 12:37:15,899:INFO:Fitting Model
2026-01-30 12:37:16,774:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:37:16,826:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014579 seconds.
2026-01-30 12:37:16,826:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:37:16,826:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:37:16,826:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:37:16,826:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:37:16,829:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:37:16,829:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:37:17,718:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:37:17,718:INFO:create_model() successfully completed......................................
2026-01-30 12:37:17,952:INFO:Initializing create_model()
2026-01-30 12:37:17,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:37:17,952:INFO:Checking exceptions
2026-01-30 12:37:17,953:INFO:Importing libraries
2026-01-30 12:37:17,953:INFO:Copying training dataset
2026-01-30 12:37:18,146:INFO:Defining folds
2026-01-30 12:37:18,146:INFO:Declaring metric variables
2026-01-30 12:37:18,146:INFO:Importing untrained model
2026-01-30 12:37:18,146:INFO:Declaring custom model
2026-01-30 12:37:18,147:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:37:18,148:INFO:Cross validation set to False
2026-01-30 12:37:18,148:INFO:Fitting Model
2026-01-30 12:37:21,144:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:37:21,144:INFO:create_model() successfully completed......................................
2026-01-30 12:37:21,313:INFO:_master_model_container: 4
2026-01-30 12:37:21,314:INFO:_display_container: 2
2026-01-30 12:37:21,315:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 12:37:21,315:INFO:compare_models() successfully completed......................................
2026-01-30 12:37:21,316:INFO:Initializing tune_model()
2026-01-30 12:37:21,316:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:37:21,316:INFO:Checking exceptions
2026-01-30 12:37:21,396:INFO:Copying training dataset
2026-01-30 12:37:21,511:INFO:Checking base model
2026-01-30 12:37:21,511:INFO:Base model : Random Forest Classifier
2026-01-30 12:37:21,511:INFO:Declaring metric variables
2026-01-30 12:37:21,511:INFO:Defining Hyperparameters
2026-01-30 12:37:21,666:INFO:Tuning with n_jobs=-1
2026-01-30 12:37:21,666:INFO:Initializing RandomizedSearchCV
2026-01-30 12:40:12,955:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 12:40:12,955:INFO:Hyperparameter search completed
2026-01-30 12:40:12,955:INFO:SubProcess create_model() called ==================================
2026-01-30 12:40:12,955:INFO:Initializing create_model()
2026-01-30 12:40:12,955:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C8C9A1D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 12:40:12,955:INFO:Checking exceptions
2026-01-30 12:40:12,955:INFO:Importing libraries
2026-01-30 12:40:12,955:INFO:Copying training dataset
2026-01-30 12:40:13,441:INFO:Defining folds
2026-01-30 12:40:13,441:INFO:Declaring metric variables
2026-01-30 12:40:13,442:INFO:Importing untrained model
2026-01-30 12:40:13,442:INFO:Declaring custom model
2026-01-30 12:40:13,444:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:40:13,444:INFO:Starting cross validation
2026-01-30 12:40:13,446:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:40:48,663:INFO:Calculating mean and std
2026-01-30 12:40:48,663:INFO:Creating metrics dataframe
2026-01-30 12:40:48,663:INFO:Finalizing model
2026-01-30 12:41:04,648:INFO:Uploading results into container
2026-01-30 12:41:04,648:INFO:Uploading model into container now
2026-01-30 12:41:04,648:INFO:_master_model_container: 5
2026-01-30 12:41:04,648:INFO:_display_container: 3
2026-01-30 12:41:04,648:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:41:04,648:INFO:create_model() successfully completed......................................
2026-01-30 12:41:04,879:INFO:SubProcess create_model() end ==================================
2026-01-30 12:41:04,880:INFO:choose_better activated
2026-01-30 12:41:04,880:INFO:SubProcess create_model() called ==================================
2026-01-30 12:41:04,881:INFO:Initializing create_model()
2026-01-30 12:41:04,881:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:41:04,881:INFO:Checking exceptions
2026-01-30 12:41:04,882:INFO:Importing libraries
2026-01-30 12:41:04,882:INFO:Copying training dataset
2026-01-30 12:41:05,083:INFO:Defining folds
2026-01-30 12:41:05,083:INFO:Declaring metric variables
2026-01-30 12:41:05,083:INFO:Importing untrained model
2026-01-30 12:41:05,083:INFO:Declaring custom model
2026-01-30 12:41:05,083:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:41:05,083:INFO:Starting cross validation
2026-01-30 12:41:05,083:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:41:28,258:INFO:Calculating mean and std
2026-01-30 12:41:28,258:INFO:Creating metrics dataframe
2026-01-30 12:41:28,261:INFO:Finalizing model
2026-01-30 12:41:39,145:INFO:Uploading results into container
2026-01-30 12:41:39,145:INFO:Uploading model into container now
2026-01-30 12:41:39,145:INFO:_master_model_container: 6
2026-01-30 12:41:39,145:INFO:_display_container: 4
2026-01-30 12:41:39,145:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:41:39,145:INFO:create_model() successfully completed......................................
2026-01-30 12:41:39,332:INFO:SubProcess create_model() end ==================================
2026-01-30 12:41:39,332:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9962
2026-01-30 12:41:39,332:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9658
2026-01-30 12:41:39,332:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 12:41:39,332:INFO:choose_better completed
2026-01-30 12:41:39,332:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 12:41:39,332:INFO:_master_model_container: 6
2026-01-30 12:41:39,332:INFO:_display_container: 3
2026-01-30 12:41:39,332:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:41:39,332:INFO:tune_model() successfully completed......................................
2026-01-30 12:41:39,500:INFO:Initializing tune_model()
2026-01-30 12:41:39,500:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:41:39,500:INFO:Checking exceptions
2026-01-30 12:41:39,579:INFO:Copying training dataset
2026-01-30 12:41:39,715:INFO:Checking base model
2026-01-30 12:41:39,715:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 12:41:39,715:INFO:Declaring metric variables
2026-01-30 12:41:39,715:INFO:Defining Hyperparameters
2026-01-30 12:41:39,879:INFO:Tuning with n_jobs=-1
2026-01-30 12:41:39,879:INFO:Initializing RandomizedSearchCV
2026-01-30 12:42:22,265:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 12:42:22,266:INFO:Hyperparameter search completed
2026-01-30 12:42:22,267:INFO:SubProcess create_model() called ==================================
2026-01-30 12:42:22,268:INFO:Initializing create_model()
2026-01-30 12:42:22,268:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0D02B1690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 12:42:22,268:INFO:Checking exceptions
2026-01-30 12:42:22,268:INFO:Importing libraries
2026-01-30 12:42:22,268:INFO:Copying training dataset
2026-01-30 12:42:22,557:INFO:Defining folds
2026-01-30 12:42:22,557:INFO:Declaring metric variables
2026-01-30 12:42:22,558:INFO:Importing untrained model
2026-01-30 12:42:22,558:INFO:Declaring custom model
2026-01-30 12:42:22,559:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:42:22,559:INFO:Starting cross validation
2026-01-30 12:42:22,560:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:42:33,355:INFO:Calculating mean and std
2026-01-30 12:42:33,357:INFO:Creating metrics dataframe
2026-01-30 12:42:33,361:INFO:Finalizing model
2026-01-30 12:42:34,136:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 12:42:34,136:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 12:42:34,137:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 12:42:34,341:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 12:42:34,341:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 12:42:34,341:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 12:42:34,342:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:42:34,400:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013876 seconds.
2026-01-30 12:42:34,400:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:42:34,400:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:42:34,401:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:42:34,402:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:42:34,407:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:42:34,407:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:42:38,068:INFO:Uploading results into container
2026-01-30 12:42:38,071:INFO:Uploading model into container now
2026-01-30 12:42:38,071:INFO:_master_model_container: 7
2026-01-30 12:42:38,073:INFO:_display_container: 4
2026-01-30 12:42:38,073:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:42:38,073:INFO:create_model() successfully completed......................................
2026-01-30 12:42:38,316:INFO:SubProcess create_model() end ==================================
2026-01-30 12:42:38,324:INFO:choose_better activated
2026-01-30 12:42:38,324:INFO:SubProcess create_model() called ==================================
2026-01-30 12:42:38,324:INFO:Initializing create_model()
2026-01-30 12:42:38,324:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:42:38,324:INFO:Checking exceptions
2026-01-30 12:42:38,324:INFO:Importing libraries
2026-01-30 12:42:38,324:INFO:Copying training dataset
2026-01-30 12:42:38,561:INFO:Defining folds
2026-01-30 12:42:38,561:INFO:Declaring metric variables
2026-01-30 12:42:38,561:INFO:Importing untrained model
2026-01-30 12:42:38,561:INFO:Declaring custom model
2026-01-30 12:42:38,561:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:42:38,561:INFO:Starting cross validation
2026-01-30 12:42:38,561:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:42:43,524:INFO:Calculating mean and std
2026-01-30 12:42:43,524:INFO:Creating metrics dataframe
2026-01-30 12:42:43,530:INFO:Finalizing model
2026-01-30 12:42:44,414:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:42:44,472:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012333 seconds.
2026-01-30 12:42:44,472:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:42:44,472:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:42:44,474:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:42:44,474:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:42:44,476:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:42:44,476:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:42:45,389:INFO:Uploading results into container
2026-01-30 12:42:45,389:INFO:Uploading model into container now
2026-01-30 12:42:45,391:INFO:_master_model_container: 8
2026-01-30 12:42:45,391:INFO:_display_container: 5
2026-01-30 12:42:45,391:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:42:45,391:INFO:create_model() successfully completed......................................
2026-01-30 12:42:45,629:INFO:SubProcess create_model() end ==================================
2026-01-30 12:42:45,629:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9766
2026-01-30 12:42:45,629:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9936
2026-01-30 12:42:45,629:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 12:42:45,629:INFO:choose_better completed
2026-01-30 12:42:45,629:INFO:_master_model_container: 8
2026-01-30 12:42:45,629:INFO:_display_container: 4
2026-01-30 12:42:45,629:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:42:45,629:INFO:tune_model() successfully completed......................................
2026-01-30 12:42:45,814:INFO:Initializing tune_model()
2026-01-30 12:42:45,814:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:42:45,814:INFO:Checking exceptions
2026-01-30 12:42:45,896:INFO:Copying training dataset
2026-01-30 12:42:46,047:INFO:Checking base model
2026-01-30 12:42:46,047:INFO:Base model : Decision Tree Classifier
2026-01-30 12:42:46,047:INFO:Declaring metric variables
2026-01-30 12:42:46,047:INFO:Defining Hyperparameters
2026-01-30 12:42:46,231:INFO:Tuning with n_jobs=-1
2026-01-30 12:42:46,231:INFO:Initializing RandomizedSearchCV
2026-01-30 12:42:54,514:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 12:42:54,516:INFO:Hyperparameter search completed
2026-01-30 12:42:54,516:INFO:SubProcess create_model() called ==================================
2026-01-30 12:42:54,516:INFO:Initializing create_model()
2026-01-30 12:42:54,516:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A074FB5810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 12:42:54,516:INFO:Checking exceptions
2026-01-30 12:42:54,516:INFO:Importing libraries
2026-01-30 12:42:54,516:INFO:Copying training dataset
2026-01-30 12:42:54,706:INFO:Defining folds
2026-01-30 12:42:54,707:INFO:Declaring metric variables
2026-01-30 12:42:54,707:INFO:Importing untrained model
2026-01-30 12:42:54,707:INFO:Declaring custom model
2026-01-30 12:42:54,707:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:42:54,707:INFO:Starting cross validation
2026-01-30 12:42:54,707:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:42:57,457:INFO:Calculating mean and std
2026-01-30 12:42:57,460:INFO:Creating metrics dataframe
2026-01-30 12:42:57,461:INFO:Finalizing model
2026-01-30 12:42:59,305:INFO:Uploading results into container
2026-01-30 12:42:59,306:INFO:Uploading model into container now
2026-01-30 12:42:59,306:INFO:_master_model_container: 9
2026-01-30 12:42:59,306:INFO:_display_container: 5
2026-01-30 12:42:59,306:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:42:59,306:INFO:create_model() successfully completed......................................
2026-01-30 12:42:59,477:INFO:SubProcess create_model() end ==================================
2026-01-30 12:42:59,477:INFO:choose_better activated
2026-01-30 12:42:59,477:INFO:SubProcess create_model() called ==================================
2026-01-30 12:42:59,478:INFO:Initializing create_model()
2026-01-30 12:42:59,478:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:42:59,478:INFO:Checking exceptions
2026-01-30 12:42:59,479:INFO:Importing libraries
2026-01-30 12:42:59,479:INFO:Copying training dataset
2026-01-30 12:42:59,659:INFO:Defining folds
2026-01-30 12:42:59,659:INFO:Declaring metric variables
2026-01-30 12:42:59,659:INFO:Importing untrained model
2026-01-30 12:42:59,659:INFO:Declaring custom model
2026-01-30 12:42:59,659:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:42:59,659:INFO:Starting cross validation
2026-01-30 12:42:59,659:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:43:03,619:INFO:Calculating mean and std
2026-01-30 12:43:03,621:INFO:Creating metrics dataframe
2026-01-30 12:43:03,624:INFO:Finalizing model
2026-01-30 12:43:07,682:INFO:Uploading results into container
2026-01-30 12:43:07,684:INFO:Uploading model into container now
2026-01-30 12:43:07,684:INFO:_master_model_container: 10
2026-01-30 12:43:07,684:INFO:_display_container: 6
2026-01-30 12:43:07,685:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:43:07,685:INFO:create_model() successfully completed......................................
2026-01-30 12:43:08,010:INFO:SubProcess create_model() end ==================================
2026-01-30 12:43:08,010:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9701
2026-01-30 12:43:08,010:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9544
2026-01-30 12:43:08,010:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 12:43:08,010:INFO:choose_better completed
2026-01-30 12:43:08,010:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 12:43:08,010:INFO:_master_model_container: 10
2026-01-30 12:43:08,010:INFO:_display_container: 5
2026-01-30 12:43:08,010:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:43:08,010:INFO:tune_model() successfully completed......................................
2026-01-30 12:43:08,205:INFO:Initializing predict_model()
2026-01-30 12:43:08,205:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A0CCB9BCE0>)
2026-01-30 12:43:08,205:INFO:Checking exceptions
2026-01-30 12:43:08,205:INFO:Preloading libraries
2026-01-30 12:43:08,205:INFO:Set up data.
2026-01-30 12:43:08,219:INFO:Set up index.
2026-01-30 12:43:08,662:INFO:Initializing predict_model()
2026-01-30 12:43:08,662:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A051942B60>)
2026-01-30 12:43:08,662:INFO:Checking exceptions
2026-01-30 12:43:08,662:INFO:Preloading libraries
2026-01-30 12:43:08,662:INFO:Set up data.
2026-01-30 12:43:08,673:INFO:Set up index.
2026-01-30 12:43:09,185:INFO:Initializing predict_model()
2026-01-30 12:43:09,186:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A051942B60>)
2026-01-30 12:43:09,186:INFO:Checking exceptions
2026-01-30 12:43:09,186:INFO:Preloading libraries
2026-01-30 12:43:09,186:INFO:Set up data.
2026-01-30 12:43:09,201:INFO:Set up index.
2026-01-30 12:43:09,818:INFO:Initializing plot_model()
2026-01-30 12:43:09,818:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 12:43:09,818:INFO:Checking exceptions
2026-01-30 12:43:10,016:INFO:Preloading libraries
2026-01-30 12:43:10,199:INFO:Copying training dataset
2026-01-30 12:43:10,199:INFO:Plot type: feature
2026-01-30 12:43:10,200:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 12:43:10,706:INFO:Visual Rendered Successfully
2026-01-30 12:43:10,902:INFO:plot_model() successfully completed......................................
2026-01-30 12:43:10,913:INFO:Initializing plot_model()
2026-01-30 12:43:10,914:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04C237610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 12:43:10,914:INFO:Checking exceptions
2026-01-30 12:43:11,031:INFO:Preloading libraries
2026-01-30 12:43:11,172:INFO:Copying training dataset
2026-01-30 12:43:11,172:INFO:Plot type: feature_all
2026-01-30 12:43:11,473:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 12:43:11,981:INFO:Visual Rendered Successfully
2026-01-30 12:43:12,220:INFO:plot_model() successfully completed......................................
2026-01-30 12:43:12,243:INFO:Initializing save_model()
2026-01-30 12:43:12,244:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 12:43:12,244:INFO:Adding model into prep_pipe
2026-01-30 12:43:12,536:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 12:43:12,545:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGAD...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 12:43:12,545:INFO:save_model() successfully completed......................................
2026-01-30 12:45:51,310:INFO:Initializing load_model()
2026-01-30 12:45:51,310:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:45:53,375:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2096248682.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:47:26,019:INFO:Initializing load_model()
2026-01-30 12:47:26,020:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:47:28,290:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\785099566.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:47:28,878:INFO:Initializing predict_model()
2026-01-30 12:47:28,878:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C38048450>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C5A86D080>)
2026-01-30 12:47:28,878:INFO:Checking exceptions
2026-01-30 12:47:28,878:INFO:Preloading libraries
2026-01-30 12:47:28,878:INFO:Set up data.
2026-01-30 12:47:28,890:INFO:Set up index.
2026-01-30 12:48:34,343:INFO:Initializing load_model()
2026-01-30 12:48:34,344:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:48:36,575:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\1243188443.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:48:37,190:INFO:Initializing predict_model()
2026-01-30 12:48:37,190:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C4AFCAF10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C65678CC0>)
2026-01-30 12:48:37,190:INFO:Checking exceptions
2026-01-30 12:48:37,190:INFO:Preloading libraries
2026-01-30 12:48:37,190:INFO:Set up data.
2026-01-30 12:48:37,206:INFO:Set up index.
2026-01-30 12:50:17,219:INFO:Initializing load_model()
2026-01-30 12:50:17,219:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:50:19,386:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\84628162.py:25: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:51:20,249:INFO:Initializing load_model()
2026-01-30 12:51:20,249:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:51:22,491:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\1593748511.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:51:57,562:INFO:Initializing load_model()
2026-01-30 12:51:57,562:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:51:59,753:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\1815458068.py:26: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:52:00,870:INFO:Initializing predict_model()
2026-01-30 12:52:00,870:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C38002250>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C4C3823E0>)
2026-01-30 12:52:00,870:INFO:Checking exceptions
2026-01-30 12:52:00,870:INFO:Preloading libraries
2026-01-30 12:52:00,870:INFO:Set up data.
2026-01-30 12:52:01,870:INFO:Set up index.
2026-01-30 12:52:39,467:INFO:Initializing load_model()
2026-01-30 12:52:39,467:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:52:41,620:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\951826523.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:52:42,786:INFO:Initializing predict_model()
2026-01-30 12:52:42,786:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C650AB290>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C5B1AA160>)
2026-01-30 12:52:42,786:INFO:Checking exceptions
2026-01-30 12:52:42,786:INFO:Preloading libraries
2026-01-30 12:52:42,786:INFO:Set up data.
2026-01-30 12:52:43,706:INFO:Set up index.
2026-01-30 12:53:31,306:INFO:Initializing load_model()
2026-01-30 12:53:31,306:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:53:33,525:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\1700848726.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:54:37,169:INFO:Initializing load_model()
2026-01-30 12:54:37,170:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 12:54:39,450:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\1700848726.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 12:57:04,918:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\2175477632.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 12:57:07,097:INFO:PyCaret ClassificationExperiment
2026-01-30 12:57:07,098:INFO:Logging name: clf-default-name
2026-01-30 12:57:07,099:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 12:57:07,099:INFO:version 3.3.2
2026-01-30 12:57:07,099:INFO:Initializing setup()
2026-01-30 12:57:07,099:INFO:self.USI: d9aa
2026-01-30 12:57:07,099:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 12:57:07,099:INFO:Checking environment
2026-01-30 12:57:07,099:INFO:python_version: 3.11.11
2026-01-30 12:57:07,099:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 12:57:07,099:INFO:machine: AMD64
2026-01-30 12:57:07,099:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 12:57:07,099:INFO:Memory: svmem(total=34009374720, available=12575571968, percent=63.0, used=21433802752, free=12575571968)
2026-01-30 12:57:07,099:INFO:Physical Core: 12
2026-01-30 12:57:07,099:INFO:Logical Core: 16
2026-01-30 12:57:07,099:INFO:Checking libraries
2026-01-30 12:57:07,099:INFO:System:
2026-01-30 12:57:07,099:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 12:57:07,099:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 12:57:07,099:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 12:57:07,099:INFO:PyCaret required dependencies:
2026-01-30 12:57:07,099:INFO:                 pip: 25.0
2026-01-30 12:57:07,099:INFO:          setuptools: 75.8.0
2026-01-30 12:57:07,099:INFO:             pycaret: 3.3.2
2026-01-30 12:57:07,099:INFO:             IPython: 9.9.0
2026-01-30 12:57:07,099:INFO:          ipywidgets: 8.1.8
2026-01-30 12:57:07,099:INFO:                tqdm: 4.67.1
2026-01-30 12:57:07,099:INFO:               numpy: 1.26.4
2026-01-30 12:57:07,099:INFO:              pandas: 2.1.4
2026-01-30 12:57:07,099:INFO:              jinja2: 3.1.6
2026-01-30 12:57:07,099:INFO:               scipy: 1.11.4
2026-01-30 12:57:07,099:INFO:              joblib: 1.3.2
2026-01-30 12:57:07,099:INFO:             sklearn: 1.4.2
2026-01-30 12:57:07,099:INFO:                pyod: 2.0.6
2026-01-30 12:57:07,099:INFO:            imblearn: 0.14.1
2026-01-30 12:57:07,099:INFO:   category_encoders: 2.7.0
2026-01-30 12:57:07,099:INFO:            lightgbm: 4.6.0
2026-01-30 12:57:07,099:INFO:               numba: 0.62.1
2026-01-30 12:57:07,099:INFO:            requests: 2.32.3
2026-01-30 12:57:07,099:INFO:          matplotlib: 3.7.5
2026-01-30 12:57:07,099:INFO:          scikitplot: 0.3.7
2026-01-30 12:57:07,099:INFO:         yellowbrick: 1.5
2026-01-30 12:57:07,099:INFO:              plotly: 5.24.1
2026-01-30 12:57:07,099:INFO:    plotly-resampler: Not installed
2026-01-30 12:57:07,099:INFO:             kaleido: 1.2.0
2026-01-30 12:57:07,099:INFO:           schemdraw: 0.15
2026-01-30 12:57:07,099:INFO:         statsmodels: 0.14.6
2026-01-30 12:57:07,099:INFO:              sktime: 0.26.0
2026-01-30 12:57:07,099:INFO:               tbats: 1.1.3
2026-01-30 12:57:07,099:INFO:            pmdarima: 2.0.4
2026-01-30 12:57:07,099:INFO:              psutil: 7.2.1
2026-01-30 12:57:07,099:INFO:          markupsafe: 3.0.3
2026-01-30 12:57:07,099:INFO:             pickle5: Not installed
2026-01-30 12:57:07,099:INFO:         cloudpickle: 3.0.0
2026-01-30 12:57:07,099:INFO:         deprecation: 2.1.0
2026-01-30 12:57:07,099:INFO:              xxhash: 3.6.0
2026-01-30 12:57:07,099:INFO:           wurlitzer: Not installed
2026-01-30 12:57:07,099:INFO:PyCaret optional dependencies:
2026-01-30 12:57:07,099:INFO:                shap: 0.44.1
2026-01-30 12:57:07,099:INFO:           interpret: 0.7.3
2026-01-30 12:57:07,099:INFO:                umap: 0.5.7
2026-01-30 12:57:07,099:INFO:     ydata_profiling: 4.18.1
2026-01-30 12:57:07,099:INFO:  explainerdashboard: 0.5.1
2026-01-30 12:57:07,099:INFO:             autoviz: Not installed
2026-01-30 12:57:07,099:INFO:           fairlearn: 0.7.0
2026-01-30 12:57:07,099:INFO:          deepchecks: Not installed
2026-01-30 12:57:07,099:INFO:             xgboost: Not installed
2026-01-30 12:57:07,099:INFO:            catboost: 1.2.8
2026-01-30 12:57:07,099:INFO:              kmodes: 0.12.2
2026-01-30 12:57:07,099:INFO:             mlxtend: 0.23.4
2026-01-30 12:57:07,099:INFO:       statsforecast: 1.5.0
2026-01-30 12:57:07,099:INFO:        tune_sklearn: Not installed
2026-01-30 12:57:07,099:INFO:                 ray: Not installed
2026-01-30 12:57:07,099:INFO:            hyperopt: 0.2.7
2026-01-30 12:57:07,099:INFO:              optuna: 4.6.0
2026-01-30 12:57:07,099:INFO:               skopt: 0.10.2
2026-01-30 12:57:07,099:INFO:              mlflow: 3.8.1
2026-01-30 12:57:07,099:INFO:              gradio: 6.3.0
2026-01-30 12:57:07,099:INFO:             fastapi: 0.128.0
2026-01-30 12:57:07,099:INFO:             uvicorn: 0.40.0
2026-01-30 12:57:07,099:INFO:              m2cgen: 0.10.0
2026-01-30 12:57:07,099:INFO:           evidently: 0.4.40
2026-01-30 12:57:07,099:INFO:               fugue: 0.8.7
2026-01-30 12:57:07,099:INFO:           streamlit: Not installed
2026-01-30 12:57:07,099:INFO:             prophet: Not installed
2026-01-30 12:57:07,099:INFO:None
2026-01-30 12:57:07,099:INFO:Set up data.
2026-01-30 12:57:07,283:INFO:Set up folding strategy.
2026-01-30 12:57:07,283:INFO:Set up train/test split.
2026-01-30 12:57:07,599:INFO:Set up index.
2026-01-30 12:57:07,615:INFO:Assigning column types.
2026-01-30 12:57:07,869:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 12:57:07,920:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 12:57:07,921:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:57:07,949:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:57:07,949:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:57:07,999:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 12:57:07,999:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:57:08,033:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:57:08,033:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:57:08,033:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 12:57:08,083:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:57:08,116:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:57:08,116:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:57:08,171:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 12:57:08,203:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:57:08,203:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:57:08,203:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 12:57:08,282:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:57:08,282:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:57:08,366:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:57:08,366:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:57:08,366:INFO:Preparing preprocessing pipeline...
2026-01-30 12:57:08,416:INFO:Set up simple imputation.
2026-01-30 12:57:08,416:INFO:Set up feature normalization.
2026-01-30 12:57:09,300:INFO:Finished creating preprocessing pipeline.
2026-01-30 12:57:09,315:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 12:57:09,316:INFO:Creating final display dataframe.
2026-01-30 12:57:10,452:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 27)
4        Transformed data shape      (482669, 27)
5   Transformed train set shape      (337868, 27)
6    Transformed test set shape      (144801, 27)
7              Numeric features                23
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              d9aa
2026-01-30 12:57:10,516:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:57:10,516:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:57:10,566:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 12:57:10,566:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 12:57:10,581:INFO:setup() successfully completed in 3.52s...............
2026-01-30 12:57:10,581:INFO:Initializing compare_models()
2026-01-30 12:57:10,581:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 12:57:10,581:INFO:Checking exceptions
2026-01-30 12:57:10,716:INFO:Preparing display monitor
2026-01-30 12:57:10,719:INFO:Initializing Logistic Regression
2026-01-30 12:57:10,719:INFO:Total runtime is 0.0 minutes
2026-01-30 12:57:10,720:INFO:SubProcess create_model() called ==================================
2026-01-30 12:57:10,720:INFO:Initializing create_model()
2026-01-30 12:57:10,720:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04EEBD690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:57:10,720:INFO:Checking exceptions
2026-01-30 12:57:10,720:INFO:Importing libraries
2026-01-30 12:57:10,720:INFO:Copying training dataset
2026-01-30 12:57:10,949:INFO:Defining folds
2026-01-30 12:57:10,949:INFO:Declaring metric variables
2026-01-30 12:57:10,949:INFO:Importing untrained model
2026-01-30 12:57:10,950:INFO:Logistic Regression Imported successfully
2026-01-30 12:57:10,950:INFO:Starting cross validation
2026-01-30 12:57:10,951:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:57:21,259:INFO:Calculating mean and std
2026-01-30 12:57:21,261:INFO:Creating metrics dataframe
2026-01-30 12:57:21,262:INFO:Uploading results into container
2026-01-30 12:57:21,263:INFO:Uploading model into container now
2026-01-30 12:57:21,263:INFO:_master_model_container: 1
2026-01-30 12:57:21,263:INFO:_display_container: 2
2026-01-30 12:57:21,264:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 12:57:21,264:INFO:create_model() successfully completed......................................
2026-01-30 12:57:21,433:INFO:SubProcess create_model() end ==================================
2026-01-30 12:57:21,433:INFO:Creating metrics dataframe
2026-01-30 12:57:21,433:INFO:Initializing Decision Tree Classifier
2026-01-30 12:57:21,433:INFO:Total runtime is 0.17856082518895466 minutes
2026-01-30 12:57:21,433:INFO:SubProcess create_model() called ==================================
2026-01-30 12:57:21,433:INFO:Initializing create_model()
2026-01-30 12:57:21,433:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04EEBD690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:57:21,433:INFO:Checking exceptions
2026-01-30 12:57:21,433:INFO:Importing libraries
2026-01-30 12:57:21,433:INFO:Copying training dataset
2026-01-30 12:57:21,642:INFO:Defining folds
2026-01-30 12:57:21,642:INFO:Declaring metric variables
2026-01-30 12:57:21,642:INFO:Importing untrained model
2026-01-30 12:57:21,642:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:57:21,642:INFO:Starting cross validation
2026-01-30 12:57:21,643:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:57:30,242:INFO:Calculating mean and std
2026-01-30 12:57:30,245:INFO:Creating metrics dataframe
2026-01-30 12:57:30,248:INFO:Uploading results into container
2026-01-30 12:57:30,250:INFO:Uploading model into container now
2026-01-30 12:57:30,250:INFO:_master_model_container: 2
2026-01-30 12:57:30,251:INFO:_display_container: 2
2026-01-30 12:57:30,252:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:57:30,252:INFO:create_model() successfully completed......................................
2026-01-30 12:57:30,456:INFO:SubProcess create_model() end ==================================
2026-01-30 12:57:30,457:INFO:Creating metrics dataframe
2026-01-30 12:57:30,458:INFO:Initializing Random Forest Classifier
2026-01-30 12:57:30,458:INFO:Total runtime is 0.3289838433265686 minutes
2026-01-30 12:57:30,458:INFO:SubProcess create_model() called ==================================
2026-01-30 12:57:30,459:INFO:Initializing create_model()
2026-01-30 12:57:30,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04EEBD690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:57:30,459:INFO:Checking exceptions
2026-01-30 12:57:30,459:INFO:Importing libraries
2026-01-30 12:57:30,459:INFO:Copying training dataset
2026-01-30 12:57:30,649:INFO:Defining folds
2026-01-30 12:57:30,665:INFO:Declaring metric variables
2026-01-30 12:57:30,665:INFO:Importing untrained model
2026-01-30 12:57:30,665:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:57:30,666:INFO:Starting cross validation
2026-01-30 12:57:30,666:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:57:52,579:INFO:Calculating mean and std
2026-01-30 12:57:52,580:INFO:Creating metrics dataframe
2026-01-30 12:57:52,582:INFO:Uploading results into container
2026-01-30 12:57:52,582:INFO:Uploading model into container now
2026-01-30 12:57:52,583:INFO:_master_model_container: 3
2026-01-30 12:57:52,583:INFO:_display_container: 2
2026-01-30 12:57:52,583:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:57:52,583:INFO:create_model() successfully completed......................................
2026-01-30 12:57:52,800:INFO:SubProcess create_model() end ==================================
2026-01-30 12:57:52,800:INFO:Creating metrics dataframe
2026-01-30 12:57:52,815:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 12:57:52,816:INFO:Total runtime is 0.7016204833984375 minutes
2026-01-30 12:57:52,816:INFO:SubProcess create_model() called ==================================
2026-01-30 12:57:52,816:INFO:Initializing create_model()
2026-01-30 12:57:52,816:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04EEBD690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:57:52,816:INFO:Checking exceptions
2026-01-30 12:57:52,816:INFO:Importing libraries
2026-01-30 12:57:52,817:INFO:Copying training dataset
2026-01-30 12:57:53,237:INFO:Defining folds
2026-01-30 12:57:53,238:INFO:Declaring metric variables
2026-01-30 12:57:53,238:INFO:Importing untrained model
2026-01-30 12:57:53,239:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:57:53,240:INFO:Starting cross validation
2026-01-30 12:57:53,241:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 12:58:03,843:INFO:Calculating mean and std
2026-01-30 12:58:03,844:INFO:Creating metrics dataframe
2026-01-30 12:58:03,846:INFO:Uploading results into container
2026-01-30 12:58:03,847:INFO:Uploading model into container now
2026-01-30 12:58:03,847:INFO:_master_model_container: 4
2026-01-30 12:58:03,847:INFO:_display_container: 2
2026-01-30 12:58:03,848:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:58:03,848:INFO:create_model() successfully completed......................................
2026-01-30 12:58:04,048:INFO:SubProcess create_model() end ==================================
2026-01-30 12:58:04,048:INFO:Creating metrics dataframe
2026-01-30 12:58:04,051:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 12:58:04,053:INFO:Initializing create_model()
2026-01-30 12:58:04,053:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:58:04,053:INFO:Checking exceptions
2026-01-30 12:58:04,054:INFO:Importing libraries
2026-01-30 12:58:04,054:INFO:Copying training dataset
2026-01-30 12:58:04,344:INFO:Defining folds
2026-01-30 12:58:04,344:INFO:Declaring metric variables
2026-01-30 12:58:04,344:INFO:Importing untrained model
2026-01-30 12:58:04,344:INFO:Declaring custom model
2026-01-30 12:58:04,345:INFO:Random Forest Classifier Imported successfully
2026-01-30 12:58:04,346:INFO:Cross validation set to False
2026-01-30 12:58:04,346:INFO:Fitting Model
2026-01-30 12:58:13,814:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 12:58:13,814:INFO:create_model() successfully completed......................................
2026-01-30 12:58:14,032:INFO:Initializing create_model()
2026-01-30 12:58:14,032:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:58:14,032:INFO:Checking exceptions
2026-01-30 12:58:14,033:INFO:Importing libraries
2026-01-30 12:58:14,033:INFO:Copying training dataset
2026-01-30 12:58:14,255:INFO:Defining folds
2026-01-30 12:58:14,255:INFO:Declaring metric variables
2026-01-30 12:58:14,255:INFO:Importing untrained model
2026-01-30 12:58:14,255:INFO:Declaring custom model
2026-01-30 12:58:14,256:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 12:58:14,257:INFO:Cross validation set to False
2026-01-30 12:58:14,257:INFO:Fitting Model
2026-01-30 12:58:15,285:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 12:58:15,336:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014229 seconds.
2026-01-30 12:58:15,337:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 12:58:15,337:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 12:58:15,337:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 12:58:15,338:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 12:58:15,340:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 12:58:15,340:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 12:58:16,047:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 12:58:16,048:INFO:create_model() successfully completed......................................
2026-01-30 12:58:16,268:INFO:Initializing create_model()
2026-01-30 12:58:16,268:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 12:58:16,268:INFO:Checking exceptions
2026-01-30 12:58:16,269:INFO:Importing libraries
2026-01-30 12:58:16,269:INFO:Copying training dataset
2026-01-30 12:58:16,502:INFO:Defining folds
2026-01-30 12:58:16,502:INFO:Declaring metric variables
2026-01-30 12:58:16,502:INFO:Importing untrained model
2026-01-30 12:58:16,502:INFO:Declaring custom model
2026-01-30 12:58:16,503:INFO:Decision Tree Classifier Imported successfully
2026-01-30 12:58:16,503:INFO:Cross validation set to False
2026-01-30 12:58:16,503:INFO:Fitting Model
2026-01-30 12:58:20,179:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 12:58:20,179:INFO:create_model() successfully completed......................................
2026-01-30 12:58:20,371:INFO:_master_model_container: 4
2026-01-30 12:58:20,372:INFO:_display_container: 2
2026-01-30 12:58:20,373:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 12:58:20,373:INFO:compare_models() successfully completed......................................
2026-01-30 12:58:20,385:INFO:Initializing tune_model()
2026-01-30 12:58:20,386:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 12:58:20,386:INFO:Checking exceptions
2026-01-30 12:58:20,465:INFO:Copying training dataset
2026-01-30 12:58:20,625:INFO:Checking base model
2026-01-30 12:58:20,625:INFO:Base model : Random Forest Classifier
2026-01-30 12:58:20,626:INFO:Declaring metric variables
2026-01-30 12:58:20,626:INFO:Defining Hyperparameters
2026-01-30 12:58:20,811:INFO:Tuning with n_jobs=-1
2026-01-30 12:58:20,811:INFO:Initializing RandomizedSearchCV
2026-01-30 12:59:29,552:INFO:Initializing load_model()
2026-01-30 12:59:29,552:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 13:00:31,813:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\322740615.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 13:00:44,568:INFO:Initializing predict_model()
2026-01-30 13:00:44,568:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C50E6DA50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C50C0E8E0>)
2026-01-30 13:00:44,568:INFO:Checking exceptions
2026-01-30 13:00:44,568:INFO:Preloading libraries
2026-01-30 13:00:44,570:INFO:Set up data.
2026-01-30 13:00:48,976:INFO:Set up index.
2026-01-30 13:00:49,252:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 13:00:49,255:INFO:Hyperparameter search completed
2026-01-30 13:00:49,256:INFO:SubProcess create_model() called ==================================
2026-01-30 13:00:49,257:INFO:Initializing create_model()
2026-01-30 13:00:49,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C3052B50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 13:00:49,258:INFO:Checking exceptions
2026-01-30 13:00:49,259:INFO:Importing libraries
2026-01-30 13:00:49,259:INFO:Copying training dataset
2026-01-30 13:00:49,937:INFO:Defining folds
2026-01-30 13:00:49,937:INFO:Declaring metric variables
2026-01-30 13:00:49,937:INFO:Importing untrained model
2026-01-30 13:00:49,938:INFO:Declaring custom model
2026-01-30 13:00:49,939:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:00:49,939:INFO:Starting cross validation
2026-01-30 13:00:49,941:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:01:23,384:INFO:Calculating mean and std
2026-01-30 13:01:23,384:INFO:Creating metrics dataframe
2026-01-30 13:01:23,384:INFO:Finalizing model
2026-01-30 13:01:39,331:INFO:Uploading results into container
2026-01-30 13:01:39,331:INFO:Uploading model into container now
2026-01-30 13:01:39,331:INFO:_master_model_container: 5
2026-01-30 13:01:39,331:INFO:_display_container: 3
2026-01-30 13:01:39,331:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:01:39,331:INFO:create_model() successfully completed......................................
2026-01-30 13:01:39,523:INFO:SubProcess create_model() end ==================================
2026-01-30 13:01:39,523:INFO:choose_better activated
2026-01-30 13:01:39,523:INFO:SubProcess create_model() called ==================================
2026-01-30 13:01:39,524:INFO:Initializing create_model()
2026-01-30 13:01:39,524:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:01:39,524:INFO:Checking exceptions
2026-01-30 13:01:39,524:INFO:Importing libraries
2026-01-30 13:01:39,524:INFO:Copying training dataset
2026-01-30 13:01:39,731:INFO:Defining folds
2026-01-30 13:01:39,731:INFO:Declaring metric variables
2026-01-30 13:01:39,732:INFO:Importing untrained model
2026-01-30 13:01:39,732:INFO:Declaring custom model
2026-01-30 13:01:39,732:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:01:39,732:INFO:Starting cross validation
2026-01-30 13:01:39,733:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:02:02,710:INFO:Calculating mean and std
2026-01-30 13:02:02,710:INFO:Creating metrics dataframe
2026-01-30 13:02:02,710:INFO:Finalizing model
2026-01-30 13:02:13,713:INFO:Uploading results into container
2026-01-30 13:02:13,714:INFO:Uploading model into container now
2026-01-30 13:02:13,715:INFO:_master_model_container: 6
2026-01-30 13:02:13,715:INFO:_display_container: 4
2026-01-30 13:02:13,715:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:02:13,716:INFO:create_model() successfully completed......................................
2026-01-30 13:02:13,893:INFO:SubProcess create_model() end ==================================
2026-01-30 13:02:13,893:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9962
2026-01-30 13:02:13,893:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9658
2026-01-30 13:02:13,905:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 13:02:13,905:INFO:choose_better completed
2026-01-30 13:02:13,905:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 13:02:13,907:INFO:_master_model_container: 6
2026-01-30 13:02:13,907:INFO:_display_container: 3
2026-01-30 13:02:13,907:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:02:13,907:INFO:tune_model() successfully completed......................................
2026-01-30 13:02:14,079:INFO:Initializing tune_model()
2026-01-30 13:02:14,079:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:02:14,079:INFO:Checking exceptions
2026-01-30 13:02:14,149:INFO:Copying training dataset
2026-01-30 13:02:14,267:INFO:Checking base model
2026-01-30 13:02:14,267:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 13:02:14,267:INFO:Declaring metric variables
2026-01-30 13:02:14,267:INFO:Defining Hyperparameters
2026-01-30 13:02:14,440:INFO:Tuning with n_jobs=-1
2026-01-30 13:02:14,440:INFO:Initializing RandomizedSearchCV
2026-01-30 13:03:02,640:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 13:03:02,642:INFO:Hyperparameter search completed
2026-01-30 13:03:02,643:INFO:SubProcess create_model() called ==================================
2026-01-30 13:03:02,645:INFO:Initializing create_model()
2026-01-30 13:03:02,645:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03C5A75D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 13:03:02,646:INFO:Checking exceptions
2026-01-30 13:03:02,647:INFO:Importing libraries
2026-01-30 13:03:02,647:INFO:Copying training dataset
2026-01-30 13:03:03,196:INFO:Defining folds
2026-01-30 13:03:03,198:INFO:Declaring metric variables
2026-01-30 13:03:03,198:INFO:Importing untrained model
2026-01-30 13:03:03,198:INFO:Declaring custom model
2026-01-30 13:03:03,200:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:03:03,202:INFO:Starting cross validation
2026-01-30 13:03:03,202:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:03:15,883:INFO:Calculating mean and std
2026-01-30 13:03:15,885:INFO:Creating metrics dataframe
2026-01-30 13:03:15,889:INFO:Finalizing model
2026-01-30 13:03:16,998:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 13:03:16,998:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 13:03:16,998:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 13:03:17,257:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 13:03:17,257:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 13:03:17,257:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 13:03:17,258:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 13:03:17,330:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016354 seconds.
2026-01-30 13:03:17,330:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:03:17,330:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:03:17,331:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 13:03:17,332:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 13:03:17,337:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 13:03:17,338:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 13:03:21,295:INFO:Uploading results into container
2026-01-30 13:03:21,295:INFO:Uploading model into container now
2026-01-30 13:03:21,297:INFO:_master_model_container: 7
2026-01-30 13:03:21,297:INFO:_display_container: 4
2026-01-30 13:03:21,299:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:03:21,299:INFO:create_model() successfully completed......................................
2026-01-30 13:03:21,544:INFO:SubProcess create_model() end ==================================
2026-01-30 13:03:21,545:INFO:choose_better activated
2026-01-30 13:03:21,546:INFO:SubProcess create_model() called ==================================
2026-01-30 13:03:21,546:INFO:Initializing create_model()
2026-01-30 13:03:21,546:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:03:21,546:INFO:Checking exceptions
2026-01-30 13:03:21,547:INFO:Importing libraries
2026-01-30 13:03:21,547:INFO:Copying training dataset
2026-01-30 13:03:21,752:INFO:Defining folds
2026-01-30 13:03:21,752:INFO:Declaring metric variables
2026-01-30 13:03:21,752:INFO:Importing untrained model
2026-01-30 13:03:21,752:INFO:Declaring custom model
2026-01-30 13:03:21,752:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:03:21,752:INFO:Starting cross validation
2026-01-30 13:03:21,755:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:03:26,582:INFO:Calculating mean and std
2026-01-30 13:03:26,584:INFO:Creating metrics dataframe
2026-01-30 13:03:26,587:INFO:Finalizing model
2026-01-30 13:03:27,513:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 13:03:27,563:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013262 seconds.
2026-01-30 13:03:27,565:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:03:27,565:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:03:27,565:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 13:03:27,565:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 13:03:27,568:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 13:03:27,568:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 13:03:28,477:INFO:Uploading results into container
2026-01-30 13:03:28,479:INFO:Uploading model into container now
2026-01-30 13:03:28,479:INFO:_master_model_container: 8
2026-01-30 13:03:28,479:INFO:_display_container: 5
2026-01-30 13:03:28,479:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:03:28,481:INFO:create_model() successfully completed......................................
2026-01-30 13:03:28,740:INFO:SubProcess create_model() end ==================================
2026-01-30 13:03:28,740:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9766
2026-01-30 13:03:28,741:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9936
2026-01-30 13:03:28,742:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 13:03:28,742:INFO:choose_better completed
2026-01-30 13:03:28,744:INFO:_master_model_container: 8
2026-01-30 13:03:28,744:INFO:_display_container: 4
2026-01-30 13:03:28,745:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:03:28,745:INFO:tune_model() successfully completed......................................
2026-01-30 13:03:28,921:INFO:Initializing tune_model()
2026-01-30 13:03:28,921:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:03:28,921:INFO:Checking exceptions
2026-01-30 13:03:28,995:INFO:Copying training dataset
2026-01-30 13:03:29,120:INFO:Checking base model
2026-01-30 13:03:29,120:INFO:Base model : Decision Tree Classifier
2026-01-30 13:03:29,120:INFO:Declaring metric variables
2026-01-30 13:03:29,120:INFO:Defining Hyperparameters
2026-01-30 13:03:29,337:INFO:Tuning with n_jobs=-1
2026-01-30 13:03:29,337:INFO:Initializing RandomizedSearchCV
2026-01-30 13:03:38,666:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 13:03:38,667:INFO:Hyperparameter search completed
2026-01-30 13:03:38,668:INFO:SubProcess create_model() called ==================================
2026-01-30 13:03:38,668:INFO:Initializing create_model()
2026-01-30 13:03:38,668:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0CA0B3250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 13:03:38,670:INFO:Checking exceptions
2026-01-30 13:03:38,670:INFO:Importing libraries
2026-01-30 13:03:38,670:INFO:Copying training dataset
2026-01-30 13:03:39,033:INFO:Defining folds
2026-01-30 13:03:39,034:INFO:Declaring metric variables
2026-01-30 13:03:39,034:INFO:Importing untrained model
2026-01-30 13:03:39,034:INFO:Declaring custom model
2026-01-30 13:03:39,035:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:03:39,036:INFO:Starting cross validation
2026-01-30 13:03:39,037:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:03:42,597:INFO:Calculating mean and std
2026-01-30 13:03:42,597:INFO:Creating metrics dataframe
2026-01-30 13:03:42,597:INFO:Finalizing model
2026-01-30 13:03:44,661:INFO:Uploading results into container
2026-01-30 13:03:44,662:INFO:Uploading model into container now
2026-01-30 13:03:44,663:INFO:_master_model_container: 9
2026-01-30 13:03:44,663:INFO:_display_container: 5
2026-01-30 13:03:44,663:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:03:44,663:INFO:create_model() successfully completed......................................
2026-01-30 13:03:44,832:INFO:SubProcess create_model() end ==================================
2026-01-30 13:03:44,833:INFO:choose_better activated
2026-01-30 13:03:44,833:INFO:SubProcess create_model() called ==================================
2026-01-30 13:03:44,833:INFO:Initializing create_model()
2026-01-30 13:03:44,833:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:03:44,833:INFO:Checking exceptions
2026-01-30 13:03:44,834:INFO:Importing libraries
2026-01-30 13:03:44,834:INFO:Copying training dataset
2026-01-30 13:03:45,037:INFO:Defining folds
2026-01-30 13:03:45,037:INFO:Declaring metric variables
2026-01-30 13:03:45,037:INFO:Importing untrained model
2026-01-30 13:03:45,039:INFO:Declaring custom model
2026-01-30 13:03:45,039:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:03:45,040:INFO:Starting cross validation
2026-01-30 13:03:45,040:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:03:49,771:INFO:Calculating mean and std
2026-01-30 13:03:49,772:INFO:Creating metrics dataframe
2026-01-30 13:03:49,775:INFO:Finalizing model
2026-01-30 13:03:53,894:INFO:Uploading results into container
2026-01-30 13:03:53,895:INFO:Uploading model into container now
2026-01-30 13:03:53,895:INFO:_master_model_container: 10
2026-01-30 13:03:53,895:INFO:_display_container: 6
2026-01-30 13:03:53,895:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:03:53,895:INFO:create_model() successfully completed......................................
2026-01-30 13:03:54,077:INFO:SubProcess create_model() end ==================================
2026-01-30 13:03:54,078:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9701
2026-01-30 13:03:54,078:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9544
2026-01-30 13:03:54,078:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 13:03:54,079:INFO:choose_better completed
2026-01-30 13:03:54,079:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 13:03:54,081:INFO:_master_model_container: 10
2026-01-30 13:03:54,081:INFO:_display_container: 5
2026-01-30 13:03:54,081:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:03:54,082:INFO:tune_model() successfully completed......................................
2026-01-30 13:03:54,283:INFO:Initializing predict_model()
2026-01-30 13:03:54,283:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A06E731760>)
2026-01-30 13:03:54,283:INFO:Checking exceptions
2026-01-30 13:03:54,283:INFO:Preloading libraries
2026-01-30 13:03:54,283:INFO:Set up data.
2026-01-30 13:03:54,294:INFO:Set up index.
2026-01-30 13:03:54,741:INFO:Initializing predict_model()
2026-01-30 13:03:54,742:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A01562A5C0>)
2026-01-30 13:03:54,742:INFO:Checking exceptions
2026-01-30 13:03:54,742:INFO:Preloading libraries
2026-01-30 13:03:54,742:INFO:Set up data.
2026-01-30 13:03:54,754:INFO:Set up index.
2026-01-30 13:03:55,266:INFO:Initializing predict_model()
2026-01-30 13:03:55,266:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A06E731760>)
2026-01-30 13:03:55,266:INFO:Checking exceptions
2026-01-30 13:03:55,266:INFO:Preloading libraries
2026-01-30 13:03:55,267:INFO:Set up data.
2026-01-30 13:03:55,279:INFO:Set up index.
2026-01-30 13:03:55,902:INFO:Initializing plot_model()
2026-01-30 13:03:55,902:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 13:03:55,903:INFO:Checking exceptions
2026-01-30 13:03:56,082:INFO:Preloading libraries
2026-01-30 13:03:56,215:INFO:Copying training dataset
2026-01-30 13:03:56,215:INFO:Plot type: feature
2026-01-30 13:03:56,216:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 13:03:56,615:INFO:Visual Rendered Successfully
2026-01-30 13:03:56,802:INFO:plot_model() successfully completed......................................
2026-01-30 13:03:56,811:INFO:Initializing plot_model()
2026-01-30 13:03:56,811:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0C9F04850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 13:03:56,811:INFO:Checking exceptions
2026-01-30 13:03:56,918:INFO:Preloading libraries
2026-01-30 13:03:57,069:INFO:Copying training dataset
2026-01-30 13:03:57,069:INFO:Plot type: feature_all
2026-01-30 13:03:57,308:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 13:03:57,841:INFO:Visual Rendered Successfully
2026-01-30 13:03:58,071:INFO:plot_model() successfully completed......................................
2026-01-30 13:03:58,090:INFO:Initializing save_model()
2026-01-30 13:03:58,091:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 13:03:58,091:INFO:Adding model into prep_pipe
2026-01-30 13:03:58,297:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 13:03:58,302:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGAD...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 13:03:58,302:INFO:save_model() successfully completed......................................
2026-01-30 13:04:34,410:INFO:Initializing load_model()
2026-01-30 13:04:34,410:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 13:04:36,686:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2381771573.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 13:04:38,408:INFO:Initializing predict_model()
2026-01-30 13:04:38,408:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C59DB9D90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C4DBEB600>)
2026-01-30 13:04:38,408:INFO:Checking exceptions
2026-01-30 13:04:38,408:INFO:Preloading libraries
2026-01-30 13:04:38,408:INFO:Set up data.
2026-01-30 13:04:39,418:INFO:Set up index.
2026-01-30 13:04:42,271:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2381771573.py:82: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.
  for grupo, g in df.groupby(grupo_col):

2026-01-30 13:04:42,322:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\sklearn\utils\_array_api.py:290: RuntimeWarning: invalid value encountered in cast
  return x.astype(dtype, copy=copy, casting=casting)

2026-01-30 13:07:31,057:INFO:Initializing load_model()
2026-01-30 13:07:31,057:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 13:07:33,290:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\270635649.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 13:07:35,087:INFO:Initializing predict_model()
2026-01-30 13:07:35,087:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C64151B50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C5DD10C20>)
2026-01-30 13:07:35,087:INFO:Checking exceptions
2026-01-30 13:07:35,087:INFO:Preloading libraries
2026-01-30 13:07:35,087:INFO:Set up data.
2026-01-30 13:07:36,020:INFO:Set up index.
2026-01-30 13:07:41,157:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\270635649.py:88: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.
  for grupo, g in df.groupby(grupo_col):

2026-01-30 13:07:42,104:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\270635649.py:113: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-01-30 13:08:49,002:INFO:Initializing load_model()
2026-01-30 13:08:49,003:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 13:08:52,362:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2637609198.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 13:08:55,170:INFO:Initializing predict_model()
2026-01-30 13:08:55,170:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C657CA750>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGADO_FINAL',
                                             'tie...
                                             'flag_CU_IMPORTE_TOTAL_na',
                                             'flag_CU_precioOrdinario_def__c_na',
                                             'flag_CU_precioAplicado_def__c_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C32162200>)
2026-01-30 13:08:55,170:INFO:Checking exceptions
2026-01-30 13:08:55,170:INFO:Preloading libraries
2026-01-30 13:08:55,170:INFO:Set up data.
2026-01-30 13:08:56,319:INFO:Set up index.
2026-01-30 13:09:01,760:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2637609198.py:120: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-01-30 13:09:02,236:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2637609198.py:120: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-01-30 13:12:09,554:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\3466513485.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 13:12:11,653:INFO:PyCaret ClassificationExperiment
2026-01-30 13:12:11,653:INFO:Logging name: clf-default-name
2026-01-30 13:12:11,653:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 13:12:11,653:INFO:version 3.3.2
2026-01-30 13:12:11,653:INFO:Initializing setup()
2026-01-30 13:12:11,653:INFO:self.USI: b075
2026-01-30 13:12:11,653:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 13:12:11,653:INFO:Checking environment
2026-01-30 13:12:11,653:INFO:python_version: 3.11.11
2026-01-30 13:12:11,653:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 13:12:11,653:INFO:machine: AMD64
2026-01-30 13:12:11,653:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 13:12:11,653:INFO:Memory: svmem(total=34009374720, available=13265219584, percent=61.0, used=20744155136, free=13265219584)
2026-01-30 13:12:11,653:INFO:Physical Core: 12
2026-01-30 13:12:11,653:INFO:Logical Core: 16
2026-01-30 13:12:11,653:INFO:Checking libraries
2026-01-30 13:12:11,653:INFO:System:
2026-01-30 13:12:11,653:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 13:12:11,653:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 13:12:11,653:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 13:12:11,653:INFO:PyCaret required dependencies:
2026-01-30 13:12:11,653:INFO:                 pip: 25.0
2026-01-30 13:12:11,653:INFO:          setuptools: 75.8.0
2026-01-30 13:12:11,653:INFO:             pycaret: 3.3.2
2026-01-30 13:12:11,653:INFO:             IPython: 9.9.0
2026-01-30 13:12:11,653:INFO:          ipywidgets: 8.1.8
2026-01-30 13:12:11,653:INFO:                tqdm: 4.67.1
2026-01-30 13:12:11,653:INFO:               numpy: 1.26.4
2026-01-30 13:12:11,653:INFO:              pandas: 2.1.4
2026-01-30 13:12:11,653:INFO:              jinja2: 3.1.6
2026-01-30 13:12:11,653:INFO:               scipy: 1.11.4
2026-01-30 13:12:11,653:INFO:              joblib: 1.3.2
2026-01-30 13:12:11,653:INFO:             sklearn: 1.4.2
2026-01-30 13:12:11,653:INFO:                pyod: 2.0.6
2026-01-30 13:12:11,653:INFO:            imblearn: 0.14.1
2026-01-30 13:12:11,666:INFO:   category_encoders: 2.7.0
2026-01-30 13:12:11,666:INFO:            lightgbm: 4.6.0
2026-01-30 13:12:11,666:INFO:               numba: 0.62.1
2026-01-30 13:12:11,666:INFO:            requests: 2.32.3
2026-01-30 13:12:11,666:INFO:          matplotlib: 3.7.5
2026-01-30 13:12:11,667:INFO:          scikitplot: 0.3.7
2026-01-30 13:12:11,667:INFO:         yellowbrick: 1.5
2026-01-30 13:12:11,667:INFO:              plotly: 5.24.1
2026-01-30 13:12:11,667:INFO:    plotly-resampler: Not installed
2026-01-30 13:12:11,667:INFO:             kaleido: 1.2.0
2026-01-30 13:12:11,667:INFO:           schemdraw: 0.15
2026-01-30 13:12:11,667:INFO:         statsmodels: 0.14.6
2026-01-30 13:12:11,667:INFO:              sktime: 0.26.0
2026-01-30 13:12:11,667:INFO:               tbats: 1.1.3
2026-01-30 13:12:11,667:INFO:            pmdarima: 2.0.4
2026-01-30 13:12:11,667:INFO:              psutil: 7.2.1
2026-01-30 13:12:11,667:INFO:          markupsafe: 3.0.3
2026-01-30 13:12:11,667:INFO:             pickle5: Not installed
2026-01-30 13:12:11,667:INFO:         cloudpickle: 3.0.0
2026-01-30 13:12:11,667:INFO:         deprecation: 2.1.0
2026-01-30 13:12:11,667:INFO:              xxhash: 3.6.0
2026-01-30 13:12:11,667:INFO:           wurlitzer: Not installed
2026-01-30 13:12:11,667:INFO:PyCaret optional dependencies:
2026-01-30 13:12:11,667:INFO:                shap: 0.44.1
2026-01-30 13:12:11,667:INFO:           interpret: 0.7.3
2026-01-30 13:12:11,667:INFO:                umap: 0.5.7
2026-01-30 13:12:11,667:INFO:     ydata_profiling: 4.18.1
2026-01-30 13:12:11,667:INFO:  explainerdashboard: 0.5.1
2026-01-30 13:12:11,667:INFO:             autoviz: Not installed
2026-01-30 13:12:11,667:INFO:           fairlearn: 0.7.0
2026-01-30 13:12:11,667:INFO:          deepchecks: Not installed
2026-01-30 13:12:11,667:INFO:             xgboost: Not installed
2026-01-30 13:12:11,667:INFO:            catboost: 1.2.8
2026-01-30 13:12:11,667:INFO:              kmodes: 0.12.2
2026-01-30 13:12:11,667:INFO:             mlxtend: 0.23.4
2026-01-30 13:12:11,667:INFO:       statsforecast: 1.5.0
2026-01-30 13:12:11,667:INFO:        tune_sklearn: Not installed
2026-01-30 13:12:11,667:INFO:                 ray: Not installed
2026-01-30 13:12:11,667:INFO:            hyperopt: 0.2.7
2026-01-30 13:12:11,667:INFO:              optuna: 4.6.0
2026-01-30 13:12:11,667:INFO:               skopt: 0.10.2
2026-01-30 13:12:11,667:INFO:              mlflow: 3.8.1
2026-01-30 13:12:11,667:INFO:              gradio: 6.3.0
2026-01-30 13:12:11,667:INFO:             fastapi: 0.128.0
2026-01-30 13:12:11,667:INFO:             uvicorn: 0.40.0
2026-01-30 13:12:11,667:INFO:              m2cgen: 0.10.0
2026-01-30 13:12:11,667:INFO:           evidently: 0.4.40
2026-01-30 13:12:11,667:INFO:               fugue: 0.8.7
2026-01-30 13:12:11,667:INFO:           streamlit: Not installed
2026-01-30 13:12:11,667:INFO:             prophet: Not installed
2026-01-30 13:12:11,667:INFO:None
2026-01-30 13:12:11,667:INFO:Set up data.
2026-01-30 13:12:11,800:INFO:Set up folding strategy.
2026-01-30 13:12:11,800:INFO:Set up train/test split.
2026-01-30 13:12:12,000:INFO:Set up index.
2026-01-30 13:12:12,017:INFO:Assigning column types.
2026-01-30 13:12:12,168:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 13:12:12,183:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 13:12:12,183:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:12:12,199:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:12:12,199:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:12:12,233:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 13:12:12,233:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:12:12,249:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:12:12,249:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:12:12,249:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 13:12:12,283:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:12:12,300:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:12:12,300:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:12:12,331:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:12:12,349:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:12:12,349:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:12:12,349:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 13:12:12,387:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:12:12,387:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:12:12,456:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:12:12,456:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:12:12,456:INFO:Preparing preprocessing pipeline...
2026-01-30 13:12:12,494:INFO:Set up simple imputation.
2026-01-30 13:12:12,494:INFO:Set up feature normalization.
2026-01-30 13:12:12,804:INFO:Finished creating preprocessing pipeline.
2026-01-30 13:12:12,804:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 13:12:12,804:INFO:Creating final display dataframe.
2026-01-30 13:12:13,600:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (482669, 27)
4        Transformed data shape      (482669, 27)
5   Transformed train set shape      (337868, 27)
6    Transformed test set shape      (144801, 27)
7              Numeric features                23
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              b075
2026-01-30 13:12:13,666:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:12:13,666:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:12:13,717:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:12:13,717:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:12:13,717:INFO:setup() successfully completed in 2.08s...............
2026-01-30 13:12:13,717:INFO:Initializing compare_models()
2026-01-30 13:12:13,717:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 13:12:13,717:INFO:Checking exceptions
2026-01-30 13:12:13,854:INFO:Preparing display monitor
2026-01-30 13:12:13,855:INFO:Initializing Logistic Regression
2026-01-30 13:12:13,855:INFO:Total runtime is 0.0 minutes
2026-01-30 13:12:13,855:INFO:SubProcess create_model() called ==================================
2026-01-30 13:12:13,855:INFO:Initializing create_model()
2026-01-30 13:12:13,855:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0484BB250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:12:13,855:INFO:Checking exceptions
2026-01-30 13:12:13,855:INFO:Importing libraries
2026-01-30 13:12:13,855:INFO:Copying training dataset
2026-01-30 13:12:14,083:INFO:Defining folds
2026-01-30 13:12:14,083:INFO:Declaring metric variables
2026-01-30 13:12:14,083:INFO:Importing untrained model
2026-01-30 13:12:14,083:INFO:Logistic Regression Imported successfully
2026-01-30 13:12:14,083:INFO:Starting cross validation
2026-01-30 13:12:14,083:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:12:22,567:INFO:Calculating mean and std
2026-01-30 13:12:22,567:INFO:Creating metrics dataframe
2026-01-30 13:12:22,567:INFO:Uploading results into container
2026-01-30 13:12:22,567:INFO:Uploading model into container now
2026-01-30 13:12:22,567:INFO:_master_model_container: 1
2026-01-30 13:12:22,567:INFO:_display_container: 2
2026-01-30 13:12:22,567:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 13:12:22,567:INFO:create_model() successfully completed......................................
2026-01-30 13:12:22,733:INFO:SubProcess create_model() end ==================================
2026-01-30 13:12:22,733:INFO:Creating metrics dataframe
2026-01-30 13:12:22,747:INFO:Initializing Decision Tree Classifier
2026-01-30 13:12:22,747:INFO:Total runtime is 0.1482052206993103 minutes
2026-01-30 13:12:22,747:INFO:SubProcess create_model() called ==================================
2026-01-30 13:12:22,747:INFO:Initializing create_model()
2026-01-30 13:12:22,747:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0484BB250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:12:22,747:INFO:Checking exceptions
2026-01-30 13:12:22,747:INFO:Importing libraries
2026-01-30 13:12:22,747:INFO:Copying training dataset
2026-01-30 13:12:22,919:INFO:Defining folds
2026-01-30 13:12:22,919:INFO:Declaring metric variables
2026-01-30 13:12:22,919:INFO:Importing untrained model
2026-01-30 13:12:22,919:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:12:22,920:INFO:Starting cross validation
2026-01-30 13:12:22,920:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:12:31,059:INFO:Calculating mean and std
2026-01-30 13:12:31,059:INFO:Creating metrics dataframe
2026-01-30 13:12:31,059:INFO:Uploading results into container
2026-01-30 13:12:31,059:INFO:Uploading model into container now
2026-01-30 13:12:31,059:INFO:_master_model_container: 2
2026-01-30 13:12:31,059:INFO:_display_container: 2
2026-01-30 13:12:31,059:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:12:31,059:INFO:create_model() successfully completed......................................
2026-01-30 13:12:31,258:INFO:SubProcess create_model() end ==================================
2026-01-30 13:12:31,258:INFO:Creating metrics dataframe
2026-01-30 13:12:31,258:INFO:Initializing Random Forest Classifier
2026-01-30 13:12:31,258:INFO:Total runtime is 0.29004819790522257 minutes
2026-01-30 13:12:31,258:INFO:SubProcess create_model() called ==================================
2026-01-30 13:12:31,258:INFO:Initializing create_model()
2026-01-30 13:12:31,258:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0484BB250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:12:31,258:INFO:Checking exceptions
2026-01-30 13:12:31,258:INFO:Importing libraries
2026-01-30 13:12:31,258:INFO:Copying training dataset
2026-01-30 13:12:31,458:INFO:Defining folds
2026-01-30 13:12:31,458:INFO:Declaring metric variables
2026-01-30 13:12:31,458:INFO:Importing untrained model
2026-01-30 13:12:31,458:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:12:31,458:INFO:Starting cross validation
2026-01-30 13:12:31,458:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:12:52,224:INFO:Calculating mean and std
2026-01-30 13:12:52,226:INFO:Creating metrics dataframe
2026-01-30 13:12:52,228:INFO:Uploading results into container
2026-01-30 13:12:52,229:INFO:Uploading model into container now
2026-01-30 13:12:52,230:INFO:_master_model_container: 3
2026-01-30 13:12:52,230:INFO:_display_container: 2
2026-01-30 13:12:52,230:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:12:52,230:INFO:create_model() successfully completed......................................
2026-01-30 13:12:52,405:INFO:SubProcess create_model() end ==================================
2026-01-30 13:12:52,405:INFO:Creating metrics dataframe
2026-01-30 13:12:52,405:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 13:12:52,405:INFO:Total runtime is 0.6425010442733765 minutes
2026-01-30 13:12:52,405:INFO:SubProcess create_model() called ==================================
2026-01-30 13:12:52,405:INFO:Initializing create_model()
2026-01-30 13:12:52,405:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0484BB250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:12:52,405:INFO:Checking exceptions
2026-01-30 13:12:52,405:INFO:Importing libraries
2026-01-30 13:12:52,405:INFO:Copying training dataset
2026-01-30 13:12:52,636:INFO:Defining folds
2026-01-30 13:12:52,636:INFO:Declaring metric variables
2026-01-30 13:12:52,636:INFO:Importing untrained model
2026-01-30 13:12:52,637:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:12:52,637:INFO:Starting cross validation
2026-01-30 13:12:52,638:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:13:00,105:INFO:Calculating mean and std
2026-01-30 13:13:00,105:INFO:Creating metrics dataframe
2026-01-30 13:13:00,105:INFO:Uploading results into container
2026-01-30 13:13:00,105:INFO:Uploading model into container now
2026-01-30 13:13:00,105:INFO:_master_model_container: 4
2026-01-30 13:13:00,105:INFO:_display_container: 2
2026-01-30 13:13:00,105:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:13:00,113:INFO:create_model() successfully completed......................................
2026-01-30 13:13:00,281:INFO:SubProcess create_model() end ==================================
2026-01-30 13:13:00,281:INFO:Creating metrics dataframe
2026-01-30 13:13:00,282:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 13:13:00,282:INFO:Initializing create_model()
2026-01-30 13:13:00,282:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:13:00,282:INFO:Checking exceptions
2026-01-30 13:13:00,282:INFO:Importing libraries
2026-01-30 13:13:00,282:INFO:Copying training dataset
2026-01-30 13:13:00,494:INFO:Defining folds
2026-01-30 13:13:00,494:INFO:Declaring metric variables
2026-01-30 13:13:00,494:INFO:Importing untrained model
2026-01-30 13:13:00,494:INFO:Declaring custom model
2026-01-30 13:13:00,494:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:13:00,494:INFO:Cross validation set to False
2026-01-30 13:13:00,494:INFO:Fitting Model
2026-01-30 13:13:08,920:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:13:08,920:INFO:create_model() successfully completed......................................
2026-01-30 13:13:09,096:INFO:Initializing create_model()
2026-01-30 13:13:09,097:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:13:09,097:INFO:Checking exceptions
2026-01-30 13:13:09,098:INFO:Importing libraries
2026-01-30 13:13:09,098:INFO:Copying training dataset
2026-01-30 13:13:09,318:INFO:Defining folds
2026-01-30 13:13:09,319:INFO:Declaring metric variables
2026-01-30 13:13:09,319:INFO:Importing untrained model
2026-01-30 13:13:09,319:INFO:Declaring custom model
2026-01-30 13:13:09,320:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:13:09,321:INFO:Cross validation set to False
2026-01-30 13:13:09,321:INFO:Fitting Model
2026-01-30 13:13:10,209:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 13:13:10,258:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012553 seconds.
2026-01-30 13:13:10,259:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:13:10,259:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:13:10,259:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 13:13:10,260:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 13:13:10,263:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 13:13:10,263:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 13:13:10,883:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:13:10,885:INFO:create_model() successfully completed......................................
2026-01-30 13:13:11,100:INFO:Initializing create_model()
2026-01-30 13:13:11,100:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:13:11,100:INFO:Checking exceptions
2026-01-30 13:13:11,100:INFO:Importing libraries
2026-01-30 13:13:11,101:INFO:Copying training dataset
2026-01-30 13:13:11,312:INFO:Defining folds
2026-01-30 13:13:11,312:INFO:Declaring metric variables
2026-01-30 13:13:11,312:INFO:Importing untrained model
2026-01-30 13:13:11,312:INFO:Declaring custom model
2026-01-30 13:13:11,313:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:13:11,313:INFO:Cross validation set to False
2026-01-30 13:13:11,313:INFO:Fitting Model
2026-01-30 13:13:14,730:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:13:14,730:INFO:create_model() successfully completed......................................
2026-01-30 13:13:14,913:INFO:_master_model_container: 4
2026-01-30 13:13:14,913:INFO:_display_container: 2
2026-01-30 13:13:14,914:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 13:13:14,915:INFO:compare_models() successfully completed......................................
2026-01-30 13:13:14,927:INFO:Initializing tune_model()
2026-01-30 13:13:14,927:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:13:14,927:INFO:Checking exceptions
2026-01-30 13:13:14,998:INFO:Copying training dataset
2026-01-30 13:13:15,133:INFO:Checking base model
2026-01-30 13:13:15,133:INFO:Base model : Random Forest Classifier
2026-01-30 13:13:15,133:INFO:Declaring metric variables
2026-01-30 13:13:15,134:INFO:Defining Hyperparameters
2026-01-30 13:13:15,312:INFO:Tuning with n_jobs=-1
2026-01-30 13:13:15,312:INFO:Initializing RandomizedSearchCV
2026-01-30 13:15:24,668:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 13:15:24,668:INFO:Hyperparameter search completed
2026-01-30 13:15:24,670:INFO:SubProcess create_model() called ==================================
2026-01-30 13:15:24,670:INFO:Initializing create_model()
2026-01-30 13:15:24,672:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A036A97B10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 13:15:24,672:INFO:Checking exceptions
2026-01-30 13:15:24,672:INFO:Importing libraries
2026-01-30 13:15:24,672:INFO:Copying training dataset
2026-01-30 13:15:24,896:INFO:Defining folds
2026-01-30 13:15:24,896:INFO:Declaring metric variables
2026-01-30 13:15:24,896:INFO:Importing untrained model
2026-01-30 13:15:24,896:INFO:Declaring custom model
2026-01-30 13:15:24,896:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:15:24,896:INFO:Starting cross validation
2026-01-30 13:15:24,896:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:15:53,726:INFO:Calculating mean and std
2026-01-30 13:15:53,729:INFO:Creating metrics dataframe
2026-01-30 13:15:53,731:INFO:Finalizing model
2026-01-30 13:16:09,128:INFO:Uploading results into container
2026-01-30 13:16:09,129:INFO:Uploading model into container now
2026-01-30 13:16:09,130:INFO:_master_model_container: 5
2026-01-30 13:16:09,131:INFO:_display_container: 3
2026-01-30 13:16:09,131:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:16:09,131:INFO:create_model() successfully completed......................................
2026-01-30 13:16:09,337:INFO:SubProcess create_model() end ==================================
2026-01-30 13:16:09,337:INFO:choose_better activated
2026-01-30 13:16:09,337:INFO:SubProcess create_model() called ==================================
2026-01-30 13:16:09,338:INFO:Initializing create_model()
2026-01-30 13:16:09,338:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:16:09,338:INFO:Checking exceptions
2026-01-30 13:16:09,338:INFO:Importing libraries
2026-01-30 13:16:09,338:INFO:Copying training dataset
2026-01-30 13:16:09,560:INFO:Defining folds
2026-01-30 13:16:09,561:INFO:Declaring metric variables
2026-01-30 13:16:09,561:INFO:Importing untrained model
2026-01-30 13:16:09,561:INFO:Declaring custom model
2026-01-30 13:16:09,561:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:16:09,562:INFO:Starting cross validation
2026-01-30 13:16:09,562:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:16:32,766:INFO:Calculating mean and std
2026-01-30 13:16:32,766:INFO:Creating metrics dataframe
2026-01-30 13:16:32,766:INFO:Finalizing model
2026-01-30 13:16:43,679:INFO:Uploading results into container
2026-01-30 13:16:43,679:INFO:Uploading model into container now
2026-01-30 13:16:43,679:INFO:_master_model_container: 6
2026-01-30 13:16:43,679:INFO:_display_container: 4
2026-01-30 13:16:43,679:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:16:43,679:INFO:create_model() successfully completed......................................
2026-01-30 13:16:43,869:INFO:SubProcess create_model() end ==================================
2026-01-30 13:16:43,870:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9962
2026-01-30 13:16:43,870:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9658
2026-01-30 13:16:43,870:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 13:16:43,870:INFO:choose_better completed
2026-01-30 13:16:43,871:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 13:16:43,873:INFO:_master_model_container: 6
2026-01-30 13:16:43,873:INFO:_display_container: 3
2026-01-30 13:16:43,873:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:16:43,873:INFO:tune_model() successfully completed......................................
2026-01-30 13:16:44,029:INFO:Initializing tune_model()
2026-01-30 13:16:44,029:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:16:44,029:INFO:Checking exceptions
2026-01-30 13:16:44,120:INFO:Copying training dataset
2026-01-30 13:16:44,253:INFO:Checking base model
2026-01-30 13:16:44,253:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 13:16:44,253:INFO:Declaring metric variables
2026-01-30 13:16:44,253:INFO:Defining Hyperparameters
2026-01-30 13:16:44,428:INFO:Tuning with n_jobs=-1
2026-01-30 13:16:44,428:INFO:Initializing RandomizedSearchCV
2026-01-30 13:17:27,569:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 13:17:27,569:INFO:Hyperparameter search completed
2026-01-30 13:17:27,569:INFO:SubProcess create_model() called ==================================
2026-01-30 13:17:27,575:INFO:Initializing create_model()
2026-01-30 13:17:27,575:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03DBC4490>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 13:17:27,575:INFO:Checking exceptions
2026-01-30 13:17:27,575:INFO:Importing libraries
2026-01-30 13:17:27,575:INFO:Copying training dataset
2026-01-30 13:17:27,776:INFO:Defining folds
2026-01-30 13:17:27,776:INFO:Declaring metric variables
2026-01-30 13:17:27,787:INFO:Importing untrained model
2026-01-30 13:17:27,787:INFO:Declaring custom model
2026-01-30 13:17:27,787:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:17:27,787:INFO:Starting cross validation
2026-01-30 13:17:27,787:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:17:37,510:INFO:Calculating mean and std
2026-01-30 13:17:37,510:INFO:Creating metrics dataframe
2026-01-30 13:17:37,515:INFO:Finalizing model
2026-01-30 13:17:38,236:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 13:17:38,236:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 13:17:38,236:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 13:17:38,418:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 13:17:38,420:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 13:17:38,420:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 13:17:38,420:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 13:17:38,488:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011528 seconds.
2026-01-30 13:17:38,488:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:17:38,488:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:17:38,488:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 13:17:38,489:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 13:17:38,494:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 13:17:38,494:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 13:17:41,992:INFO:Uploading results into container
2026-01-30 13:17:41,995:INFO:Uploading model into container now
2026-01-30 13:17:41,996:INFO:_master_model_container: 7
2026-01-30 13:17:41,996:INFO:_display_container: 4
2026-01-30 13:17:41,998:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:17:41,998:INFO:create_model() successfully completed......................................
2026-01-30 13:17:42,229:INFO:SubProcess create_model() end ==================================
2026-01-30 13:17:42,229:INFO:choose_better activated
2026-01-30 13:17:42,229:INFO:SubProcess create_model() called ==================================
2026-01-30 13:17:42,230:INFO:Initializing create_model()
2026-01-30 13:17:42,230:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:17:42,230:INFO:Checking exceptions
2026-01-30 13:17:42,231:INFO:Importing libraries
2026-01-30 13:17:42,231:INFO:Copying training dataset
2026-01-30 13:17:42,441:INFO:Defining folds
2026-01-30 13:17:42,441:INFO:Declaring metric variables
2026-01-30 13:17:42,442:INFO:Importing untrained model
2026-01-30 13:17:42,442:INFO:Declaring custom model
2026-01-30 13:17:42,443:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:17:42,443:INFO:Starting cross validation
2026-01-30 13:17:42,443:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:17:47,233:INFO:Calculating mean and std
2026-01-30 13:17:47,233:INFO:Creating metrics dataframe
2026-01-30 13:17:47,233:INFO:Finalizing model
2026-01-30 13:17:48,090:INFO:[LightGBM] [Info] Number of positive: 146121, number of negative: 191747
2026-01-30 13:17:48,237:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.070962 seconds.
2026-01-30 13:17:48,237:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:17:48,237:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:17:48,237:INFO:[LightGBM] [Info] Total Bins 2868
2026-01-30 13:17:48,239:INFO:[LightGBM] [Info] Number of data points in the train set: 337868, number of used features: 26
2026-01-30 13:17:48,242:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.432480 -> initscore=-0.271742
2026-01-30 13:17:48,244:INFO:[LightGBM] [Info] Start training from score -0.271742
2026-01-30 13:17:49,359:INFO:Uploading results into container
2026-01-30 13:17:49,361:INFO:Uploading model into container now
2026-01-30 13:17:49,361:INFO:_master_model_container: 8
2026-01-30 13:17:49,361:INFO:_display_container: 5
2026-01-30 13:17:49,362:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:17:49,362:INFO:create_model() successfully completed......................................
2026-01-30 13:17:49,594:INFO:SubProcess create_model() end ==================================
2026-01-30 13:17:49,594:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9766
2026-01-30 13:17:49,610:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9936
2026-01-30 13:17:49,610:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 13:17:49,610:INFO:choose_better completed
2026-01-30 13:17:49,610:INFO:_master_model_container: 8
2026-01-30 13:17:49,610:INFO:_display_container: 4
2026-01-30 13:17:49,610:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:17:49,610:INFO:tune_model() successfully completed......................................
2026-01-30 13:17:49,803:INFO:Initializing tune_model()
2026-01-30 13:17:49,803:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:17:49,803:INFO:Checking exceptions
2026-01-30 13:17:49,880:INFO:Copying training dataset
2026-01-30 13:17:50,009:INFO:Checking base model
2026-01-30 13:17:50,009:INFO:Base model : Decision Tree Classifier
2026-01-30 13:17:50,010:INFO:Declaring metric variables
2026-01-30 13:17:50,010:INFO:Defining Hyperparameters
2026-01-30 13:17:50,176:INFO:Tuning with n_jobs=-1
2026-01-30 13:17:50,176:INFO:Initializing RandomizedSearchCV
2026-01-30 13:17:58,442:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 13:17:58,442:INFO:Hyperparameter search completed
2026-01-30 13:17:58,442:INFO:SubProcess create_model() called ==================================
2026-01-30 13:17:58,442:INFO:Initializing create_model()
2026-01-30 13:17:58,442:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A03CED7290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 13:17:58,442:INFO:Checking exceptions
2026-01-30 13:17:58,442:INFO:Importing libraries
2026-01-30 13:17:58,442:INFO:Copying training dataset
2026-01-30 13:17:58,711:INFO:Defining folds
2026-01-30 13:17:58,711:INFO:Declaring metric variables
2026-01-30 13:17:58,712:INFO:Importing untrained model
2026-01-30 13:17:58,712:INFO:Declaring custom model
2026-01-30 13:17:58,712:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:17:58,712:INFO:Starting cross validation
2026-01-30 13:17:58,713:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:18:01,511:INFO:Calculating mean and std
2026-01-30 13:18:01,512:INFO:Creating metrics dataframe
2026-01-30 13:18:01,514:INFO:Finalizing model
2026-01-30 13:18:03,446:INFO:Uploading results into container
2026-01-30 13:18:03,446:INFO:Uploading model into container now
2026-01-30 13:18:03,449:INFO:_master_model_container: 9
2026-01-30 13:18:03,449:INFO:_display_container: 5
2026-01-30 13:18:03,449:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:18:03,449:INFO:create_model() successfully completed......................................
2026-01-30 13:18:03,625:INFO:SubProcess create_model() end ==================================
2026-01-30 13:18:03,625:INFO:choose_better activated
2026-01-30 13:18:03,625:INFO:SubProcess create_model() called ==================================
2026-01-30 13:18:03,625:INFO:Initializing create_model()
2026-01-30 13:18:03,625:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:18:03,625:INFO:Checking exceptions
2026-01-30 13:18:03,625:INFO:Importing libraries
2026-01-30 13:18:03,627:INFO:Copying training dataset
2026-01-30 13:18:03,838:INFO:Defining folds
2026-01-30 13:18:03,838:INFO:Declaring metric variables
2026-01-30 13:18:03,838:INFO:Importing untrained model
2026-01-30 13:18:03,838:INFO:Declaring custom model
2026-01-30 13:18:03,838:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:18:03,838:INFO:Starting cross validation
2026-01-30 13:18:03,838:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:18:07,806:INFO:Calculating mean and std
2026-01-30 13:18:07,806:INFO:Creating metrics dataframe
2026-01-30 13:18:07,806:INFO:Finalizing model
2026-01-30 13:18:11,176:INFO:Uploading results into container
2026-01-30 13:18:11,177:INFO:Uploading model into container now
2026-01-30 13:18:11,177:INFO:_master_model_container: 10
2026-01-30 13:18:11,177:INFO:_display_container: 6
2026-01-30 13:18:11,177:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:18:11,177:INFO:create_model() successfully completed......................................
2026-01-30 13:18:11,341:INFO:SubProcess create_model() end ==================================
2026-01-30 13:18:11,341:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9701
2026-01-30 13:18:11,341:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9544
2026-01-30 13:18:11,341:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 13:18:11,341:INFO:choose_better completed
2026-01-30 13:18:11,341:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 13:18:11,341:INFO:_master_model_container: 10
2026-01-30 13:18:11,341:INFO:_display_container: 5
2026-01-30 13:18:11,341:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:18:11,341:INFO:tune_model() successfully completed......................................
2026-01-30 13:18:11,511:INFO:Initializing predict_model()
2026-01-30 13:18:11,511:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A04CF2B560>)
2026-01-30 13:18:11,511:INFO:Checking exceptions
2026-01-30 13:18:11,511:INFO:Preloading libraries
2026-01-30 13:18:11,511:INFO:Set up data.
2026-01-30 13:18:11,511:INFO:Set up index.
2026-01-30 13:18:11,911:INFO:Initializing predict_model()
2026-01-30 13:18:11,911:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A04CF2B560>)
2026-01-30 13:18:11,911:INFO:Checking exceptions
2026-01-30 13:18:11,911:INFO:Preloading libraries
2026-01-30 13:18:11,911:INFO:Set up data.
2026-01-30 13:18:11,926:INFO:Set up index.
2026-01-30 13:18:12,407:INFO:Initializing predict_model()
2026-01-30 13:18:12,407:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03ED62E80>)
2026-01-30 13:18:12,407:INFO:Checking exceptions
2026-01-30 13:18:12,407:INFO:Preloading libraries
2026-01-30 13:18:12,407:INFO:Set up data.
2026-01-30 13:18:12,429:INFO:Set up index.
2026-01-30 13:18:12,980:INFO:Initializing plot_model()
2026-01-30 13:18:12,980:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 13:18:12,980:INFO:Checking exceptions
2026-01-30 13:18:13,074:INFO:Preloading libraries
2026-01-30 13:18:13,215:INFO:Copying training dataset
2026-01-30 13:18:13,215:INFO:Plot type: feature
2026-01-30 13:18:13,215:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 13:18:13,545:INFO:Visual Rendered Successfully
2026-01-30 13:18:13,711:INFO:plot_model() successfully completed......................................
2026-01-30 13:18:13,711:INFO:Initializing plot_model()
2026-01-30 13:18:13,711:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A04EA9F590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 13:18:13,711:INFO:Checking exceptions
2026-01-30 13:18:13,811:INFO:Preloading libraries
2026-01-30 13:18:13,927:INFO:Copying training dataset
2026-01-30 13:18:13,927:INFO:Plot type: feature_all
2026-01-30 13:18:14,133:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 13:18:14,573:INFO:Visual Rendered Successfully
2026-01-30 13:18:14,764:INFO:plot_model() successfully completed......................................
2026-01-30 13:18:14,779:INFO:Initializing save_model()
2026-01-30 13:18:14,780:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'C...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 13:18:14,780:INFO:Adding model into prep_pipe
2026-01-30 13:18:14,955:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 13:18:14,959:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'FO_rentaFam_ges__c',
                                             'CU_precioOrdinario_def__c',
                                             'CU_precioAplicado_def__c',
                                             'PORCENTAJE_PAGAD...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 13:18:14,959:INFO:save_model() successfully completed......................................
2026-01-30 13:27:44,320:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\3810660801.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 13:27:46,576:INFO:PyCaret ClassificationExperiment
2026-01-30 13:27:46,576:INFO:Logging name: clf-default-name
2026-01-30 13:27:46,576:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 13:27:46,576:INFO:version 3.3.2
2026-01-30 13:27:46,577:INFO:Initializing setup()
2026-01-30 13:27:46,577:INFO:self.USI: 1663
2026-01-30 13:27:46,578:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 13:27:46,578:INFO:Checking environment
2026-01-30 13:27:46,578:INFO:python_version: 3.11.11
2026-01-30 13:27:46,579:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 13:27:46,579:INFO:machine: AMD64
2026-01-30 13:27:46,579:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 13:27:46,580:INFO:Memory: svmem(total=34009374720, available=13250928640, percent=61.0, used=20758446080, free=13250928640)
2026-01-30 13:27:46,581:INFO:Physical Core: 12
2026-01-30 13:27:46,581:INFO:Logical Core: 16
2026-01-30 13:27:46,582:INFO:Checking libraries
2026-01-30 13:27:46,583:INFO:System:
2026-01-30 13:27:46,583:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 13:27:46,583:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 13:27:46,583:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 13:27:46,583:INFO:PyCaret required dependencies:
2026-01-30 13:27:46,584:INFO:                 pip: 25.0
2026-01-30 13:27:46,585:INFO:          setuptools: 75.8.0
2026-01-30 13:27:46,585:INFO:             pycaret: 3.3.2
2026-01-30 13:27:46,585:INFO:             IPython: 9.9.0
2026-01-30 13:27:46,585:INFO:          ipywidgets: 8.1.8
2026-01-30 13:27:46,587:INFO:                tqdm: 4.67.1
2026-01-30 13:27:46,587:INFO:               numpy: 1.26.4
2026-01-30 13:27:46,587:INFO:              pandas: 2.1.4
2026-01-30 13:27:46,587:INFO:              jinja2: 3.1.6
2026-01-30 13:27:46,587:INFO:               scipy: 1.11.4
2026-01-30 13:27:46,587:INFO:              joblib: 1.3.2
2026-01-30 13:27:46,587:INFO:             sklearn: 1.4.2
2026-01-30 13:27:46,587:INFO:                pyod: 2.0.6
2026-01-30 13:27:46,589:INFO:            imblearn: 0.14.1
2026-01-30 13:27:46,589:INFO:   category_encoders: 2.7.0
2026-01-30 13:27:46,590:INFO:            lightgbm: 4.6.0
2026-01-30 13:27:46,590:INFO:               numba: 0.62.1
2026-01-30 13:27:46,591:INFO:            requests: 2.32.3
2026-01-30 13:27:46,591:INFO:          matplotlib: 3.7.5
2026-01-30 13:27:46,591:INFO:          scikitplot: 0.3.7
2026-01-30 13:27:46,591:INFO:         yellowbrick: 1.5
2026-01-30 13:27:46,591:INFO:              plotly: 5.24.1
2026-01-30 13:27:46,592:INFO:    plotly-resampler: Not installed
2026-01-30 13:27:46,592:INFO:             kaleido: 1.2.0
2026-01-30 13:27:46,592:INFO:           schemdraw: 0.15
2026-01-30 13:27:46,592:INFO:         statsmodels: 0.14.6
2026-01-30 13:27:46,593:INFO:              sktime: 0.26.0
2026-01-30 13:27:46,593:INFO:               tbats: 1.1.3
2026-01-30 13:27:46,594:INFO:            pmdarima: 2.0.4
2026-01-30 13:27:46,594:INFO:              psutil: 7.2.1
2026-01-30 13:27:46,594:INFO:          markupsafe: 3.0.3
2026-01-30 13:27:46,595:INFO:             pickle5: Not installed
2026-01-30 13:27:46,595:INFO:         cloudpickle: 3.0.0
2026-01-30 13:27:46,595:INFO:         deprecation: 2.1.0
2026-01-30 13:27:46,595:INFO:              xxhash: 3.6.0
2026-01-30 13:27:46,595:INFO:           wurlitzer: Not installed
2026-01-30 13:27:46,595:INFO:PyCaret optional dependencies:
2026-01-30 13:27:46,595:INFO:                shap: 0.44.1
2026-01-30 13:27:46,595:INFO:           interpret: 0.7.3
2026-01-30 13:27:46,595:INFO:                umap: 0.5.7
2026-01-30 13:27:46,595:INFO:     ydata_profiling: 4.18.1
2026-01-30 13:27:46,595:INFO:  explainerdashboard: 0.5.1
2026-01-30 13:27:46,595:INFO:             autoviz: Not installed
2026-01-30 13:27:46,595:INFO:           fairlearn: 0.7.0
2026-01-30 13:27:46,596:INFO:          deepchecks: Not installed
2026-01-30 13:27:46,596:INFO:             xgboost: Not installed
2026-01-30 13:27:46,596:INFO:            catboost: 1.2.8
2026-01-30 13:27:46,597:INFO:              kmodes: 0.12.2
2026-01-30 13:27:46,597:INFO:             mlxtend: 0.23.4
2026-01-30 13:27:46,597:INFO:       statsforecast: 1.5.0
2026-01-30 13:27:46,597:INFO:        tune_sklearn: Not installed
2026-01-30 13:27:46,597:INFO:                 ray: Not installed
2026-01-30 13:27:46,597:INFO:            hyperopt: 0.2.7
2026-01-30 13:27:46,598:INFO:              optuna: 4.6.0
2026-01-30 13:27:46,598:INFO:               skopt: 0.10.2
2026-01-30 13:27:46,598:INFO:              mlflow: 3.8.1
2026-01-30 13:27:46,598:INFO:              gradio: 6.3.0
2026-01-30 13:27:46,598:INFO:             fastapi: 0.128.0
2026-01-30 13:27:46,598:INFO:             uvicorn: 0.40.0
2026-01-30 13:27:46,598:INFO:              m2cgen: 0.10.0
2026-01-30 13:27:46,598:INFO:           evidently: 0.4.40
2026-01-30 13:27:46,598:INFO:               fugue: 0.8.7
2026-01-30 13:27:46,598:INFO:           streamlit: Not installed
2026-01-30 13:27:46,598:INFO:             prophet: Not installed
2026-01-30 13:27:46,598:INFO:None
2026-01-30 13:27:46,598:INFO:Set up data.
2026-01-30 13:27:46,672:INFO:Set up folding strategy.
2026-01-30 13:27:46,672:INFO:Set up train/test split.
2026-01-30 13:27:46,821:INFO:Set up index.
2026-01-30 13:27:46,826:INFO:Assigning column types.
2026-01-30 13:27:46,909:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 13:27:46,943:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 13:27:46,943:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:27:46,959:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:27:46,959:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:27:46,989:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 13:27:46,990:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:27:47,007:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:27:47,011:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:27:47,011:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 13:27:47,041:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:27:47,058:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:27:47,058:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:27:47,087:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:27:47,104:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:27:47,104:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:27:47,104:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 13:27:47,151:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:27:47,151:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:27:47,187:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:27:47,187:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:27:47,187:INFO:Preparing preprocessing pipeline...
2026-01-30 13:27:47,203:INFO:Set up simple imputation.
2026-01-30 13:27:47,203:INFO:Set up feature normalization.
2026-01-30 13:27:47,520:INFO:Finished creating preprocessing pipeline.
2026-01-30 13:27:47,520:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 13:27:47,520:INFO:Creating final display dataframe.
2026-01-30 13:27:48,636:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 21)
4        Transformed data shape      (373023, 21)
5   Transformed train set shape      (261116, 21)
6    Transformed test set shape      (111907, 21)
7              Numeric features                17
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              1663
2026-01-30 13:27:48,720:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:27:48,720:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:27:48,787:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:27:48,787:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:27:48,787:INFO:setup() successfully completed in 2.22s...............
2026-01-30 13:27:48,787:INFO:Initializing compare_models()
2026-01-30 13:27:48,787:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 13:27:48,787:INFO:Checking exceptions
2026-01-30 13:27:48,903:INFO:Preparing display monitor
2026-01-30 13:27:48,903:INFO:Initializing Logistic Regression
2026-01-30 13:27:48,903:INFO:Total runtime is 0.0 minutes
2026-01-30 13:27:48,903:INFO:SubProcess create_model() called ==================================
2026-01-30 13:27:48,903:INFO:Initializing create_model()
2026-01-30 13:27:48,903:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A048A95610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:27:48,903:INFO:Checking exceptions
2026-01-30 13:27:48,903:INFO:Importing libraries
2026-01-30 13:27:48,903:INFO:Copying training dataset
2026-01-30 13:27:49,086:INFO:Defining folds
2026-01-30 13:27:49,086:INFO:Declaring metric variables
2026-01-30 13:27:49,086:INFO:Importing untrained model
2026-01-30 13:27:49,086:INFO:Logistic Regression Imported successfully
2026-01-30 13:27:49,086:INFO:Starting cross validation
2026-01-30 13:27:49,086:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:27:57,898:INFO:Calculating mean and std
2026-01-30 13:27:57,899:INFO:Creating metrics dataframe
2026-01-30 13:27:57,901:INFO:Uploading results into container
2026-01-30 13:27:57,901:INFO:Uploading model into container now
2026-01-30 13:27:57,902:INFO:_master_model_container: 1
2026-01-30 13:27:57,902:INFO:_display_container: 2
2026-01-30 13:27:57,902:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 13:27:57,903:INFO:create_model() successfully completed......................................
2026-01-30 13:27:58,070:INFO:SubProcess create_model() end ==================================
2026-01-30 13:27:58,070:INFO:Creating metrics dataframe
2026-01-30 13:27:58,070:INFO:Initializing Decision Tree Classifier
2026-01-30 13:27:58,070:INFO:Total runtime is 0.15277994473775228 minutes
2026-01-30 13:27:58,070:INFO:SubProcess create_model() called ==================================
2026-01-30 13:27:58,070:INFO:Initializing create_model()
2026-01-30 13:27:58,070:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A048A95610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:27:58,070:INFO:Checking exceptions
2026-01-30 13:27:58,070:INFO:Importing libraries
2026-01-30 13:27:58,070:INFO:Copying training dataset
2026-01-30 13:27:58,186:INFO:Defining folds
2026-01-30 13:27:58,186:INFO:Declaring metric variables
2026-01-30 13:27:58,186:INFO:Importing untrained model
2026-01-30 13:27:58,186:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:27:58,186:INFO:Starting cross validation
2026-01-30 13:27:58,186:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:28:04,144:INFO:Calculating mean and std
2026-01-30 13:28:04,144:INFO:Creating metrics dataframe
2026-01-30 13:28:04,152:INFO:Uploading results into container
2026-01-30 13:28:04,152:INFO:Uploading model into container now
2026-01-30 13:28:04,152:INFO:_master_model_container: 2
2026-01-30 13:28:04,153:INFO:_display_container: 2
2026-01-30 13:28:04,153:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:28:04,153:INFO:create_model() successfully completed......................................
2026-01-30 13:28:04,336:INFO:SubProcess create_model() end ==================================
2026-01-30 13:28:04,336:INFO:Creating metrics dataframe
2026-01-30 13:28:04,336:INFO:Initializing Random Forest Classifier
2026-01-30 13:28:04,336:INFO:Total runtime is 0.25721621910730996 minutes
2026-01-30 13:28:04,336:INFO:SubProcess create_model() called ==================================
2026-01-30 13:28:04,336:INFO:Initializing create_model()
2026-01-30 13:28:04,336:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A048A95610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:28:04,336:INFO:Checking exceptions
2026-01-30 13:28:04,336:INFO:Importing libraries
2026-01-30 13:28:04,336:INFO:Copying training dataset
2026-01-30 13:28:04,453:INFO:Defining folds
2026-01-30 13:28:04,453:INFO:Declaring metric variables
2026-01-30 13:28:04,454:INFO:Importing untrained model
2026-01-30 13:28:04,454:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:28:04,454:INFO:Starting cross validation
2026-01-30 13:28:04,454:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:28:18,012:INFO:Calculating mean and std
2026-01-30 13:28:18,012:INFO:Creating metrics dataframe
2026-01-30 13:28:18,012:INFO:Uploading results into container
2026-01-30 13:28:18,012:INFO:Uploading model into container now
2026-01-30 13:28:18,012:INFO:_master_model_container: 3
2026-01-30 13:28:18,012:INFO:_display_container: 2
2026-01-30 13:28:18,012:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:28:18,012:INFO:create_model() successfully completed......................................
2026-01-30 13:28:18,203:INFO:SubProcess create_model() end ==================================
2026-01-30 13:28:18,203:INFO:Creating metrics dataframe
2026-01-30 13:28:18,203:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 13:28:18,203:INFO:Total runtime is 0.4883267601331075 minutes
2026-01-30 13:28:18,203:INFO:SubProcess create_model() called ==================================
2026-01-30 13:28:18,203:INFO:Initializing create_model()
2026-01-30 13:28:18,203:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A048A95610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:28:18,203:INFO:Checking exceptions
2026-01-30 13:28:18,203:INFO:Importing libraries
2026-01-30 13:28:18,203:INFO:Copying training dataset
2026-01-30 13:28:18,319:INFO:Defining folds
2026-01-30 13:28:18,319:INFO:Declaring metric variables
2026-01-30 13:28:18,319:INFO:Importing untrained model
2026-01-30 13:28:18,319:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:28:18,319:INFO:Starting cross validation
2026-01-30 13:28:18,319:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:28:24,645:INFO:Calculating mean and std
2026-01-30 13:28:24,645:INFO:Creating metrics dataframe
2026-01-30 13:28:24,645:INFO:Uploading results into container
2026-01-30 13:28:24,645:INFO:Uploading model into container now
2026-01-30 13:28:24,645:INFO:_master_model_container: 4
2026-01-30 13:28:24,645:INFO:_display_container: 2
2026-01-30 13:28:24,645:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:28:24,645:INFO:create_model() successfully completed......................................
2026-01-30 13:28:24,818:INFO:SubProcess create_model() end ==================================
2026-01-30 13:28:24,818:INFO:Creating metrics dataframe
2026-01-30 13:28:24,819:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 13:28:24,819:INFO:Initializing create_model()
2026-01-30 13:28:24,819:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:28:24,819:INFO:Checking exceptions
2026-01-30 13:28:24,819:INFO:Importing libraries
2026-01-30 13:28:24,823:INFO:Copying training dataset
2026-01-30 13:28:24,939:INFO:Defining folds
2026-01-30 13:28:24,939:INFO:Declaring metric variables
2026-01-30 13:28:24,939:INFO:Importing untrained model
2026-01-30 13:28:24,939:INFO:Declaring custom model
2026-01-30 13:28:24,939:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:28:24,939:INFO:Cross validation set to False
2026-01-30 13:28:24,939:INFO:Fitting Model
2026-01-30 13:28:30,123:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:28:30,123:INFO:create_model() successfully completed......................................
2026-01-30 13:28:30,303:INFO:Initializing create_model()
2026-01-30 13:28:30,303:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:28:30,303:INFO:Checking exceptions
2026-01-30 13:28:30,303:INFO:Importing libraries
2026-01-30 13:28:30,303:INFO:Copying training dataset
2026-01-30 13:28:30,436:INFO:Defining folds
2026-01-30 13:28:30,436:INFO:Declaring metric variables
2026-01-30 13:28:30,436:INFO:Importing untrained model
2026-01-30 13:28:30,436:INFO:Declaring custom model
2026-01-30 13:28:30,436:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:28:30,436:INFO:Cross validation set to False
2026-01-30 13:28:30,436:INFO:Fitting Model
2026-01-30 13:28:31,015:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-01-30 13:28:31,033:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009011 seconds.
2026-01-30 13:28:31,033:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:28:31,033:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:28:31,034:INFO:[LightGBM] [Info] Total Bins 2108
2026-01-30 13:28:31,035:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-01-30 13:28:31,037:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-01-30 13:28:31,037:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-01-30 13:28:31,687:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:28:31,688:INFO:create_model() successfully completed......................................
2026-01-30 13:28:31,886:INFO:Initializing create_model()
2026-01-30 13:28:31,886:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:28:31,886:INFO:Checking exceptions
2026-01-30 13:28:31,886:INFO:Importing libraries
2026-01-30 13:28:31,886:INFO:Copying training dataset
2026-01-30 13:28:32,019:INFO:Defining folds
2026-01-30 13:28:32,019:INFO:Declaring metric variables
2026-01-30 13:28:32,019:INFO:Importing untrained model
2026-01-30 13:28:32,019:INFO:Declaring custom model
2026-01-30 13:28:32,019:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:28:32,019:INFO:Cross validation set to False
2026-01-30 13:28:32,019:INFO:Fitting Model
2026-01-30 13:28:34,086:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:28:34,086:INFO:create_model() successfully completed......................................
2026-01-30 13:28:34,269:INFO:_master_model_container: 4
2026-01-30 13:28:34,269:INFO:_display_container: 2
2026-01-30 13:28:34,269:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 13:28:34,269:INFO:compare_models() successfully completed......................................
2026-01-30 13:28:34,286:INFO:Initializing tune_model()
2026-01-30 13:28:34,286:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:28:34,286:INFO:Checking exceptions
2026-01-30 13:28:34,339:INFO:Copying training dataset
2026-01-30 13:28:34,425:INFO:Checking base model
2026-01-30 13:28:34,425:INFO:Base model : Random Forest Classifier
2026-01-30 13:28:34,425:INFO:Declaring metric variables
2026-01-30 13:28:34,425:INFO:Defining Hyperparameters
2026-01-30 13:28:34,589:INFO:Tuning with n_jobs=-1
2026-01-30 13:28:34,589:INFO:Initializing RandomizedSearchCV
2026-01-30 13:30:02,275:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 13:30:02,276:INFO:Hyperparameter search completed
2026-01-30 13:30:02,277:INFO:SubProcess create_model() called ==================================
2026-01-30 13:30:02,277:INFO:Initializing create_model()
2026-01-30 13:30:02,277:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04850B250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 13:30:02,278:INFO:Checking exceptions
2026-01-30 13:30:02,278:INFO:Importing libraries
2026-01-30 13:30:02,278:INFO:Copying training dataset
2026-01-30 13:30:02,458:INFO:Defining folds
2026-01-30 13:30:02,458:INFO:Declaring metric variables
2026-01-30 13:30:02,458:INFO:Importing untrained model
2026-01-30 13:30:02,458:INFO:Declaring custom model
2026-01-30 13:30:02,459:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:30:02,459:INFO:Starting cross validation
2026-01-30 13:30:02,460:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:30:20,902:INFO:Calculating mean and std
2026-01-30 13:30:20,902:INFO:Creating metrics dataframe
2026-01-30 13:30:20,902:INFO:Finalizing model
2026-01-30 13:30:29,787:INFO:Uploading results into container
2026-01-30 13:30:29,787:INFO:Uploading model into container now
2026-01-30 13:30:29,787:INFO:_master_model_container: 5
2026-01-30 13:30:29,787:INFO:_display_container: 3
2026-01-30 13:30:29,787:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:30:29,787:INFO:create_model() successfully completed......................................
2026-01-30 13:30:29,982:INFO:SubProcess create_model() end ==================================
2026-01-30 13:30:29,982:INFO:choose_better activated
2026-01-30 13:30:29,982:INFO:SubProcess create_model() called ==================================
2026-01-30 13:30:29,982:INFO:Initializing create_model()
2026-01-30 13:30:29,982:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:30:29,982:INFO:Checking exceptions
2026-01-30 13:30:29,982:INFO:Importing libraries
2026-01-30 13:30:29,982:INFO:Copying training dataset
2026-01-30 13:30:30,150:INFO:Defining folds
2026-01-30 13:30:30,150:INFO:Declaring metric variables
2026-01-30 13:30:30,150:INFO:Importing untrained model
2026-01-30 13:30:30,150:INFO:Declaring custom model
2026-01-30 13:30:30,150:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:30:30,150:INFO:Starting cross validation
2026-01-30 13:30:30,150:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:30:42,988:INFO:Calculating mean and std
2026-01-30 13:30:42,988:INFO:Creating metrics dataframe
2026-01-30 13:30:42,989:INFO:Finalizing model
2026-01-30 13:30:49,274:INFO:Uploading results into container
2026-01-30 13:30:49,275:INFO:Uploading model into container now
2026-01-30 13:30:49,276:INFO:_master_model_container: 6
2026-01-30 13:30:49,276:INFO:_display_container: 4
2026-01-30 13:30:49,276:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:30:49,277:INFO:create_model() successfully completed......................................
2026-01-30 13:30:49,448:INFO:SubProcess create_model() end ==================================
2026-01-30 13:30:49,448:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9957
2026-01-30 13:30:49,448:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9656
2026-01-30 13:30:49,448:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 13:30:49,448:INFO:choose_better completed
2026-01-30 13:30:49,448:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 13:30:49,448:INFO:_master_model_container: 6
2026-01-30 13:30:49,448:INFO:_display_container: 3
2026-01-30 13:30:49,448:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:30:49,448:INFO:tune_model() successfully completed......................................
2026-01-30 13:30:49,621:INFO:Initializing tune_model()
2026-01-30 13:30:49,621:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:30:49,621:INFO:Checking exceptions
2026-01-30 13:30:49,674:INFO:Copying training dataset
2026-01-30 13:30:49,750:INFO:Checking base model
2026-01-30 13:30:49,750:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 13:30:49,750:INFO:Declaring metric variables
2026-01-30 13:30:49,750:INFO:Defining Hyperparameters
2026-01-30 13:30:49,936:INFO:Tuning with n_jobs=-1
2026-01-30 13:30:49,936:INFO:Initializing RandomizedSearchCV
2026-01-30 13:31:18,131:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 13:31:18,134:INFO:Hyperparameter search completed
2026-01-30 13:31:18,134:INFO:SubProcess create_model() called ==================================
2026-01-30 13:31:18,134:INFO:Initializing create_model()
2026-01-30 13:31:18,134:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0CCBF3BD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 13:31:18,134:INFO:Checking exceptions
2026-01-30 13:31:18,134:INFO:Importing libraries
2026-01-30 13:31:18,134:INFO:Copying training dataset
2026-01-30 13:31:18,270:INFO:Defining folds
2026-01-30 13:31:18,270:INFO:Declaring metric variables
2026-01-30 13:31:18,270:INFO:Importing untrained model
2026-01-30 13:31:18,270:INFO:Declaring custom model
2026-01-30 13:31:18,270:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:31:18,270:INFO:Starting cross validation
2026-01-30 13:31:18,281:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:31:25,355:INFO:Calculating mean and std
2026-01-30 13:31:25,355:INFO:Creating metrics dataframe
2026-01-30 13:31:25,355:INFO:Finalizing model
2026-01-30 13:31:25,803:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 13:31:25,803:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 13:31:25,803:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 13:31:25,959:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 13:31:25,959:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 13:31:25,959:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 13:31:25,961:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-01-30 13:31:25,978:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009307 seconds.
2026-01-30 13:31:25,978:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:31:25,978:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:31:25,980:INFO:[LightGBM] [Info] Total Bins 2108
2026-01-30 13:31:25,980:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-01-30 13:31:25,984:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-01-30 13:31:25,984:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-01-30 13:31:28,378:INFO:Uploading results into container
2026-01-30 13:31:28,378:INFO:Uploading model into container now
2026-01-30 13:31:28,380:INFO:_master_model_container: 7
2026-01-30 13:31:28,380:INFO:_display_container: 4
2026-01-30 13:31:28,382:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:31:28,382:INFO:create_model() successfully completed......................................
2026-01-30 13:31:28,599:INFO:SubProcess create_model() end ==================================
2026-01-30 13:31:28,599:INFO:choose_better activated
2026-01-30 13:31:28,615:INFO:SubProcess create_model() called ==================================
2026-01-30 13:31:28,615:INFO:Initializing create_model()
2026-01-30 13:31:28,616:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:31:28,616:INFO:Checking exceptions
2026-01-30 13:31:28,616:INFO:Importing libraries
2026-01-30 13:31:28,616:INFO:Copying training dataset
2026-01-30 13:31:28,744:INFO:Defining folds
2026-01-30 13:31:28,744:INFO:Declaring metric variables
2026-01-30 13:31:28,744:INFO:Importing untrained model
2026-01-30 13:31:28,744:INFO:Declaring custom model
2026-01-30 13:31:28,744:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:31:28,744:INFO:Starting cross validation
2026-01-30 13:31:28,744:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:31:32,270:INFO:Calculating mean and std
2026-01-30 13:31:32,270:INFO:Creating metrics dataframe
2026-01-30 13:31:32,270:INFO:Finalizing model
2026-01-30 13:31:32,843:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-01-30 13:31:32,863:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011038 seconds.
2026-01-30 13:31:32,863:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:31:32,863:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:31:32,863:INFO:[LightGBM] [Info] Total Bins 2108
2026-01-30 13:31:32,863:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-01-30 13:31:32,867:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-01-30 13:31:32,867:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-01-30 13:31:33,714:INFO:Uploading results into container
2026-01-30 13:31:33,716:INFO:Uploading model into container now
2026-01-30 13:31:33,716:INFO:_master_model_container: 8
2026-01-30 13:31:33,718:INFO:_display_container: 5
2026-01-30 13:31:33,718:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:31:33,718:INFO:create_model() successfully completed......................................
2026-01-30 13:31:33,950:INFO:SubProcess create_model() end ==================================
2026-01-30 13:31:33,950:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9785
2026-01-30 13:31:33,950:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9938
2026-01-30 13:31:33,950:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 13:31:33,950:INFO:choose_better completed
2026-01-30 13:31:33,950:INFO:_master_model_container: 8
2026-01-30 13:31:33,950:INFO:_display_container: 4
2026-01-30 13:31:33,950:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:31:33,950:INFO:tune_model() successfully completed......................................
2026-01-30 13:31:34,133:INFO:Initializing tune_model()
2026-01-30 13:31:34,137:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:31:34,137:INFO:Checking exceptions
2026-01-30 13:31:34,187:INFO:Copying training dataset
2026-01-30 13:31:34,267:INFO:Checking base model
2026-01-30 13:31:34,267:INFO:Base model : Decision Tree Classifier
2026-01-30 13:31:34,267:INFO:Declaring metric variables
2026-01-30 13:31:34,267:INFO:Defining Hyperparameters
2026-01-30 13:31:34,436:INFO:Tuning with n_jobs=-1
2026-01-30 13:31:34,436:INFO:Initializing RandomizedSearchCV
2026-01-30 13:31:39,777:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 13:31:39,777:INFO:Hyperparameter search completed
2026-01-30 13:31:39,777:INFO:SubProcess create_model() called ==================================
2026-01-30 13:31:39,777:INFO:Initializing create_model()
2026-01-30 13:31:39,777:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C2373D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 13:31:39,777:INFO:Checking exceptions
2026-01-30 13:31:39,777:INFO:Importing libraries
2026-01-30 13:31:39,777:INFO:Copying training dataset
2026-01-30 13:31:39,902:INFO:Defining folds
2026-01-30 13:31:39,902:INFO:Declaring metric variables
2026-01-30 13:31:39,902:INFO:Importing untrained model
2026-01-30 13:31:39,902:INFO:Declaring custom model
2026-01-30 13:31:39,902:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:31:39,902:INFO:Starting cross validation
2026-01-30 13:31:39,902:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:31:41,710:INFO:Calculating mean and std
2026-01-30 13:31:41,710:INFO:Creating metrics dataframe
2026-01-30 13:31:41,710:INFO:Finalizing model
2026-01-30 13:31:42,970:INFO:Uploading results into container
2026-01-30 13:31:42,970:INFO:Uploading model into container now
2026-01-30 13:31:42,970:INFO:_master_model_container: 9
2026-01-30 13:31:42,970:INFO:_display_container: 5
2026-01-30 13:31:42,970:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:31:42,970:INFO:create_model() successfully completed......................................
2026-01-30 13:31:43,148:INFO:SubProcess create_model() end ==================================
2026-01-30 13:31:43,148:INFO:choose_better activated
2026-01-30 13:31:43,148:INFO:SubProcess create_model() called ==================================
2026-01-30 13:31:43,148:INFO:Initializing create_model()
2026-01-30 13:31:43,148:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:31:43,148:INFO:Checking exceptions
2026-01-30 13:31:43,160:INFO:Importing libraries
2026-01-30 13:31:43,160:INFO:Copying training dataset
2026-01-30 13:31:43,283:INFO:Defining folds
2026-01-30 13:31:43,283:INFO:Declaring metric variables
2026-01-30 13:31:43,283:INFO:Importing untrained model
2026-01-30 13:31:43,283:INFO:Declaring custom model
2026-01-30 13:31:43,283:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:31:43,283:INFO:Starting cross validation
2026-01-30 13:31:43,283:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:31:45,774:INFO:Calculating mean and std
2026-01-30 13:31:45,774:INFO:Creating metrics dataframe
2026-01-30 13:31:45,774:INFO:Finalizing model
2026-01-30 13:31:47,851:INFO:Uploading results into container
2026-01-30 13:31:47,852:INFO:Uploading model into container now
2026-01-30 13:31:47,853:INFO:_master_model_container: 10
2026-01-30 13:31:47,853:INFO:_display_container: 6
2026-01-30 13:31:47,853:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:31:47,853:INFO:create_model() successfully completed......................................
2026-01-30 13:31:48,024:INFO:SubProcess create_model() end ==================================
2026-01-30 13:31:48,026:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9752
2026-01-30 13:31:48,026:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9572
2026-01-30 13:31:48,026:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 13:31:48,026:INFO:choose_better completed
2026-01-30 13:31:48,026:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 13:31:48,029:INFO:_master_model_container: 10
2026-01-30 13:31:48,029:INFO:_display_container: 5
2026-01-30 13:31:48,029:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:31:48,029:INFO:tune_model() successfully completed......................................
2026-01-30 13:31:48,199:INFO:Initializing predict_model()
2026-01-30 13:31:48,199:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A03D1EFA90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A03B48FA60>)
2026-01-30 13:31:48,199:INFO:Checking exceptions
2026-01-30 13:31:48,199:INFO:Preloading libraries
2026-01-30 13:31:48,199:INFO:Set up data.
2026-01-30 13:31:48,199:INFO:Set up index.
2026-01-30 13:34:41,174:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_26880\2655064592.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-01-30 13:34:43,384:INFO:PyCaret ClassificationExperiment
2026-01-30 13:34:43,384:INFO:Logging name: clf-default-name
2026-01-30 13:34:43,384:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-01-30 13:34:43,384:INFO:version 3.3.2
2026-01-30 13:34:43,384:INFO:Initializing setup()
2026-01-30 13:34:43,384:INFO:self.USI: fb0c
2026-01-30 13:34:43,384:INFO:self._variable_keys: {'fold_groups_param', 'is_multiclass', 'n_jobs_param', 'data', 'X', 'idx', 'y_test', 'log_plots_param', 'html_param', 'fold_shuffle_param', 'USI', 'target_param', 'fix_imbalance', '_ml_usecase', 'X_train', 'memory', 'exp_name_log', '_available_plots', 'y_train', 'X_test', 'seed', 'gpu_param', 'gpu_n_jobs_param', 'y', 'logging_param', 'pipeline', 'fold_generator', 'exp_id'}
2026-01-30 13:34:43,384:INFO:Checking environment
2026-01-30 13:34:43,384:INFO:python_version: 3.11.11
2026-01-30 13:34:43,384:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-01-30 13:34:43,384:INFO:machine: AMD64
2026-01-30 13:34:43,384:INFO:platform: Windows-10-10.0.26100-SP0
2026-01-30 13:34:43,384:INFO:Memory: svmem(total=34009374720, available=9635639296, percent=71.7, used=24373735424, free=9635639296)
2026-01-30 13:34:43,384:INFO:Physical Core: 12
2026-01-30 13:34:43,384:INFO:Logical Core: 16
2026-01-30 13:34:43,384:INFO:Checking libraries
2026-01-30 13:34:43,384:INFO:System:
2026-01-30 13:34:43,384:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-01-30 13:34:43,384:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-01-30 13:34:43,384:INFO:   machine: Windows-10-10.0.26100-SP0
2026-01-30 13:34:43,384:INFO:PyCaret required dependencies:
2026-01-30 13:34:43,384:INFO:                 pip: 25.0
2026-01-30 13:34:43,384:INFO:          setuptools: 75.8.0
2026-01-30 13:34:43,384:INFO:             pycaret: 3.3.2
2026-01-30 13:34:43,384:INFO:             IPython: 9.9.0
2026-01-30 13:34:43,384:INFO:          ipywidgets: 8.1.8
2026-01-30 13:34:43,384:INFO:                tqdm: 4.67.1
2026-01-30 13:34:43,384:INFO:               numpy: 1.26.4
2026-01-30 13:34:43,384:INFO:              pandas: 2.1.4
2026-01-30 13:34:43,384:INFO:              jinja2: 3.1.6
2026-01-30 13:34:43,384:INFO:               scipy: 1.11.4
2026-01-30 13:34:43,384:INFO:              joblib: 1.3.2
2026-01-30 13:34:43,384:INFO:             sklearn: 1.4.2
2026-01-30 13:34:43,384:INFO:                pyod: 2.0.6
2026-01-30 13:34:43,384:INFO:            imblearn: 0.14.1
2026-01-30 13:34:43,384:INFO:   category_encoders: 2.7.0
2026-01-30 13:34:43,384:INFO:            lightgbm: 4.6.0
2026-01-30 13:34:43,384:INFO:               numba: 0.62.1
2026-01-30 13:34:43,384:INFO:            requests: 2.32.3
2026-01-30 13:34:43,384:INFO:          matplotlib: 3.7.5
2026-01-30 13:34:43,384:INFO:          scikitplot: 0.3.7
2026-01-30 13:34:43,384:INFO:         yellowbrick: 1.5
2026-01-30 13:34:43,384:INFO:              plotly: 5.24.1
2026-01-30 13:34:43,384:INFO:    plotly-resampler: Not installed
2026-01-30 13:34:43,384:INFO:             kaleido: 1.2.0
2026-01-30 13:34:43,384:INFO:           schemdraw: 0.15
2026-01-30 13:34:43,384:INFO:         statsmodels: 0.14.6
2026-01-30 13:34:43,384:INFO:              sktime: 0.26.0
2026-01-30 13:34:43,384:INFO:               tbats: 1.1.3
2026-01-30 13:34:43,384:INFO:            pmdarima: 2.0.4
2026-01-30 13:34:43,384:INFO:              psutil: 7.2.1
2026-01-30 13:34:43,384:INFO:          markupsafe: 3.0.3
2026-01-30 13:34:43,384:INFO:             pickle5: Not installed
2026-01-30 13:34:43,384:INFO:         cloudpickle: 3.0.0
2026-01-30 13:34:43,384:INFO:         deprecation: 2.1.0
2026-01-30 13:34:43,384:INFO:              xxhash: 3.6.0
2026-01-30 13:34:43,384:INFO:           wurlitzer: Not installed
2026-01-30 13:34:43,384:INFO:PyCaret optional dependencies:
2026-01-30 13:34:43,384:INFO:                shap: 0.44.1
2026-01-30 13:34:43,384:INFO:           interpret: 0.7.3
2026-01-30 13:34:43,384:INFO:                umap: 0.5.7
2026-01-30 13:34:43,384:INFO:     ydata_profiling: 4.18.1
2026-01-30 13:34:43,384:INFO:  explainerdashboard: 0.5.1
2026-01-30 13:34:43,384:INFO:             autoviz: Not installed
2026-01-30 13:34:43,384:INFO:           fairlearn: 0.7.0
2026-01-30 13:34:43,384:INFO:          deepchecks: Not installed
2026-01-30 13:34:43,384:INFO:             xgboost: Not installed
2026-01-30 13:34:43,384:INFO:            catboost: 1.2.8
2026-01-30 13:34:43,384:INFO:              kmodes: 0.12.2
2026-01-30 13:34:43,384:INFO:             mlxtend: 0.23.4
2026-01-30 13:34:43,384:INFO:       statsforecast: 1.5.0
2026-01-30 13:34:43,384:INFO:        tune_sklearn: Not installed
2026-01-30 13:34:43,396:INFO:                 ray: Not installed
2026-01-30 13:34:43,396:INFO:            hyperopt: 0.2.7
2026-01-30 13:34:43,396:INFO:              optuna: 4.6.0
2026-01-30 13:34:43,396:INFO:               skopt: 0.10.2
2026-01-30 13:34:43,396:INFO:              mlflow: 3.8.1
2026-01-30 13:34:43,396:INFO:              gradio: 6.3.0
2026-01-30 13:34:43,396:INFO:             fastapi: 0.128.0
2026-01-30 13:34:43,396:INFO:             uvicorn: 0.40.0
2026-01-30 13:34:43,396:INFO:              m2cgen: 0.10.0
2026-01-30 13:34:43,396:INFO:           evidently: 0.4.40
2026-01-30 13:34:43,396:INFO:               fugue: 0.8.7
2026-01-30 13:34:43,397:INFO:           streamlit: Not installed
2026-01-30 13:34:43,397:INFO:             prophet: Not installed
2026-01-30 13:34:43,397:INFO:None
2026-01-30 13:34:43,397:INFO:Set up data.
2026-01-30 13:34:43,464:INFO:Set up folding strategy.
2026-01-30 13:34:43,464:INFO:Set up train/test split.
2026-01-30 13:34:43,627:INFO:Set up index.
2026-01-30 13:34:43,636:INFO:Assigning column types.
2026-01-30 13:34:43,729:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-01-30 13:34:43,757:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 13:34:43,757:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:34:43,764:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:34:43,764:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:34:43,800:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-01-30 13:34:43,800:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:34:43,819:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:34:43,819:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:34:43,819:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-01-30 13:34:43,847:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:34:43,864:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:34:43,864:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:34:43,897:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-01-30 13:34:43,914:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:34:43,914:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:34:43,914:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-01-30 13:34:43,963:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:34:43,963:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:34:43,997:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:34:43,997:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:34:43,997:INFO:Preparing preprocessing pipeline...
2026-01-30 13:34:44,019:INFO:Set up simple imputation.
2026-01-30 13:34:44,019:INFO:Set up feature normalization.
2026-01-30 13:34:44,291:INFO:Finished creating preprocessing pipeline.
2026-01-30 13:34:44,299:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-01-30 13:34:44,299:INFO:Creating final display dataframe.
2026-01-30 13:34:44,695:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 21)
4        Transformed data shape      (373023, 21)
5   Transformed train set shape      (261116, 21)
6    Transformed test set shape      (111907, 21)
7              Numeric features                17
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              fb0c
2026-01-30 13:34:44,742:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:34:44,742:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:34:44,790:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-01-30 13:34:44,790:INFO:Soft dependency imported: catboost: 1.2.8
2026-01-30 13:34:44,796:INFO:setup() successfully completed in 1.43s...............
2026-01-30 13:34:44,796:INFO:Initializing compare_models()
2026-01-30 13:34:44,796:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-01-30 13:34:44,796:INFO:Checking exceptions
2026-01-30 13:34:44,864:INFO:Preparing display monitor
2026-01-30 13:34:44,864:INFO:Initializing Logistic Regression
2026-01-30 13:34:44,864:INFO:Total runtime is 0.0 minutes
2026-01-30 13:34:44,864:INFO:SubProcess create_model() called ==================================
2026-01-30 13:34:44,864:INFO:Initializing create_model()
2026-01-30 13:34:44,864:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C609A10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:34:44,864:INFO:Checking exceptions
2026-01-30 13:34:44,864:INFO:Importing libraries
2026-01-30 13:34:44,864:INFO:Copying training dataset
2026-01-30 13:34:44,980:INFO:Defining folds
2026-01-30 13:34:44,980:INFO:Declaring metric variables
2026-01-30 13:34:44,980:INFO:Importing untrained model
2026-01-30 13:34:44,980:INFO:Logistic Regression Imported successfully
2026-01-30 13:34:44,980:INFO:Starting cross validation
2026-01-30 13:34:44,980:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:34:46,649:INFO:Calculating mean and std
2026-01-30 13:34:46,649:INFO:Creating metrics dataframe
2026-01-30 13:34:46,650:INFO:Uploading results into container
2026-01-30 13:34:46,650:INFO:Uploading model into container now
2026-01-30 13:34:46,650:INFO:_master_model_container: 1
2026-01-30 13:34:46,650:INFO:_display_container: 2
2026-01-30 13:34:46,650:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-01-30 13:34:46,650:INFO:create_model() successfully completed......................................
2026-01-30 13:34:46,831:INFO:SubProcess create_model() end ==================================
2026-01-30 13:34:46,831:INFO:Creating metrics dataframe
2026-01-30 13:34:46,831:INFO:Initializing Decision Tree Classifier
2026-01-30 13:34:46,831:INFO:Total runtime is 0.03278047243754069 minutes
2026-01-30 13:34:46,831:INFO:SubProcess create_model() called ==================================
2026-01-30 13:34:46,831:INFO:Initializing create_model()
2026-01-30 13:34:46,831:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C609A10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:34:46,831:INFO:Checking exceptions
2026-01-30 13:34:46,831:INFO:Importing libraries
2026-01-30 13:34:46,831:INFO:Copying training dataset
2026-01-30 13:34:46,950:INFO:Defining folds
2026-01-30 13:34:46,950:INFO:Declaring metric variables
2026-01-30 13:34:46,950:INFO:Importing untrained model
2026-01-30 13:34:46,950:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:34:46,950:INFO:Starting cross validation
2026-01-30 13:34:46,950:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:34:49,178:INFO:Calculating mean and std
2026-01-30 13:34:49,178:INFO:Creating metrics dataframe
2026-01-30 13:34:49,178:INFO:Uploading results into container
2026-01-30 13:34:49,178:INFO:Uploading model into container now
2026-01-30 13:34:49,178:INFO:_master_model_container: 2
2026-01-30 13:34:49,178:INFO:_display_container: 2
2026-01-30 13:34:49,184:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:34:49,184:INFO:create_model() successfully completed......................................
2026-01-30 13:34:49,370:INFO:SubProcess create_model() end ==================================
2026-01-30 13:34:49,370:INFO:Creating metrics dataframe
2026-01-30 13:34:49,372:INFO:Initializing Random Forest Classifier
2026-01-30 13:34:49,372:INFO:Total runtime is 0.0751427133878072 minutes
2026-01-30 13:34:49,372:INFO:SubProcess create_model() called ==================================
2026-01-30 13:34:49,372:INFO:Initializing create_model()
2026-01-30 13:34:49,372:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C609A10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:34:49,373:INFO:Checking exceptions
2026-01-30 13:34:49,373:INFO:Importing libraries
2026-01-30 13:34:49,373:INFO:Copying training dataset
2026-01-30 13:34:49,480:INFO:Defining folds
2026-01-30 13:34:49,480:INFO:Declaring metric variables
2026-01-30 13:34:49,480:INFO:Importing untrained model
2026-01-30 13:34:49,480:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:34:49,480:INFO:Starting cross validation
2026-01-30 13:34:49,480:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:35:01,089:INFO:Calculating mean and std
2026-01-30 13:35:01,089:INFO:Creating metrics dataframe
2026-01-30 13:35:01,089:INFO:Uploading results into container
2026-01-30 13:35:01,097:INFO:Uploading model into container now
2026-01-30 13:35:01,097:INFO:_master_model_container: 3
2026-01-30 13:35:01,097:INFO:_display_container: 2
2026-01-30 13:35:01,097:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:35:01,097:INFO:create_model() successfully completed......................................
2026-01-30 13:35:01,280:INFO:SubProcess create_model() end ==================================
2026-01-30 13:35:01,280:INFO:Creating metrics dataframe
2026-01-30 13:35:01,280:INFO:Initializing Light Gradient Boosting Machine
2026-01-30 13:35:01,280:INFO:Total runtime is 0.27360623677571616 minutes
2026-01-30 13:35:01,280:INFO:SubProcess create_model() called ==================================
2026-01-30 13:35:01,280:INFO:Initializing create_model()
2026-01-30 13:35:01,280:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A04C609A10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:35:01,280:INFO:Checking exceptions
2026-01-30 13:35:01,280:INFO:Importing libraries
2026-01-30 13:35:01,280:INFO:Copying training dataset
2026-01-30 13:35:01,413:INFO:Defining folds
2026-01-30 13:35:01,413:INFO:Declaring metric variables
2026-01-30 13:35:01,413:INFO:Importing untrained model
2026-01-30 13:35:01,413:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:35:01,413:INFO:Starting cross validation
2026-01-30 13:35:01,413:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:35:04,700:INFO:Calculating mean and std
2026-01-30 13:35:04,700:INFO:Creating metrics dataframe
2026-01-30 13:35:04,700:INFO:Uploading results into container
2026-01-30 13:35:04,700:INFO:Uploading model into container now
2026-01-30 13:35:04,700:INFO:_master_model_container: 4
2026-01-30 13:35:04,700:INFO:_display_container: 2
2026-01-30 13:35:04,700:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:35:04,700:INFO:create_model() successfully completed......................................
2026-01-30 13:35:04,867:INFO:SubProcess create_model() end ==================================
2026-01-30 13:35:04,867:INFO:Creating metrics dataframe
2026-01-30 13:35:04,880:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-01-30 13:35:04,880:INFO:Initializing create_model()
2026-01-30 13:35:04,880:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:35:04,880:INFO:Checking exceptions
2026-01-30 13:35:04,880:INFO:Importing libraries
2026-01-30 13:35:04,880:INFO:Copying training dataset
2026-01-30 13:35:04,997:INFO:Defining folds
2026-01-30 13:35:04,997:INFO:Declaring metric variables
2026-01-30 13:35:04,997:INFO:Importing untrained model
2026-01-30 13:35:04,997:INFO:Declaring custom model
2026-01-30 13:35:04,997:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:35:04,997:INFO:Cross validation set to False
2026-01-30 13:35:04,997:INFO:Fitting Model
2026-01-30 13:35:11,081:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:35:11,081:INFO:create_model() successfully completed......................................
2026-01-30 13:35:11,263:INFO:Initializing create_model()
2026-01-30 13:35:11,263:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:35:11,263:INFO:Checking exceptions
2026-01-30 13:35:11,263:INFO:Importing libraries
2026-01-30 13:35:11,263:INFO:Copying training dataset
2026-01-30 13:35:11,380:INFO:Defining folds
2026-01-30 13:35:11,380:INFO:Declaring metric variables
2026-01-30 13:35:11,380:INFO:Importing untrained model
2026-01-30 13:35:11,380:INFO:Declaring custom model
2026-01-30 13:35:11,380:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:35:11,380:INFO:Cross validation set to False
2026-01-30 13:35:11,380:INFO:Fitting Model
2026-01-30 13:35:11,927:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-01-30 13:35:11,945:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009485 seconds.
2026-01-30 13:35:11,945:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:35:11,945:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:35:11,946:INFO:[LightGBM] [Info] Total Bins 2108
2026-01-30 13:35:11,946:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-01-30 13:35:11,946:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-01-30 13:35:11,946:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-01-30 13:35:12,547:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:35:12,547:INFO:create_model() successfully completed......................................
2026-01-30 13:35:12,763:INFO:Initializing create_model()
2026-01-30 13:35:12,763:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:35:12,763:INFO:Checking exceptions
2026-01-30 13:35:12,763:INFO:Importing libraries
2026-01-30 13:35:12,763:INFO:Copying training dataset
2026-01-30 13:35:12,897:INFO:Defining folds
2026-01-30 13:35:12,897:INFO:Declaring metric variables
2026-01-30 13:35:12,897:INFO:Importing untrained model
2026-01-30 13:35:12,897:INFO:Declaring custom model
2026-01-30 13:35:12,897:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:35:12,897:INFO:Cross validation set to False
2026-01-30 13:35:12,897:INFO:Fitting Model
2026-01-30 13:35:14,813:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:35:14,813:INFO:create_model() successfully completed......................................
2026-01-30 13:35:14,997:INFO:_master_model_container: 4
2026-01-30 13:35:14,997:INFO:_display_container: 2
2026-01-30 13:35:14,997:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-01-30 13:35:14,997:INFO:compare_models() successfully completed......................................
2026-01-30 13:35:15,012:INFO:Initializing tune_model()
2026-01-30 13:35:15,012:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:35:15,012:INFO:Checking exceptions
2026-01-30 13:35:15,050:INFO:Copying training dataset
2026-01-30 13:35:15,138:INFO:Checking base model
2026-01-30 13:35:15,138:INFO:Base model : Random Forest Classifier
2026-01-30 13:35:15,138:INFO:Declaring metric variables
2026-01-30 13:35:15,138:INFO:Defining Hyperparameters
2026-01-30 13:35:15,297:INFO:Tuning with n_jobs=-1
2026-01-30 13:35:15,297:INFO:Initializing RandomizedSearchCV
2026-01-30 13:37:02,023:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-01-30 13:37:02,023:INFO:Hyperparameter search completed
2026-01-30 13:37:02,023:INFO:SubProcess create_model() called ==================================
2026-01-30 13:37:02,028:INFO:Initializing create_model()
2026-01-30 13:37:02,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0530E6C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-01-30 13:37:02,028:INFO:Checking exceptions
2026-01-30 13:37:02,028:INFO:Importing libraries
2026-01-30 13:37:02,028:INFO:Copying training dataset
2026-01-30 13:37:02,195:INFO:Defining folds
2026-01-30 13:37:02,195:INFO:Declaring metric variables
2026-01-30 13:37:02,195:INFO:Importing untrained model
2026-01-30 13:37:02,195:INFO:Declaring custom model
2026-01-30 13:37:02,195:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:37:02,195:INFO:Starting cross validation
2026-01-30 13:37:02,195:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:37:25,680:INFO:Calculating mean and std
2026-01-30 13:37:25,680:INFO:Creating metrics dataframe
2026-01-30 13:37:25,680:INFO:Finalizing model
2026-01-30 13:37:36,898:INFO:Uploading results into container
2026-01-30 13:37:36,901:INFO:Uploading model into container now
2026-01-30 13:37:36,901:INFO:_master_model_container: 5
2026-01-30 13:37:36,901:INFO:_display_container: 3
2026-01-30 13:37:36,901:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:37:36,901:INFO:create_model() successfully completed......................................
2026-01-30 13:37:37,120:INFO:SubProcess create_model() end ==================================
2026-01-30 13:37:37,121:INFO:choose_better activated
2026-01-30 13:37:37,121:INFO:SubProcess create_model() called ==================================
2026-01-30 13:37:37,121:INFO:Initializing create_model()
2026-01-30 13:37:37,121:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:37:37,121:INFO:Checking exceptions
2026-01-30 13:37:37,123:INFO:Importing libraries
2026-01-30 13:37:37,123:INFO:Copying training dataset
2026-01-30 13:37:37,245:INFO:Defining folds
2026-01-30 13:37:37,245:INFO:Declaring metric variables
2026-01-30 13:37:37,245:INFO:Importing untrained model
2026-01-30 13:37:37,245:INFO:Declaring custom model
2026-01-30 13:37:37,245:INFO:Random Forest Classifier Imported successfully
2026-01-30 13:37:37,245:INFO:Starting cross validation
2026-01-30 13:37:37,245:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:37:52,727:INFO:Calculating mean and std
2026-01-30 13:37:52,727:INFO:Creating metrics dataframe
2026-01-30 13:37:52,727:INFO:Finalizing model
2026-01-30 13:37:59,667:INFO:Uploading results into container
2026-01-30 13:37:59,667:INFO:Uploading model into container now
2026-01-30 13:37:59,669:INFO:_master_model_container: 6
2026-01-30 13:37:59,669:INFO:_display_container: 4
2026-01-30 13:37:59,669:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:37:59,669:INFO:create_model() successfully completed......................................
2026-01-30 13:37:59,861:INFO:SubProcess create_model() end ==================================
2026-01-30 13:37:59,861:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9957
2026-01-30 13:37:59,861:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9656
2026-01-30 13:37:59,861:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-01-30 13:37:59,861:INFO:choose_better completed
2026-01-30 13:37:59,861:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 13:37:59,861:INFO:_master_model_container: 6
2026-01-30 13:37:59,861:INFO:_display_container: 3
2026-01-30 13:37:59,861:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-01-30 13:37:59,861:INFO:tune_model() successfully completed......................................
2026-01-30 13:38:00,046:INFO:Initializing tune_model()
2026-01-30 13:38:00,047:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:38:00,047:INFO:Checking exceptions
2026-01-30 13:38:00,078:INFO:Copying training dataset
2026-01-30 13:38:00,169:INFO:Checking base model
2026-01-30 13:38:00,169:INFO:Base model : Light Gradient Boosting Machine
2026-01-30 13:38:00,169:INFO:Declaring metric variables
2026-01-30 13:38:00,170:INFO:Defining Hyperparameters
2026-01-30 13:38:00,331:INFO:Tuning with n_jobs=-1
2026-01-30 13:38:00,331:INFO:Initializing RandomizedSearchCV
2026-01-30 13:38:33,138:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-01-30 13:38:33,138:INFO:Hyperparameter search completed
2026-01-30 13:38:33,138:INFO:SubProcess create_model() called ==================================
2026-01-30 13:38:33,144:INFO:Initializing create_model()
2026-01-30 13:38:33,144:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A0C3631650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-01-30 13:38:33,144:INFO:Checking exceptions
2026-01-30 13:38:33,144:INFO:Importing libraries
2026-01-30 13:38:33,144:INFO:Copying training dataset
2026-01-30 13:38:33,322:INFO:Defining folds
2026-01-30 13:38:33,322:INFO:Declaring metric variables
2026-01-30 13:38:33,322:INFO:Importing untrained model
2026-01-30 13:38:33,323:INFO:Declaring custom model
2026-01-30 13:38:33,324:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:38:33,324:INFO:Starting cross validation
2026-01-30 13:38:33,325:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:38:41,089:INFO:Calculating mean and std
2026-01-30 13:38:41,089:INFO:Creating metrics dataframe
2026-01-30 13:38:41,093:INFO:Finalizing model
2026-01-30 13:38:41,526:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 13:38:41,526:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 13:38:41,526:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 13:38:41,686:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-01-30 13:38:41,686:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-01-30 13:38:41,686:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-01-30 13:38:41,686:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-01-30 13:38:41,706:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009164 seconds.
2026-01-30 13:38:41,706:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:38:41,706:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:38:41,706:INFO:[LightGBM] [Info] Total Bins 2108
2026-01-30 13:38:41,708:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-01-30 13:38:41,712:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-01-30 13:38:41,714:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-01-30 13:38:44,729:INFO:Uploading results into container
2026-01-30 13:38:44,731:INFO:Uploading model into container now
2026-01-30 13:38:44,732:INFO:_master_model_container: 7
2026-01-30 13:38:44,732:INFO:_display_container: 4
2026-01-30 13:38:44,734:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:38:44,734:INFO:create_model() successfully completed......................................
2026-01-30 13:38:44,977:INFO:SubProcess create_model() end ==================================
2026-01-30 13:38:44,977:INFO:choose_better activated
2026-01-30 13:38:44,977:INFO:SubProcess create_model() called ==================================
2026-01-30 13:38:44,977:INFO:Initializing create_model()
2026-01-30 13:38:44,977:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:38:44,977:INFO:Checking exceptions
2026-01-30 13:38:44,977:INFO:Importing libraries
2026-01-30 13:38:44,977:INFO:Copying training dataset
2026-01-30 13:38:45,110:INFO:Defining folds
2026-01-30 13:38:45,110:INFO:Declaring metric variables
2026-01-30 13:38:45,110:INFO:Importing untrained model
2026-01-30 13:38:45,110:INFO:Declaring custom model
2026-01-30 13:38:45,110:INFO:Light Gradient Boosting Machine Imported successfully
2026-01-30 13:38:45,110:INFO:Starting cross validation
2026-01-30 13:38:45,110:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:38:48,885:INFO:Calculating mean and std
2026-01-30 13:38:48,885:INFO:Creating metrics dataframe
2026-01-30 13:38:48,885:INFO:Finalizing model
2026-01-30 13:38:49,539:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-01-30 13:38:49,559:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009335 seconds.
2026-01-30 13:38:49,559:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-01-30 13:38:49,560:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-01-30 13:38:49,560:INFO:[LightGBM] [Info] Total Bins 2108
2026-01-30 13:38:49,560:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-01-30 13:38:49,563:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-01-30 13:38:49,563:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-01-30 13:38:50,348:INFO:Uploading results into container
2026-01-30 13:38:50,348:INFO:Uploading model into container now
2026-01-30 13:38:50,350:INFO:_master_model_container: 8
2026-01-30 13:38:50,350:INFO:_display_container: 5
2026-01-30 13:38:50,350:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:38:50,350:INFO:create_model() successfully completed......................................
2026-01-30 13:38:50,595:INFO:SubProcess create_model() end ==================================
2026-01-30 13:38:50,596:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9785
2026-01-30 13:38:50,597:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9938
2026-01-30 13:38:50,598:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-01-30 13:38:50,598:INFO:choose_better completed
2026-01-30 13:38:50,599:INFO:_master_model_container: 8
2026-01-30 13:38:50,599:INFO:_display_container: 4
2026-01-30 13:38:50,599:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-01-30 13:38:50,599:INFO:tune_model() successfully completed......................................
2026-01-30 13:38:50,777:INFO:Initializing tune_model()
2026-01-30 13:38:50,777:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-01-30 13:38:50,777:INFO:Checking exceptions
2026-01-30 13:38:50,827:INFO:Copying training dataset
2026-01-30 13:38:50,896:INFO:Checking base model
2026-01-30 13:38:50,896:INFO:Base model : Decision Tree Classifier
2026-01-30 13:38:50,896:INFO:Declaring metric variables
2026-01-30 13:38:50,896:INFO:Defining Hyperparameters
2026-01-30 13:38:51,080:INFO:Tuning with n_jobs=-1
2026-01-30 13:38:51,080:INFO:Initializing RandomizedSearchCV
2026-01-30 13:38:56,533:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-01-30 13:38:56,533:INFO:Hyperparameter search completed
2026-01-30 13:38:56,533:INFO:SubProcess create_model() called ==================================
2026-01-30 13:38:56,533:INFO:Initializing create_model()
2026-01-30 13:38:56,533:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002A048A82750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-01-30 13:38:56,533:INFO:Checking exceptions
2026-01-30 13:38:56,533:INFO:Importing libraries
2026-01-30 13:38:56,533:INFO:Copying training dataset
2026-01-30 13:38:56,665:INFO:Defining folds
2026-01-30 13:38:56,665:INFO:Declaring metric variables
2026-01-30 13:38:56,665:INFO:Importing untrained model
2026-01-30 13:38:56,665:INFO:Declaring custom model
2026-01-30 13:38:56,667:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:38:56,667:INFO:Starting cross validation
2026-01-30 13:38:56,667:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:38:58,462:INFO:Calculating mean and std
2026-01-30 13:38:58,462:INFO:Creating metrics dataframe
2026-01-30 13:38:58,462:INFO:Finalizing model
2026-01-30 13:38:59,643:INFO:Uploading results into container
2026-01-30 13:38:59,643:INFO:Uploading model into container now
2026-01-30 13:38:59,643:INFO:_master_model_container: 9
2026-01-30 13:38:59,643:INFO:_display_container: 5
2026-01-30 13:38:59,643:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:38:59,643:INFO:create_model() successfully completed......................................
2026-01-30 13:38:59,810:INFO:SubProcess create_model() end ==================================
2026-01-30 13:38:59,810:INFO:choose_better activated
2026-01-30 13:38:59,810:INFO:SubProcess create_model() called ==================================
2026-01-30 13:38:59,810:INFO:Initializing create_model()
2026-01-30 13:38:59,810:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-01-30 13:38:59,810:INFO:Checking exceptions
2026-01-30 13:38:59,810:INFO:Importing libraries
2026-01-30 13:38:59,810:INFO:Copying training dataset
2026-01-30 13:38:59,943:INFO:Defining folds
2026-01-30 13:38:59,943:INFO:Declaring metric variables
2026-01-30 13:38:59,943:INFO:Importing untrained model
2026-01-30 13:38:59,943:INFO:Declaring custom model
2026-01-30 13:38:59,943:INFO:Decision Tree Classifier Imported successfully
2026-01-30 13:38:59,943:INFO:Starting cross validation
2026-01-30 13:38:59,943:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-01-30 13:39:02,504:INFO:Calculating mean and std
2026-01-30 13:39:02,504:INFO:Creating metrics dataframe
2026-01-30 13:39:02,504:INFO:Finalizing model
2026-01-30 13:39:04,410:INFO:Uploading results into container
2026-01-30 13:39:04,410:INFO:Uploading model into container now
2026-01-30 13:39:04,410:INFO:_master_model_container: 10
2026-01-30 13:39:04,410:INFO:_display_container: 6
2026-01-30 13:39:04,410:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:39:04,410:INFO:create_model() successfully completed......................................
2026-01-30 13:39:04,585:INFO:SubProcess create_model() end ==================================
2026-01-30 13:39:04,585:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9752
2026-01-30 13:39:04,585:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9572
2026-01-30 13:39:04,585:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-01-30 13:39:04,585:INFO:choose_better completed
2026-01-30 13:39:04,585:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-01-30 13:39:04,593:INFO:_master_model_container: 10
2026-01-30 13:39:04,593:INFO:_display_container: 5
2026-01-30 13:39:04,593:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-01-30 13:39:04,593:INFO:tune_model() successfully completed......................................
2026-01-30 13:39:04,781:INFO:Initializing predict_model()
2026-01-30 13:39:04,781:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A120455A80>)
2026-01-30 13:39:04,781:INFO:Checking exceptions
2026-01-30 13:39:04,781:INFO:Preloading libraries
2026-01-30 13:39:04,781:INFO:Set up data.
2026-01-30 13:39:04,793:INFO:Set up index.
2026-01-30 13:39:05,560:INFO:Initializing predict_model()
2026-01-30 13:39:05,560:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A01562A5C0>)
2026-01-30 13:39:05,560:INFO:Checking exceptions
2026-01-30 13:39:05,560:INFO:Preloading libraries
2026-01-30 13:39:05,560:INFO:Set up data.
2026-01-30 13:39:05,580:INFO:Set up index.
2026-01-30 13:39:06,399:INFO:Initializing predict_model()
2026-01-30 13:39:06,399:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002A01562A5C0>)
2026-01-30 13:39:06,399:INFO:Checking exceptions
2026-01-30 13:39:06,399:INFO:Preloading libraries
2026-01-30 13:39:06,399:INFO:Set up data.
2026-01-30 13:39:06,421:INFO:Set up index.
2026-01-30 13:39:07,148:INFO:Initializing plot_model()
2026-01-30 13:39:07,148:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 13:39:07,148:INFO:Checking exceptions
2026-01-30 13:39:07,227:INFO:Preloading libraries
2026-01-30 13:39:07,343:INFO:Copying training dataset
2026-01-30 13:39:07,343:INFO:Plot type: feature
2026-01-30 13:39:07,343:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 13:39:07,610:INFO:Visual Rendered Successfully
2026-01-30 13:39:07,776:INFO:plot_model() successfully completed......................................
2026-01-30 13:39:07,797:INFO:Initializing plot_model()
2026-01-30 13:39:07,799:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002A0CCBD1090>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-01-30 13:39:07,799:INFO:Checking exceptions
2026-01-30 13:39:07,879:INFO:Preloading libraries
2026-01-30 13:39:07,979:INFO:Copying training dataset
2026-01-30 13:39:07,979:INFO:Plot type: feature_all
2026-01-30 13:39:08,108:WARNING:No coef_ found. Trying feature_importances_
2026-01-30 13:39:08,437:INFO:Visual Rendered Successfully
2026-01-30 13:39:08,632:INFO:plot_model() successfully completed......................................
2026-01-30 13:39:08,648:INFO:Initializing save_model()
2026-01-30 13:39:08,648:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-01-30 13:39:08,648:INFO:Adding model into prep_pipe
2026-01-30 13:39:08,835:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-01-30 13:39:08,835:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-01-30 13:39:08,835:INFO:save_model() successfully completed......................................
2026-01-30 13:39:51,489:INFO:Initializing load_model()
2026-01-30 13:39:51,489:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-01-30 13:39:53,676:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2637609198.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-01-30 13:39:55,593:INFO:Initializing predict_model()
2026-01-30 13:39:55,593:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029C50F0B310>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum', 'PCA1',
                                             'PCA2'...
                                             'flag_NU_NOTA_MEDIA_1_BACH__PC_na',
                                             'flag_NU_RESULTADO_ADMISION_PUNTOS_na',
                                             'flag_CU_IMPORTE_TOTAL_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000029C60835B20>)
2026-01-30 13:39:55,593:INFO:Checking exceptions
2026-01-30 13:39:55,593:INFO:Preloading libraries
2026-01-30 13:39:55,593:INFO:Set up data.
2026-01-30 13:39:56,546:INFO:Set up index.
2026-01-30 13:39:59,680:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2637609198.py:120: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-01-30 13:40:00,067:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_27688\2637609198.py:120: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-02-02 09:12:11,303:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-02 09:12:11,303:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-02 09:12:11,303:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-02 09:12:11,303:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-02 09:13:05,503:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\2655064592.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-02-02 09:13:07,703:INFO:PyCaret ClassificationExperiment
2026-02-02 09:13:07,703:INFO:Logging name: clf-default-name
2026-02-02 09:13:07,703:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 09:13:07,703:INFO:version 3.3.2
2026-02-02 09:13:07,703:INFO:Initializing setup()
2026-02-02 09:13:07,703:INFO:self.USI: e742
2026-02-02 09:13:07,703:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 09:13:07,703:INFO:Checking environment
2026-02-02 09:13:07,703:INFO:python_version: 3.11.11
2026-02-02 09:13:07,703:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 09:13:07,703:INFO:machine: AMD64
2026-02-02 09:13:07,703:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 09:13:07,703:INFO:Memory: svmem(total=34009374720, available=17036472320, percent=49.9, used=16972902400, free=17036472320)
2026-02-02 09:13:07,703:INFO:Physical Core: 12
2026-02-02 09:13:07,703:INFO:Logical Core: 16
2026-02-02 09:13:07,703:INFO:Checking libraries
2026-02-02 09:13:07,703:INFO:System:
2026-02-02 09:13:07,703:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 09:13:07,703:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 09:13:07,703:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 09:13:07,703:INFO:PyCaret required dependencies:
2026-02-02 09:13:09,160:INFO:                 pip: 25.0
2026-02-02 09:13:09,160:INFO:          setuptools: 75.8.0
2026-02-02 09:13:09,160:INFO:             pycaret: 3.3.2
2026-02-02 09:13:09,160:INFO:             IPython: 9.9.0
2026-02-02 09:13:09,160:INFO:          ipywidgets: 8.1.8
2026-02-02 09:13:09,160:INFO:                tqdm: 4.67.1
2026-02-02 09:13:09,168:INFO:               numpy: 1.26.4
2026-02-02 09:13:09,168:INFO:              pandas: 2.1.4
2026-02-02 09:13:09,168:INFO:              jinja2: 3.1.6
2026-02-02 09:13:09,168:INFO:               scipy: 1.11.4
2026-02-02 09:13:09,168:INFO:              joblib: 1.3.2
2026-02-02 09:13:09,168:INFO:             sklearn: 1.4.2
2026-02-02 09:13:09,168:INFO:                pyod: 2.0.6
2026-02-02 09:13:09,168:INFO:            imblearn: 0.14.1
2026-02-02 09:13:09,169:INFO:   category_encoders: 2.7.0
2026-02-02 09:13:09,169:INFO:            lightgbm: 4.6.0
2026-02-02 09:13:09,169:INFO:               numba: 0.62.1
2026-02-02 09:13:09,169:INFO:            requests: 2.32.3
2026-02-02 09:13:09,169:INFO:          matplotlib: 3.7.5
2026-02-02 09:13:09,169:INFO:          scikitplot: 0.3.7
2026-02-02 09:13:09,169:INFO:         yellowbrick: 1.5
2026-02-02 09:13:09,169:INFO:              plotly: 5.24.1
2026-02-02 09:13:09,169:INFO:    plotly-resampler: Not installed
2026-02-02 09:13:09,169:INFO:             kaleido: 1.2.0
2026-02-02 09:13:09,169:INFO:           schemdraw: 0.15
2026-02-02 09:13:09,169:INFO:         statsmodels: 0.14.6
2026-02-02 09:13:09,169:INFO:              sktime: 0.26.0
2026-02-02 09:13:09,169:INFO:               tbats: 1.1.3
2026-02-02 09:13:09,169:INFO:            pmdarima: 2.0.4
2026-02-02 09:13:09,169:INFO:              psutil: 7.2.1
2026-02-02 09:13:09,169:INFO:          markupsafe: 3.0.3
2026-02-02 09:13:09,169:INFO:             pickle5: Not installed
2026-02-02 09:13:09,169:INFO:         cloudpickle: 3.0.0
2026-02-02 09:13:09,169:INFO:         deprecation: 2.1.0
2026-02-02 09:13:09,169:INFO:              xxhash: 3.6.0
2026-02-02 09:13:09,169:INFO:           wurlitzer: Not installed
2026-02-02 09:13:09,169:INFO:PyCaret optional dependencies:
2026-02-02 09:13:13,303:INFO:                shap: 0.44.1
2026-02-02 09:13:13,303:INFO:           interpret: 0.7.3
2026-02-02 09:13:13,303:INFO:                umap: 0.5.7
2026-02-02 09:13:13,303:INFO:     ydata_profiling: 4.18.1
2026-02-02 09:13:13,303:INFO:  explainerdashboard: 0.5.1
2026-02-02 09:13:13,303:INFO:             autoviz: Not installed
2026-02-02 09:13:13,303:INFO:           fairlearn: 0.7.0
2026-02-02 09:13:13,303:INFO:          deepchecks: Not installed
2026-02-02 09:13:13,303:INFO:             xgboost: Not installed
2026-02-02 09:13:13,303:INFO:            catboost: 1.2.8
2026-02-02 09:13:13,303:INFO:              kmodes: 0.12.2
2026-02-02 09:13:13,303:INFO:             mlxtend: 0.23.4
2026-02-02 09:13:13,303:INFO:       statsforecast: 1.5.0
2026-02-02 09:13:13,303:INFO:        tune_sklearn: Not installed
2026-02-02 09:13:13,303:INFO:                 ray: Not installed
2026-02-02 09:13:13,303:INFO:            hyperopt: 0.2.7
2026-02-02 09:13:13,303:INFO:              optuna: 4.6.0
2026-02-02 09:13:13,303:INFO:               skopt: 0.10.2
2026-02-02 09:13:13,303:INFO:              mlflow: 3.8.1
2026-02-02 09:13:13,303:INFO:              gradio: 6.3.0
2026-02-02 09:13:13,303:INFO:             fastapi: 0.128.0
2026-02-02 09:13:13,303:INFO:             uvicorn: 0.40.0
2026-02-02 09:13:13,303:INFO:              m2cgen: 0.10.0
2026-02-02 09:13:13,303:INFO:           evidently: 0.4.40
2026-02-02 09:13:13,303:INFO:               fugue: 0.8.7
2026-02-02 09:13:13,303:INFO:           streamlit: Not installed
2026-02-02 09:13:13,303:INFO:             prophet: Not installed
2026-02-02 09:13:13,303:INFO:None
2026-02-02 09:13:13,303:INFO:Set up data.
2026-02-02 09:13:13,386:INFO:Set up folding strategy.
2026-02-02 09:13:13,386:INFO:Set up train/test split.
2026-02-02 09:13:13,535:INFO:Set up index.
2026-02-02 09:13:13,552:INFO:Assigning column types.
2026-02-02 09:13:13,636:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 09:13:13,669:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 09:13:13,669:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 09:13:13,688:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:13:13,688:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:13:13,852:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 09:13:13,852:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 09:13:13,869:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:13:13,869:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:13:13,869:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 09:13:13,886:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 09:13:13,905:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:13:13,905:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:13:13,936:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 09:13:13,953:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:13:13,953:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:13:13,953:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 09:13:14,002:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:13:14,002:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:13:14,051:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:13:14,052:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:13:14,053:INFO:Preparing preprocessing pipeline...
2026-02-02 09:13:14,070:INFO:Set up simple imputation.
2026-02-02 09:13:14,070:INFO:Set up feature normalization.
2026-02-02 09:13:14,352:INFO:Finished creating preprocessing pipeline.
2026-02-02 09:13:14,352:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 09:13:14,352:INFO:Creating final display dataframe.
2026-02-02 09:13:14,886:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 21)
4        Transformed data shape      (373023, 21)
5   Transformed train set shape      (261116, 21)
6    Transformed test set shape      (111907, 21)
7              Numeric features                17
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              e742
2026-02-02 09:13:14,935:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:13:14,935:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:13:14,986:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:13:14,986:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:13:14,987:INFO:setup() successfully completed in 7.32s...............
2026-02-02 09:13:14,988:INFO:Initializing compare_models()
2026-02-02 09:13:14,988:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 09:13:14,988:INFO:Checking exceptions
2026-02-02 09:13:15,053:INFO:Preparing display monitor
2026-02-02 09:13:15,067:INFO:Initializing Logistic Regression
2026-02-02 09:13:15,067:INFO:Total runtime is 0.0 minutes
2026-02-02 09:13:15,067:INFO:SubProcess create_model() called ==================================
2026-02-02 09:13:15,067:INFO:Initializing create_model()
2026-02-02 09:13:15,067:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002286B2DA690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:13:15,067:INFO:Checking exceptions
2026-02-02 09:13:15,067:INFO:Importing libraries
2026-02-02 09:13:15,067:INFO:Copying training dataset
2026-02-02 09:13:15,185:INFO:Defining folds
2026-02-02 09:13:15,185:INFO:Declaring metric variables
2026-02-02 09:13:15,185:INFO:Importing untrained model
2026-02-02 09:13:15,185:INFO:Logistic Regression Imported successfully
2026-02-02 09:13:15,185:INFO:Starting cross validation
2026-02-02 09:13:15,185:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:13:24,933:INFO:Calculating mean and std
2026-02-02 09:13:24,936:INFO:Creating metrics dataframe
2026-02-02 09:13:24,936:INFO:Uploading results into container
2026-02-02 09:13:24,936:INFO:Uploading model into container now
2026-02-02 09:13:24,936:INFO:_master_model_container: 1
2026-02-02 09:13:24,936:INFO:_display_container: 2
2026-02-02 09:13:24,936:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 09:13:24,936:INFO:create_model() successfully completed......................................
2026-02-02 09:13:25,056:INFO:SubProcess create_model() end ==================================
2026-02-02 09:13:25,056:INFO:Creating metrics dataframe
2026-02-02 09:13:25,065:INFO:Initializing Decision Tree Classifier
2026-02-02 09:13:25,065:INFO:Total runtime is 0.16663879950841268 minutes
2026-02-02 09:13:25,065:INFO:SubProcess create_model() called ==================================
2026-02-02 09:13:25,065:INFO:Initializing create_model()
2026-02-02 09:13:25,065:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002286B2DA690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:13:25,065:INFO:Checking exceptions
2026-02-02 09:13:25,065:INFO:Importing libraries
2026-02-02 09:13:25,065:INFO:Copying training dataset
2026-02-02 09:13:25,186:INFO:Defining folds
2026-02-02 09:13:25,186:INFO:Declaring metric variables
2026-02-02 09:13:25,186:INFO:Importing untrained model
2026-02-02 09:13:25,186:INFO:Decision Tree Classifier Imported successfully
2026-02-02 09:13:25,186:INFO:Starting cross validation
2026-02-02 09:13:25,186:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:13:33,208:INFO:Calculating mean and std
2026-02-02 09:13:33,210:INFO:Creating metrics dataframe
2026-02-02 09:13:33,213:INFO:Uploading results into container
2026-02-02 09:13:33,214:INFO:Uploading model into container now
2026-02-02 09:13:33,215:INFO:_master_model_container: 2
2026-02-02 09:13:33,216:INFO:_display_container: 2
2026-02-02 09:13:33,216:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:13:33,217:INFO:create_model() successfully completed......................................
2026-02-02 09:13:33,353:INFO:SubProcess create_model() end ==================================
2026-02-02 09:13:33,354:INFO:Creating metrics dataframe
2026-02-02 09:13:33,356:INFO:Initializing Random Forest Classifier
2026-02-02 09:13:33,356:INFO:Total runtime is 0.30481323798497517 minutes
2026-02-02 09:13:33,356:INFO:SubProcess create_model() called ==================================
2026-02-02 09:13:33,356:INFO:Initializing create_model()
2026-02-02 09:13:33,357:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002286B2DA690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:13:33,357:INFO:Checking exceptions
2026-02-02 09:13:33,357:INFO:Importing libraries
2026-02-02 09:13:33,357:INFO:Copying training dataset
2026-02-02 09:13:33,494:INFO:Defining folds
2026-02-02 09:13:33,494:INFO:Declaring metric variables
2026-02-02 09:13:33,495:INFO:Importing untrained model
2026-02-02 09:13:33,495:INFO:Random Forest Classifier Imported successfully
2026-02-02 09:13:33,495:INFO:Starting cross validation
2026-02-02 09:13:33,496:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:13:53,087:INFO:Calculating mean and std
2026-02-02 09:13:53,087:INFO:Creating metrics dataframe
2026-02-02 09:13:53,087:INFO:Uploading results into container
2026-02-02 09:13:53,087:INFO:Uploading model into container now
2026-02-02 09:13:53,087:INFO:_master_model_container: 3
2026-02-02 09:13:53,087:INFO:_display_container: 2
2026-02-02 09:13:53,087:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:13:53,087:INFO:create_model() successfully completed......................................
2026-02-02 09:13:53,218:INFO:SubProcess create_model() end ==================================
2026-02-02 09:13:53,218:INFO:Creating metrics dataframe
2026-02-02 09:13:53,235:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 09:13:53,235:INFO:Total runtime is 0.6361277302106221 minutes
2026-02-02 09:13:53,235:INFO:SubProcess create_model() called ==================================
2026-02-02 09:13:53,235:INFO:Initializing create_model()
2026-02-02 09:13:53,235:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002286B2DA690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:13:53,235:INFO:Checking exceptions
2026-02-02 09:13:53,235:INFO:Importing libraries
2026-02-02 09:13:53,235:INFO:Copying training dataset
2026-02-02 09:13:53,361:INFO:Defining folds
2026-02-02 09:13:53,362:INFO:Declaring metric variables
2026-02-02 09:13:53,362:INFO:Importing untrained model
2026-02-02 09:13:53,363:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 09:13:53,363:INFO:Starting cross validation
2026-02-02 09:13:53,363:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:14:01,326:INFO:Calculating mean and std
2026-02-02 09:14:01,326:INFO:Creating metrics dataframe
2026-02-02 09:14:01,326:INFO:Uploading results into container
2026-02-02 09:14:01,326:INFO:Uploading model into container now
2026-02-02 09:14:01,326:INFO:_master_model_container: 4
2026-02-02 09:14:01,326:INFO:_display_container: 2
2026-02-02 09:14:01,326:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:14:01,326:INFO:create_model() successfully completed......................................
2026-02-02 09:14:01,481:INFO:SubProcess create_model() end ==================================
2026-02-02 09:14:01,481:INFO:Creating metrics dataframe
2026-02-02 09:14:01,488:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 09:14:01,488:INFO:Initializing create_model()
2026-02-02 09:14:01,488:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:14:01,488:INFO:Checking exceptions
2026-02-02 09:14:01,488:INFO:Importing libraries
2026-02-02 09:14:01,488:INFO:Copying training dataset
2026-02-02 09:14:01,638:INFO:Defining folds
2026-02-02 09:14:01,638:INFO:Declaring metric variables
2026-02-02 09:14:01,638:INFO:Importing untrained model
2026-02-02 09:14:01,638:INFO:Declaring custom model
2026-02-02 09:14:01,638:INFO:Random Forest Classifier Imported successfully
2026-02-02 09:14:01,638:INFO:Cross validation set to False
2026-02-02 09:14:01,638:INFO:Fitting Model
2026-02-02 09:14:09,063:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:14:09,063:INFO:create_model() successfully completed......................................
2026-02-02 09:14:09,224:INFO:Initializing create_model()
2026-02-02 09:14:09,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:14:09,224:INFO:Checking exceptions
2026-02-02 09:14:09,224:INFO:Importing libraries
2026-02-02 09:14:09,224:INFO:Copying training dataset
2026-02-02 09:14:09,369:INFO:Defining folds
2026-02-02 09:14:09,369:INFO:Declaring metric variables
2026-02-02 09:14:09,369:INFO:Importing untrained model
2026-02-02 09:14:09,369:INFO:Declaring custom model
2026-02-02 09:14:09,369:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 09:14:09,369:INFO:Cross validation set to False
2026-02-02 09:14:09,369:INFO:Fitting Model
2026-02-02 09:14:09,981:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 09:14:09,997:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008518 seconds.
2026-02-02 09:14:09,997:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 09:14:09,997:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 09:14:09,999:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 09:14:09,999:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 09:14:10,002:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 09:14:10,002:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 09:14:10,859:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:14:10,859:INFO:create_model() successfully completed......................................
2026-02-02 09:14:11,039:INFO:Initializing create_model()
2026-02-02 09:14:11,039:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:14:11,039:INFO:Checking exceptions
2026-02-02 09:14:11,039:INFO:Importing libraries
2026-02-02 09:14:11,039:INFO:Copying training dataset
2026-02-02 09:14:11,170:INFO:Defining folds
2026-02-02 09:14:11,170:INFO:Declaring metric variables
2026-02-02 09:14:11,170:INFO:Importing untrained model
2026-02-02 09:14:11,170:INFO:Declaring custom model
2026-02-02 09:14:11,170:INFO:Decision Tree Classifier Imported successfully
2026-02-02 09:14:11,185:INFO:Cross validation set to False
2026-02-02 09:14:11,185:INFO:Fitting Model
2026-02-02 09:14:13,071:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:14:13,071:INFO:create_model() successfully completed......................................
2026-02-02 09:14:13,185:INFO:_master_model_container: 4
2026-02-02 09:14:13,185:INFO:_display_container: 2
2026-02-02 09:14:13,185:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-02-02 09:14:13,185:INFO:compare_models() successfully completed......................................
2026-02-02 09:14:13,200:INFO:Initializing tune_model()
2026-02-02 09:14:13,200:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 09:14:13,200:INFO:Checking exceptions
2026-02-02 09:14:13,243:INFO:Copying training dataset
2026-02-02 09:14:13,306:INFO:Checking base model
2026-02-02 09:14:13,306:INFO:Base model : Random Forest Classifier
2026-02-02 09:14:13,306:INFO:Declaring metric variables
2026-02-02 09:14:13,306:INFO:Defining Hyperparameters
2026-02-02 09:14:13,433:INFO:Tuning with n_jobs=-1
2026-02-02 09:14:13,433:INFO:Initializing RandomizedSearchCV
2026-02-02 09:16:11,450:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 09:16:11,450:INFO:Hyperparameter search completed
2026-02-02 09:16:11,450:INFO:SubProcess create_model() called ==================================
2026-02-02 09:16:11,450:INFO:Initializing create_model()
2026-02-02 09:16:11,450:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022866B40850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-02 09:16:11,450:INFO:Checking exceptions
2026-02-02 09:16:11,450:INFO:Importing libraries
2026-02-02 09:16:11,450:INFO:Copying training dataset
2026-02-02 09:16:11,667:INFO:Defining folds
2026-02-02 09:16:11,667:INFO:Declaring metric variables
2026-02-02 09:16:11,667:INFO:Importing untrained model
2026-02-02 09:16:11,667:INFO:Declaring custom model
2026-02-02 09:16:11,667:INFO:Random Forest Classifier Imported successfully
2026-02-02 09:16:11,667:INFO:Starting cross validation
2026-02-02 09:16:11,667:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:16:34,013:INFO:Calculating mean and std
2026-02-02 09:16:34,013:INFO:Creating metrics dataframe
2026-02-02 09:16:34,016:INFO:Finalizing model
2026-02-02 09:16:45,275:INFO:Uploading results into container
2026-02-02 09:16:45,276:INFO:Uploading model into container now
2026-02-02 09:16:45,276:INFO:_master_model_container: 5
2026-02-02 09:16:45,276:INFO:_display_container: 3
2026-02-02 09:16:45,276:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:16:45,276:INFO:create_model() successfully completed......................................
2026-02-02 09:16:45,433:INFO:SubProcess create_model() end ==================================
2026-02-02 09:16:45,433:INFO:choose_better activated
2026-02-02 09:16:45,433:INFO:SubProcess create_model() called ==================================
2026-02-02 09:16:45,433:INFO:Initializing create_model()
2026-02-02 09:16:45,433:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:16:45,433:INFO:Checking exceptions
2026-02-02 09:16:45,433:INFO:Importing libraries
2026-02-02 09:16:45,433:INFO:Copying training dataset
2026-02-02 09:16:45,608:INFO:Defining folds
2026-02-02 09:16:45,608:INFO:Declaring metric variables
2026-02-02 09:16:45,608:INFO:Importing untrained model
2026-02-02 09:16:45,608:INFO:Declaring custom model
2026-02-02 09:16:45,608:INFO:Random Forest Classifier Imported successfully
2026-02-02 09:16:45,608:INFO:Starting cross validation
2026-02-02 09:16:45,608:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:17:00,939:INFO:Calculating mean and std
2026-02-02 09:17:00,939:INFO:Creating metrics dataframe
2026-02-02 09:17:00,941:INFO:Finalizing model
2026-02-02 09:17:07,852:INFO:Uploading results into container
2026-02-02 09:17:07,852:INFO:Uploading model into container now
2026-02-02 09:17:07,852:INFO:_master_model_container: 6
2026-02-02 09:17:07,852:INFO:_display_container: 4
2026-02-02 09:17:07,852:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:17:07,852:INFO:create_model() successfully completed......................................
2026-02-02 09:17:07,983:INFO:SubProcess create_model() end ==================================
2026-02-02 09:17:07,983:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9957
2026-02-02 09:17:07,983:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9656
2026-02-02 09:17:07,983:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 09:17:07,983:INFO:choose_better completed
2026-02-02 09:17:07,983:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 09:17:07,999:INFO:_master_model_container: 6
2026-02-02 09:17:07,999:INFO:_display_container: 3
2026-02-02 09:17:07,999:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:17:07,999:INFO:tune_model() successfully completed......................................
2026-02-02 09:17:08,116:INFO:Initializing tune_model()
2026-02-02 09:17:08,116:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 09:17:08,116:INFO:Checking exceptions
2026-02-02 09:17:08,165:INFO:Copying training dataset
2026-02-02 09:17:08,249:INFO:Checking base model
2026-02-02 09:17:08,249:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 09:17:08,249:INFO:Declaring metric variables
2026-02-02 09:17:08,249:INFO:Defining Hyperparameters
2026-02-02 09:17:08,366:INFO:Tuning with n_jobs=-1
2026-02-02 09:17:08,366:INFO:Initializing RandomizedSearchCV
2026-02-02 09:17:38,852:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 09:17:38,859:INFO:Hyperparameter search completed
2026-02-02 09:17:38,859:INFO:SubProcess create_model() called ==================================
2026-02-02 09:17:38,859:INFO:Initializing create_model()
2026-02-02 09:17:38,864:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022866C7EF10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 09:17:38,865:INFO:Checking exceptions
2026-02-02 09:17:38,865:INFO:Importing libraries
2026-02-02 09:17:38,865:INFO:Copying training dataset
2026-02-02 09:17:39,016:INFO:Defining folds
2026-02-02 09:17:39,016:INFO:Declaring metric variables
2026-02-02 09:17:39,016:INFO:Importing untrained model
2026-02-02 09:17:39,016:INFO:Declaring custom model
2026-02-02 09:17:39,022:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 09:17:39,022:INFO:Starting cross validation
2026-02-02 09:17:39,022:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:17:46,883:INFO:Calculating mean and std
2026-02-02 09:17:46,885:INFO:Creating metrics dataframe
2026-02-02 09:17:46,887:INFO:Finalizing model
2026-02-02 09:17:47,299:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 09:17:47,299:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 09:17:47,299:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 09:17:47,447:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 09:17:47,448:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 09:17:47,448:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 09:17:47,448:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 09:17:47,464:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006898 seconds.
2026-02-02 09:17:47,464:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 09:17:47,464:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 09:17:47,464:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 09:17:47,465:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 09:17:47,469:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 09:17:47,469:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 09:17:50,055:INFO:Uploading results into container
2026-02-02 09:17:50,055:INFO:Uploading model into container now
2026-02-02 09:17:50,057:INFO:_master_model_container: 7
2026-02-02 09:17:50,057:INFO:_display_container: 4
2026-02-02 09:17:50,059:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:17:50,059:INFO:create_model() successfully completed......................................
2026-02-02 09:17:50,283:INFO:SubProcess create_model() end ==================================
2026-02-02 09:17:50,283:INFO:choose_better activated
2026-02-02 09:17:50,283:INFO:SubProcess create_model() called ==================================
2026-02-02 09:17:50,283:INFO:Initializing create_model()
2026-02-02 09:17:50,283:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:17:50,283:INFO:Checking exceptions
2026-02-02 09:17:50,283:INFO:Importing libraries
2026-02-02 09:17:50,283:INFO:Copying training dataset
2026-02-02 09:17:50,521:INFO:Defining folds
2026-02-02 09:17:50,521:INFO:Declaring metric variables
2026-02-02 09:17:50,521:INFO:Importing untrained model
2026-02-02 09:17:50,521:INFO:Declaring custom model
2026-02-02 09:17:50,521:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 09:17:50,521:INFO:Starting cross validation
2026-02-02 09:17:50,521:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:17:54,639:INFO:Calculating mean and std
2026-02-02 09:17:54,639:INFO:Creating metrics dataframe
2026-02-02 09:17:54,639:INFO:Finalizing model
2026-02-02 09:17:55,177:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 09:17:55,192:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007614 seconds.
2026-02-02 09:17:55,194:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 09:17:55,194:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 09:17:55,194:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 09:17:55,194:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 09:17:55,196:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 09:17:55,197:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 09:17:56,029:INFO:Uploading results into container
2026-02-02 09:17:56,029:INFO:Uploading model into container now
2026-02-02 09:17:56,031:INFO:_master_model_container: 8
2026-02-02 09:17:56,031:INFO:_display_container: 5
2026-02-02 09:17:56,032:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:17:56,032:INFO:create_model() successfully completed......................................
2026-02-02 09:17:56,216:INFO:SubProcess create_model() end ==================================
2026-02-02 09:17:56,217:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9785
2026-02-02 09:17:56,218:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9938
2026-02-02 09:17:56,219:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 09:17:56,219:INFO:choose_better completed
2026-02-02 09:17:56,221:INFO:_master_model_container: 8
2026-02-02 09:17:56,221:INFO:_display_container: 4
2026-02-02 09:17:56,221:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:17:56,221:INFO:tune_model() successfully completed......................................
2026-02-02 09:17:56,349:INFO:Initializing tune_model()
2026-02-02 09:17:56,349:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 09:17:56,349:INFO:Checking exceptions
2026-02-02 09:17:56,405:INFO:Copying training dataset
2026-02-02 09:17:56,505:INFO:Checking base model
2026-02-02 09:17:56,505:INFO:Base model : Decision Tree Classifier
2026-02-02 09:17:56,505:INFO:Declaring metric variables
2026-02-02 09:17:56,505:INFO:Defining Hyperparameters
2026-02-02 09:17:56,646:INFO:Tuning with n_jobs=-1
2026-02-02 09:17:56,646:INFO:Initializing RandomizedSearchCV
2026-02-02 09:18:02,124:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-02-02 09:18:02,124:INFO:Hyperparameter search completed
2026-02-02 09:18:02,124:INFO:SubProcess create_model() called ==================================
2026-02-02 09:18:02,127:INFO:Initializing create_model()
2026-02-02 09:18:02,127:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022866DD5510>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-02-02 09:18:02,127:INFO:Checking exceptions
2026-02-02 09:18:02,127:INFO:Importing libraries
2026-02-02 09:18:02,127:INFO:Copying training dataset
2026-02-02 09:18:02,265:INFO:Defining folds
2026-02-02 09:18:02,265:INFO:Declaring metric variables
2026-02-02 09:18:02,265:INFO:Importing untrained model
2026-02-02 09:18:02,265:INFO:Declaring custom model
2026-02-02 09:18:02,265:INFO:Decision Tree Classifier Imported successfully
2026-02-02 09:18:02,265:INFO:Starting cross validation
2026-02-02 09:18:02,265:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:18:04,166:INFO:Calculating mean and std
2026-02-02 09:18:04,166:INFO:Creating metrics dataframe
2026-02-02 09:18:04,166:INFO:Finalizing model
2026-02-02 09:18:05,398:INFO:Uploading results into container
2026-02-02 09:18:05,398:INFO:Uploading model into container now
2026-02-02 09:18:05,398:INFO:_master_model_container: 9
2026-02-02 09:18:05,398:INFO:_display_container: 5
2026-02-02 09:18:05,398:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:18:05,398:INFO:create_model() successfully completed......................................
2026-02-02 09:18:05,519:INFO:SubProcess create_model() end ==================================
2026-02-02 09:18:05,519:INFO:choose_better activated
2026-02-02 09:18:05,519:INFO:SubProcess create_model() called ==================================
2026-02-02 09:18:05,519:INFO:Initializing create_model()
2026-02-02 09:18:05,519:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:18:05,519:INFO:Checking exceptions
2026-02-02 09:18:05,519:INFO:Importing libraries
2026-02-02 09:18:05,519:INFO:Copying training dataset
2026-02-02 09:18:05,635:INFO:Defining folds
2026-02-02 09:18:05,635:INFO:Declaring metric variables
2026-02-02 09:18:05,635:INFO:Importing untrained model
2026-02-02 09:18:05,635:INFO:Declaring custom model
2026-02-02 09:18:05,635:INFO:Decision Tree Classifier Imported successfully
2026-02-02 09:18:05,635:INFO:Starting cross validation
2026-02-02 09:18:05,635:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:18:08,430:INFO:Calculating mean and std
2026-02-02 09:18:08,430:INFO:Creating metrics dataframe
2026-02-02 09:18:08,434:INFO:Finalizing model
2026-02-02 09:18:10,431:INFO:Uploading results into container
2026-02-02 09:18:10,431:INFO:Uploading model into container now
2026-02-02 09:18:10,431:INFO:_master_model_container: 10
2026-02-02 09:18:10,431:INFO:_display_container: 6
2026-02-02 09:18:10,431:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:18:10,431:INFO:create_model() successfully completed......................................
2026-02-02 09:18:10,548:INFO:SubProcess create_model() end ==================================
2026-02-02 09:18:10,548:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9752
2026-02-02 09:18:10,548:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9572
2026-02-02 09:18:10,548:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 09:18:10,548:INFO:choose_better completed
2026-02-02 09:18:10,548:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 09:18:10,548:INFO:_master_model_container: 10
2026-02-02 09:18:10,548:INFO:_display_container: 5
2026-02-02 09:18:10,548:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:18:10,548:INFO:tune_model() successfully completed......................................
2026-02-02 09:18:10,682:INFO:Initializing predict_model()
2026-02-02 09:18:10,682:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228308A1620>)
2026-02-02 09:18:10,682:INFO:Checking exceptions
2026-02-02 09:18:10,682:INFO:Preloading libraries
2026-02-02 09:18:10,682:INFO:Set up data.
2026-02-02 09:18:10,698:INFO:Set up index.
2026-02-02 09:18:11,282:INFO:Initializing predict_model()
2026-02-02 09:18:11,282:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228308A1620>)
2026-02-02 09:18:11,282:INFO:Checking exceptions
2026-02-02 09:18:11,282:INFO:Preloading libraries
2026-02-02 09:18:11,282:INFO:Set up data.
2026-02-02 09:18:11,298:INFO:Set up index.
2026-02-02 09:18:12,015:INFO:Initializing predict_model()
2026-02-02 09:18:12,015:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228308A1620>)
2026-02-02 09:18:12,015:INFO:Checking exceptions
2026-02-02 09:18:12,015:INFO:Preloading libraries
2026-02-02 09:18:12,015:INFO:Set up data.
2026-02-02 09:18:12,032:INFO:Set up index.
2026-02-02 09:18:12,581:INFO:Initializing plot_model()
2026-02-02 09:18:12,581:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 09:18:12,581:INFO:Checking exceptions
2026-02-02 09:18:12,648:INFO:Preloading libraries
2026-02-02 09:18:12,748:INFO:Copying training dataset
2026-02-02 09:18:12,748:INFO:Plot type: feature
2026-02-02 09:18:12,748:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 09:18:13,031:INFO:Visual Rendered Successfully
2026-02-02 09:18:13,146:INFO:plot_model() successfully completed......................................
2026-02-02 09:18:13,167:INFO:Initializing plot_model()
2026-02-02 09:18:13,167:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022837EDEA10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 09:18:13,167:INFO:Checking exceptions
2026-02-02 09:18:13,256:INFO:Preloading libraries
2026-02-02 09:18:13,347:INFO:Copying training dataset
2026-02-02 09:18:13,347:INFO:Plot type: feature_all
2026-02-02 09:18:13,477:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 09:18:13,766:INFO:Visual Rendered Successfully
2026-02-02 09:18:13,889:INFO:plot_model() successfully completed......................................
2026-02-02 09:18:13,898:INFO:Initializing save_model()
2026-02-02 09:18:13,898:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 09:18:13,898:INFO:Adding model into prep_pipe
2026-02-02 09:20:09,798:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\3137887059.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 09:20:12,098:INFO:PyCaret ClassificationExperiment
2026-02-02 09:20:12,098:INFO:Logging name: clf-default-name
2026-02-02 09:20:12,098:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 09:20:12,098:INFO:version 3.3.2
2026-02-02 09:20:12,098:INFO:Initializing setup()
2026-02-02 09:20:12,098:INFO:self.USI: fbe9
2026-02-02 09:20:12,098:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 09:20:12,098:INFO:Checking environment
2026-02-02 09:20:12,098:INFO:python_version: 3.11.11
2026-02-02 09:20:12,098:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 09:20:12,098:INFO:machine: AMD64
2026-02-02 09:20:12,098:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 09:20:12,098:INFO:Memory: svmem(total=34009374720, available=13240107008, percent=61.1, used=20769267712, free=13240107008)
2026-02-02 09:20:12,098:INFO:Physical Core: 12
2026-02-02 09:20:12,098:INFO:Logical Core: 16
2026-02-02 09:20:12,098:INFO:Checking libraries
2026-02-02 09:20:12,098:INFO:System:
2026-02-02 09:20:12,098:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 09:20:12,098:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 09:20:12,098:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 09:20:12,098:INFO:PyCaret required dependencies:
2026-02-02 09:20:12,098:INFO:                 pip: 25.0
2026-02-02 09:20:12,098:INFO:          setuptools: 75.8.0
2026-02-02 09:20:12,098:INFO:             pycaret: 3.3.2
2026-02-02 09:20:12,098:INFO:             IPython: 9.9.0
2026-02-02 09:20:12,098:INFO:          ipywidgets: 8.1.8
2026-02-02 09:20:12,098:INFO:                tqdm: 4.67.1
2026-02-02 09:20:12,098:INFO:               numpy: 1.26.4
2026-02-02 09:20:12,098:INFO:              pandas: 2.1.4
2026-02-02 09:20:12,098:INFO:              jinja2: 3.1.6
2026-02-02 09:20:12,098:INFO:               scipy: 1.11.4
2026-02-02 09:20:12,098:INFO:              joblib: 1.3.2
2026-02-02 09:20:12,098:INFO:             sklearn: 1.4.2
2026-02-02 09:20:12,098:INFO:                pyod: 2.0.6
2026-02-02 09:20:12,098:INFO:            imblearn: 0.14.1
2026-02-02 09:20:12,098:INFO:   category_encoders: 2.7.0
2026-02-02 09:20:12,098:INFO:            lightgbm: 4.6.0
2026-02-02 09:20:12,098:INFO:               numba: 0.62.1
2026-02-02 09:20:12,098:INFO:            requests: 2.32.3
2026-02-02 09:20:12,098:INFO:          matplotlib: 3.7.5
2026-02-02 09:20:12,098:INFO:          scikitplot: 0.3.7
2026-02-02 09:20:12,098:INFO:         yellowbrick: 1.5
2026-02-02 09:20:12,098:INFO:              plotly: 5.24.1
2026-02-02 09:20:12,098:INFO:    plotly-resampler: Not installed
2026-02-02 09:20:12,098:INFO:             kaleido: 1.2.0
2026-02-02 09:20:12,098:INFO:           schemdraw: 0.15
2026-02-02 09:20:12,098:INFO:         statsmodels: 0.14.6
2026-02-02 09:20:12,098:INFO:              sktime: 0.26.0
2026-02-02 09:20:12,098:INFO:               tbats: 1.1.3
2026-02-02 09:20:12,098:INFO:            pmdarima: 2.0.4
2026-02-02 09:20:12,098:INFO:              psutil: 7.2.1
2026-02-02 09:20:12,098:INFO:          markupsafe: 3.0.3
2026-02-02 09:20:12,098:INFO:             pickle5: Not installed
2026-02-02 09:20:12,098:INFO:         cloudpickle: 3.0.0
2026-02-02 09:20:12,098:INFO:         deprecation: 2.1.0
2026-02-02 09:20:12,098:INFO:              xxhash: 3.6.0
2026-02-02 09:20:12,098:INFO:           wurlitzer: Not installed
2026-02-02 09:20:12,098:INFO:PyCaret optional dependencies:
2026-02-02 09:20:12,098:INFO:                shap: 0.44.1
2026-02-02 09:20:12,098:INFO:           interpret: 0.7.3
2026-02-02 09:20:12,098:INFO:                umap: 0.5.7
2026-02-02 09:20:12,098:INFO:     ydata_profiling: 4.18.1
2026-02-02 09:20:12,098:INFO:  explainerdashboard: 0.5.1
2026-02-02 09:20:12,098:INFO:             autoviz: Not installed
2026-02-02 09:20:12,098:INFO:           fairlearn: 0.7.0
2026-02-02 09:20:12,098:INFO:          deepchecks: Not installed
2026-02-02 09:20:12,098:INFO:             xgboost: Not installed
2026-02-02 09:20:12,098:INFO:            catboost: 1.2.8
2026-02-02 09:20:12,098:INFO:              kmodes: 0.12.2
2026-02-02 09:20:12,098:INFO:             mlxtend: 0.23.4
2026-02-02 09:20:12,098:INFO:       statsforecast: 1.5.0
2026-02-02 09:20:12,098:INFO:        tune_sklearn: Not installed
2026-02-02 09:20:12,098:INFO:                 ray: Not installed
2026-02-02 09:20:12,098:INFO:            hyperopt: 0.2.7
2026-02-02 09:20:12,098:INFO:              optuna: 4.6.0
2026-02-02 09:20:12,098:INFO:               skopt: 0.10.2
2026-02-02 09:20:12,098:INFO:              mlflow: 3.8.1
2026-02-02 09:20:12,098:INFO:              gradio: 6.3.0
2026-02-02 09:20:12,098:INFO:             fastapi: 0.128.0
2026-02-02 09:20:12,098:INFO:             uvicorn: 0.40.0
2026-02-02 09:20:12,098:INFO:              m2cgen: 0.10.0
2026-02-02 09:20:12,098:INFO:           evidently: 0.4.40
2026-02-02 09:20:12,098:INFO:               fugue: 0.8.7
2026-02-02 09:20:12,098:INFO:           streamlit: Not installed
2026-02-02 09:20:12,098:INFO:             prophet: Not installed
2026-02-02 09:20:12,112:INFO:None
2026-02-02 09:20:12,112:INFO:Set up data.
2026-02-02 09:20:12,198:INFO:Set up folding strategy.
2026-02-02 09:20:12,198:INFO:Set up train/test split.
2026-02-02 09:20:12,346:INFO:Set up index.
2026-02-02 09:20:12,346:INFO:Assigning column types.
2026-02-02 09:20:12,448:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 09:20:12,478:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 09:20:12,478:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 09:20:12,498:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:20:12,498:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:20:12,533:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 09:20:12,533:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 09:20:12,552:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:20:12,552:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:20:12,552:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 09:20:12,580:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 09:20:12,600:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:20:12,601:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:20:12,629:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 09:20:12,647:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:20:12,647:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:20:12,648:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 09:20:12,686:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:20:12,686:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:20:12,733:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:20:12,733:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:20:12,733:INFO:Preparing preprocessing pipeline...
2026-02-02 09:20:12,746:INFO:Set up simple imputation.
2026-02-02 09:20:12,746:INFO:Set up feature normalization.
2026-02-02 09:20:12,982:INFO:Finished creating preprocessing pipeline.
2026-02-02 09:20:12,982:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 09:20:12,982:INFO:Creating final display dataframe.
2026-02-02 09:20:13,432:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 21)
4        Transformed data shape      (373023, 21)
5   Transformed train set shape      (261116, 21)
6    Transformed test set shape      (111907, 21)
7              Numeric features                17
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              fbe9
2026-02-02 09:20:13,497:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:20:13,497:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:20:13,582:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 09:20:13,582:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 09:20:13,584:INFO:setup() successfully completed in 1.5s...............
2026-02-02 09:20:13,584:INFO:Initializing compare_models()
2026-02-02 09:20:13,584:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 09:20:13,584:INFO:Checking exceptions
2026-02-02 09:20:13,687:INFO:Preparing display monitor
2026-02-02 09:20:13,687:INFO:Initializing Logistic Regression
2026-02-02 09:20:13,687:INFO:Total runtime is 0.0 minutes
2026-02-02 09:20:13,687:INFO:SubProcess create_model() called ==================================
2026-02-02 09:20:13,687:INFO:Initializing create_model()
2026-02-02 09:20:13,687:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002283A8E2450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:20:13,687:INFO:Checking exceptions
2026-02-02 09:20:13,687:INFO:Importing libraries
2026-02-02 09:20:13,687:INFO:Copying training dataset
2026-02-02 09:20:13,853:INFO:Defining folds
2026-02-02 09:20:13,853:INFO:Declaring metric variables
2026-02-02 09:20:13,853:INFO:Importing untrained model
2026-02-02 09:20:13,863:INFO:Logistic Regression Imported successfully
2026-02-02 09:20:13,863:INFO:Starting cross validation
2026-02-02 09:20:13,863:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:20:16,147:INFO:Calculating mean and std
2026-02-02 09:20:16,147:INFO:Creating metrics dataframe
2026-02-02 09:20:16,147:INFO:Uploading results into container
2026-02-02 09:20:16,147:INFO:Uploading model into container now
2026-02-02 09:20:16,147:INFO:_master_model_container: 1
2026-02-02 09:20:16,147:INFO:_display_container: 2
2026-02-02 09:20:16,147:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 09:20:16,147:INFO:create_model() successfully completed......................................
2026-02-02 09:20:16,296:INFO:SubProcess create_model() end ==================================
2026-02-02 09:20:16,296:INFO:Creating metrics dataframe
2026-02-02 09:20:16,296:INFO:Initializing Decision Tree Classifier
2026-02-02 09:20:16,296:INFO:Total runtime is 0.0434844175974528 minutes
2026-02-02 09:20:16,296:INFO:SubProcess create_model() called ==================================
2026-02-02 09:20:16,296:INFO:Initializing create_model()
2026-02-02 09:20:16,296:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002283A8E2450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:20:16,296:INFO:Checking exceptions
2026-02-02 09:20:16,296:INFO:Importing libraries
2026-02-02 09:20:16,296:INFO:Copying training dataset
2026-02-02 09:20:16,413:INFO:Defining folds
2026-02-02 09:20:16,413:INFO:Declaring metric variables
2026-02-02 09:20:16,413:INFO:Importing untrained model
2026-02-02 09:20:16,413:INFO:Decision Tree Classifier Imported successfully
2026-02-02 09:20:16,413:INFO:Starting cross validation
2026-02-02 09:20:16,413:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:20:19,688:INFO:Calculating mean and std
2026-02-02 09:20:19,691:INFO:Creating metrics dataframe
2026-02-02 09:20:19,693:INFO:Uploading results into container
2026-02-02 09:20:19,694:INFO:Uploading model into container now
2026-02-02 09:20:19,694:INFO:_master_model_container: 2
2026-02-02 09:20:19,694:INFO:_display_container: 2
2026-02-02 09:20:19,694:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:20:19,694:INFO:create_model() successfully completed......................................
2026-02-02 09:20:19,827:INFO:SubProcess create_model() end ==================================
2026-02-02 09:20:19,827:INFO:Creating metrics dataframe
2026-02-02 09:20:19,829:INFO:Initializing Random Forest Classifier
2026-02-02 09:20:19,829:INFO:Total runtime is 0.1023644208908081 minutes
2026-02-02 09:20:19,831:INFO:SubProcess create_model() called ==================================
2026-02-02 09:20:19,831:INFO:Initializing create_model()
2026-02-02 09:20:19,831:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002283A8E2450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:20:19,831:INFO:Checking exceptions
2026-02-02 09:20:19,831:INFO:Importing libraries
2026-02-02 09:20:19,831:INFO:Copying training dataset
2026-02-02 09:20:19,947:INFO:Defining folds
2026-02-02 09:20:19,947:INFO:Declaring metric variables
2026-02-02 09:20:19,947:INFO:Importing untrained model
2026-02-02 09:20:19,948:INFO:Random Forest Classifier Imported successfully
2026-02-02 09:20:19,948:INFO:Starting cross validation
2026-02-02 09:20:19,949:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:20:33,152:INFO:Calculating mean and std
2026-02-02 09:20:33,154:INFO:Creating metrics dataframe
2026-02-02 09:20:33,157:INFO:Uploading results into container
2026-02-02 09:20:33,157:INFO:Uploading model into container now
2026-02-02 09:20:33,158:INFO:_master_model_container: 3
2026-02-02 09:20:33,158:INFO:_display_container: 2
2026-02-02 09:20:33,158:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:20:33,158:INFO:create_model() successfully completed......................................
2026-02-02 09:20:33,295:INFO:SubProcess create_model() end ==================================
2026-02-02 09:20:33,296:INFO:Creating metrics dataframe
2026-02-02 09:20:33,297:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 09:20:33,297:INFO:Total runtime is 0.3268291195233663 minutes
2026-02-02 09:20:33,297:INFO:SubProcess create_model() called ==================================
2026-02-02 09:20:33,297:INFO:Initializing create_model()
2026-02-02 09:20:33,297:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002283A8E2450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:20:33,297:INFO:Checking exceptions
2026-02-02 09:20:33,297:INFO:Importing libraries
2026-02-02 09:20:33,298:INFO:Copying training dataset
2026-02-02 09:20:33,402:INFO:Defining folds
2026-02-02 09:20:33,404:INFO:Declaring metric variables
2026-02-02 09:20:33,404:INFO:Importing untrained model
2026-02-02 09:20:33,405:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 09:20:33,405:INFO:Starting cross validation
2026-02-02 09:20:33,406:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:20:36,379:INFO:Calculating mean and std
2026-02-02 09:20:36,382:INFO:Creating metrics dataframe
2026-02-02 09:20:36,385:INFO:Uploading results into container
2026-02-02 09:20:36,386:INFO:Uploading model into container now
2026-02-02 09:20:36,387:INFO:_master_model_container: 4
2026-02-02 09:20:36,387:INFO:_display_container: 2
2026-02-02 09:20:36,388:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:20:36,388:INFO:create_model() successfully completed......................................
2026-02-02 09:20:36,556:INFO:SubProcess create_model() end ==================================
2026-02-02 09:20:36,557:INFO:Creating metrics dataframe
2026-02-02 09:20:36,559:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 09:20:36,560:INFO:Initializing create_model()
2026-02-02 09:20:36,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:20:36,560:INFO:Checking exceptions
2026-02-02 09:20:36,560:INFO:Importing libraries
2026-02-02 09:20:36,561:INFO:Copying training dataset
2026-02-02 09:20:36,675:INFO:Defining folds
2026-02-02 09:20:36,676:INFO:Declaring metric variables
2026-02-02 09:20:36,676:INFO:Importing untrained model
2026-02-02 09:20:36,676:INFO:Declaring custom model
2026-02-02 09:20:36,676:INFO:Random Forest Classifier Imported successfully
2026-02-02 09:20:36,677:INFO:Cross validation set to False
2026-02-02 09:20:36,677:INFO:Fitting Model
2026-02-02 09:20:41,894:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:20:41,895:INFO:create_model() successfully completed......................................
2026-02-02 09:20:42,028:INFO:Initializing create_model()
2026-02-02 09:20:42,028:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:20:42,028:INFO:Checking exceptions
2026-02-02 09:20:42,029:INFO:Importing libraries
2026-02-02 09:20:42,030:INFO:Copying training dataset
2026-02-02 09:20:42,136:INFO:Defining folds
2026-02-02 09:20:42,136:INFO:Declaring metric variables
2026-02-02 09:20:42,136:INFO:Importing untrained model
2026-02-02 09:20:42,136:INFO:Declaring custom model
2026-02-02 09:20:42,138:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 09:20:42,139:INFO:Cross validation set to False
2026-02-02 09:20:42,139:INFO:Fitting Model
2026-02-02 09:20:42,700:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 09:20:42,714:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006828 seconds.
2026-02-02 09:20:42,714:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 09:20:42,714:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 09:20:42,714:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 09:20:42,715:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 09:20:42,716:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 09:20:42,717:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 09:20:43,207:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:20:43,207:INFO:create_model() successfully completed......................................
2026-02-02 09:20:43,385:INFO:Initializing create_model()
2026-02-02 09:20:43,385:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:20:43,385:INFO:Checking exceptions
2026-02-02 09:20:43,386:INFO:Importing libraries
2026-02-02 09:20:43,386:INFO:Copying training dataset
2026-02-02 09:20:43,500:INFO:Defining folds
2026-02-02 09:20:43,501:INFO:Declaring metric variables
2026-02-02 09:20:43,501:INFO:Importing untrained model
2026-02-02 09:20:43,501:INFO:Declaring custom model
2026-02-02 09:20:43,501:INFO:Decision Tree Classifier Imported successfully
2026-02-02 09:20:43,502:INFO:Cross validation set to False
2026-02-02 09:20:43,502:INFO:Fitting Model
2026-02-02 09:20:45,347:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:20:45,349:INFO:create_model() successfully completed......................................
2026-02-02 09:20:45,511:INFO:_master_model_container: 4
2026-02-02 09:20:45,512:INFO:_display_container: 2
2026-02-02 09:20:45,513:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-02-02 09:20:45,513:INFO:compare_models() successfully completed......................................
2026-02-02 09:20:45,526:INFO:Initializing tune_model()
2026-02-02 09:20:45,526:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 09:20:45,527:INFO:Checking exceptions
2026-02-02 09:20:45,572:INFO:Copying training dataset
2026-02-02 09:20:45,647:INFO:Checking base model
2026-02-02 09:20:45,647:INFO:Base model : Random Forest Classifier
2026-02-02 09:20:45,648:INFO:Declaring metric variables
2026-02-02 09:20:45,648:INFO:Defining Hyperparameters
2026-02-02 09:20:45,801:INFO:Tuning with n_jobs=-1
2026-02-02 09:20:45,801:INFO:Initializing RandomizedSearchCV
2026-02-02 09:22:11,892:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 09:22:11,894:INFO:Hyperparameter search completed
2026-02-02 09:22:11,895:INFO:SubProcess create_model() called ==================================
2026-02-02 09:22:11,896:INFO:Initializing create_model()
2026-02-02 09:22:11,896:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022870726210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-02 09:22:11,896:INFO:Checking exceptions
2026-02-02 09:22:11,896:INFO:Importing libraries
2026-02-02 09:22:11,896:INFO:Copying training dataset
2026-02-02 09:22:12,057:INFO:Defining folds
2026-02-02 09:22:12,058:INFO:Declaring metric variables
2026-02-02 09:22:12,058:INFO:Importing untrained model
2026-02-02 09:22:12,058:INFO:Declaring custom model
2026-02-02 09:22:12,059:INFO:Random Forest Classifier Imported successfully
2026-02-02 09:22:12,060:INFO:Starting cross validation
2026-02-02 09:22:12,061:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:22:30,492:INFO:Calculating mean and std
2026-02-02 09:22:30,495:INFO:Creating metrics dataframe
2026-02-02 09:22:30,498:INFO:Finalizing model
2026-02-02 09:22:39,769:INFO:Uploading results into container
2026-02-02 09:22:39,769:INFO:Uploading model into container now
2026-02-02 09:22:39,769:INFO:_master_model_container: 5
2026-02-02 09:22:39,769:INFO:_display_container: 3
2026-02-02 09:22:39,769:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:22:39,769:INFO:create_model() successfully completed......................................
2026-02-02 09:22:39,984:INFO:SubProcess create_model() end ==================================
2026-02-02 09:22:39,985:INFO:choose_better activated
2026-02-02 09:22:39,986:INFO:SubProcess create_model() called ==================================
2026-02-02 09:22:39,986:INFO:Initializing create_model()
2026-02-02 09:22:39,986:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:22:39,986:INFO:Checking exceptions
2026-02-02 09:22:39,986:INFO:Importing libraries
2026-02-02 09:22:39,987:INFO:Copying training dataset
2026-02-02 09:22:40,135:INFO:Defining folds
2026-02-02 09:22:40,136:INFO:Declaring metric variables
2026-02-02 09:22:40,136:INFO:Importing untrained model
2026-02-02 09:22:40,136:INFO:Declaring custom model
2026-02-02 09:22:40,137:INFO:Random Forest Classifier Imported successfully
2026-02-02 09:22:40,137:INFO:Starting cross validation
2026-02-02 09:22:40,138:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:22:52,772:INFO:Calculating mean and std
2026-02-02 09:22:52,773:INFO:Creating metrics dataframe
2026-02-02 09:22:52,775:INFO:Finalizing model
2026-02-02 09:22:58,846:INFO:Uploading results into container
2026-02-02 09:22:58,853:INFO:Uploading model into container now
2026-02-02 09:22:58,854:INFO:_master_model_container: 6
2026-02-02 09:22:58,855:INFO:_display_container: 4
2026-02-02 09:22:58,855:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:22:58,856:INFO:create_model() successfully completed......................................
2026-02-02 09:22:59,015:INFO:SubProcess create_model() end ==================================
2026-02-02 09:22:59,016:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9957
2026-02-02 09:22:59,017:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9656
2026-02-02 09:22:59,018:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 09:22:59,018:INFO:choose_better completed
2026-02-02 09:22:59,018:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 09:22:59,021:INFO:_master_model_container: 6
2026-02-02 09:22:59,021:INFO:_display_container: 3
2026-02-02 09:22:59,021:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 09:22:59,021:INFO:tune_model() successfully completed......................................
2026-02-02 09:22:59,201:INFO:Initializing tune_model()
2026-02-02 09:22:59,201:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 09:22:59,201:INFO:Checking exceptions
2026-02-02 09:22:59,271:INFO:Copying training dataset
2026-02-02 09:22:59,373:INFO:Checking base model
2026-02-02 09:22:59,373:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 09:22:59,373:INFO:Declaring metric variables
2026-02-02 09:22:59,375:INFO:Defining Hyperparameters
2026-02-02 09:22:59,516:INFO:Tuning with n_jobs=-1
2026-02-02 09:22:59,517:INFO:Initializing RandomizedSearchCV
2026-02-02 09:23:26,358:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 09:23:26,359:INFO:Hyperparameter search completed
2026-02-02 09:23:26,359:INFO:SubProcess create_model() called ==================================
2026-02-02 09:23:26,360:INFO:Initializing create_model()
2026-02-02 09:23:26,360:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022871D6CA90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 09:23:26,360:INFO:Checking exceptions
2026-02-02 09:23:26,360:INFO:Importing libraries
2026-02-02 09:23:26,360:INFO:Copying training dataset
2026-02-02 09:23:26,499:INFO:Defining folds
2026-02-02 09:23:26,499:INFO:Declaring metric variables
2026-02-02 09:23:26,500:INFO:Importing untrained model
2026-02-02 09:23:26,500:INFO:Declaring custom model
2026-02-02 09:23:26,501:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 09:23:26,502:INFO:Starting cross validation
2026-02-02 09:23:26,502:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:23:33,359:INFO:Calculating mean and std
2026-02-02 09:23:33,361:INFO:Creating metrics dataframe
2026-02-02 09:23:33,365:INFO:Finalizing model
2026-02-02 09:23:33,814:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 09:23:33,814:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 09:23:33,815:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 09:23:33,968:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 09:23:33,968:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 09:23:33,968:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 09:23:33,968:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 09:23:33,985:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007532 seconds.
2026-02-02 09:23:33,987:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 09:23:33,987:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 09:23:33,987:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 09:23:33,987:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 09:23:33,992:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 09:23:33,992:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 09:23:36,149:INFO:Uploading results into container
2026-02-02 09:23:36,151:INFO:Uploading model into container now
2026-02-02 09:23:36,151:INFO:_master_model_container: 7
2026-02-02 09:23:36,152:INFO:_display_container: 4
2026-02-02 09:23:36,153:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:23:36,153:INFO:create_model() successfully completed......................................
2026-02-02 09:23:36,334:INFO:SubProcess create_model() end ==================================
2026-02-02 09:23:36,334:INFO:choose_better activated
2026-02-02 09:23:36,334:INFO:SubProcess create_model() called ==================================
2026-02-02 09:23:36,335:INFO:Initializing create_model()
2026-02-02 09:23:36,335:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:23:36,335:INFO:Checking exceptions
2026-02-02 09:23:36,335:INFO:Importing libraries
2026-02-02 09:23:36,336:INFO:Copying training dataset
2026-02-02 09:23:36,469:INFO:Defining folds
2026-02-02 09:23:36,469:INFO:Declaring metric variables
2026-02-02 09:23:36,470:INFO:Importing untrained model
2026-02-02 09:23:36,470:INFO:Declaring custom model
2026-02-02 09:23:36,471:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 09:23:36,471:INFO:Starting cross validation
2026-02-02 09:23:36,471:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:23:40,125:INFO:Calculating mean and std
2026-02-02 09:23:40,127:INFO:Creating metrics dataframe
2026-02-02 09:23:40,129:INFO:Finalizing model
2026-02-02 09:23:40,712:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 09:23:40,728:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009207 seconds.
2026-02-02 09:23:40,729:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 09:23:40,729:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 09:23:40,729:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 09:23:40,730:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 09:23:40,732:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 09:23:40,732:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 09:23:41,382:INFO:Uploading results into container
2026-02-02 09:23:41,383:INFO:Uploading model into container now
2026-02-02 09:23:41,383:INFO:_master_model_container: 8
2026-02-02 09:23:41,383:INFO:_display_container: 5
2026-02-02 09:23:41,384:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:23:41,384:INFO:create_model() successfully completed......................................
2026-02-02 09:23:41,564:INFO:SubProcess create_model() end ==================================
2026-02-02 09:23:41,565:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9785
2026-02-02 09:23:41,565:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9938
2026-02-02 09:23:41,566:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 09:23:41,566:INFO:choose_better completed
2026-02-02 09:23:41,568:INFO:_master_model_container: 8
2026-02-02 09:23:41,569:INFO:_display_container: 4
2026-02-02 09:23:41,569:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 09:23:41,569:INFO:tune_model() successfully completed......................................
2026-02-02 09:23:41,727:INFO:Initializing tune_model()
2026-02-02 09:23:41,727:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 09:23:41,727:INFO:Checking exceptions
2026-02-02 09:23:41,782:INFO:Copying training dataset
2026-02-02 09:23:41,877:INFO:Checking base model
2026-02-02 09:23:41,877:INFO:Base model : Decision Tree Classifier
2026-02-02 09:23:41,877:INFO:Declaring metric variables
2026-02-02 09:23:41,877:INFO:Defining Hyperparameters
2026-02-02 09:23:42,012:INFO:Tuning with n_jobs=-1
2026-02-02 09:23:42,012:INFO:Initializing RandomizedSearchCV
2026-02-02 09:23:47,300:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-02-02 09:23:47,302:INFO:Hyperparameter search completed
2026-02-02 09:23:47,302:INFO:SubProcess create_model() called ==================================
2026-02-02 09:23:47,303:INFO:Initializing create_model()
2026-02-02 09:23:47,303:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022833980F90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-02-02 09:23:47,303:INFO:Checking exceptions
2026-02-02 09:23:47,303:INFO:Importing libraries
2026-02-02 09:23:47,303:INFO:Copying training dataset
2026-02-02 09:23:47,454:INFO:Defining folds
2026-02-02 09:23:47,455:INFO:Declaring metric variables
2026-02-02 09:23:47,455:INFO:Importing untrained model
2026-02-02 09:23:47,455:INFO:Declaring custom model
2026-02-02 09:23:47,456:INFO:Decision Tree Classifier Imported successfully
2026-02-02 09:23:47,457:INFO:Starting cross validation
2026-02-02 09:23:47,457:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:23:49,663:INFO:Calculating mean and std
2026-02-02 09:23:49,665:INFO:Creating metrics dataframe
2026-02-02 09:23:49,666:INFO:Finalizing model
2026-02-02 09:23:51,131:INFO:Uploading results into container
2026-02-02 09:23:51,132:INFO:Uploading model into container now
2026-02-02 09:23:51,133:INFO:_master_model_container: 9
2026-02-02 09:23:51,134:INFO:_display_container: 5
2026-02-02 09:23:51,134:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:23:51,134:INFO:create_model() successfully completed......................................
2026-02-02 09:23:51,277:INFO:SubProcess create_model() end ==================================
2026-02-02 09:23:51,277:INFO:choose_better activated
2026-02-02 09:23:51,277:INFO:SubProcess create_model() called ==================================
2026-02-02 09:23:51,277:INFO:Initializing create_model()
2026-02-02 09:23:51,277:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 09:23:51,277:INFO:Checking exceptions
2026-02-02 09:23:51,278:INFO:Importing libraries
2026-02-02 09:23:51,278:INFO:Copying training dataset
2026-02-02 09:23:51,412:INFO:Defining folds
2026-02-02 09:23:51,412:INFO:Declaring metric variables
2026-02-02 09:23:51,412:INFO:Importing untrained model
2026-02-02 09:23:51,412:INFO:Declaring custom model
2026-02-02 09:23:51,412:INFO:Decision Tree Classifier Imported successfully
2026-02-02 09:23:51,412:INFO:Starting cross validation
2026-02-02 09:23:51,413:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 09:23:54,457:INFO:Calculating mean and std
2026-02-02 09:23:54,458:INFO:Creating metrics dataframe
2026-02-02 09:23:54,461:INFO:Finalizing model
2026-02-02 09:23:56,630:INFO:Uploading results into container
2026-02-02 09:23:56,631:INFO:Uploading model into container now
2026-02-02 09:23:56,631:INFO:_master_model_container: 10
2026-02-02 09:23:56,631:INFO:_display_container: 6
2026-02-02 09:23:56,631:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:23:56,631:INFO:create_model() successfully completed......................................
2026-02-02 09:23:56,764:INFO:SubProcess create_model() end ==================================
2026-02-02 09:23:56,764:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9752
2026-02-02 09:23:56,765:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9572
2026-02-02 09:23:56,765:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 09:23:56,765:INFO:choose_better completed
2026-02-02 09:23:56,765:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 09:23:56,767:INFO:_master_model_container: 10
2026-02-02 09:23:56,767:INFO:_display_container: 5
2026-02-02 09:23:56,768:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 09:23:56,768:INFO:tune_model() successfully completed......................................
2026-02-02 09:23:56,916:INFO:Initializing predict_model()
2026-02-02 09:23:56,916:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022866C43D80>)
2026-02-02 09:23:56,916:INFO:Checking exceptions
2026-02-02 09:23:56,916:INFO:Preloading libraries
2026-02-02 09:23:56,916:INFO:Set up data.
2026-02-02 09:23:56,932:INFO:Set up index.
2026-02-02 09:23:57,538:INFO:Initializing predict_model()
2026-02-02 09:23:57,538:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022866C43D80>)
2026-02-02 09:23:57,538:INFO:Checking exceptions
2026-02-02 09:23:57,538:INFO:Preloading libraries
2026-02-02 09:23:57,539:INFO:Set up data.
2026-02-02 09:23:57,552:INFO:Set up index.
2026-02-02 09:23:58,203:INFO:Initializing predict_model()
2026-02-02 09:23:58,204:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022866C43D80>)
2026-02-02 09:23:58,204:INFO:Checking exceptions
2026-02-02 09:23:58,204:INFO:Preloading libraries
2026-02-02 09:23:58,204:INFO:Set up data.
2026-02-02 09:23:58,219:INFO:Set up index.
2026-02-02 09:23:58,489:INFO:Initializing predict_model()
2026-02-02 09:23:58,489:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228387B1D00>)
2026-02-02 09:23:58,489:INFO:Checking exceptions
2026-02-02 09:23:58,489:INFO:Preloading libraries
2026-02-02 09:23:58,489:INFO:Set up data.
2026-02-02 09:23:58,502:INFO:Set up index.
2026-02-02 09:23:59,105:INFO:Initializing plot_model()
2026-02-02 09:23:59,105:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 09:23:59,105:INFO:Checking exceptions
2026-02-02 09:23:59,183:INFO:Preloading libraries
2026-02-02 09:23:59,285:INFO:Copying training dataset
2026-02-02 09:23:59,285:INFO:Plot type: feature
2026-02-02 09:23:59,286:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 09:23:59,597:INFO:Visual Rendered Successfully
2026-02-02 09:23:59,731:INFO:plot_model() successfully completed......................................
2026-02-02 09:23:59,749:INFO:Initializing plot_model()
2026-02-02 09:23:59,749:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866B40650>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 09:23:59,749:INFO:Checking exceptions
2026-02-02 09:23:59,839:INFO:Preloading libraries
2026-02-02 09:23:59,945:INFO:Copying training dataset
2026-02-02 09:23:59,945:INFO:Plot type: feature_all
2026-02-02 09:24:00,089:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 09:24:00,450:INFO:Visual Rendered Successfully
2026-02-02 09:24:00,588:INFO:plot_model() successfully completed......................................
2026-02-02 09:24:00,605:INFO:Initializing save_model()
2026-02-02 09:24:00,605:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 09:24:00,605:INFO:Adding model into prep_pipe
2026-02-02 09:24:00,747:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-02 09:24:00,751:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-02 09:24:00,751:INFO:save_model() successfully completed......................................
2026-02-02 10:43:52,236:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\1570328503.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 10:43:54,326:INFO:PyCaret ClassificationExperiment
2026-02-02 10:43:54,332:INFO:Logging name: clf-default-name
2026-02-02 10:43:54,332:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 10:43:54,332:INFO:version 3.3.2
2026-02-02 10:43:54,332:INFO:Initializing setup()
2026-02-02 10:43:54,332:INFO:self.USI: 28b2
2026-02-02 10:43:54,332:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 10:43:54,332:INFO:Checking environment
2026-02-02 10:43:54,332:INFO:python_version: 3.11.11
2026-02-02 10:43:54,332:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 10:43:54,332:INFO:machine: AMD64
2026-02-02 10:43:54,332:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 10:43:54,332:INFO:Memory: svmem(total=34009374720, available=15785521152, percent=53.6, used=18223853568, free=15785521152)
2026-02-02 10:43:54,332:INFO:Physical Core: 12
2026-02-02 10:43:54,332:INFO:Logical Core: 16
2026-02-02 10:43:54,332:INFO:Checking libraries
2026-02-02 10:43:54,332:INFO:System:
2026-02-02 10:43:54,332:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 10:43:54,332:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 10:43:54,332:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 10:43:54,332:INFO:PyCaret required dependencies:
2026-02-02 10:43:54,332:INFO:                 pip: 25.0
2026-02-02 10:43:54,332:INFO:          setuptools: 75.8.0
2026-02-02 10:43:54,341:INFO:             pycaret: 3.3.2
2026-02-02 10:43:54,341:INFO:             IPython: 9.9.0
2026-02-02 10:43:54,341:INFO:          ipywidgets: 8.1.8
2026-02-02 10:43:54,341:INFO:                tqdm: 4.67.1
2026-02-02 10:43:54,341:INFO:               numpy: 1.26.4
2026-02-02 10:43:54,341:INFO:              pandas: 2.1.4
2026-02-02 10:43:54,341:INFO:              jinja2: 3.1.6
2026-02-02 10:43:54,341:INFO:               scipy: 1.11.4
2026-02-02 10:43:54,341:INFO:              joblib: 1.3.2
2026-02-02 10:43:54,341:INFO:             sklearn: 1.4.2
2026-02-02 10:43:54,341:INFO:                pyod: 2.0.6
2026-02-02 10:43:54,341:INFO:            imblearn: 0.14.1
2026-02-02 10:43:54,341:INFO:   category_encoders: 2.7.0
2026-02-02 10:43:54,341:INFO:            lightgbm: 4.6.0
2026-02-02 10:43:54,341:INFO:               numba: 0.62.1
2026-02-02 10:43:54,341:INFO:            requests: 2.32.3
2026-02-02 10:43:54,341:INFO:          matplotlib: 3.7.5
2026-02-02 10:43:54,341:INFO:          scikitplot: 0.3.7
2026-02-02 10:43:54,341:INFO:         yellowbrick: 1.5
2026-02-02 10:43:54,341:INFO:              plotly: 5.24.1
2026-02-02 10:43:54,347:INFO:    plotly-resampler: Not installed
2026-02-02 10:43:54,347:INFO:             kaleido: 1.2.0
2026-02-02 10:43:54,347:INFO:           schemdraw: 0.15
2026-02-02 10:43:54,347:INFO:         statsmodels: 0.14.6
2026-02-02 10:43:54,347:INFO:              sktime: 0.26.0
2026-02-02 10:43:54,348:INFO:               tbats: 1.1.3
2026-02-02 10:43:54,348:INFO:            pmdarima: 2.0.4
2026-02-02 10:43:54,348:INFO:              psutil: 7.2.1
2026-02-02 10:43:54,348:INFO:          markupsafe: 3.0.3
2026-02-02 10:43:54,348:INFO:             pickle5: Not installed
2026-02-02 10:43:54,348:INFO:         cloudpickle: 3.0.0
2026-02-02 10:43:54,348:INFO:         deprecation: 2.1.0
2026-02-02 10:43:54,348:INFO:              xxhash: 3.6.0
2026-02-02 10:43:54,348:INFO:           wurlitzer: Not installed
2026-02-02 10:43:54,348:INFO:PyCaret optional dependencies:
2026-02-02 10:43:54,349:INFO:                shap: 0.44.1
2026-02-02 10:43:54,349:INFO:           interpret: 0.7.3
2026-02-02 10:43:54,349:INFO:                umap: 0.5.7
2026-02-02 10:43:54,349:INFO:     ydata_profiling: 4.18.1
2026-02-02 10:43:54,349:INFO:  explainerdashboard: 0.5.1
2026-02-02 10:43:54,349:INFO:             autoviz: Not installed
2026-02-02 10:43:54,349:INFO:           fairlearn: 0.7.0
2026-02-02 10:43:54,349:INFO:          deepchecks: Not installed
2026-02-02 10:43:54,349:INFO:             xgboost: Not installed
2026-02-02 10:43:54,349:INFO:            catboost: 1.2.8
2026-02-02 10:43:54,349:INFO:              kmodes: 0.12.2
2026-02-02 10:43:54,349:INFO:             mlxtend: 0.23.4
2026-02-02 10:43:54,349:INFO:       statsforecast: 1.5.0
2026-02-02 10:43:54,349:INFO:        tune_sklearn: Not installed
2026-02-02 10:43:54,349:INFO:                 ray: Not installed
2026-02-02 10:43:54,349:INFO:            hyperopt: 0.2.7
2026-02-02 10:43:54,349:INFO:              optuna: 4.6.0
2026-02-02 10:43:54,349:INFO:               skopt: 0.10.2
2026-02-02 10:43:54,349:INFO:              mlflow: 3.8.1
2026-02-02 10:43:54,349:INFO:              gradio: 6.3.0
2026-02-02 10:43:54,349:INFO:             fastapi: 0.128.0
2026-02-02 10:43:54,349:INFO:             uvicorn: 0.40.0
2026-02-02 10:43:54,349:INFO:              m2cgen: 0.10.0
2026-02-02 10:43:54,349:INFO:           evidently: 0.4.40
2026-02-02 10:43:54,349:INFO:               fugue: 0.8.7
2026-02-02 10:43:54,349:INFO:           streamlit: Not installed
2026-02-02 10:43:54,349:INFO:             prophet: Not installed
2026-02-02 10:43:54,349:INFO:None
2026-02-02 10:43:54,349:INFO:Set up data.
2026-02-02 10:43:54,434:INFO:Set up folding strategy.
2026-02-02 10:43:54,434:INFO:Set up train/test split.
2026-02-02 10:43:54,601:INFO:Set up index.
2026-02-02 10:43:54,607:INFO:Assigning column types.
2026-02-02 10:43:54,690:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 10:43:54,710:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 10:43:54,710:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 10:43:54,726:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 10:43:54,726:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 10:43:54,758:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 10:43:54,758:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 10:43:54,774:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 10:43:54,774:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 10:43:54,774:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 10:43:54,801:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 10:43:54,818:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 10:43:54,818:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 10:43:54,848:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 10:43:54,866:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 10:43:54,866:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 10:43:54,867:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 10:43:54,915:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 10:43:54,915:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 10:43:54,962:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 10:43:54,962:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 10:43:54,963:INFO:Preparing preprocessing pipeline...
2026-02-02 10:43:54,978:INFO:Set up simple imputation.
2026-02-02 10:43:54,978:INFO:Set up feature normalization.
2026-02-02 10:43:55,197:INFO:Finished creating preprocessing pipeline.
2026-02-02 10:43:55,202:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 10:43:55,202:INFO:Creating final display dataframe.
2026-02-02 10:43:55,560:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 21)
4        Transformed data shape      (373023, 21)
5   Transformed train set shape      (261116, 21)
6    Transformed test set shape      (111907, 21)
7              Numeric features                17
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              28b2
2026-02-02 10:43:55,604:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 10:43:55,605:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 10:43:55,648:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 10:43:55,649:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 10:43:55,649:INFO:setup() successfully completed in 1.34s...............
2026-02-02 10:43:55,650:INFO:Initializing compare_models()
2026-02-02 10:43:55,650:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 10:43:55,650:INFO:Checking exceptions
2026-02-02 10:43:55,708:INFO:Preparing display monitor
2026-02-02 10:43:55,717:INFO:Initializing Logistic Regression
2026-02-02 10:43:55,717:INFO:Total runtime is 0.0 minutes
2026-02-02 10:43:55,717:INFO:SubProcess create_model() called ==================================
2026-02-02 10:43:55,717:INFO:Initializing create_model()
2026-02-02 10:43:55,718:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D529A450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:43:55,718:INFO:Checking exceptions
2026-02-02 10:43:55,718:INFO:Importing libraries
2026-02-02 10:43:55,718:INFO:Copying training dataset
2026-02-02 10:43:55,807:INFO:Defining folds
2026-02-02 10:43:55,807:INFO:Declaring metric variables
2026-02-02 10:43:55,807:INFO:Importing untrained model
2026-02-02 10:43:55,807:INFO:Logistic Regression Imported successfully
2026-02-02 10:43:55,807:INFO:Starting cross validation
2026-02-02 10:43:55,807:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:44:03,459:INFO:Calculating mean and std
2026-02-02 10:44:03,461:INFO:Creating metrics dataframe
2026-02-02 10:44:03,463:INFO:Uploading results into container
2026-02-02 10:44:03,463:INFO:Uploading model into container now
2026-02-02 10:44:03,465:INFO:_master_model_container: 1
2026-02-02 10:44:03,465:INFO:_display_container: 2
2026-02-02 10:44:03,465:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 10:44:03,465:INFO:create_model() successfully completed......................................
2026-02-02 10:44:03,609:INFO:SubProcess create_model() end ==================================
2026-02-02 10:44:03,609:INFO:Creating metrics dataframe
2026-02-02 10:44:03,611:INFO:Initializing Decision Tree Classifier
2026-02-02 10:44:03,611:INFO:Total runtime is 0.13157376050949096 minutes
2026-02-02 10:44:03,611:INFO:SubProcess create_model() called ==================================
2026-02-02 10:44:03,612:INFO:Initializing create_model()
2026-02-02 10:44:03,612:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D529A450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:44:03,612:INFO:Checking exceptions
2026-02-02 10:44:03,612:INFO:Importing libraries
2026-02-02 10:44:03,612:INFO:Copying training dataset
2026-02-02 10:44:03,715:INFO:Defining folds
2026-02-02 10:44:03,715:INFO:Declaring metric variables
2026-02-02 10:44:03,715:INFO:Importing untrained model
2026-02-02 10:44:03,716:INFO:Decision Tree Classifier Imported successfully
2026-02-02 10:44:03,716:INFO:Starting cross validation
2026-02-02 10:44:03,717:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:44:10,769:INFO:Calculating mean and std
2026-02-02 10:44:10,771:INFO:Creating metrics dataframe
2026-02-02 10:44:10,773:INFO:Uploading results into container
2026-02-02 10:44:10,773:INFO:Uploading model into container now
2026-02-02 10:44:10,774:INFO:_master_model_container: 2
2026-02-02 10:44:10,775:INFO:_display_container: 2
2026-02-02 10:44:10,775:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 10:44:10,776:INFO:create_model() successfully completed......................................
2026-02-02 10:44:10,931:INFO:SubProcess create_model() end ==================================
2026-02-02 10:44:10,932:INFO:Creating metrics dataframe
2026-02-02 10:44:10,933:INFO:Initializing Random Forest Classifier
2026-02-02 10:44:10,933:INFO:Total runtime is 0.2536033034324646 minutes
2026-02-02 10:44:10,933:INFO:SubProcess create_model() called ==================================
2026-02-02 10:44:10,934:INFO:Initializing create_model()
2026-02-02 10:44:10,934:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D529A450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:44:10,934:INFO:Checking exceptions
2026-02-02 10:44:10,934:INFO:Importing libraries
2026-02-02 10:44:10,934:INFO:Copying training dataset
2026-02-02 10:44:11,079:INFO:Defining folds
2026-02-02 10:44:11,081:INFO:Declaring metric variables
2026-02-02 10:44:11,081:INFO:Importing untrained model
2026-02-02 10:44:11,081:INFO:Random Forest Classifier Imported successfully
2026-02-02 10:44:11,081:INFO:Starting cross validation
2026-02-02 10:44:11,082:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:44:25,447:INFO:Calculating mean and std
2026-02-02 10:44:25,449:INFO:Creating metrics dataframe
2026-02-02 10:44:25,452:INFO:Uploading results into container
2026-02-02 10:44:25,454:INFO:Uploading model into container now
2026-02-02 10:44:25,455:INFO:_master_model_container: 3
2026-02-02 10:44:25,455:INFO:_display_container: 2
2026-02-02 10:44:25,456:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 10:44:25,456:INFO:create_model() successfully completed......................................
2026-02-02 10:44:25,610:INFO:SubProcess create_model() end ==================================
2026-02-02 10:44:25,610:INFO:Creating metrics dataframe
2026-02-02 10:44:25,613:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 10:44:25,613:INFO:Total runtime is 0.4982746084531149 minutes
2026-02-02 10:44:25,614:INFO:SubProcess create_model() called ==================================
2026-02-02 10:44:25,614:INFO:Initializing create_model()
2026-02-02 10:44:25,614:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D529A450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:44:25,614:INFO:Checking exceptions
2026-02-02 10:44:25,614:INFO:Importing libraries
2026-02-02 10:44:25,614:INFO:Copying training dataset
2026-02-02 10:44:25,800:INFO:Defining folds
2026-02-02 10:44:25,800:INFO:Declaring metric variables
2026-02-02 10:44:25,800:INFO:Importing untrained model
2026-02-02 10:44:25,801:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 10:44:25,801:INFO:Starting cross validation
2026-02-02 10:44:25,802:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:44:33,489:INFO:Calculating mean and std
2026-02-02 10:44:33,490:INFO:Creating metrics dataframe
2026-02-02 10:44:33,492:INFO:Uploading results into container
2026-02-02 10:44:33,493:INFO:Uploading model into container now
2026-02-02 10:44:33,493:INFO:_master_model_container: 4
2026-02-02 10:44:33,494:INFO:_display_container: 2
2026-02-02 10:44:33,494:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 10:44:33,494:INFO:create_model() successfully completed......................................
2026-02-02 10:44:33,624:INFO:SubProcess create_model() end ==================================
2026-02-02 10:44:33,624:INFO:Creating metrics dataframe
2026-02-02 10:44:33,624:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 10:44:33,624:INFO:Initializing create_model()
2026-02-02 10:44:33,624:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:44:33,624:INFO:Checking exceptions
2026-02-02 10:44:33,624:INFO:Importing libraries
2026-02-02 10:44:33,624:INFO:Copying training dataset
2026-02-02 10:44:33,741:INFO:Defining folds
2026-02-02 10:44:33,741:INFO:Declaring metric variables
2026-02-02 10:44:33,741:INFO:Importing untrained model
2026-02-02 10:44:33,741:INFO:Declaring custom model
2026-02-02 10:44:33,741:INFO:Random Forest Classifier Imported successfully
2026-02-02 10:44:33,741:INFO:Cross validation set to False
2026-02-02 10:44:33,741:INFO:Fitting Model
2026-02-02 10:44:38,582:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 10:44:38,582:INFO:create_model() successfully completed......................................
2026-02-02 10:44:38,716:INFO:Initializing create_model()
2026-02-02 10:44:38,716:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:44:38,716:INFO:Checking exceptions
2026-02-02 10:44:38,716:INFO:Importing libraries
2026-02-02 10:44:38,716:INFO:Copying training dataset
2026-02-02 10:44:38,841:INFO:Defining folds
2026-02-02 10:44:38,841:INFO:Declaring metric variables
2026-02-02 10:44:38,841:INFO:Importing untrained model
2026-02-02 10:44:38,841:INFO:Declaring custom model
2026-02-02 10:44:38,841:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 10:44:38,841:INFO:Cross validation set to False
2026-02-02 10:44:38,841:INFO:Fitting Model
2026-02-02 10:44:39,425:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 10:44:39,439:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006923 seconds.
2026-02-02 10:44:39,440:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 10:44:39,440:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 10:44:39,440:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 10:44:39,441:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 10:44:39,443:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 10:44:39,443:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 10:44:39,977:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 10:44:39,977:INFO:create_model() successfully completed......................................
2026-02-02 10:44:40,141:INFO:Initializing create_model()
2026-02-02 10:44:40,141:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:44:40,141:INFO:Checking exceptions
2026-02-02 10:44:40,141:INFO:Importing libraries
2026-02-02 10:44:40,141:INFO:Copying training dataset
2026-02-02 10:44:40,257:INFO:Defining folds
2026-02-02 10:44:40,257:INFO:Declaring metric variables
2026-02-02 10:44:40,257:INFO:Importing untrained model
2026-02-02 10:44:40,257:INFO:Declaring custom model
2026-02-02 10:44:40,257:INFO:Decision Tree Classifier Imported successfully
2026-02-02 10:44:40,257:INFO:Cross validation set to False
2026-02-02 10:44:40,257:INFO:Fitting Model
2026-02-02 10:44:42,158:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 10:44:42,158:INFO:create_model() successfully completed......................................
2026-02-02 10:44:42,291:INFO:_master_model_container: 4
2026-02-02 10:44:42,291:INFO:_display_container: 2
2026-02-02 10:44:42,291:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-02-02 10:44:42,291:INFO:compare_models() successfully completed......................................
2026-02-02 10:44:42,311:INFO:Initializing tune_model()
2026-02-02 10:44:42,311:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 10:44:42,311:INFO:Checking exceptions
2026-02-02 10:44:42,341:INFO:Copying training dataset
2026-02-02 10:44:42,435:INFO:Checking base model
2026-02-02 10:44:42,435:INFO:Base model : Random Forest Classifier
2026-02-02 10:44:42,435:INFO:Declaring metric variables
2026-02-02 10:44:42,435:INFO:Defining Hyperparameters
2026-02-02 10:44:42,557:INFO:Tuning with n_jobs=-1
2026-02-02 10:44:42,557:INFO:Initializing RandomizedSearchCV
2026-02-02 10:46:14,765:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 10:46:14,766:INFO:Hyperparameter search completed
2026-02-02 10:46:14,766:INFO:SubProcess create_model() called ==================================
2026-02-02 10:46:14,767:INFO:Initializing create_model()
2026-02-02 10:46:14,767:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228669EAA90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-02 10:46:14,767:INFO:Checking exceptions
2026-02-02 10:46:14,767:INFO:Importing libraries
2026-02-02 10:46:14,767:INFO:Copying training dataset
2026-02-02 10:46:14,973:INFO:Defining folds
2026-02-02 10:46:14,973:INFO:Declaring metric variables
2026-02-02 10:46:14,973:INFO:Importing untrained model
2026-02-02 10:46:14,973:INFO:Declaring custom model
2026-02-02 10:46:14,973:INFO:Random Forest Classifier Imported successfully
2026-02-02 10:46:14,973:INFO:Starting cross validation
2026-02-02 10:46:14,973:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:46:34,510:INFO:Calculating mean and std
2026-02-02 10:46:34,510:INFO:Creating metrics dataframe
2026-02-02 10:46:34,510:INFO:Finalizing model
2026-02-02 10:46:43,904:INFO:Uploading results into container
2026-02-02 10:46:43,906:INFO:Uploading model into container now
2026-02-02 10:46:43,906:INFO:_master_model_container: 5
2026-02-02 10:46:43,906:INFO:_display_container: 3
2026-02-02 10:46:43,906:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 10:46:43,906:INFO:create_model() successfully completed......................................
2026-02-02 10:46:44,043:INFO:SubProcess create_model() end ==================================
2026-02-02 10:46:44,043:INFO:choose_better activated
2026-02-02 10:46:44,043:INFO:SubProcess create_model() called ==================================
2026-02-02 10:46:44,043:INFO:Initializing create_model()
2026-02-02 10:46:44,043:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:46:44,043:INFO:Checking exceptions
2026-02-02 10:46:44,043:INFO:Importing libraries
2026-02-02 10:46:44,043:INFO:Copying training dataset
2026-02-02 10:46:44,172:INFO:Defining folds
2026-02-02 10:46:44,172:INFO:Declaring metric variables
2026-02-02 10:46:44,172:INFO:Importing untrained model
2026-02-02 10:46:44,172:INFO:Declaring custom model
2026-02-02 10:46:44,172:INFO:Random Forest Classifier Imported successfully
2026-02-02 10:46:44,172:INFO:Starting cross validation
2026-02-02 10:46:44,172:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:46:57,821:INFO:Calculating mean and std
2026-02-02 10:46:57,822:INFO:Creating metrics dataframe
2026-02-02 10:46:57,822:INFO:Finalizing model
2026-02-02 10:47:04,534:INFO:Uploading results into container
2026-02-02 10:47:04,536:INFO:Uploading model into container now
2026-02-02 10:47:04,537:INFO:_master_model_container: 6
2026-02-02 10:47:04,537:INFO:_display_container: 4
2026-02-02 10:47:04,538:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 10:47:04,538:INFO:create_model() successfully completed......................................
2026-02-02 10:47:04,696:INFO:SubProcess create_model() end ==================================
2026-02-02 10:47:04,697:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9957
2026-02-02 10:47:04,697:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9656
2026-02-02 10:47:04,697:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 10:47:04,698:INFO:choose_better completed
2026-02-02 10:47:04,698:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 10:47:04,700:INFO:_master_model_container: 6
2026-02-02 10:47:04,700:INFO:_display_container: 3
2026-02-02 10:47:04,701:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 10:47:04,701:INFO:tune_model() successfully completed......................................
2026-02-02 10:47:04,838:INFO:Initializing tune_model()
2026-02-02 10:47:04,838:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 10:47:04,839:INFO:Checking exceptions
2026-02-02 10:47:04,881:INFO:Copying training dataset
2026-02-02 10:47:04,960:INFO:Checking base model
2026-02-02 10:47:04,960:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 10:47:04,961:INFO:Declaring metric variables
2026-02-02 10:47:04,961:INFO:Defining Hyperparameters
2026-02-02 10:47:05,089:INFO:Tuning with n_jobs=-1
2026-02-02 10:47:05,089:INFO:Initializing RandomizedSearchCV
2026-02-02 10:47:35,915:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 10:47:35,915:INFO:Hyperparameter search completed
2026-02-02 10:47:35,915:INFO:SubProcess create_model() called ==================================
2026-02-02 10:47:35,915:INFO:Initializing create_model()
2026-02-02 10:47:35,915:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228CB20D250>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 10:47:35,921:INFO:Checking exceptions
2026-02-02 10:47:35,921:INFO:Importing libraries
2026-02-02 10:47:35,921:INFO:Copying training dataset
2026-02-02 10:47:36,105:INFO:Defining folds
2026-02-02 10:47:36,120:INFO:Declaring metric variables
2026-02-02 10:47:36,120:INFO:Importing untrained model
2026-02-02 10:47:36,120:INFO:Declaring custom model
2026-02-02 10:47:36,122:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 10:47:36,122:INFO:Starting cross validation
2026-02-02 10:47:36,122:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:47:43,906:INFO:Calculating mean and std
2026-02-02 10:47:43,906:INFO:Creating metrics dataframe
2026-02-02 10:47:43,906:INFO:Finalizing model
2026-02-02 10:47:44,306:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 10:47:44,306:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 10:47:44,307:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 10:47:44,444:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 10:47:44,446:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 10:47:44,446:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 10:47:44,446:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 10:47:44,465:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008624 seconds.
2026-02-02 10:47:44,465:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 10:47:44,465:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 10:47:44,465:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 10:47:44,465:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 10:47:44,468:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 10:47:44,468:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 10:47:47,130:INFO:Uploading results into container
2026-02-02 10:47:47,132:INFO:Uploading model into container now
2026-02-02 10:47:47,132:INFO:_master_model_container: 7
2026-02-02 10:47:47,132:INFO:_display_container: 4
2026-02-02 10:47:47,134:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 10:47:47,134:INFO:create_model() successfully completed......................................
2026-02-02 10:47:47,339:INFO:SubProcess create_model() end ==================================
2026-02-02 10:47:47,339:INFO:choose_better activated
2026-02-02 10:47:47,339:INFO:SubProcess create_model() called ==================================
2026-02-02 10:47:47,339:INFO:Initializing create_model()
2026-02-02 10:47:47,339:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:47:47,339:INFO:Checking exceptions
2026-02-02 10:47:47,339:INFO:Importing libraries
2026-02-02 10:47:47,339:INFO:Copying training dataset
2026-02-02 10:47:47,490:INFO:Defining folds
2026-02-02 10:47:47,490:INFO:Declaring metric variables
2026-02-02 10:47:47,490:INFO:Importing untrained model
2026-02-02 10:47:47,490:INFO:Declaring custom model
2026-02-02 10:47:47,493:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 10:47:47,493:INFO:Starting cross validation
2026-02-02 10:47:47,493:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:47:51,226:INFO:Calculating mean and std
2026-02-02 10:47:51,226:INFO:Creating metrics dataframe
2026-02-02 10:47:51,226:INFO:Finalizing model
2026-02-02 10:47:51,763:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 10:47:51,779:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007761 seconds.
2026-02-02 10:47:51,779:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 10:47:51,779:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 10:47:51,779:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 10:47:51,779:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 10:47:51,783:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 10:47:51,783:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 10:47:52,538:INFO:Uploading results into container
2026-02-02 10:47:52,538:INFO:Uploading model into container now
2026-02-02 10:47:52,540:INFO:_master_model_container: 8
2026-02-02 10:47:52,540:INFO:_display_container: 5
2026-02-02 10:47:52,540:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 10:47:52,540:INFO:create_model() successfully completed......................................
2026-02-02 10:47:52,734:INFO:SubProcess create_model() end ==================================
2026-02-02 10:47:52,735:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9785
2026-02-02 10:47:52,735:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9938
2026-02-02 10:47:52,736:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 10:47:52,736:INFO:choose_better completed
2026-02-02 10:47:52,739:INFO:_master_model_container: 8
2026-02-02 10:47:52,739:INFO:_display_container: 4
2026-02-02 10:47:52,739:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 10:47:52,739:INFO:tune_model() successfully completed......................................
2026-02-02 10:47:52,872:INFO:Initializing tune_model()
2026-02-02 10:47:52,872:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 10:47:52,872:INFO:Checking exceptions
2026-02-02 10:47:52,924:INFO:Copying training dataset
2026-02-02 10:47:53,005:INFO:Checking base model
2026-02-02 10:47:53,005:INFO:Base model : Decision Tree Classifier
2026-02-02 10:47:53,005:INFO:Declaring metric variables
2026-02-02 10:47:53,005:INFO:Defining Hyperparameters
2026-02-02 10:47:53,138:INFO:Tuning with n_jobs=-1
2026-02-02 10:47:53,138:INFO:Initializing RandomizedSearchCV
2026-02-02 10:47:58,569:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-02-02 10:47:58,570:INFO:Hyperparameter search completed
2026-02-02 10:47:58,571:INFO:SubProcess create_model() called ==================================
2026-02-02 10:47:58,571:INFO:Initializing create_model()
2026-02-02 10:47:58,571:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022866FCC750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-02-02 10:47:58,571:INFO:Checking exceptions
2026-02-02 10:47:58,571:INFO:Importing libraries
2026-02-02 10:47:58,571:INFO:Copying training dataset
2026-02-02 10:47:58,708:INFO:Defining folds
2026-02-02 10:47:58,708:INFO:Declaring metric variables
2026-02-02 10:47:58,721:INFO:Importing untrained model
2026-02-02 10:47:58,721:INFO:Declaring custom model
2026-02-02 10:47:58,722:INFO:Decision Tree Classifier Imported successfully
2026-02-02 10:47:58,722:INFO:Starting cross validation
2026-02-02 10:47:58,723:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:48:00,621:INFO:Calculating mean and std
2026-02-02 10:48:00,623:INFO:Creating metrics dataframe
2026-02-02 10:48:00,625:INFO:Finalizing model
2026-02-02 10:48:01,774:INFO:Uploading results into container
2026-02-02 10:48:01,774:INFO:Uploading model into container now
2026-02-02 10:48:01,774:INFO:_master_model_container: 9
2026-02-02 10:48:01,774:INFO:_display_container: 5
2026-02-02 10:48:01,774:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 10:48:01,774:INFO:create_model() successfully completed......................................
2026-02-02 10:48:01,905:INFO:SubProcess create_model() end ==================================
2026-02-02 10:48:01,905:INFO:choose_better activated
2026-02-02 10:48:01,905:INFO:SubProcess create_model() called ==================================
2026-02-02 10:48:01,912:INFO:Initializing create_model()
2026-02-02 10:48:01,912:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 10:48:01,912:INFO:Checking exceptions
2026-02-02 10:48:01,912:INFO:Importing libraries
2026-02-02 10:48:01,912:INFO:Copying training dataset
2026-02-02 10:48:02,025:INFO:Defining folds
2026-02-02 10:48:02,025:INFO:Declaring metric variables
2026-02-02 10:48:02,025:INFO:Importing untrained model
2026-02-02 10:48:02,025:INFO:Declaring custom model
2026-02-02 10:48:02,025:INFO:Decision Tree Classifier Imported successfully
2026-02-02 10:48:02,025:INFO:Starting cross validation
2026-02-02 10:48:02,025:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 10:48:04,567:INFO:Calculating mean and std
2026-02-02 10:48:04,567:INFO:Creating metrics dataframe
2026-02-02 10:48:04,567:INFO:Finalizing model
2026-02-02 10:48:06,438:INFO:Uploading results into container
2026-02-02 10:48:06,438:INFO:Uploading model into container now
2026-02-02 10:48:06,438:INFO:_master_model_container: 10
2026-02-02 10:48:06,438:INFO:_display_container: 6
2026-02-02 10:48:06,438:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 10:48:06,438:INFO:create_model() successfully completed......................................
2026-02-02 10:48:06,571:INFO:SubProcess create_model() end ==================================
2026-02-02 10:48:06,571:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9752
2026-02-02 10:48:06,571:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9572
2026-02-02 10:48:06,571:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 10:48:06,571:INFO:choose_better completed
2026-02-02 10:48:06,571:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 10:48:06,571:INFO:_master_model_container: 10
2026-02-02 10:48:06,571:INFO:_display_container: 5
2026-02-02 10:48:06,571:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 10:48:06,571:INFO:tune_model() successfully completed......................................
2026-02-02 10:48:06,721:INFO:Initializing predict_model()
2026-02-02 10:48:06,721:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022866C43D80>)
2026-02-02 10:48:06,721:INFO:Checking exceptions
2026-02-02 10:48:06,721:INFO:Preloading libraries
2026-02-02 10:48:06,721:INFO:Set up data.
2026-02-02 10:48:06,733:INFO:Set up index.
2026-02-02 10:48:07,255:INFO:Initializing predict_model()
2026-02-02 10:48:07,255:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022866C43D80>)
2026-02-02 10:48:07,255:INFO:Checking exceptions
2026-02-02 10:48:07,255:INFO:Preloading libraries
2026-02-02 10:48:07,255:INFO:Set up data.
2026-02-02 10:48:07,271:INFO:Set up index.
2026-02-02 10:48:07,957:INFO:Initializing predict_model()
2026-02-02 10:48:07,957:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022866C43D80>)
2026-02-02 10:48:07,957:INFO:Checking exceptions
2026-02-02 10:48:07,957:INFO:Preloading libraries
2026-02-02 10:48:07,957:INFO:Set up data.
2026-02-02 10:48:07,971:INFO:Set up index.
2026-02-02 10:48:08,254:INFO:Initializing predict_model()
2026-02-02 10:48:08,254:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228387B1D00>)
2026-02-02 10:48:08,254:INFO:Checking exceptions
2026-02-02 10:48:08,254:INFO:Preloading libraries
2026-02-02 10:48:08,254:INFO:Set up data.
2026-02-02 10:48:08,274:INFO:Set up index.
2026-02-02 10:48:08,888:INFO:Initializing plot_model()
2026-02-02 10:48:08,888:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 10:48:08,888:INFO:Checking exceptions
2026-02-02 10:48:08,971:INFO:Preloading libraries
2026-02-02 10:48:09,088:INFO:Copying training dataset
2026-02-02 10:48:09,088:INFO:Plot type: feature
2026-02-02 10:48:09,088:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 10:48:09,335:INFO:Visual Rendered Successfully
2026-02-02 10:48:09,458:INFO:plot_model() successfully completed......................................
2026-02-02 10:48:09,471:INFO:Initializing plot_model()
2026-02-02 10:48:09,471:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022833969150>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 10:48:09,471:INFO:Checking exceptions
2026-02-02 10:48:09,538:INFO:Preloading libraries
2026-02-02 10:48:09,638:INFO:Copying training dataset
2026-02-02 10:48:09,641:INFO:Plot type: feature_all
2026-02-02 10:48:09,754:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 10:48:10,054:INFO:Visual Rendered Successfully
2026-02-02 10:48:10,171:INFO:plot_model() successfully completed......................................
2026-02-02 10:48:10,194:INFO:Initializing save_model()
2026-02-02 10:48:10,194:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 10:48:10,194:INFO:Adding model into prep_pipe
2026-02-02 10:48:10,347:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-02 10:48:10,347:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-02 10:48:10,347:INFO:save_model() successfully completed......................................
2026-02-02 11:34:41,057:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-02 11:34:41,058:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-02 11:34:41,059:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-02 11:34:41,059:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-02 11:34:45,893:INFO:Initializing load_model()
2026-02-02 11:34:45,895:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-02-02 11:34:50,385:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\2637609198.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-02-02 11:34:53,204:INFO:Initializing predict_model()
2026-02-02 11:34:53,204:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000214674F3B50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum', 'PCA1',
                                             'PCA2'...
                                             'flag_NU_NOTA_MEDIA_1_BACH__PC_na',
                                             'flag_NU_RESULTADO_ADMISION_PUNTOS_na',
                                             'flag_CU_IMPORTE_TOTAL_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002146B5EB100>)
2026-02-02 11:34:53,205:INFO:Checking exceptions
2026-02-02 11:34:53,205:INFO:Preloading libraries
2026-02-02 11:34:53,205:INFO:Set up data.
2026-02-02 11:34:54,510:INFO:Set up index.
2026-02-02 11:34:59,022:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\2637609198.py:120: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-02-02 11:34:59,401:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\2637609198.py:120: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-02-02 11:54:23,126:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\442305277.py:20: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 11:54:26,545:INFO:PyCaret ClassificationExperiment
2026-02-02 11:54:26,547:INFO:Logging name: clf-default-name
2026-02-02 11:54:26,547:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 11:54:26,548:INFO:version 3.3.2
2026-02-02 11:54:26,548:INFO:Initializing setup()
2026-02-02 11:54:26,548:INFO:self.USI: 946d
2026-02-02 11:54:26,549:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 11:54:26,549:INFO:Checking environment
2026-02-02 11:54:26,549:INFO:python_version: 3.11.11
2026-02-02 11:54:26,549:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 11:54:26,550:INFO:machine: AMD64
2026-02-02 11:54:26,550:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 11:54:26,550:INFO:Memory: svmem(total=34009374720, available=14171508736, percent=58.3, used=19837865984, free=14171508736)
2026-02-02 11:54:26,551:INFO:Physical Core: 12
2026-02-02 11:54:26,551:INFO:Logical Core: 16
2026-02-02 11:54:26,552:INFO:Checking libraries
2026-02-02 11:54:26,552:INFO:System:
2026-02-02 11:54:26,552:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 11:54:26,553:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 11:54:26,554:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 11:54:26,554:INFO:PyCaret required dependencies:
2026-02-02 11:54:26,554:INFO:                 pip: 25.0
2026-02-02 11:54:26,554:INFO:          setuptools: 75.8.0
2026-02-02 11:54:26,554:INFO:             pycaret: 3.3.2
2026-02-02 11:54:26,555:INFO:             IPython: 9.9.0
2026-02-02 11:54:26,555:INFO:          ipywidgets: 8.1.8
2026-02-02 11:54:26,555:INFO:                tqdm: 4.67.1
2026-02-02 11:54:26,555:INFO:               numpy: 1.26.4
2026-02-02 11:54:26,556:INFO:              pandas: 2.1.4
2026-02-02 11:54:26,556:INFO:              jinja2: 3.1.6
2026-02-02 11:54:26,556:INFO:               scipy: 1.11.4
2026-02-02 11:54:26,556:INFO:              joblib: 1.3.2
2026-02-02 11:54:26,557:INFO:             sklearn: 1.4.2
2026-02-02 11:54:26,557:INFO:                pyod: 2.0.6
2026-02-02 11:54:26,557:INFO:            imblearn: 0.14.1
2026-02-02 11:54:26,557:INFO:   category_encoders: 2.7.0
2026-02-02 11:54:26,558:INFO:            lightgbm: 4.6.0
2026-02-02 11:54:26,558:INFO:               numba: 0.62.1
2026-02-02 11:54:26,558:INFO:            requests: 2.32.3
2026-02-02 11:54:26,558:INFO:          matplotlib: 3.7.5
2026-02-02 11:54:26,559:INFO:          scikitplot: 0.3.7
2026-02-02 11:54:26,559:INFO:         yellowbrick: 1.5
2026-02-02 11:54:26,559:INFO:              plotly: 5.24.1
2026-02-02 11:54:26,559:INFO:    plotly-resampler: Not installed
2026-02-02 11:54:26,560:INFO:             kaleido: 1.2.0
2026-02-02 11:54:26,560:INFO:           schemdraw: 0.15
2026-02-02 11:54:26,560:INFO:         statsmodels: 0.14.6
2026-02-02 11:54:26,560:INFO:              sktime: 0.26.0
2026-02-02 11:54:26,561:INFO:               tbats: 1.1.3
2026-02-02 11:54:26,561:INFO:            pmdarima: 2.0.4
2026-02-02 11:54:26,561:INFO:              psutil: 7.2.1
2026-02-02 11:54:26,561:INFO:          markupsafe: 3.0.3
2026-02-02 11:54:26,561:INFO:             pickle5: Not installed
2026-02-02 11:54:26,561:INFO:         cloudpickle: 3.0.0
2026-02-02 11:54:26,561:INFO:         deprecation: 2.1.0
2026-02-02 11:54:26,561:INFO:              xxhash: 3.6.0
2026-02-02 11:54:26,561:INFO:           wurlitzer: Not installed
2026-02-02 11:54:26,561:INFO:PyCaret optional dependencies:
2026-02-02 11:54:26,561:INFO:                shap: 0.44.1
2026-02-02 11:54:26,561:INFO:           interpret: 0.7.3
2026-02-02 11:54:26,561:INFO:                umap: 0.5.7
2026-02-02 11:54:26,561:INFO:     ydata_profiling: 4.18.1
2026-02-02 11:54:26,561:INFO:  explainerdashboard: 0.5.1
2026-02-02 11:54:26,561:INFO:             autoviz: Not installed
2026-02-02 11:54:26,561:INFO:           fairlearn: 0.7.0
2026-02-02 11:54:26,561:INFO:          deepchecks: Not installed
2026-02-02 11:54:26,561:INFO:             xgboost: Not installed
2026-02-02 11:54:26,561:INFO:            catboost: 1.2.8
2026-02-02 11:54:26,561:INFO:              kmodes: 0.12.2
2026-02-02 11:54:26,561:INFO:             mlxtend: 0.23.4
2026-02-02 11:54:26,563:INFO:       statsforecast: 1.5.0
2026-02-02 11:54:26,563:INFO:        tune_sklearn: Not installed
2026-02-02 11:54:26,563:INFO:                 ray: Not installed
2026-02-02 11:54:26,563:INFO:            hyperopt: 0.2.7
2026-02-02 11:54:26,563:INFO:              optuna: 4.6.0
2026-02-02 11:54:26,563:INFO:               skopt: 0.10.2
2026-02-02 11:54:26,563:INFO:              mlflow: 3.8.1
2026-02-02 11:54:26,563:INFO:              gradio: 6.3.0
2026-02-02 11:54:26,563:INFO:             fastapi: 0.128.0
2026-02-02 11:54:26,563:INFO:             uvicorn: 0.40.0
2026-02-02 11:54:26,563:INFO:              m2cgen: 0.10.0
2026-02-02 11:54:26,563:INFO:           evidently: 0.4.40
2026-02-02 11:54:26,563:INFO:               fugue: 0.8.7
2026-02-02 11:54:26,563:INFO:           streamlit: Not installed
2026-02-02 11:54:26,563:INFO:             prophet: Not installed
2026-02-02 11:54:26,563:INFO:None
2026-02-02 11:54:26,563:INFO:Set up data.
2026-02-02 11:54:26,689:INFO:Set up folding strategy.
2026-02-02 11:54:26,689:INFO:Set up train/test split.
2026-02-02 11:54:26,921:INFO:Set up index.
2026-02-02 11:54:26,932:INFO:Assigning column types.
2026-02-02 11:54:27,063:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 11:54:27,105:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 11:54:27,107:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 11:54:27,133:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 11:54:27,133:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 11:54:27,176:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 11:54:27,177:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 11:54:27,204:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 11:54:27,204:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 11:54:27,205:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 11:54:27,253:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 11:54:27,280:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 11:54:27,280:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 11:54:27,327:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 11:54:27,355:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 11:54:27,355:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 11:54:27,356:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 11:54:27,426:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 11:54:27,426:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 11:54:27,501:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 11:54:27,501:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 11:54:27,503:INFO:Preparing preprocessing pipeline...
2026-02-02 11:54:27,530:INFO:Set up simple imputation.
2026-02-02 11:54:27,530:INFO:Set up feature normalization.
2026-02-02 11:54:27,851:INFO:Finished creating preprocessing pipeline.
2026-02-02 11:54:27,855:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 11:54:27,855:INFO:Creating final display dataframe.
2026-02-02 11:54:28,373:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 21)
4        Transformed data shape      (373023, 21)
5   Transformed train set shape      (261116, 21)
6    Transformed test set shape      (111907, 21)
7              Numeric features                17
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              946d
2026-02-02 11:54:28,433:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 11:54:28,435:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 11:54:28,501:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 11:54:28,501:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 11:54:28,504:INFO:setup() successfully completed in 1.98s...............
2026-02-02 11:54:28,504:INFO:Initializing compare_models()
2026-02-02 11:54:28,504:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 11:54:28,504:INFO:Checking exceptions
2026-02-02 11:54:28,625:INFO:Preparing display monitor
2026-02-02 11:54:28,628:INFO:Initializing Logistic Regression
2026-02-02 11:54:28,629:INFO:Total runtime is 1.6510486602783203e-05 minutes
2026-02-02 11:54:28,629:INFO:SubProcess create_model() called ==================================
2026-02-02 11:54:28,629:INFO:Initializing create_model()
2026-02-02 11:54:28,629:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D653DD10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:54:28,630:INFO:Checking exceptions
2026-02-02 11:54:28,630:INFO:Importing libraries
2026-02-02 11:54:28,630:INFO:Copying training dataset
2026-02-02 11:54:28,794:INFO:Defining folds
2026-02-02 11:54:28,794:INFO:Declaring metric variables
2026-02-02 11:54:28,794:INFO:Importing untrained model
2026-02-02 11:54:28,795:INFO:Logistic Regression Imported successfully
2026-02-02 11:54:28,795:INFO:Starting cross validation
2026-02-02 11:54:28,796:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 11:54:39,483:INFO:Calculating mean and std
2026-02-02 11:54:39,486:INFO:Creating metrics dataframe
2026-02-02 11:54:39,489:INFO:Uploading results into container
2026-02-02 11:54:39,490:INFO:Uploading model into container now
2026-02-02 11:54:39,490:INFO:_master_model_container: 1
2026-02-02 11:54:39,491:INFO:_display_container: 2
2026-02-02 11:54:39,491:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 11:54:39,491:INFO:create_model() successfully completed......................................
2026-02-02 11:54:39,663:INFO:SubProcess create_model() end ==================================
2026-02-02 11:54:39,663:INFO:Creating metrics dataframe
2026-02-02 11:54:39,666:INFO:Initializing Decision Tree Classifier
2026-02-02 11:54:39,667:INFO:Total runtime is 0.1839731216430664 minutes
2026-02-02 11:54:39,667:INFO:SubProcess create_model() called ==================================
2026-02-02 11:54:39,667:INFO:Initializing create_model()
2026-02-02 11:54:39,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D653DD10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:54:39,667:INFO:Checking exceptions
2026-02-02 11:54:39,667:INFO:Importing libraries
2026-02-02 11:54:39,667:INFO:Copying training dataset
2026-02-02 11:54:39,809:INFO:Defining folds
2026-02-02 11:54:39,810:INFO:Declaring metric variables
2026-02-02 11:54:39,810:INFO:Importing untrained model
2026-02-02 11:54:39,810:INFO:Decision Tree Classifier Imported successfully
2026-02-02 11:54:39,810:INFO:Starting cross validation
2026-02-02 11:54:39,811:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 11:54:49,791:INFO:Calculating mean and std
2026-02-02 11:54:49,795:INFO:Creating metrics dataframe
2026-02-02 11:54:49,799:INFO:Uploading results into container
2026-02-02 11:54:49,801:INFO:Uploading model into container now
2026-02-02 11:54:49,802:INFO:_master_model_container: 2
2026-02-02 11:54:49,802:INFO:_display_container: 2
2026-02-02 11:54:49,803:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 11:54:49,803:INFO:create_model() successfully completed......................................
2026-02-02 11:54:50,023:INFO:SubProcess create_model() end ==================================
2026-02-02 11:54:50,024:INFO:Creating metrics dataframe
2026-02-02 11:54:50,028:INFO:Initializing Random Forest Classifier
2026-02-02 11:54:50,029:INFO:Total runtime is 0.3566737731297811 minutes
2026-02-02 11:54:50,029:INFO:SubProcess create_model() called ==================================
2026-02-02 11:54:50,029:INFO:Initializing create_model()
2026-02-02 11:54:50,030:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D653DD10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:54:50,030:INFO:Checking exceptions
2026-02-02 11:54:50,030:INFO:Importing libraries
2026-02-02 11:54:50,030:INFO:Copying training dataset
2026-02-02 11:54:50,276:INFO:Defining folds
2026-02-02 11:54:50,277:INFO:Declaring metric variables
2026-02-02 11:54:50,277:INFO:Importing untrained model
2026-02-02 11:54:50,278:INFO:Random Forest Classifier Imported successfully
2026-02-02 11:54:50,279:INFO:Starting cross validation
2026-02-02 11:54:50,281:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 11:55:12,757:INFO:Calculating mean and std
2026-02-02 11:55:12,759:INFO:Creating metrics dataframe
2026-02-02 11:55:12,761:INFO:Uploading results into container
2026-02-02 11:55:12,763:INFO:Uploading model into container now
2026-02-02 11:55:12,763:INFO:_master_model_container: 3
2026-02-02 11:55:12,765:INFO:_display_container: 2
2026-02-02 11:55:12,765:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 11:55:12,765:INFO:create_model() successfully completed......................................
2026-02-02 11:55:12,926:INFO:SubProcess create_model() end ==================================
2026-02-02 11:55:12,926:INFO:Creating metrics dataframe
2026-02-02 11:55:12,928:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 11:55:12,928:INFO:Total runtime is 0.7383299072583517 minutes
2026-02-02 11:55:12,928:INFO:SubProcess create_model() called ==================================
2026-02-02 11:55:12,928:INFO:Initializing create_model()
2026-02-02 11:55:12,928:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D653DD10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:55:12,928:INFO:Checking exceptions
2026-02-02 11:55:12,928:INFO:Importing libraries
2026-02-02 11:55:12,929:INFO:Copying training dataset
2026-02-02 11:55:13,071:INFO:Defining folds
2026-02-02 11:55:13,071:INFO:Declaring metric variables
2026-02-02 11:55:13,073:INFO:Importing untrained model
2026-02-02 11:55:13,074:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 11:55:13,074:INFO:Starting cross validation
2026-02-02 11:55:13,075:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 11:55:23,183:INFO:Calculating mean and std
2026-02-02 11:55:23,186:INFO:Creating metrics dataframe
2026-02-02 11:55:23,187:INFO:Uploading results into container
2026-02-02 11:55:23,188:INFO:Uploading model into container now
2026-02-02 11:55:23,188:INFO:_master_model_container: 4
2026-02-02 11:55:23,188:INFO:_display_container: 2
2026-02-02 11:55:23,189:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 11:55:23,189:INFO:create_model() successfully completed......................................
2026-02-02 11:55:23,343:INFO:SubProcess create_model() end ==================================
2026-02-02 11:55:23,343:INFO:Creating metrics dataframe
2026-02-02 11:55:23,346:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 11:55:23,347:INFO:Initializing create_model()
2026-02-02 11:55:23,348:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:55:23,348:INFO:Checking exceptions
2026-02-02 11:55:23,348:INFO:Importing libraries
2026-02-02 11:55:23,348:INFO:Copying training dataset
2026-02-02 11:55:23,474:INFO:Defining folds
2026-02-02 11:55:23,474:INFO:Declaring metric variables
2026-02-02 11:55:23,474:INFO:Importing untrained model
2026-02-02 11:55:23,474:INFO:Declaring custom model
2026-02-02 11:55:23,475:INFO:Random Forest Classifier Imported successfully
2026-02-02 11:55:23,475:INFO:Cross validation set to False
2026-02-02 11:55:23,475:INFO:Fitting Model
2026-02-02 11:55:31,545:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 11:55:31,546:INFO:create_model() successfully completed......................................
2026-02-02 11:55:31,715:INFO:Initializing create_model()
2026-02-02 11:55:31,715:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:55:31,715:INFO:Checking exceptions
2026-02-02 11:55:31,716:INFO:Importing libraries
2026-02-02 11:55:31,716:INFO:Copying training dataset
2026-02-02 11:55:31,847:INFO:Defining folds
2026-02-02 11:55:31,847:INFO:Declaring metric variables
2026-02-02 11:55:31,847:INFO:Importing untrained model
2026-02-02 11:55:31,847:INFO:Declaring custom model
2026-02-02 11:55:31,848:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 11:55:31,849:INFO:Cross validation set to False
2026-02-02 11:55:31,849:INFO:Fitting Model
2026-02-02 11:55:32,403:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 11:55:32,421:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009294 seconds.
2026-02-02 11:55:32,421:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 11:55:32,421:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 11:55:32,421:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 11:55:32,422:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 11:55:32,423:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 11:55:32,425:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 11:55:33,337:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 11:55:33,337:INFO:create_model() successfully completed......................................
2026-02-02 11:55:33,543:INFO:Initializing create_model()
2026-02-02 11:55:33,543:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:55:33,543:INFO:Checking exceptions
2026-02-02 11:55:33,545:INFO:Importing libraries
2026-02-02 11:55:33,545:INFO:Copying training dataset
2026-02-02 11:55:33,711:INFO:Defining folds
2026-02-02 11:55:33,711:INFO:Declaring metric variables
2026-02-02 11:55:33,712:INFO:Importing untrained model
2026-02-02 11:55:33,712:INFO:Declaring custom model
2026-02-02 11:55:33,713:INFO:Decision Tree Classifier Imported successfully
2026-02-02 11:55:33,713:INFO:Cross validation set to False
2026-02-02 11:55:33,713:INFO:Fitting Model
2026-02-02 11:55:35,759:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 11:55:35,759:INFO:create_model() successfully completed......................................
2026-02-02 11:55:35,923:INFO:_master_model_container: 4
2026-02-02 11:55:35,923:INFO:_display_container: 2
2026-02-02 11:55:35,924:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-02-02 11:55:35,924:INFO:compare_models() successfully completed......................................
2026-02-02 11:55:35,933:INFO:Initializing tune_model()
2026-02-02 11:55:35,933:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 11:55:35,933:INFO:Checking exceptions
2026-02-02 11:55:35,988:INFO:Copying training dataset
2026-02-02 11:55:36,077:INFO:Checking base model
2026-02-02 11:55:36,077:INFO:Base model : Random Forest Classifier
2026-02-02 11:55:36,078:INFO:Declaring metric variables
2026-02-02 11:55:36,078:INFO:Defining Hyperparameters
2026-02-02 11:55:36,211:INFO:Tuning with n_jobs=-1
2026-02-02 11:55:36,212:INFO:Initializing RandomizedSearchCV
2026-02-02 11:57:42,292:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 11:57:42,294:INFO:Hyperparameter search completed
2026-02-02 11:57:42,294:INFO:SubProcess create_model() called ==================================
2026-02-02 11:57:42,296:INFO:Initializing create_model()
2026-02-02 11:57:42,296:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228CAF4F050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-02 11:57:42,296:INFO:Checking exceptions
2026-02-02 11:57:42,297:INFO:Importing libraries
2026-02-02 11:57:42,297:INFO:Copying training dataset
2026-02-02 11:57:42,502:INFO:Defining folds
2026-02-02 11:57:42,502:INFO:Declaring metric variables
2026-02-02 11:57:42,503:INFO:Importing untrained model
2026-02-02 11:57:42,503:INFO:Declaring custom model
2026-02-02 11:57:42,504:INFO:Random Forest Classifier Imported successfully
2026-02-02 11:57:42,504:INFO:Starting cross validation
2026-02-02 11:57:42,505:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 11:58:06,282:INFO:Calculating mean and std
2026-02-02 11:58:06,283:INFO:Creating metrics dataframe
2026-02-02 11:58:06,285:INFO:Finalizing model
2026-02-02 11:58:17,880:INFO:Uploading results into container
2026-02-02 11:58:17,882:INFO:Uploading model into container now
2026-02-02 11:58:17,882:INFO:_master_model_container: 5
2026-02-02 11:58:17,882:INFO:_display_container: 3
2026-02-02 11:58:17,884:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 11:58:17,884:INFO:create_model() successfully completed......................................
2026-02-02 11:58:18,054:INFO:SubProcess create_model() end ==================================
2026-02-02 11:58:18,054:INFO:choose_better activated
2026-02-02 11:58:18,054:INFO:SubProcess create_model() called ==================================
2026-02-02 11:58:18,055:INFO:Initializing create_model()
2026-02-02 11:58:18,055:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:58:18,055:INFO:Checking exceptions
2026-02-02 11:58:18,056:INFO:Importing libraries
2026-02-02 11:58:18,056:INFO:Copying training dataset
2026-02-02 11:58:18,203:INFO:Defining folds
2026-02-02 11:58:18,204:INFO:Declaring metric variables
2026-02-02 11:58:18,204:INFO:Importing untrained model
2026-02-02 11:58:18,204:INFO:Declaring custom model
2026-02-02 11:58:18,205:INFO:Random Forest Classifier Imported successfully
2026-02-02 11:58:18,205:INFO:Starting cross validation
2026-02-02 11:58:18,206:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 11:58:34,402:INFO:Calculating mean and std
2026-02-02 11:58:34,403:INFO:Creating metrics dataframe
2026-02-02 11:58:34,404:INFO:Finalizing model
2026-02-02 11:58:41,782:INFO:Uploading results into container
2026-02-02 11:58:41,783:INFO:Uploading model into container now
2026-02-02 11:58:41,784:INFO:_master_model_container: 6
2026-02-02 11:58:41,784:INFO:_display_container: 4
2026-02-02 11:58:41,785:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 11:58:41,785:INFO:create_model() successfully completed......................................
2026-02-02 11:58:41,955:INFO:SubProcess create_model() end ==================================
2026-02-02 11:58:41,956:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9957
2026-02-02 11:58:41,956:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9656
2026-02-02 11:58:41,956:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 11:58:41,956:INFO:choose_better completed
2026-02-02 11:58:41,957:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 11:58:41,959:INFO:_master_model_container: 6
2026-02-02 11:58:41,959:INFO:_display_container: 3
2026-02-02 11:58:41,960:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 11:58:41,960:INFO:tune_model() successfully completed......................................
2026-02-02 11:58:42,103:INFO:Initializing tune_model()
2026-02-02 11:58:42,103:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 11:58:42,103:INFO:Checking exceptions
2026-02-02 11:58:42,151:INFO:Copying training dataset
2026-02-02 11:58:42,233:INFO:Checking base model
2026-02-02 11:58:42,233:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 11:58:42,233:INFO:Declaring metric variables
2026-02-02 11:58:42,233:INFO:Defining Hyperparameters
2026-02-02 11:58:42,366:INFO:Tuning with n_jobs=-1
2026-02-02 11:58:42,367:INFO:Initializing RandomizedSearchCV
2026-02-02 11:59:28,729:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 11:59:28,730:INFO:Hyperparameter search completed
2026-02-02 11:59:28,730:INFO:SubProcess create_model() called ==================================
2026-02-02 11:59:28,733:INFO:Initializing create_model()
2026-02-02 11:59:28,733:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022867165C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 11:59:28,734:INFO:Checking exceptions
2026-02-02 11:59:28,734:INFO:Importing libraries
2026-02-02 11:59:28,734:INFO:Copying training dataset
2026-02-02 11:59:29,064:INFO:Defining folds
2026-02-02 11:59:29,064:INFO:Declaring metric variables
2026-02-02 11:59:29,065:INFO:Importing untrained model
2026-02-02 11:59:29,066:INFO:Declaring custom model
2026-02-02 11:59:29,068:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 11:59:29,068:INFO:Starting cross validation
2026-02-02 11:59:29,070:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 11:59:39,872:INFO:Calculating mean and std
2026-02-02 11:59:39,875:INFO:Creating metrics dataframe
2026-02-02 11:59:39,877:INFO:Finalizing model
2026-02-02 11:59:40,502:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 11:59:40,502:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 11:59:40,502:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 11:59:40,690:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 11:59:40,690:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 11:59:40,690:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 11:59:40,690:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 11:59:40,712:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011229 seconds.
2026-02-02 11:59:40,713:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 11:59:40,713:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 11:59:40,713:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 11:59:40,714:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 11:59:40,718:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 11:59:40,718:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 11:59:44,255:INFO:Uploading results into container
2026-02-02 11:59:44,257:INFO:Uploading model into container now
2026-02-02 11:59:44,258:INFO:_master_model_container: 7
2026-02-02 11:59:44,258:INFO:_display_container: 4
2026-02-02 11:59:44,259:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 11:59:44,259:INFO:create_model() successfully completed......................................
2026-02-02 11:59:44,445:INFO:SubProcess create_model() end ==================================
2026-02-02 11:59:44,445:INFO:choose_better activated
2026-02-02 11:59:44,446:INFO:SubProcess create_model() called ==================================
2026-02-02 11:59:44,446:INFO:Initializing create_model()
2026-02-02 11:59:44,447:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 11:59:44,447:INFO:Checking exceptions
2026-02-02 11:59:44,448:INFO:Importing libraries
2026-02-02 11:59:44,448:INFO:Copying training dataset
2026-02-02 11:59:44,768:INFO:Defining folds
2026-02-02 11:59:44,768:INFO:Declaring metric variables
2026-02-02 11:59:44,768:INFO:Importing untrained model
2026-02-02 11:59:44,769:INFO:Declaring custom model
2026-02-02 11:59:44,770:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 11:59:44,770:INFO:Starting cross validation
2026-02-02 11:59:44,771:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 11:59:49,774:INFO:Calculating mean and std
2026-02-02 11:59:49,776:INFO:Creating metrics dataframe
2026-02-02 11:59:49,779:INFO:Finalizing model
2026-02-02 11:59:50,552:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 11:59:50,570:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007852 seconds.
2026-02-02 11:59:50,570:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 11:59:50,570:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 11:59:50,572:INFO:[LightGBM] [Info] Total Bins 2108
2026-02-02 11:59:50,572:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 20
2026-02-02 11:59:50,574:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 11:59:50,574:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 11:59:51,594:INFO:Uploading results into container
2026-02-02 11:59:51,595:INFO:Uploading model into container now
2026-02-02 11:59:51,596:INFO:_master_model_container: 8
2026-02-02 11:59:51,596:INFO:_display_container: 5
2026-02-02 11:59:51,597:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 11:59:51,597:INFO:create_model() successfully completed......................................
2026-02-02 11:59:51,772:INFO:SubProcess create_model() end ==================================
2026-02-02 11:59:51,773:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9785
2026-02-02 11:59:51,773:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9938
2026-02-02 11:59:51,774:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 11:59:51,774:INFO:choose_better completed
2026-02-02 11:59:51,777:INFO:_master_model_container: 8
2026-02-02 11:59:51,777:INFO:_display_container: 4
2026-02-02 11:59:51,778:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 11:59:51,778:INFO:tune_model() successfully completed......................................
2026-02-02 11:59:51,936:INFO:Initializing tune_model()
2026-02-02 11:59:51,937:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 11:59:51,937:INFO:Checking exceptions
2026-02-02 11:59:52,034:INFO:Copying training dataset
2026-02-02 11:59:52,272:INFO:Checking base model
2026-02-02 11:59:52,272:INFO:Base model : Decision Tree Classifier
2026-02-02 11:59:52,272:INFO:Declaring metric variables
2026-02-02 11:59:52,274:INFO:Defining Hyperparameters
2026-02-02 11:59:52,438:INFO:Tuning with n_jobs=-1
2026-02-02 11:59:52,439:INFO:Initializing RandomizedSearchCV
2026-02-02 12:00:00,224:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-02-02 12:00:00,226:INFO:Hyperparameter search completed
2026-02-02 12:00:00,227:INFO:SubProcess create_model() called ==================================
2026-02-02 12:00:00,228:INFO:Initializing create_model()
2026-02-02 12:00:00,228:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022839138A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-02-02 12:00:00,229:INFO:Checking exceptions
2026-02-02 12:00:00,229:INFO:Importing libraries
2026-02-02 12:00:00,229:INFO:Copying training dataset
2026-02-02 12:00:00,509:INFO:Defining folds
2026-02-02 12:00:00,509:INFO:Declaring metric variables
2026-02-02 12:00:00,509:INFO:Importing untrained model
2026-02-02 12:00:00,509:INFO:Declaring custom model
2026-02-02 12:00:00,510:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:00:00,511:INFO:Starting cross validation
2026-02-02 12:00:00,512:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:00:02,712:INFO:Calculating mean and std
2026-02-02 12:00:02,716:INFO:Creating metrics dataframe
2026-02-02 12:00:02,722:INFO:Finalizing model
2026-02-02 12:00:04,335:INFO:Uploading results into container
2026-02-02 12:00:04,335:INFO:Uploading model into container now
2026-02-02 12:00:04,337:INFO:_master_model_container: 9
2026-02-02 12:00:04,337:INFO:_display_container: 5
2026-02-02 12:00:04,337:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:00:04,337:INFO:create_model() successfully completed......................................
2026-02-02 12:00:04,532:INFO:SubProcess create_model() end ==================================
2026-02-02 12:00:04,532:INFO:choose_better activated
2026-02-02 12:00:04,532:INFO:SubProcess create_model() called ==================================
2026-02-02 12:00:04,532:INFO:Initializing create_model()
2026-02-02 12:00:04,532:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:00:04,532:INFO:Checking exceptions
2026-02-02 12:00:04,533:INFO:Importing libraries
2026-02-02 12:00:04,533:INFO:Copying training dataset
2026-02-02 12:00:04,783:INFO:Defining folds
2026-02-02 12:00:04,783:INFO:Declaring metric variables
2026-02-02 12:00:04,784:INFO:Importing untrained model
2026-02-02 12:00:04,784:INFO:Declaring custom model
2026-02-02 12:00:04,784:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:00:04,784:INFO:Starting cross validation
2026-02-02 12:00:04,785:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:00:07,618:INFO:Calculating mean and std
2026-02-02 12:00:07,621:INFO:Creating metrics dataframe
2026-02-02 12:00:07,625:INFO:Finalizing model
2026-02-02 12:00:10,122:INFO:Uploading results into container
2026-02-02 12:00:10,122:INFO:Uploading model into container now
2026-02-02 12:00:10,124:INFO:_master_model_container: 10
2026-02-02 12:00:10,124:INFO:_display_container: 6
2026-02-02 12:00:10,124:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:00:10,124:INFO:create_model() successfully completed......................................
2026-02-02 12:00:10,272:INFO:SubProcess create_model() end ==================================
2026-02-02 12:00:10,272:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9752
2026-02-02 12:00:10,272:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9572
2026-02-02 12:00:10,274:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 12:00:10,274:INFO:choose_better completed
2026-02-02 12:00:10,274:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:00:10,277:INFO:_master_model_container: 10
2026-02-02 12:00:10,277:INFO:_display_container: 5
2026-02-02 12:00:10,278:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:00:10,278:INFO:tune_model() successfully completed......................................
2026-02-02 12:00:10,441:INFO:Initializing predict_model()
2026-02-02 12:00:10,441:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002283816F600>)
2026-02-02 12:00:10,441:INFO:Checking exceptions
2026-02-02 12:00:10,442:INFO:Preloading libraries
2026-02-02 12:00:10,442:INFO:Set up data.
2026-02-02 12:00:10,461:INFO:Set up index.
2026-02-02 12:00:11,325:INFO:Initializing predict_model()
2026-02-02 12:00:11,326:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002283816F600>)
2026-02-02 12:00:11,326:INFO:Checking exceptions
2026-02-02 12:00:11,326:INFO:Preloading libraries
2026-02-02 12:00:11,326:INFO:Set up data.
2026-02-02 12:00:11,344:INFO:Set up index.
2026-02-02 12:00:12,306:INFO:Initializing predict_model()
2026-02-02 12:00:12,307:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002283816F600>)
2026-02-02 12:00:12,307:INFO:Checking exceptions
2026-02-02 12:00:12,307:INFO:Preloading libraries
2026-02-02 12:00:12,307:INFO:Set up data.
2026-02-02 12:00:12,327:INFO:Set up index.
2026-02-02 12:00:12,673:INFO:Initializing predict_model()
2026-02-02 12:00:12,673:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022871D18AE0>)
2026-02-02 12:00:12,673:INFO:Checking exceptions
2026-02-02 12:00:12,673:INFO:Preloading libraries
2026-02-02 12:00:12,674:INFO:Set up data.
2026-02-02 12:00:12,711:INFO:Set up index.
2026-02-02 12:00:13,668:INFO:Initializing plot_model()
2026-02-02 12:00:13,669:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:00:13,669:INFO:Checking exceptions
2026-02-02 12:00:13,849:INFO:Preloading libraries
2026-02-02 12:00:13,978:INFO:Copying training dataset
2026-02-02 12:00:13,978:INFO:Plot type: feature
2026-02-02 12:00:13,978:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:00:14,491:INFO:Visual Rendered Successfully
2026-02-02 12:00:14,660:INFO:plot_model() successfully completed......................................
2026-02-02 12:00:14,672:INFO:Initializing plot_model()
2026-02-02 12:00:14,673:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022871D399D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:00:14,673:INFO:Checking exceptions
2026-02-02 12:00:14,889:INFO:Preloading libraries
2026-02-02 12:00:15,049:INFO:Copying training dataset
2026-02-02 12:00:15,050:INFO:Plot type: feature_all
2026-02-02 12:00:15,295:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:00:15,896:INFO:Visual Rendered Successfully
2026-02-02 12:00:16,048:INFO:plot_model() successfully completed......................................
2026-02-02 12:00:16,063:INFO:Initializing save_model()
2026-02-02 12:00:16,064:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 12:00:16,064:INFO:Adding model into prep_pipe
2026-02-02 12:00:16,298:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-02 12:00:16,307:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-02 12:00:16,307:INFO:save_model() successfully completed......................................
2026-02-02 12:05:03,621:INFO:Initializing load_model()
2026-02-02 12:05:03,622:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_explicable, platform=None, authentication=None, verbose=True)
2026-02-02 12:05:07,155:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\2637609198.py:23: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-02-02 12:05:09,867:INFO:Initializing predict_model()
2026-02-02 12:05:09,867:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000214BBE7A910>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum', 'PCA1',
                                             'PCA2'...
                                             'flag_NU_NOTA_MEDIA_1_BACH__PC_na',
                                             'flag_NU_RESULTADO_ADMISION_PUNTOS_na',
                                             'flag_CU_IMPORTE_TOTAL_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model',
                 RandomForestClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000214674FB060>)
2026-02-02 12:05:09,868:INFO:Checking exceptions
2026-02-02 12:05:09,868:INFO:Preloading libraries
2026-02-02 12:05:09,868:INFO:Set up data.
2026-02-02 12:05:10,844:INFO:Set up index.
2026-02-02 12:05:15,063:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\2637609198.py:120: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-02-02 12:05:15,481:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\2637609198.py:120: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-02-02 12:11:36,787:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\2739491150.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 12:11:37,550:INFO:PyCaret ClassificationExperiment
2026-02-02 12:11:37,550:INFO:Logging name: clf-default-name
2026-02-02 12:11:37,550:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 12:11:37,550:INFO:version 3.3.2
2026-02-02 12:11:37,550:INFO:Initializing setup()
2026-02-02 12:11:37,550:INFO:self.USI: 27f0
2026-02-02 12:11:37,550:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 12:11:37,550:INFO:Checking environment
2026-02-02 12:11:37,550:INFO:python_version: 3.11.11
2026-02-02 12:11:37,550:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 12:11:37,550:INFO:machine: AMD64
2026-02-02 12:11:37,550:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 12:11:37,550:INFO:Memory: svmem(total=34009374720, available=13568651264, percent=60.1, used=20440723456, free=13568651264)
2026-02-02 12:11:37,553:INFO:Physical Core: 12
2026-02-02 12:11:37,554:INFO:Logical Core: 16
2026-02-02 12:11:37,554:INFO:Checking libraries
2026-02-02 12:11:37,554:INFO:System:
2026-02-02 12:11:37,554:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 12:11:37,554:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 12:11:37,554:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 12:11:37,554:INFO:PyCaret required dependencies:
2026-02-02 12:11:37,554:INFO:                 pip: 25.0
2026-02-02 12:11:37,554:INFO:          setuptools: 75.8.0
2026-02-02 12:11:37,554:INFO:             pycaret: 3.3.2
2026-02-02 12:11:37,554:INFO:             IPython: 9.9.0
2026-02-02 12:11:37,554:INFO:          ipywidgets: 8.1.8
2026-02-02 12:11:37,554:INFO:                tqdm: 4.67.1
2026-02-02 12:11:37,554:INFO:               numpy: 1.26.4
2026-02-02 12:11:37,554:INFO:              pandas: 2.1.4
2026-02-02 12:11:37,554:INFO:              jinja2: 3.1.6
2026-02-02 12:11:37,554:INFO:               scipy: 1.11.4
2026-02-02 12:11:37,554:INFO:              joblib: 1.3.2
2026-02-02 12:11:37,554:INFO:             sklearn: 1.4.2
2026-02-02 12:11:37,554:INFO:                pyod: 2.0.6
2026-02-02 12:11:37,554:INFO:            imblearn: 0.14.1
2026-02-02 12:11:37,554:INFO:   category_encoders: 2.7.0
2026-02-02 12:11:37,554:INFO:            lightgbm: 4.6.0
2026-02-02 12:11:37,554:INFO:               numba: 0.62.1
2026-02-02 12:11:37,554:INFO:            requests: 2.32.3
2026-02-02 12:11:37,554:INFO:          matplotlib: 3.7.5
2026-02-02 12:11:37,554:INFO:          scikitplot: 0.3.7
2026-02-02 12:11:37,554:INFO:         yellowbrick: 1.5
2026-02-02 12:11:37,554:INFO:              plotly: 5.24.1
2026-02-02 12:11:37,554:INFO:    plotly-resampler: Not installed
2026-02-02 12:11:37,554:INFO:             kaleido: 1.2.0
2026-02-02 12:11:37,554:INFO:           schemdraw: 0.15
2026-02-02 12:11:37,554:INFO:         statsmodels: 0.14.6
2026-02-02 12:11:37,554:INFO:              sktime: 0.26.0
2026-02-02 12:11:37,554:INFO:               tbats: 1.1.3
2026-02-02 12:11:37,554:INFO:            pmdarima: 2.0.4
2026-02-02 12:11:37,554:INFO:              psutil: 7.2.1
2026-02-02 12:11:37,554:INFO:          markupsafe: 3.0.3
2026-02-02 12:11:37,554:INFO:             pickle5: Not installed
2026-02-02 12:11:37,554:INFO:         cloudpickle: 3.0.0
2026-02-02 12:11:37,554:INFO:         deprecation: 2.1.0
2026-02-02 12:11:37,554:INFO:              xxhash: 3.6.0
2026-02-02 12:11:37,554:INFO:           wurlitzer: Not installed
2026-02-02 12:11:37,554:INFO:PyCaret optional dependencies:
2026-02-02 12:11:37,554:INFO:                shap: 0.44.1
2026-02-02 12:11:37,554:INFO:           interpret: 0.7.3
2026-02-02 12:11:37,554:INFO:                umap: 0.5.7
2026-02-02 12:11:37,554:INFO:     ydata_profiling: 4.18.1
2026-02-02 12:11:37,554:INFO:  explainerdashboard: 0.5.1
2026-02-02 12:11:37,554:INFO:             autoviz: Not installed
2026-02-02 12:11:37,554:INFO:           fairlearn: 0.7.0
2026-02-02 12:11:37,554:INFO:          deepchecks: Not installed
2026-02-02 12:11:37,554:INFO:             xgboost: Not installed
2026-02-02 12:11:37,554:INFO:            catboost: 1.2.8
2026-02-02 12:11:37,554:INFO:              kmodes: 0.12.2
2026-02-02 12:11:37,554:INFO:             mlxtend: 0.23.4
2026-02-02 12:11:37,554:INFO:       statsforecast: 1.5.0
2026-02-02 12:11:37,554:INFO:        tune_sklearn: Not installed
2026-02-02 12:11:37,554:INFO:                 ray: Not installed
2026-02-02 12:11:37,554:INFO:            hyperopt: 0.2.7
2026-02-02 12:11:37,554:INFO:              optuna: 4.6.0
2026-02-02 12:11:37,554:INFO:               skopt: 0.10.2
2026-02-02 12:11:37,554:INFO:              mlflow: 3.8.1
2026-02-02 12:11:37,554:INFO:              gradio: 6.3.0
2026-02-02 12:11:37,554:INFO:             fastapi: 0.128.0
2026-02-02 12:11:37,554:INFO:             uvicorn: 0.40.0
2026-02-02 12:11:37,554:INFO:              m2cgen: 0.10.0
2026-02-02 12:11:37,554:INFO:           evidently: 0.4.40
2026-02-02 12:11:37,554:INFO:               fugue: 0.8.7
2026-02-02 12:11:37,554:INFO:           streamlit: Not installed
2026-02-02 12:11:37,554:INFO:             prophet: Not installed
2026-02-02 12:11:37,554:INFO:None
2026-02-02 12:11:37,554:INFO:Set up data.
2026-02-02 12:11:37,564:INFO:Set up folding strategy.
2026-02-02 12:11:37,564:INFO:Set up train/test split.
2026-02-02 12:11:37,573:INFO:Set up index.
2026-02-02 12:11:37,573:INFO:Assigning column types.
2026-02-02 12:11:37,573:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 12:11:37,608:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:11:37,608:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:11:37,623:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:11:37,623:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:11:37,654:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:11:37,654:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:11:37,674:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:11:37,675:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:11:37,676:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 12:11:37,705:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:11:37,722:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:11:37,722:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:11:37,752:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:11:37,770:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:11:37,771:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:11:37,771:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 12:11:37,804:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:11:37,804:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:11:37,863:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:11:37,864:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:11:37,865:INFO:Preparing preprocessing pipeline...
2026-02-02 12:11:37,867:INFO:Set up simple imputation.
2026-02-02 12:11:37,867:INFO:Set up feature normalization.
2026-02-02 12:11:37,890:INFO:Finished creating preprocessing pipeline.
2026-02-02 12:11:37,890:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2',
                                             'flag_NU_NOTA_MEDIA_1_BACH__PC_na',
                                             'flag_CU_IM...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 12:11:37,890:INFO:Creating final display dataframe.
2026-02-02 12:11:37,990:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 21)
4        Transformed data shape       (10381, 21)
5   Transformed train set shape        (7266, 21)
6    Transformed test set shape        (3115, 21)
7              Numeric features                10
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              27f0
2026-02-02 12:11:38,037:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:11:38,037:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:11:38,070:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:11:38,070:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:11:38,070:INFO:setup() successfully completed in 0.55s...............
2026-02-02 12:11:38,070:INFO:Initializing compare_models()
2026-02-02 12:11:38,070:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 12:11:38,070:INFO:Checking exceptions
2026-02-02 12:11:38,090:INFO:Preparing display monitor
2026-02-02 12:11:38,090:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 12:11:38,090:INFO:Total runtime is 0.0 minutes
2026-02-02 12:11:38,090:INFO:SubProcess create_model() called ==================================
2026-02-02 12:11:38,090:INFO:Initializing create_model()
2026-02-02 12:11:38,090:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228390D2450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:11:38,090:INFO:Checking exceptions
2026-02-02 12:11:38,090:INFO:Importing libraries
2026-02-02 12:11:38,090:INFO:Copying training dataset
2026-02-02 12:11:38,090:INFO:Defining folds
2026-02-02 12:11:38,090:INFO:Declaring metric variables
2026-02-02 12:11:38,090:INFO:Importing untrained model
2026-02-02 12:11:38,090:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:11:38,090:INFO:Starting cross validation
2026-02-02 12:11:38,090:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:11:48,898:INFO:Calculating mean and std
2026-02-02 12:11:48,902:INFO:Creating metrics dataframe
2026-02-02 12:11:48,907:INFO:Uploading results into container
2026-02-02 12:11:48,910:INFO:Uploading model into container now
2026-02-02 12:11:48,911:INFO:_master_model_container: 1
2026-02-02 12:11:48,912:INFO:_display_container: 2
2026-02-02 12:11:48,914:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:11:48,915:INFO:create_model() successfully completed......................................
2026-02-02 12:11:49,896:INFO:SubProcess create_model() end ==================================
2026-02-02 12:11:49,896:INFO:Creating metrics dataframe
2026-02-02 12:11:49,899:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 12:11:49,901:INFO:Initializing create_model()
2026-02-02 12:11:49,901:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:11:49,901:INFO:Checking exceptions
2026-02-02 12:11:49,902:INFO:Importing libraries
2026-02-02 12:11:49,902:INFO:Copying training dataset
2026-02-02 12:11:49,910:INFO:Defining folds
2026-02-02 12:11:49,910:INFO:Declaring metric variables
2026-02-02 12:11:49,910:INFO:Importing untrained model
2026-02-02 12:11:49,910:INFO:Declaring custom model
2026-02-02 12:11:49,911:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:11:49,912:INFO:Cross validation set to False
2026-02-02 12:11:49,912:INFO:Fitting Model
2026-02-02 12:11:49,943:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:11:49,945:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000873 seconds.
2026-02-02 12:11:49,945:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:11:49,945:INFO:[LightGBM] [Info] Total Bins 1107
2026-02-02 12:11:49,945:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 19
2026-02-02 12:11:49,946:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:11:49,946:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:11:50,077:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:11:50,077:INFO:create_model() successfully completed......................................
2026-02-02 12:11:50,311:INFO:_master_model_container: 1
2026-02-02 12:11:50,312:INFO:_display_container: 2
2026-02-02 12:11:50,313:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:11:50,314:INFO:compare_models() successfully completed......................................
2026-02-02 12:11:50,315:INFO:Initializing tune_model()
2026-02-02 12:11:50,315:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:11:50,316:INFO:Checking exceptions
2026-02-02 12:11:50,330:INFO:Copying training dataset
2026-02-02 12:11:50,342:INFO:Checking base model
2026-02-02 12:11:50,343:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 12:11:50,344:INFO:Declaring metric variables
2026-02-02 12:11:50,344:INFO:Defining Hyperparameters
2026-02-02 12:11:50,554:INFO:Tuning with n_jobs=-1
2026-02-02 12:11:50,554:INFO:Initializing RandomizedSearchCV
2026-02-02 12:12:06,869:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 12:12:06,871:INFO:Hyperparameter search completed
2026-02-02 12:12:06,871:INFO:SubProcess create_model() called ==================================
2026-02-02 12:12:06,872:INFO:Initializing create_model()
2026-02-02 12:12:06,872:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228339A7050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 12:12:06,872:INFO:Checking exceptions
2026-02-02 12:12:06,872:INFO:Importing libraries
2026-02-02 12:12:06,873:INFO:Copying training dataset
2026-02-02 12:12:06,884:INFO:Defining folds
2026-02-02 12:12:06,884:INFO:Declaring metric variables
2026-02-02 12:12:06,884:INFO:Importing untrained model
2026-02-02 12:12:06,884:INFO:Declaring custom model
2026-02-02 12:12:06,886:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:12:06,886:INFO:Starting cross validation
2026-02-02 12:12:06,888:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:12:07,514:INFO:Calculating mean and std
2026-02-02 12:12:07,515:INFO:Creating metrics dataframe
2026-02-02 12:12:07,518:INFO:Finalizing model
2026-02-02 12:12:07,546:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:12:07,546:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:12:07,546:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:12:07,552:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:12:07,552:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:12:07,552:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:12:07,552:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:12:07,555:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001554 seconds.
2026-02-02 12:12:07,555:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:12:07,556:INFO:[LightGBM] [Info] Total Bins 1107
2026-02-02 12:12:07,556:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 19
2026-02-02 12:12:07,556:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:12:07,557:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:12:07,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,651:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,709:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:12:07,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:12:07,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,737:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:12:07,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,769:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:12:07,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:12:07,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:12:07,844:INFO:Uploading results into container
2026-02-02 12:12:07,845:INFO:Uploading model into container now
2026-02-02 12:12:07,846:INFO:_master_model_container: 2
2026-02-02 12:12:07,846:INFO:_display_container: 3
2026-02-02 12:12:07,847:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:12:07,847:INFO:create_model() successfully completed......................................
2026-02-02 12:12:08,021:INFO:SubProcess create_model() end ==================================
2026-02-02 12:12:08,021:INFO:choose_better activated
2026-02-02 12:12:08,021:INFO:SubProcess create_model() called ==================================
2026-02-02 12:12:08,022:INFO:Initializing create_model()
2026-02-02 12:12:08,022:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:12:08,022:INFO:Checking exceptions
2026-02-02 12:12:08,023:INFO:Importing libraries
2026-02-02 12:12:08,023:INFO:Copying training dataset
2026-02-02 12:12:08,029:INFO:Defining folds
2026-02-02 12:12:08,029:INFO:Declaring metric variables
2026-02-02 12:12:08,030:INFO:Importing untrained model
2026-02-02 12:12:08,030:INFO:Declaring custom model
2026-02-02 12:12:08,030:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:12:08,031:INFO:Starting cross validation
2026-02-02 12:12:08,031:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:12:08,765:INFO:Calculating mean and std
2026-02-02 12:12:08,766:INFO:Creating metrics dataframe
2026-02-02 12:12:08,767:INFO:Finalizing model
2026-02-02 12:12:08,798:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:12:08,800:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001345 seconds.
2026-02-02 12:12:08,800:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:12:08,801:INFO:[LightGBM] [Info] Total Bins 1107
2026-02-02 12:12:08,801:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 19
2026-02-02 12:12:08,801:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:12:08,801:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:12:08,954:INFO:Uploading results into container
2026-02-02 12:12:08,954:INFO:Uploading model into container now
2026-02-02 12:12:08,955:INFO:_master_model_container: 3
2026-02-02 12:12:08,955:INFO:_display_container: 4
2026-02-02 12:12:08,956:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:12:08,956:INFO:create_model() successfully completed......................................
2026-02-02 12:12:09,129:INFO:SubProcess create_model() end ==================================
2026-02-02 12:12:09,130:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9977
2026-02-02 12:12:09,130:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9947
2026-02-02 12:12:09,131:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 12:12:09,131:INFO:choose_better completed
2026-02-02 12:12:09,131:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:12:09,133:INFO:_master_model_container: 3
2026-02-02 12:12:09,133:INFO:_display_container: 3
2026-02-02 12:12:09,134:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:12:09,134:INFO:tune_model() successfully completed......................................
2026-02-02 12:12:09,267:INFO:Initializing predict_model()
2026-02-02 12:12:09,267:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D66EB2E0>)
2026-02-02 12:12:09,267:INFO:Checking exceptions
2026-02-02 12:12:09,267:INFO:Preloading libraries
2026-02-02 12:12:09,267:INFO:Set up data.
2026-02-02 12:12:09,269:INFO:Set up index.
2026-02-02 12:12:11,822:INFO:Initializing plot_model()
2026-02-02 12:12:11,822:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:12:11,822:INFO:Checking exceptions
2026-02-02 12:12:11,825:INFO:Preloading libraries
2026-02-02 12:12:11,832:INFO:Copying training dataset
2026-02-02 12:12:11,832:INFO:Plot type: feature
2026-02-02 12:12:11,833:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:12:12,018:INFO:Visual Rendered Successfully
2026-02-02 12:12:12,205:INFO:plot_model() successfully completed......................................
2026-02-02 12:12:12,206:INFO:Initializing plot_model()
2026-02-02 12:12:12,206:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228386E0990>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:12:12,206:INFO:Checking exceptions
2026-02-02 12:12:12,210:INFO:Preloading libraries
2026-02-02 12:12:12,213:INFO:Copying training dataset
2026-02-02 12:12:12,214:INFO:Plot type: feature_all
2026-02-02 12:12:12,262:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:12:12,473:INFO:Visual Rendered Successfully
2026-02-02 12:12:12,633:INFO:plot_model() successfully completed......................................
2026-02-02 12:12:12,637:INFO:Initializing save_model()
2026-02-02 12:12:12,638:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2',
                                             'flag_NU_NOTA_MEDIA_1_BACH__PC_na',
                                             'flag_CU_IM...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 12:12:12,638:INFO:Adding model into prep_pipe
2026-02-02 12:12:12,648:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-02 12:12:12,655:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2',
                                             'flag_NU_NOTA_MEDIA_1_BACH__PC_na',
                                             'flag_CU_IMPORTE_TOTAL_na'],
                                    transformer=SimpleImputer...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-02 12:12:12,656:INFO:save_model() successfully completed......................................
2026-02-02 12:14:14,035:INFO:Initializing load_model()
2026-02-02 12:14:14,036:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_master, platform=None, authentication=None, verbose=True)
2026-02-02 12:14:16,153:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\3167323388.py:25: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-02-02 12:14:16,868:INFO:Initializing predict_model()
2026-02-02 12:14:16,868:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000214BC3CBD10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2',
                                             'flag_NU_NOTA_MEDIA_1_BACH__PC_na',
                                             'flag_CU_IMPORTE_TOTAL_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', LGBMClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021407598860>)
2026-02-02 12:14:16,868:INFO:Checking exceptions
2026-02-02 12:14:16,868:INFO:Preloading libraries
2026-02-02 12:14:16,868:INFO:Set up data.
2026-02-02 12:14:16,907:INFO:Set up index.
2026-02-02 12:15:20,365:INFO:Initializing load_model()
2026-02-02 12:15:20,365:INFO:load_model(model_name=..\datos\04. Modelos\modelo_final_master, platform=None, authentication=None, verbose=True)
2026-02-02 12:15:22,458:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\1569962658.py:25: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-02-02 12:15:23,100:INFO:Initializing predict_model()
2026-02-02 12:15:23,100:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000021420DD2610>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2',
                                             'flag_NU_NOTA_MEDIA_1_BACH__PC_na',
                                             'flag_CU_IMPORTE_TOTAL_na'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('normalize', TransformerWrapper(transformer=StandardScaler())),
                ('trained_model', LGBMClassifier(n_jobs=-1, random_state=42))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000021468F00220>)
2026-02-02 12:15:23,100:INFO:Checking exceptions
2026-02-02 12:15:23,101:INFO:Preloading libraries
2026-02-02 12:15:23,101:INFO:Set up data.
2026-02-02 12:15:23,164:INFO:Set up index.
2026-02-02 12:15:23,417:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\1569962658.py:134: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-02-02 12:15:23,596:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_6924\1569962658.py:134: FutureWarning: 

Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.

  sns.barplot(data=df_plot, x=metric, y=col_name, palette="viridis")

2026-02-02 12:33:14,298:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\1247590406.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 12:33:16,990:INFO:PyCaret ClassificationExperiment
2026-02-02 12:33:16,991:INFO:Logging name: clf-default-name
2026-02-02 12:33:16,991:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 12:33:16,991:INFO:version 3.3.2
2026-02-02 12:33:16,992:INFO:Initializing setup()
2026-02-02 12:33:16,992:INFO:self.USI: 2f4f
2026-02-02 12:33:16,992:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 12:33:16,992:INFO:Checking environment
2026-02-02 12:33:16,993:INFO:python_version: 3.11.11
2026-02-02 12:33:16,993:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 12:33:16,993:INFO:machine: AMD64
2026-02-02 12:33:16,993:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 12:33:16,994:INFO:Memory: svmem(total=34009374720, available=14672113664, percent=56.9, used=19337261056, free=14672113664)
2026-02-02 12:33:16,994:INFO:Physical Core: 12
2026-02-02 12:33:16,994:INFO:Logical Core: 16
2026-02-02 12:33:16,994:INFO:Checking libraries
2026-02-02 12:33:16,995:INFO:System:
2026-02-02 12:33:16,995:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 12:33:16,995:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 12:33:16,995:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 12:33:16,996:INFO:PyCaret required dependencies:
2026-02-02 12:33:16,996:INFO:                 pip: 25.0
2026-02-02 12:33:16,997:INFO:          setuptools: 75.8.0
2026-02-02 12:33:16,997:INFO:             pycaret: 3.3.2
2026-02-02 12:33:16,998:INFO:             IPython: 9.9.0
2026-02-02 12:33:17,000:INFO:          ipywidgets: 8.1.8
2026-02-02 12:33:17,000:INFO:                tqdm: 4.67.1
2026-02-02 12:33:17,000:INFO:               numpy: 1.26.4
2026-02-02 12:33:17,000:INFO:              pandas: 2.1.4
2026-02-02 12:33:17,000:INFO:              jinja2: 3.1.6
2026-02-02 12:33:17,000:INFO:               scipy: 1.11.4
2026-02-02 12:33:17,000:INFO:              joblib: 1.3.2
2026-02-02 12:33:17,000:INFO:             sklearn: 1.4.2
2026-02-02 12:33:17,000:INFO:                pyod: 2.0.6
2026-02-02 12:33:17,000:INFO:            imblearn: 0.14.1
2026-02-02 12:33:17,001:INFO:   category_encoders: 2.7.0
2026-02-02 12:33:17,001:INFO:            lightgbm: 4.6.0
2026-02-02 12:33:17,001:INFO:               numba: 0.62.1
2026-02-02 12:33:17,001:INFO:            requests: 2.32.3
2026-02-02 12:33:17,001:INFO:          matplotlib: 3.7.5
2026-02-02 12:33:17,001:INFO:          scikitplot: 0.3.7
2026-02-02 12:33:17,001:INFO:         yellowbrick: 1.5
2026-02-02 12:33:17,001:INFO:              plotly: 5.24.1
2026-02-02 12:33:17,001:INFO:    plotly-resampler: Not installed
2026-02-02 12:33:17,002:INFO:             kaleido: 1.2.0
2026-02-02 12:33:17,002:INFO:           schemdraw: 0.15
2026-02-02 12:33:17,002:INFO:         statsmodels: 0.14.6
2026-02-02 12:33:17,002:INFO:              sktime: 0.26.0
2026-02-02 12:33:17,002:INFO:               tbats: 1.1.3
2026-02-02 12:33:17,002:INFO:            pmdarima: 2.0.4
2026-02-02 12:33:17,002:INFO:              psutil: 7.2.1
2026-02-02 12:33:17,002:INFO:          markupsafe: 3.0.3
2026-02-02 12:33:17,002:INFO:             pickle5: Not installed
2026-02-02 12:33:17,002:INFO:         cloudpickle: 3.0.0
2026-02-02 12:33:17,002:INFO:         deprecation: 2.1.0
2026-02-02 12:33:17,003:INFO:              xxhash: 3.6.0
2026-02-02 12:33:17,003:INFO:           wurlitzer: Not installed
2026-02-02 12:33:17,003:INFO:PyCaret optional dependencies:
2026-02-02 12:33:17,003:INFO:                shap: 0.44.1
2026-02-02 12:33:17,003:INFO:           interpret: 0.7.3
2026-02-02 12:33:17,003:INFO:                umap: 0.5.7
2026-02-02 12:33:17,004:INFO:     ydata_profiling: 4.18.1
2026-02-02 12:33:17,004:INFO:  explainerdashboard: 0.5.1
2026-02-02 12:33:17,004:INFO:             autoviz: Not installed
2026-02-02 12:33:17,004:INFO:           fairlearn: 0.7.0
2026-02-02 12:33:17,005:INFO:          deepchecks: Not installed
2026-02-02 12:33:17,005:INFO:             xgboost: Not installed
2026-02-02 12:33:17,005:INFO:            catboost: 1.2.8
2026-02-02 12:33:17,005:INFO:              kmodes: 0.12.2
2026-02-02 12:33:17,005:INFO:             mlxtend: 0.23.4
2026-02-02 12:33:17,005:INFO:       statsforecast: 1.5.0
2026-02-02 12:33:17,006:INFO:        tune_sklearn: Not installed
2026-02-02 12:33:17,006:INFO:                 ray: Not installed
2026-02-02 12:33:17,006:INFO:            hyperopt: 0.2.7
2026-02-02 12:33:17,007:INFO:              optuna: 4.6.0
2026-02-02 12:33:17,007:INFO:               skopt: 0.10.2
2026-02-02 12:33:17,007:INFO:              mlflow: 3.8.1
2026-02-02 12:33:17,007:INFO:              gradio: 6.3.0
2026-02-02 12:33:17,007:INFO:             fastapi: 0.128.0
2026-02-02 12:33:17,008:INFO:             uvicorn: 0.40.0
2026-02-02 12:33:17,008:INFO:              m2cgen: 0.10.0
2026-02-02 12:33:17,008:INFO:           evidently: 0.4.40
2026-02-02 12:33:17,009:INFO:               fugue: 0.8.7
2026-02-02 12:33:17,009:INFO:           streamlit: Not installed
2026-02-02 12:33:17,009:INFO:             prophet: Not installed
2026-02-02 12:33:17,009:INFO:None
2026-02-02 12:33:17,009:INFO:Set up data.
2026-02-02 12:33:17,106:INFO:Set up folding strategy.
2026-02-02 12:33:17,106:INFO:Set up train/test split.
2026-02-02 12:33:17,284:INFO:Set up index.
2026-02-02 12:33:17,292:INFO:Assigning column types.
2026-02-02 12:33:17,394:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 12:33:17,430:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:33:17,431:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:33:17,455:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:33:17,455:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:33:17,492:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:33:17,493:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:33:17,517:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:33:17,517:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:33:17,518:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 12:33:17,556:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:33:17,579:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:33:17,579:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:33:17,618:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:33:17,641:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:33:17,641:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:33:17,642:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 12:33:17,700:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:33:17,701:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:33:17,761:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:33:17,761:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:33:17,763:INFO:Preparing preprocessing pipeline...
2026-02-02 12:33:17,782:INFO:Set up simple imputation.
2026-02-02 12:33:17,782:INFO:Set up feature normalization.
2026-02-02 12:33:18,258:INFO:Finished creating preprocessing pipeline.
2026-02-02 12:33:18,264:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 12:33:18,264:INFO:Creating final display dataframe.
2026-02-02 12:33:19,249:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 17)
4        Transformed data shape      (373023, 17)
5   Transformed train set shape      (261116, 17)
6    Transformed test set shape      (111907, 17)
7              Numeric features                13
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              2f4f
2026-02-02 12:33:19,310:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:33:19,310:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:33:19,373:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:33:19,373:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:33:19,375:INFO:setup() successfully completed in 2.41s...............
2026-02-02 12:33:19,376:INFO:Initializing compare_models()
2026-02-02 12:33:19,376:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 12:33:19,376:INFO:Checking exceptions
2026-02-02 12:33:19,455:INFO:Preparing display monitor
2026-02-02 12:33:19,459:INFO:Initializing Logistic Regression
2026-02-02 12:33:19,459:INFO:Total runtime is 0.0 minutes
2026-02-02 12:33:19,460:INFO:SubProcess create_model() called ==================================
2026-02-02 12:33:19,460:INFO:Initializing create_model()
2026-02-02 12:33:19,460:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D4272450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:33:19,460:INFO:Checking exceptions
2026-02-02 12:33:19,460:INFO:Importing libraries
2026-02-02 12:33:19,460:INFO:Copying training dataset
2026-02-02 12:33:19,590:INFO:Defining folds
2026-02-02 12:33:19,590:INFO:Declaring metric variables
2026-02-02 12:33:19,591:INFO:Importing untrained model
2026-02-02 12:33:19,591:INFO:Logistic Regression Imported successfully
2026-02-02 12:33:19,591:INFO:Starting cross validation
2026-02-02 12:33:19,592:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:33:26,611:INFO:Calculating mean and std
2026-02-02 12:33:26,615:INFO:Creating metrics dataframe
2026-02-02 12:33:26,617:INFO:Uploading results into container
2026-02-02 12:33:26,617:INFO:Uploading model into container now
2026-02-02 12:33:26,617:INFO:_master_model_container: 1
2026-02-02 12:33:26,617:INFO:_display_container: 2
2026-02-02 12:33:26,618:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 12:33:26,618:INFO:create_model() successfully completed......................................
2026-02-02 12:33:26,759:INFO:SubProcess create_model() end ==================================
2026-02-02 12:33:26,759:INFO:Creating metrics dataframe
2026-02-02 12:33:26,761:INFO:Initializing Decision Tree Classifier
2026-02-02 12:33:26,761:INFO:Total runtime is 0.12170821825663249 minutes
2026-02-02 12:33:26,761:INFO:SubProcess create_model() called ==================================
2026-02-02 12:33:26,761:INFO:Initializing create_model()
2026-02-02 12:33:26,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D4272450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:33:26,762:INFO:Checking exceptions
2026-02-02 12:33:26,762:INFO:Importing libraries
2026-02-02 12:33:26,762:INFO:Copying training dataset
2026-02-02 12:33:26,864:INFO:Defining folds
2026-02-02 12:33:26,864:INFO:Declaring metric variables
2026-02-02 12:33:26,865:INFO:Importing untrained model
2026-02-02 12:33:26,865:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:33:26,865:INFO:Starting cross validation
2026-02-02 12:33:26,866:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:33:32,288:INFO:Calculating mean and std
2026-02-02 12:33:32,291:INFO:Creating metrics dataframe
2026-02-02 12:33:32,293:INFO:Uploading results into container
2026-02-02 12:33:32,293:INFO:Uploading model into container now
2026-02-02 12:33:32,294:INFO:_master_model_container: 2
2026-02-02 12:33:32,294:INFO:_display_container: 2
2026-02-02 12:33:32,295:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:33:32,295:INFO:create_model() successfully completed......................................
2026-02-02 12:33:32,425:INFO:SubProcess create_model() end ==================================
2026-02-02 12:33:32,425:INFO:Creating metrics dataframe
2026-02-02 12:33:32,427:INFO:Initializing Random Forest Classifier
2026-02-02 12:33:32,427:INFO:Total runtime is 0.2161434769630432 minutes
2026-02-02 12:33:32,427:INFO:SubProcess create_model() called ==================================
2026-02-02 12:33:32,427:INFO:Initializing create_model()
2026-02-02 12:33:32,427:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D4272450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:33:32,427:INFO:Checking exceptions
2026-02-02 12:33:32,427:INFO:Importing libraries
2026-02-02 12:33:32,428:INFO:Copying training dataset
2026-02-02 12:33:32,522:INFO:Defining folds
2026-02-02 12:33:32,523:INFO:Declaring metric variables
2026-02-02 12:33:32,523:INFO:Importing untrained model
2026-02-02 12:33:32,523:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:33:32,523:INFO:Starting cross validation
2026-02-02 12:33:32,524:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:33:48,841:INFO:Calculating mean and std
2026-02-02 12:33:48,842:INFO:Creating metrics dataframe
2026-02-02 12:33:48,843:INFO:Uploading results into container
2026-02-02 12:33:48,844:INFO:Uploading model into container now
2026-02-02 12:33:48,844:INFO:_master_model_container: 3
2026-02-02 12:33:48,844:INFO:_display_container: 2
2026-02-02 12:33:48,845:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:33:48,845:INFO:create_model() successfully completed......................................
2026-02-02 12:33:48,995:INFO:SubProcess create_model() end ==================================
2026-02-02 12:33:48,995:INFO:Creating metrics dataframe
2026-02-02 12:33:48,996:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 12:33:48,997:INFO:Total runtime is 0.4922961354255676 minutes
2026-02-02 12:33:48,997:INFO:SubProcess create_model() called ==================================
2026-02-02 12:33:48,997:INFO:Initializing create_model()
2026-02-02 12:33:48,997:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D4272450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:33:48,997:INFO:Checking exceptions
2026-02-02 12:33:48,997:INFO:Importing libraries
2026-02-02 12:33:48,997:INFO:Copying training dataset
2026-02-02 12:33:49,102:INFO:Defining folds
2026-02-02 12:33:49,102:INFO:Declaring metric variables
2026-02-02 12:33:49,103:INFO:Importing untrained model
2026-02-02 12:33:49,103:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:33:49,104:INFO:Starting cross validation
2026-02-02 12:33:49,104:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:33:56,828:INFO:Calculating mean and std
2026-02-02 12:33:56,831:INFO:Creating metrics dataframe
2026-02-02 12:33:56,833:INFO:Uploading results into container
2026-02-02 12:33:56,833:INFO:Uploading model into container now
2026-02-02 12:33:56,834:INFO:_master_model_container: 4
2026-02-02 12:33:56,835:INFO:_display_container: 2
2026-02-02 12:33:56,836:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:33:56,836:INFO:create_model() successfully completed......................................
2026-02-02 12:33:56,994:INFO:SubProcess create_model() end ==================================
2026-02-02 12:33:56,995:INFO:Creating metrics dataframe
2026-02-02 12:33:56,997:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 12:33:56,998:INFO:Initializing create_model()
2026-02-02 12:33:56,998:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:33:56,999:INFO:Checking exceptions
2026-02-02 12:33:57,000:INFO:Importing libraries
2026-02-02 12:33:57,000:INFO:Copying training dataset
2026-02-02 12:33:57,109:INFO:Defining folds
2026-02-02 12:33:57,109:INFO:Declaring metric variables
2026-02-02 12:33:57,109:INFO:Importing untrained model
2026-02-02 12:33:57,109:INFO:Declaring custom model
2026-02-02 12:33:57,109:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:33:57,110:INFO:Cross validation set to False
2026-02-02 12:33:57,110:INFO:Fitting Model
2026-02-02 12:34:03,394:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:34:03,395:INFO:create_model() successfully completed......................................
2026-02-02 12:34:03,570:INFO:Initializing create_model()
2026-02-02 12:34:03,570:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:34:03,570:INFO:Checking exceptions
2026-02-02 12:34:03,571:INFO:Importing libraries
2026-02-02 12:34:03,572:INFO:Copying training dataset
2026-02-02 12:34:03,692:INFO:Defining folds
2026-02-02 12:34:03,692:INFO:Declaring metric variables
2026-02-02 12:34:03,692:INFO:Importing untrained model
2026-02-02 12:34:03,692:INFO:Declaring custom model
2026-02-02 12:34:03,693:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:34:03,694:INFO:Cross validation set to False
2026-02-02 12:34:03,694:INFO:Fitting Model
2026-02-02 12:34:04,181:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 12:34:04,195:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005440 seconds.
2026-02-02 12:34:04,195:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 12:34:04,195:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 12:34:04,195:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 12:34:04,196:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 12:34:04,198:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 12:34:04,198:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 12:34:04,771:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:34:04,772:INFO:create_model() successfully completed......................................
2026-02-02 12:34:04,965:INFO:Initializing create_model()
2026-02-02 12:34:04,965:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:34:04,965:INFO:Checking exceptions
2026-02-02 12:34:04,966:INFO:Importing libraries
2026-02-02 12:34:04,966:INFO:Copying training dataset
2026-02-02 12:34:05,085:INFO:Defining folds
2026-02-02 12:34:05,085:INFO:Declaring metric variables
2026-02-02 12:34:05,086:INFO:Importing untrained model
2026-02-02 12:34:05,086:INFO:Declaring custom model
2026-02-02 12:34:05,086:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:34:05,088:INFO:Cross validation set to False
2026-02-02 12:34:05,088:INFO:Fitting Model
2026-02-02 12:34:07,094:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:34:07,094:INFO:create_model() successfully completed......................................
2026-02-02 12:34:07,265:INFO:_master_model_container: 4
2026-02-02 12:34:07,265:INFO:_display_container: 2
2026-02-02 12:34:07,267:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-02-02 12:34:07,267:INFO:compare_models() successfully completed......................................
2026-02-02 12:34:07,278:INFO:Initializing tune_model()
2026-02-02 12:34:07,278:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:34:07,278:INFO:Checking exceptions
2026-02-02 12:34:07,328:INFO:Copying training dataset
2026-02-02 12:34:07,413:INFO:Checking base model
2026-02-02 12:34:07,413:INFO:Base model : Random Forest Classifier
2026-02-02 12:34:07,413:INFO:Declaring metric variables
2026-02-02 12:34:07,413:INFO:Defining Hyperparameters
2026-02-02 12:34:07,579:INFO:Tuning with n_jobs=-1
2026-02-02 12:34:07,580:INFO:Initializing RandomizedSearchCV
2026-02-02 12:35:52,211:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 12:35:52,212:INFO:Hyperparameter search completed
2026-02-02 12:35:52,213:INFO:SubProcess create_model() called ==================================
2026-02-02 12:35:52,213:INFO:Initializing create_model()
2026-02-02 12:35:52,214:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D4272550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-02 12:35:52,214:INFO:Checking exceptions
2026-02-02 12:35:52,214:INFO:Importing libraries
2026-02-02 12:35:52,214:INFO:Copying training dataset
2026-02-02 12:35:52,389:INFO:Defining folds
2026-02-02 12:35:52,390:INFO:Declaring metric variables
2026-02-02 12:35:52,390:INFO:Importing untrained model
2026-02-02 12:35:52,390:INFO:Declaring custom model
2026-02-02 12:35:52,391:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:35:52,391:INFO:Starting cross validation
2026-02-02 12:35:52,392:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:36:16,673:INFO:Calculating mean and std
2026-02-02 12:36:16,675:INFO:Creating metrics dataframe
2026-02-02 12:36:16,676:INFO:Finalizing model
2026-02-02 12:36:29,221:INFO:Uploading results into container
2026-02-02 12:36:29,222:INFO:Uploading model into container now
2026-02-02 12:36:29,222:INFO:_master_model_container: 5
2026-02-02 12:36:29,223:INFO:_display_container: 3
2026-02-02 12:36:29,223:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:36:29,223:INFO:create_model() successfully completed......................................
2026-02-02 12:36:29,410:INFO:SubProcess create_model() end ==================================
2026-02-02 12:36:29,411:INFO:choose_better activated
2026-02-02 12:36:29,411:INFO:SubProcess create_model() called ==================================
2026-02-02 12:36:29,412:INFO:Initializing create_model()
2026-02-02 12:36:29,413:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:36:29,413:INFO:Checking exceptions
2026-02-02 12:36:29,414:INFO:Importing libraries
2026-02-02 12:36:29,414:INFO:Copying training dataset
2026-02-02 12:36:29,541:INFO:Defining folds
2026-02-02 12:36:29,541:INFO:Declaring metric variables
2026-02-02 12:36:29,542:INFO:Importing untrained model
2026-02-02 12:36:29,542:INFO:Declaring custom model
2026-02-02 12:36:29,543:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:36:29,543:INFO:Starting cross validation
2026-02-02 12:36:29,544:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:36:47,706:INFO:Calculating mean and std
2026-02-02 12:36:47,707:INFO:Creating metrics dataframe
2026-02-02 12:36:47,710:INFO:Finalizing model
2026-02-02 12:36:56,617:INFO:Uploading results into container
2026-02-02 12:36:56,619:INFO:Uploading model into container now
2026-02-02 12:36:56,620:INFO:_master_model_container: 6
2026-02-02 12:36:56,620:INFO:_display_container: 4
2026-02-02 12:36:56,621:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:36:56,622:INFO:create_model() successfully completed......................................
2026-02-02 12:36:56,829:INFO:SubProcess create_model() end ==================================
2026-02-02 12:36:56,829:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9956
2026-02-02 12:36:56,830:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9648
2026-02-02 12:36:56,830:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 12:36:56,830:INFO:choose_better completed
2026-02-02 12:36:56,830:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:36:56,834:INFO:_master_model_container: 6
2026-02-02 12:36:56,834:INFO:_display_container: 3
2026-02-02 12:36:56,835:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:36:56,835:INFO:tune_model() successfully completed......................................
2026-02-02 12:36:57,030:INFO:Initializing tune_model()
2026-02-02 12:36:57,031:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:36:57,031:INFO:Checking exceptions
2026-02-02 12:36:57,096:INFO:Copying training dataset
2026-02-02 12:36:57,202:INFO:Checking base model
2026-02-02 12:36:57,203:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 12:36:57,204:INFO:Declaring metric variables
2026-02-02 12:36:57,204:INFO:Defining Hyperparameters
2026-02-02 12:36:57,411:INFO:Tuning with n_jobs=-1
2026-02-02 12:36:57,411:INFO:Initializing RandomizedSearchCV
2026-02-02 12:37:30,900:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 12:37:30,902:INFO:Hyperparameter search completed
2026-02-02 12:37:30,902:INFO:SubProcess create_model() called ==================================
2026-02-02 12:37:30,904:INFO:Initializing create_model()
2026-02-02 12:37:30,904:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022866A04910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 12:37:30,904:INFO:Checking exceptions
2026-02-02 12:37:30,904:INFO:Importing libraries
2026-02-02 12:37:30,904:INFO:Copying training dataset
2026-02-02 12:37:31,074:INFO:Defining folds
2026-02-02 12:37:31,075:INFO:Declaring metric variables
2026-02-02 12:37:31,075:INFO:Importing untrained model
2026-02-02 12:37:31,075:INFO:Declaring custom model
2026-02-02 12:37:31,077:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:37:31,077:INFO:Starting cross validation
2026-02-02 12:37:31,078:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:37:39,410:INFO:Calculating mean and std
2026-02-02 12:37:39,413:INFO:Creating metrics dataframe
2026-02-02 12:37:39,418:INFO:Finalizing model
2026-02-02 12:37:39,806:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:37:39,806:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:37:39,806:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:37:39,918:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:37:39,919:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:37:39,919:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:37:39,919:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 12:37:39,930:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004793 seconds.
2026-02-02 12:37:39,930:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 12:37:39,930:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 12:37:39,931:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 12:37:39,931:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 12:37:39,935:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 12:37:39,935:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 12:37:42,259:INFO:Uploading results into container
2026-02-02 12:37:42,261:INFO:Uploading model into container now
2026-02-02 12:37:42,262:INFO:_master_model_container: 7
2026-02-02 12:37:42,263:INFO:_display_container: 4
2026-02-02 12:37:42,264:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:37:42,266:INFO:create_model() successfully completed......................................
2026-02-02 12:37:42,499:INFO:SubProcess create_model() end ==================================
2026-02-02 12:37:42,500:INFO:choose_better activated
2026-02-02 12:37:42,501:INFO:SubProcess create_model() called ==================================
2026-02-02 12:37:42,502:INFO:Initializing create_model()
2026-02-02 12:37:42,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:37:42,502:INFO:Checking exceptions
2026-02-02 12:37:42,503:INFO:Importing libraries
2026-02-02 12:37:42,503:INFO:Copying training dataset
2026-02-02 12:37:42,656:INFO:Defining folds
2026-02-02 12:37:42,656:INFO:Declaring metric variables
2026-02-02 12:37:42,657:INFO:Importing untrained model
2026-02-02 12:37:42,657:INFO:Declaring custom model
2026-02-02 12:37:42,658:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:37:42,658:INFO:Starting cross validation
2026-02-02 12:37:42,659:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:37:46,756:INFO:Calculating mean and std
2026-02-02 12:37:46,758:INFO:Creating metrics dataframe
2026-02-02 12:37:46,763:INFO:Finalizing model
2026-02-02 12:37:47,269:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 12:37:47,280:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004375 seconds.
2026-02-02 12:37:47,281:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 12:37:47,281:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 12:37:47,281:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 12:37:47,282:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 12:37:47,284:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 12:37:47,284:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 12:37:48,008:INFO:Uploading results into container
2026-02-02 12:37:48,009:INFO:Uploading model into container now
2026-02-02 12:37:48,010:INFO:_master_model_container: 8
2026-02-02 12:37:48,011:INFO:_display_container: 5
2026-02-02 12:37:48,012:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:37:48,012:INFO:create_model() successfully completed......................................
2026-02-02 12:37:48,242:INFO:SubProcess create_model() end ==================================
2026-02-02 12:37:48,243:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9767
2026-02-02 12:37:48,244:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9928
2026-02-02 12:37:48,244:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 12:37:48,245:INFO:choose_better completed
2026-02-02 12:37:48,247:INFO:_master_model_container: 8
2026-02-02 12:37:48,248:INFO:_display_container: 4
2026-02-02 12:37:48,251:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:37:48,251:INFO:tune_model() successfully completed......................................
2026-02-02 12:37:48,433:INFO:Initializing tune_model()
2026-02-02 12:37:48,434:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:37:48,434:INFO:Checking exceptions
2026-02-02 12:37:48,481:INFO:Copying training dataset
2026-02-02 12:37:48,568:INFO:Checking base model
2026-02-02 12:37:48,568:INFO:Base model : Decision Tree Classifier
2026-02-02 12:37:48,569:INFO:Declaring metric variables
2026-02-02 12:37:48,569:INFO:Defining Hyperparameters
2026-02-02 12:37:48,736:INFO:Tuning with n_jobs=-1
2026-02-02 12:37:48,737:INFO:Initializing RandomizedSearchCV
2026-02-02 12:37:54,197:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-02-02 12:37:54,200:INFO:Hyperparameter search completed
2026-02-02 12:37:54,201:INFO:SubProcess create_model() called ==================================
2026-02-02 12:37:54,203:INFO:Initializing create_model()
2026-02-02 12:37:54,204:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228390B8DD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-02-02 12:37:54,204:INFO:Checking exceptions
2026-02-02 12:37:54,204:INFO:Importing libraries
2026-02-02 12:37:54,205:INFO:Copying training dataset
2026-02-02 12:37:54,411:INFO:Defining folds
2026-02-02 12:37:54,411:INFO:Declaring metric variables
2026-02-02 12:37:54,412:INFO:Importing untrained model
2026-02-02 12:37:54,412:INFO:Declaring custom model
2026-02-02 12:37:54,413:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:37:54,413:INFO:Starting cross validation
2026-02-02 12:37:54,415:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:37:56,665:INFO:Calculating mean and std
2026-02-02 12:37:56,670:INFO:Creating metrics dataframe
2026-02-02 12:37:56,676:INFO:Finalizing model
2026-02-02 12:37:58,177:INFO:Uploading results into container
2026-02-02 12:37:58,178:INFO:Uploading model into container now
2026-02-02 12:37:58,178:INFO:_master_model_container: 9
2026-02-02 12:37:58,178:INFO:_display_container: 5
2026-02-02 12:37:58,179:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:37:58,179:INFO:create_model() successfully completed......................................
2026-02-02 12:37:58,377:INFO:SubProcess create_model() end ==================================
2026-02-02 12:37:58,377:INFO:choose_better activated
2026-02-02 12:37:58,377:INFO:SubProcess create_model() called ==================================
2026-02-02 12:37:58,378:INFO:Initializing create_model()
2026-02-02 12:37:58,378:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:37:58,378:INFO:Checking exceptions
2026-02-02 12:37:58,379:INFO:Importing libraries
2026-02-02 12:37:58,379:INFO:Copying training dataset
2026-02-02 12:37:58,507:INFO:Defining folds
2026-02-02 12:37:58,507:INFO:Declaring metric variables
2026-02-02 12:37:58,507:INFO:Importing untrained model
2026-02-02 12:37:58,507:INFO:Declaring custom model
2026-02-02 12:37:58,508:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:37:58,508:INFO:Starting cross validation
2026-02-02 12:37:58,509:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:38:01,577:INFO:Calculating mean and std
2026-02-02 12:38:01,578:INFO:Creating metrics dataframe
2026-02-02 12:38:01,581:INFO:Finalizing model
2026-02-02 12:38:03,693:INFO:Uploading results into container
2026-02-02 12:38:03,694:INFO:Uploading model into container now
2026-02-02 12:38:03,694:INFO:_master_model_container: 10
2026-02-02 12:38:03,694:INFO:_display_container: 6
2026-02-02 12:38:03,695:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:38:03,695:INFO:create_model() successfully completed......................................
2026-02-02 12:38:03,861:INFO:SubProcess create_model() end ==================================
2026-02-02 12:38:03,862:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9732
2026-02-02 12:38:03,862:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9559
2026-02-02 12:38:03,862:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 12:38:03,862:INFO:choose_better completed
2026-02-02 12:38:03,862:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:38:03,865:INFO:_master_model_container: 10
2026-02-02 12:38:03,865:INFO:_display_container: 5
2026-02-02 12:38:03,865:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:38:03,865:INFO:tune_model() successfully completed......................................
2026-02-02 12:38:04,047:INFO:Initializing predict_model()
2026-02-02 12:38:04,048:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002283399ED40>)
2026-02-02 12:38:04,048:INFO:Checking exceptions
2026-02-02 12:38:04,048:INFO:Preloading libraries
2026-02-02 12:38:04,048:INFO:Set up data.
2026-02-02 12:38:04,063:INFO:Set up index.
2026-02-02 12:38:04,945:INFO:Initializing predict_model()
2026-02-02 12:38:04,945:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002283399ED40>)
2026-02-02 12:38:04,945:INFO:Checking exceptions
2026-02-02 12:38:04,945:INFO:Preloading libraries
2026-02-02 12:38:04,946:INFO:Set up data.
2026-02-02 12:38:04,961:INFO:Set up index.
2026-02-02 12:38:05,777:INFO:Initializing predict_model()
2026-02-02 12:38:05,777:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002283399ED40>)
2026-02-02 12:38:05,778:INFO:Checking exceptions
2026-02-02 12:38:05,778:INFO:Preloading libraries
2026-02-02 12:38:05,778:INFO:Set up data.
2026-02-02 12:38:05,794:INFO:Set up index.
2026-02-02 12:38:06,104:INFO:Initializing predict_model()
2026-02-02 12:38:06,104:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022902FB96C0>)
2026-02-02 12:38:06,104:INFO:Checking exceptions
2026-02-02 12:38:06,104:INFO:Preloading libraries
2026-02-02 12:38:06,104:INFO:Set up data.
2026-02-02 12:38:06,118:INFO:Set up index.
2026-02-02 12:38:06,887:INFO:Initializing plot_model()
2026-02-02 12:38:06,887:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:38:06,887:INFO:Checking exceptions
2026-02-02 12:38:06,985:INFO:Preloading libraries
2026-02-02 12:38:07,114:INFO:Copying training dataset
2026-02-02 12:38:07,114:INFO:Plot type: feature
2026-02-02 12:38:07,115:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:38:07,424:INFO:Visual Rendered Successfully
2026-02-02 12:38:07,597:INFO:plot_model() successfully completed......................................
2026-02-02 12:38:07,608:INFO:Initializing plot_model()
2026-02-02 12:38:07,608:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022901306850>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:38:07,608:INFO:Checking exceptions
2026-02-02 12:38:07,694:INFO:Preloading libraries
2026-02-02 12:38:07,802:INFO:Copying training dataset
2026-02-02 12:38:07,803:INFO:Plot type: feature_all
2026-02-02 12:38:07,923:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:38:08,261:INFO:Visual Rendered Successfully
2026-02-02 12:38:08,424:INFO:plot_model() successfully completed......................................
2026-02-02 12:38:08,436:INFO:Initializing save_model()
2026-02-02 12:38:08,436:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 12:38:08,436:INFO:Adding model into prep_pipe
2026-02-02 12:38:08,618:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-02 12:38:08,631:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-02 12:38:08,631:INFO:save_model() successfully completed......................................
2026-02-02 12:39:19,788:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\2885474599.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 12:39:20,657:INFO:PyCaret ClassificationExperiment
2026-02-02 12:39:20,657:INFO:Logging name: clf-default-name
2026-02-02 12:39:20,657:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 12:39:20,657:INFO:version 3.3.2
2026-02-02 12:39:20,657:INFO:Initializing setup()
2026-02-02 12:39:20,657:INFO:self.USI: ca92
2026-02-02 12:39:20,657:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 12:39:20,657:INFO:Checking environment
2026-02-02 12:39:20,657:INFO:python_version: 3.11.11
2026-02-02 12:39:20,657:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 12:39:20,657:INFO:machine: AMD64
2026-02-02 12:39:20,657:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 12:39:20,658:INFO:Memory: svmem(total=34009374720, available=11835367424, percent=65.2, used=22174007296, free=11835367424)
2026-02-02 12:39:20,658:INFO:Physical Core: 12
2026-02-02 12:39:20,658:INFO:Logical Core: 16
2026-02-02 12:39:20,658:INFO:Checking libraries
2026-02-02 12:39:20,658:INFO:System:
2026-02-02 12:39:20,658:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 12:39:20,658:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 12:39:20,658:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 12:39:20,658:INFO:PyCaret required dependencies:
2026-02-02 12:39:20,658:INFO:                 pip: 25.0
2026-02-02 12:39:20,658:INFO:          setuptools: 75.8.0
2026-02-02 12:39:20,658:INFO:             pycaret: 3.3.2
2026-02-02 12:39:20,658:INFO:             IPython: 9.9.0
2026-02-02 12:39:20,658:INFO:          ipywidgets: 8.1.8
2026-02-02 12:39:20,659:INFO:                tqdm: 4.67.1
2026-02-02 12:39:20,659:INFO:               numpy: 1.26.4
2026-02-02 12:39:20,659:INFO:              pandas: 2.1.4
2026-02-02 12:39:20,659:INFO:              jinja2: 3.1.6
2026-02-02 12:39:20,660:INFO:               scipy: 1.11.4
2026-02-02 12:39:20,660:INFO:              joblib: 1.3.2
2026-02-02 12:39:20,660:INFO:             sklearn: 1.4.2
2026-02-02 12:39:20,660:INFO:                pyod: 2.0.6
2026-02-02 12:39:20,660:INFO:            imblearn: 0.14.1
2026-02-02 12:39:20,660:INFO:   category_encoders: 2.7.0
2026-02-02 12:39:20,660:INFO:            lightgbm: 4.6.0
2026-02-02 12:39:20,660:INFO:               numba: 0.62.1
2026-02-02 12:39:20,660:INFO:            requests: 2.32.3
2026-02-02 12:39:20,660:INFO:          matplotlib: 3.7.5
2026-02-02 12:39:20,660:INFO:          scikitplot: 0.3.7
2026-02-02 12:39:20,660:INFO:         yellowbrick: 1.5
2026-02-02 12:39:20,661:INFO:              plotly: 5.24.1
2026-02-02 12:39:20,661:INFO:    plotly-resampler: Not installed
2026-02-02 12:39:20,661:INFO:             kaleido: 1.2.0
2026-02-02 12:39:20,661:INFO:           schemdraw: 0.15
2026-02-02 12:39:20,661:INFO:         statsmodels: 0.14.6
2026-02-02 12:39:20,661:INFO:              sktime: 0.26.0
2026-02-02 12:39:20,661:INFO:               tbats: 1.1.3
2026-02-02 12:39:20,661:INFO:            pmdarima: 2.0.4
2026-02-02 12:39:20,661:INFO:              psutil: 7.2.1
2026-02-02 12:39:20,661:INFO:          markupsafe: 3.0.3
2026-02-02 12:39:20,661:INFO:             pickle5: Not installed
2026-02-02 12:39:20,661:INFO:         cloudpickle: 3.0.0
2026-02-02 12:39:20,662:INFO:         deprecation: 2.1.0
2026-02-02 12:39:20,662:INFO:              xxhash: 3.6.0
2026-02-02 12:39:20,662:INFO:           wurlitzer: Not installed
2026-02-02 12:39:20,662:INFO:PyCaret optional dependencies:
2026-02-02 12:39:20,662:INFO:                shap: 0.44.1
2026-02-02 12:39:20,662:INFO:           interpret: 0.7.3
2026-02-02 12:39:20,662:INFO:                umap: 0.5.7
2026-02-02 12:39:20,662:INFO:     ydata_profiling: 4.18.1
2026-02-02 12:39:20,662:INFO:  explainerdashboard: 0.5.1
2026-02-02 12:39:20,662:INFO:             autoviz: Not installed
2026-02-02 12:39:20,662:INFO:           fairlearn: 0.7.0
2026-02-02 12:39:20,662:INFO:          deepchecks: Not installed
2026-02-02 12:39:20,662:INFO:             xgboost: Not installed
2026-02-02 12:39:20,662:INFO:            catboost: 1.2.8
2026-02-02 12:39:20,663:INFO:              kmodes: 0.12.2
2026-02-02 12:39:20,663:INFO:             mlxtend: 0.23.4
2026-02-02 12:39:20,663:INFO:       statsforecast: 1.5.0
2026-02-02 12:39:20,663:INFO:        tune_sklearn: Not installed
2026-02-02 12:39:20,663:INFO:                 ray: Not installed
2026-02-02 12:39:20,663:INFO:            hyperopt: 0.2.7
2026-02-02 12:39:20,663:INFO:              optuna: 4.6.0
2026-02-02 12:39:20,663:INFO:               skopt: 0.10.2
2026-02-02 12:39:20,663:INFO:              mlflow: 3.8.1
2026-02-02 12:39:20,664:INFO:              gradio: 6.3.0
2026-02-02 12:39:20,665:INFO:             fastapi: 0.128.0
2026-02-02 12:39:20,665:INFO:             uvicorn: 0.40.0
2026-02-02 12:39:20,665:INFO:              m2cgen: 0.10.0
2026-02-02 12:39:20,665:INFO:           evidently: 0.4.40
2026-02-02 12:39:20,665:INFO:               fugue: 0.8.7
2026-02-02 12:39:20,665:INFO:           streamlit: Not installed
2026-02-02 12:39:20,665:INFO:             prophet: Not installed
2026-02-02 12:39:20,665:INFO:None
2026-02-02 12:39:20,665:INFO:Set up data.
2026-02-02 12:39:20,670:INFO:Set up folding strategy.
2026-02-02 12:39:20,670:INFO:Set up train/test split.
2026-02-02 12:39:20,677:INFO:Set up index.
2026-02-02 12:39:20,677:INFO:Assigning column types.
2026-02-02 12:39:20,683:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 12:39:20,720:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:39:20,721:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:39:20,744:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:39:20,745:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:39:20,783:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:39:20,784:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:39:20,841:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:39:20,842:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:39:20,842:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 12:39:20,876:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:39:20,899:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:39:20,899:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:39:20,936:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:39:20,959:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:39:20,959:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:39:20,960:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 12:39:21,018:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:39:21,018:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:39:21,079:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:39:21,079:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:39:21,081:INFO:Preparing preprocessing pipeline...
2026-02-02 12:39:21,083:INFO:Set up simple imputation.
2026-02-02 12:39:21,083:INFO:Set up feature normalization.
2026-02-02 12:39:21,112:INFO:Finished creating preprocessing pipeline.
2026-02-02 12:39:21,116:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 12:39:21,117:INFO:Creating final display dataframe.
2026-02-02 12:39:21,224:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 19)
4        Transformed data shape       (10381, 19)
5   Transformed train set shape        (7266, 19)
6    Transformed test set shape        (3115, 19)
7              Numeric features                 8
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              ca92
2026-02-02 12:39:21,277:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:39:21,278:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:39:21,334:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:39:21,335:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:39:21,336:INFO:setup() successfully completed in 0.69s...............
2026-02-02 12:39:21,337:INFO:Initializing compare_models()
2026-02-02 12:39:21,337:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 12:39:21,337:INFO:Checking exceptions
2026-02-02 12:39:21,341:INFO:Preparing display monitor
2026-02-02 12:39:21,343:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 12:39:21,344:INFO:Total runtime is 1.6673405965169272e-05 minutes
2026-02-02 12:39:21,344:INFO:SubProcess create_model() called ==================================
2026-02-02 12:39:21,344:INFO:Initializing create_model()
2026-02-02 12:39:21,344:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022864B78890>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:39:21,344:INFO:Checking exceptions
2026-02-02 12:39:21,344:INFO:Importing libraries
2026-02-02 12:39:21,344:INFO:Copying training dataset
2026-02-02 12:39:21,349:INFO:Defining folds
2026-02-02 12:39:21,349:INFO:Declaring metric variables
2026-02-02 12:39:21,349:INFO:Importing untrained model
2026-02-02 12:39:21,350:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:39:21,350:INFO:Starting cross validation
2026-02-02 12:39:21,351:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:39:22,126:INFO:Calculating mean and std
2026-02-02 12:39:22,128:INFO:Creating metrics dataframe
2026-02-02 12:39:22,132:INFO:Uploading results into container
2026-02-02 12:39:22,132:INFO:Uploading model into container now
2026-02-02 12:39:22,134:INFO:_master_model_container: 1
2026-02-02 12:39:22,134:INFO:_display_container: 2
2026-02-02 12:39:22,136:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:39:22,136:INFO:create_model() successfully completed......................................
2026-02-02 12:39:22,935:INFO:SubProcess create_model() end ==================================
2026-02-02 12:39:22,935:INFO:Creating metrics dataframe
2026-02-02 12:39:22,937:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 12:39:22,939:INFO:Initializing create_model()
2026-02-02 12:39:22,939:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:39:22,939:INFO:Checking exceptions
2026-02-02 12:39:22,940:INFO:Importing libraries
2026-02-02 12:39:22,940:INFO:Copying training dataset
2026-02-02 12:39:22,947:INFO:Defining folds
2026-02-02 12:39:22,947:INFO:Declaring metric variables
2026-02-02 12:39:22,947:INFO:Importing untrained model
2026-02-02 12:39:22,947:INFO:Declaring custom model
2026-02-02 12:39:22,949:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:39:22,950:INFO:Cross validation set to False
2026-02-02 12:39:22,950:INFO:Fitting Model
2026-02-02 12:39:22,976:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:39:22,978:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001078 seconds.
2026-02-02 12:39:22,978:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:39:22,978:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 12:39:22,979:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 12:39:22,979:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:39:22,979:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:39:23,199:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:39:23,199:INFO:create_model() successfully completed......................................
2026-02-02 12:39:23,401:INFO:_master_model_container: 1
2026-02-02 12:39:23,401:INFO:_display_container: 2
2026-02-02 12:39:23,402:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:39:23,402:INFO:compare_models() successfully completed......................................
2026-02-02 12:39:23,403:INFO:Initializing tune_model()
2026-02-02 12:39:23,403:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:39:23,403:INFO:Checking exceptions
2026-02-02 12:39:23,409:INFO:Copying training dataset
2026-02-02 12:39:23,415:INFO:Checking base model
2026-02-02 12:39:23,416:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 12:39:23,416:INFO:Declaring metric variables
2026-02-02 12:39:23,416:INFO:Defining Hyperparameters
2026-02-02 12:39:23,570:INFO:Tuning with n_jobs=-1
2026-02-02 12:39:23,570:INFO:Initializing RandomizedSearchCV
2026-02-02 12:39:29,273:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 12:39:29,279:INFO:Hyperparameter search completed
2026-02-02 12:39:29,279:INFO:SubProcess create_model() called ==================================
2026-02-02 12:39:29,280:INFO:Initializing create_model()
2026-02-02 12:39:29,281:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022908D96550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 12:39:29,281:INFO:Checking exceptions
2026-02-02 12:39:29,282:INFO:Importing libraries
2026-02-02 12:39:29,282:INFO:Copying training dataset
2026-02-02 12:39:29,294:INFO:Defining folds
2026-02-02 12:39:29,295:INFO:Declaring metric variables
2026-02-02 12:39:29,295:INFO:Importing untrained model
2026-02-02 12:39:29,295:INFO:Declaring custom model
2026-02-02 12:39:29,298:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:39:29,298:INFO:Starting cross validation
2026-02-02 12:39:29,301:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:39:30,196:INFO:Calculating mean and std
2026-02-02 12:39:30,197:INFO:Creating metrics dataframe
2026-02-02 12:39:30,201:INFO:Finalizing model
2026-02-02 12:39:30,235:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:39:30,235:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:39:30,235:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:39:30,243:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:39:30,243:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:39:30,243:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:39:30,244:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:39:30,247:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002305 seconds.
2026-02-02 12:39:30,247:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:39:30,248:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 12:39:30,248:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 12:39:30,249:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:39:30,249:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:39:30,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,369:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,371:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,409:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,424:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:39:30,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,433:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,437:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,448:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,450:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,480:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,482:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,485:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,489:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,497:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,501:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,519:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,538:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,541:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:39:30,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,567:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:39:30,600:INFO:Uploading results into container
2026-02-02 12:39:30,601:INFO:Uploading model into container now
2026-02-02 12:39:30,603:INFO:_master_model_container: 2
2026-02-02 12:39:30,603:INFO:_display_container: 3
2026-02-02 12:39:30,604:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:39:30,605:INFO:create_model() successfully completed......................................
2026-02-02 12:39:30,807:INFO:SubProcess create_model() end ==================================
2026-02-02 12:39:30,807:INFO:choose_better activated
2026-02-02 12:39:30,807:INFO:SubProcess create_model() called ==================================
2026-02-02 12:39:30,808:INFO:Initializing create_model()
2026-02-02 12:39:30,808:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:39:30,809:INFO:Checking exceptions
2026-02-02 12:39:30,809:INFO:Importing libraries
2026-02-02 12:39:30,811:INFO:Copying training dataset
2026-02-02 12:39:30,821:INFO:Defining folds
2026-02-02 12:39:30,821:INFO:Declaring metric variables
2026-02-02 12:39:30,822:INFO:Importing untrained model
2026-02-02 12:39:30,822:INFO:Declaring custom model
2026-02-02 12:39:30,823:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:39:30,823:INFO:Starting cross validation
2026-02-02 12:39:30,824:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:39:32,008:INFO:Calculating mean and std
2026-02-02 12:39:32,009:INFO:Creating metrics dataframe
2026-02-02 12:39:32,011:INFO:Finalizing model
2026-02-02 12:39:32,052:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:39:32,055:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001913 seconds.
2026-02-02 12:39:32,055:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:39:32,055:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 12:39:32,056:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 12:39:32,056:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:39:32,056:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:39:32,224:INFO:Uploading results into container
2026-02-02 12:39:32,225:INFO:Uploading model into container now
2026-02-02 12:39:32,225:INFO:_master_model_container: 3
2026-02-02 12:39:32,226:INFO:_display_container: 4
2026-02-02 12:39:32,227:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:39:32,227:INFO:create_model() successfully completed......................................
2026-02-02 12:39:32,423:INFO:SubProcess create_model() end ==================================
2026-02-02 12:39:32,424:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9974
2026-02-02 12:39:32,424:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9946
2026-02-02 12:39:32,425:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 12:39:32,425:INFO:choose_better completed
2026-02-02 12:39:32,425:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:39:32,428:INFO:_master_model_container: 3
2026-02-02 12:39:32,428:INFO:_display_container: 3
2026-02-02 12:39:32,430:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:39:32,430:INFO:tune_model() successfully completed......................................
2026-02-02 12:39:32,577:INFO:Initializing predict_model()
2026-02-02 12:39:32,578:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002286EDE37E0>)
2026-02-02 12:39:32,578:INFO:Checking exceptions
2026-02-02 12:39:32,578:INFO:Preloading libraries
2026-02-02 12:39:32,578:INFO:Set up data.
2026-02-02 12:39:32,581:INFO:Set up index.
2026-02-02 12:39:34,989:INFO:Initializing plot_model()
2026-02-02 12:39:34,989:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:39:34,990:INFO:Checking exceptions
2026-02-02 12:39:34,992:INFO:Preloading libraries
2026-02-02 12:39:34,999:INFO:Copying training dataset
2026-02-02 12:39:34,999:INFO:Plot type: feature
2026-02-02 12:39:35,000:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:39:35,164:INFO:Visual Rendered Successfully
2026-02-02 12:39:35,345:INFO:plot_model() successfully completed......................................
2026-02-02 12:39:35,346:INFO:Initializing plot_model()
2026-02-02 12:39:35,346:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228390F1110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:39:35,347:INFO:Checking exceptions
2026-02-02 12:39:35,349:INFO:Preloading libraries
2026-02-02 12:39:35,355:INFO:Copying training dataset
2026-02-02 12:39:35,355:INFO:Plot type: feature_all
2026-02-02 12:39:35,422:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:39:35,628:INFO:Visual Rendered Successfully
2026-02-02 12:39:35,778:INFO:plot_model() successfully completed......................................
2026-02-02 12:39:35,783:INFO:Initializing save_model()
2026-02-02 12:39:35,783:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 12:39:35,783:INFO:Adding model into prep_pipe
2026-02-02 12:39:35,794:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-02 12:39:35,803:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_feat...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-02 12:39:35,803:INFO:save_model() successfully completed......................................
2026-02-02 12:45:37,570:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\3521832023.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 12:45:39,953:INFO:PyCaret ClassificationExperiment
2026-02-02 12:45:39,954:INFO:Logging name: clf-default-name
2026-02-02 12:45:39,954:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 12:45:39,955:INFO:version 3.3.2
2026-02-02 12:45:39,955:INFO:Initializing setup()
2026-02-02 12:45:39,955:INFO:self.USI: 79eb
2026-02-02 12:45:39,956:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 12:45:39,956:INFO:Checking environment
2026-02-02 12:45:39,956:INFO:python_version: 3.11.11
2026-02-02 12:45:39,957:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 12:45:39,957:INFO:machine: AMD64
2026-02-02 12:45:39,957:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 12:45:39,958:INFO:Memory: svmem(total=34009374720, available=14549790720, percent=57.2, used=19459584000, free=14549790720)
2026-02-02 12:45:39,958:INFO:Physical Core: 12
2026-02-02 12:45:39,958:INFO:Logical Core: 16
2026-02-02 12:45:39,959:INFO:Checking libraries
2026-02-02 12:45:39,959:INFO:System:
2026-02-02 12:45:39,959:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 12:45:39,959:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 12:45:39,960:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 12:45:39,960:INFO:PyCaret required dependencies:
2026-02-02 12:45:39,960:INFO:                 pip: 25.0
2026-02-02 12:45:39,960:INFO:          setuptools: 75.8.0
2026-02-02 12:45:39,960:INFO:             pycaret: 3.3.2
2026-02-02 12:45:39,960:INFO:             IPython: 9.9.0
2026-02-02 12:45:39,960:INFO:          ipywidgets: 8.1.8
2026-02-02 12:45:39,960:INFO:                tqdm: 4.67.1
2026-02-02 12:45:39,960:INFO:               numpy: 1.26.4
2026-02-02 12:45:39,960:INFO:              pandas: 2.1.4
2026-02-02 12:45:39,960:INFO:              jinja2: 3.1.6
2026-02-02 12:45:39,960:INFO:               scipy: 1.11.4
2026-02-02 12:45:39,960:INFO:              joblib: 1.3.2
2026-02-02 12:45:39,960:INFO:             sklearn: 1.4.2
2026-02-02 12:45:39,961:INFO:                pyod: 2.0.6
2026-02-02 12:45:39,961:INFO:            imblearn: 0.14.1
2026-02-02 12:45:39,961:INFO:   category_encoders: 2.7.0
2026-02-02 12:45:39,961:INFO:            lightgbm: 4.6.0
2026-02-02 12:45:39,961:INFO:               numba: 0.62.1
2026-02-02 12:45:39,961:INFO:            requests: 2.32.3
2026-02-02 12:45:39,961:INFO:          matplotlib: 3.7.5
2026-02-02 12:45:39,961:INFO:          scikitplot: 0.3.7
2026-02-02 12:45:39,961:INFO:         yellowbrick: 1.5
2026-02-02 12:45:39,961:INFO:              plotly: 5.24.1
2026-02-02 12:45:39,961:INFO:    plotly-resampler: Not installed
2026-02-02 12:45:39,961:INFO:             kaleido: 1.2.0
2026-02-02 12:45:39,961:INFO:           schemdraw: 0.15
2026-02-02 12:45:39,961:INFO:         statsmodels: 0.14.6
2026-02-02 12:45:39,962:INFO:              sktime: 0.26.0
2026-02-02 12:45:39,962:INFO:               tbats: 1.1.3
2026-02-02 12:45:39,962:INFO:            pmdarima: 2.0.4
2026-02-02 12:45:39,962:INFO:              psutil: 7.2.1
2026-02-02 12:45:39,962:INFO:          markupsafe: 3.0.3
2026-02-02 12:45:39,962:INFO:             pickle5: Not installed
2026-02-02 12:45:39,962:INFO:         cloudpickle: 3.0.0
2026-02-02 12:45:39,963:INFO:         deprecation: 2.1.0
2026-02-02 12:45:39,963:INFO:              xxhash: 3.6.0
2026-02-02 12:45:39,963:INFO:           wurlitzer: Not installed
2026-02-02 12:45:39,963:INFO:PyCaret optional dependencies:
2026-02-02 12:45:39,963:INFO:                shap: 0.44.1
2026-02-02 12:45:39,963:INFO:           interpret: 0.7.3
2026-02-02 12:45:39,963:INFO:                umap: 0.5.7
2026-02-02 12:45:39,963:INFO:     ydata_profiling: 4.18.1
2026-02-02 12:45:39,963:INFO:  explainerdashboard: 0.5.1
2026-02-02 12:45:39,964:INFO:             autoviz: Not installed
2026-02-02 12:45:39,964:INFO:           fairlearn: 0.7.0
2026-02-02 12:45:39,964:INFO:          deepchecks: Not installed
2026-02-02 12:45:39,964:INFO:             xgboost: Not installed
2026-02-02 12:45:39,964:INFO:            catboost: 1.2.8
2026-02-02 12:45:39,964:INFO:              kmodes: 0.12.2
2026-02-02 12:45:39,964:INFO:             mlxtend: 0.23.4
2026-02-02 12:45:39,964:INFO:       statsforecast: 1.5.0
2026-02-02 12:45:39,964:INFO:        tune_sklearn: Not installed
2026-02-02 12:45:39,964:INFO:                 ray: Not installed
2026-02-02 12:45:39,965:INFO:            hyperopt: 0.2.7
2026-02-02 12:45:39,965:INFO:              optuna: 4.6.0
2026-02-02 12:45:39,965:INFO:               skopt: 0.10.2
2026-02-02 12:45:39,965:INFO:              mlflow: 3.8.1
2026-02-02 12:45:39,966:INFO:              gradio: 6.3.0
2026-02-02 12:45:39,966:INFO:             fastapi: 0.128.0
2026-02-02 12:45:39,966:INFO:             uvicorn: 0.40.0
2026-02-02 12:45:39,966:INFO:              m2cgen: 0.10.0
2026-02-02 12:45:39,966:INFO:           evidently: 0.4.40
2026-02-02 12:45:39,966:INFO:               fugue: 0.8.7
2026-02-02 12:45:39,967:INFO:           streamlit: Not installed
2026-02-02 12:45:39,967:INFO:             prophet: Not installed
2026-02-02 12:45:39,967:INFO:None
2026-02-02 12:45:39,967:INFO:Set up data.
2026-02-02 12:45:40,057:INFO:Set up folding strategy.
2026-02-02 12:45:40,057:INFO:Set up train/test split.
2026-02-02 12:45:40,200:INFO:Set up index.
2026-02-02 12:45:40,206:INFO:Assigning column types.
2026-02-02 12:45:40,286:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 12:45:40,314:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:45:40,314:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:45:40,332:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:45:40,332:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:45:40,361:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:45:40,361:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:45:40,381:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:45:40,384:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:45:40,384:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 12:45:40,414:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:45:40,432:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:45:40,432:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:45:40,463:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:45:40,484:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:45:40,485:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:45:40,485:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 12:45:40,535:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:45:40,535:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:45:40,585:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:45:40,586:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:45:40,587:INFO:Preparing preprocessing pipeline...
2026-02-02 12:45:40,604:INFO:Set up simple imputation.
2026-02-02 12:45:40,604:INFO:Set up feature normalization.
2026-02-02 12:45:40,828:INFO:Finished creating preprocessing pipeline.
2026-02-02 12:45:40,831:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 12:45:40,832:INFO:Creating final display dataframe.
2026-02-02 12:45:41,237:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 17)
4        Transformed data shape      (373023, 17)
5   Transformed train set shape      (261116, 17)
6    Transformed test set shape      (111907, 17)
7              Numeric features                13
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              79eb
2026-02-02 12:45:41,295:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:45:41,296:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:45:41,350:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:45:41,350:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:45:41,352:INFO:setup() successfully completed in 1.42s...............
2026-02-02 12:45:41,352:INFO:Initializing compare_models()
2026-02-02 12:45:41,352:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 12:45:41,352:INFO:Checking exceptions
2026-02-02 12:45:41,424:INFO:Preparing display monitor
2026-02-02 12:45:41,427:INFO:Initializing Logistic Regression
2026-02-02 12:45:41,427:INFO:Total runtime is 0.0 minutes
2026-02-02 12:45:41,427:INFO:SubProcess create_model() called ==================================
2026-02-02 12:45:41,427:INFO:Initializing create_model()
2026-02-02 12:45:41,427:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D51824D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:45:41,428:INFO:Checking exceptions
2026-02-02 12:45:41,428:INFO:Importing libraries
2026-02-02 12:45:41,428:INFO:Copying training dataset
2026-02-02 12:45:41,543:INFO:Defining folds
2026-02-02 12:45:41,543:INFO:Declaring metric variables
2026-02-02 12:45:41,543:INFO:Importing untrained model
2026-02-02 12:45:41,544:INFO:Logistic Regression Imported successfully
2026-02-02 12:45:41,544:INFO:Starting cross validation
2026-02-02 12:45:41,545:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:45:49,408:INFO:Calculating mean and std
2026-02-02 12:45:49,410:INFO:Creating metrics dataframe
2026-02-02 12:45:49,413:INFO:Uploading results into container
2026-02-02 12:45:49,414:INFO:Uploading model into container now
2026-02-02 12:45:49,414:INFO:_master_model_container: 1
2026-02-02 12:45:49,414:INFO:_display_container: 2
2026-02-02 12:45:49,415:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 12:45:49,415:INFO:create_model() successfully completed......................................
2026-02-02 12:45:49,569:INFO:SubProcess create_model() end ==================================
2026-02-02 12:45:49,570:INFO:Creating metrics dataframe
2026-02-02 12:45:49,571:INFO:Initializing Decision Tree Classifier
2026-02-02 12:45:49,572:INFO:Total runtime is 0.13573660055796305 minutes
2026-02-02 12:45:49,572:INFO:SubProcess create_model() called ==================================
2026-02-02 12:45:49,572:INFO:Initializing create_model()
2026-02-02 12:45:49,573:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D51824D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:45:49,573:INFO:Checking exceptions
2026-02-02 12:45:49,573:INFO:Importing libraries
2026-02-02 12:45:49,573:INFO:Copying training dataset
2026-02-02 12:45:49,669:INFO:Defining folds
2026-02-02 12:45:49,669:INFO:Declaring metric variables
2026-02-02 12:45:49,669:INFO:Importing untrained model
2026-02-02 12:45:49,670:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:45:49,671:INFO:Starting cross validation
2026-02-02 12:45:49,672:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:45:56,276:INFO:Calculating mean and std
2026-02-02 12:45:56,277:INFO:Creating metrics dataframe
2026-02-02 12:45:56,279:INFO:Uploading results into container
2026-02-02 12:45:56,279:INFO:Uploading model into container now
2026-02-02 12:45:56,280:INFO:_master_model_container: 2
2026-02-02 12:45:56,280:INFO:_display_container: 2
2026-02-02 12:45:56,280:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:45:56,280:INFO:create_model() successfully completed......................................
2026-02-02 12:45:56,439:INFO:SubProcess create_model() end ==================================
2026-02-02 12:45:56,440:INFO:Creating metrics dataframe
2026-02-02 12:45:56,441:INFO:Initializing Random Forest Classifier
2026-02-02 12:45:56,442:INFO:Total runtime is 0.2502434810002645 minutes
2026-02-02 12:45:56,442:INFO:SubProcess create_model() called ==================================
2026-02-02 12:45:56,442:INFO:Initializing create_model()
2026-02-02 12:45:56,442:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D51824D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:45:56,442:INFO:Checking exceptions
2026-02-02 12:45:56,442:INFO:Importing libraries
2026-02-02 12:45:56,442:INFO:Copying training dataset
2026-02-02 12:45:56,555:INFO:Defining folds
2026-02-02 12:45:56,555:INFO:Declaring metric variables
2026-02-02 12:45:56,555:INFO:Importing untrained model
2026-02-02 12:45:56,556:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:45:56,556:INFO:Starting cross validation
2026-02-02 12:45:56,556:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:46:18,413:INFO:Calculating mean and std
2026-02-02 12:46:18,415:INFO:Creating metrics dataframe
2026-02-02 12:46:18,418:INFO:Uploading results into container
2026-02-02 12:46:18,419:INFO:Uploading model into container now
2026-02-02 12:46:18,420:INFO:_master_model_container: 3
2026-02-02 12:46:18,420:INFO:_display_container: 2
2026-02-02 12:46:18,420:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:46:18,420:INFO:create_model() successfully completed......................................
2026-02-02 12:46:18,604:INFO:SubProcess create_model() end ==================================
2026-02-02 12:46:18,605:INFO:Creating metrics dataframe
2026-02-02 12:46:18,611:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 12:46:18,611:INFO:Total runtime is 0.6197278698285421 minutes
2026-02-02 12:46:18,611:INFO:SubProcess create_model() called ==================================
2026-02-02 12:46:18,612:INFO:Initializing create_model()
2026-02-02 12:46:18,612:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D51824D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:46:18,612:INFO:Checking exceptions
2026-02-02 12:46:18,612:INFO:Importing libraries
2026-02-02 12:46:18,612:INFO:Copying training dataset
2026-02-02 12:46:18,757:INFO:Defining folds
2026-02-02 12:46:18,759:INFO:Declaring metric variables
2026-02-02 12:46:18,759:INFO:Importing untrained model
2026-02-02 12:46:18,760:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:46:18,760:INFO:Starting cross validation
2026-02-02 12:46:18,761:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:46:26,953:INFO:Calculating mean and std
2026-02-02 12:46:26,954:INFO:Creating metrics dataframe
2026-02-02 12:46:26,958:INFO:Uploading results into container
2026-02-02 12:46:26,959:INFO:Uploading model into container now
2026-02-02 12:46:26,960:INFO:_master_model_container: 4
2026-02-02 12:46:26,960:INFO:_display_container: 2
2026-02-02 12:46:26,961:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:46:26,961:INFO:create_model() successfully completed......................................
2026-02-02 12:46:27,152:INFO:SubProcess create_model() end ==================================
2026-02-02 12:46:27,153:INFO:Creating metrics dataframe
2026-02-02 12:46:27,157:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 12:46:27,159:INFO:Initializing create_model()
2026-02-02 12:46:27,159:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:46:27,160:INFO:Checking exceptions
2026-02-02 12:46:27,160:INFO:Importing libraries
2026-02-02 12:46:27,161:INFO:Copying training dataset
2026-02-02 12:46:27,329:INFO:Defining folds
2026-02-02 12:46:27,329:INFO:Declaring metric variables
2026-02-02 12:46:27,329:INFO:Importing untrained model
2026-02-02 12:46:27,329:INFO:Declaring custom model
2026-02-02 12:46:27,330:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:46:27,331:INFO:Cross validation set to False
2026-02-02 12:46:27,331:INFO:Fitting Model
2026-02-02 12:46:35,453:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:46:35,453:INFO:create_model() successfully completed......................................
2026-02-02 12:46:35,611:INFO:Initializing create_model()
2026-02-02 12:46:35,613:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:46:35,613:INFO:Checking exceptions
2026-02-02 12:46:35,613:INFO:Importing libraries
2026-02-02 12:46:35,613:INFO:Copying training dataset
2026-02-02 12:46:35,716:INFO:Defining folds
2026-02-02 12:46:35,716:INFO:Declaring metric variables
2026-02-02 12:46:35,716:INFO:Importing untrained model
2026-02-02 12:46:35,716:INFO:Declaring custom model
2026-02-02 12:46:35,717:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:46:35,717:INFO:Cross validation set to False
2026-02-02 12:46:35,717:INFO:Fitting Model
2026-02-02 12:46:36,159:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 12:46:36,170:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006835 seconds.
2026-02-02 12:46:36,171:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:46:36,171:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 12:46:36,172:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 12:46:36,174:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 12:46:36,174:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 12:46:37,030:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:46:37,030:INFO:create_model() successfully completed......................................
2026-02-02 12:46:37,238:INFO:Initializing create_model()
2026-02-02 12:46:37,238:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:46:37,239:INFO:Checking exceptions
2026-02-02 12:46:37,240:INFO:Importing libraries
2026-02-02 12:46:37,240:INFO:Copying training dataset
2026-02-02 12:46:37,394:INFO:Defining folds
2026-02-02 12:46:37,395:INFO:Declaring metric variables
2026-02-02 12:46:37,395:INFO:Importing untrained model
2026-02-02 12:46:37,395:INFO:Declaring custom model
2026-02-02 12:46:37,395:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:46:37,396:INFO:Cross validation set to False
2026-02-02 12:46:37,396:INFO:Fitting Model
2026-02-02 12:46:39,235:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:46:39,235:INFO:create_model() successfully completed......................................
2026-02-02 12:46:39,386:INFO:_master_model_container: 4
2026-02-02 12:46:39,387:INFO:_display_container: 2
2026-02-02 12:46:39,387:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-02-02 12:46:39,388:INFO:compare_models() successfully completed......................................
2026-02-02 12:46:39,395:INFO:Initializing tune_model()
2026-02-02 12:46:39,395:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:46:39,395:INFO:Checking exceptions
2026-02-02 12:46:39,438:INFO:Copying training dataset
2026-02-02 12:46:39,508:INFO:Checking base model
2026-02-02 12:46:39,509:INFO:Base model : Random Forest Classifier
2026-02-02 12:46:39,509:INFO:Declaring metric variables
2026-02-02 12:46:39,510:INFO:Defining Hyperparameters
2026-02-02 12:46:39,643:INFO:Tuning with n_jobs=-1
2026-02-02 12:46:39,643:INFO:Initializing RandomizedSearchCV
2026-02-02 12:48:48,958:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 12:48:48,959:INFO:Hyperparameter search completed
2026-02-02 12:48:48,960:INFO:SubProcess create_model() called ==================================
2026-02-02 12:48:48,961:INFO:Initializing create_model()
2026-02-02 12:48:48,961:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022866F03E10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-02 12:48:48,962:INFO:Checking exceptions
2026-02-02 12:48:48,962:INFO:Importing libraries
2026-02-02 12:48:48,962:INFO:Copying training dataset
2026-02-02 12:48:49,266:INFO:Defining folds
2026-02-02 12:48:49,266:INFO:Declaring metric variables
2026-02-02 12:48:49,267:INFO:Importing untrained model
2026-02-02 12:48:49,267:INFO:Declaring custom model
2026-02-02 12:48:49,268:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:48:49,268:INFO:Starting cross validation
2026-02-02 12:48:49,269:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:49:15,474:INFO:Calculating mean and std
2026-02-02 12:49:15,476:INFO:Creating metrics dataframe
2026-02-02 12:49:15,479:INFO:Finalizing model
2026-02-02 12:49:29,084:INFO:Uploading results into container
2026-02-02 12:49:29,085:INFO:Uploading model into container now
2026-02-02 12:49:29,086:INFO:_master_model_container: 5
2026-02-02 12:49:29,086:INFO:_display_container: 3
2026-02-02 12:49:29,087:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:49:29,088:INFO:create_model() successfully completed......................................
2026-02-02 12:49:29,246:INFO:SubProcess create_model() end ==================================
2026-02-02 12:49:29,246:INFO:choose_better activated
2026-02-02 12:49:29,247:INFO:SubProcess create_model() called ==================================
2026-02-02 12:49:29,247:INFO:Initializing create_model()
2026-02-02 12:49:29,247:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:49:29,247:INFO:Checking exceptions
2026-02-02 12:49:29,248:INFO:Importing libraries
2026-02-02 12:49:29,248:INFO:Copying training dataset
2026-02-02 12:49:29,375:INFO:Defining folds
2026-02-02 12:49:29,375:INFO:Declaring metric variables
2026-02-02 12:49:29,375:INFO:Importing untrained model
2026-02-02 12:49:29,375:INFO:Declaring custom model
2026-02-02 12:49:29,376:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:49:29,376:INFO:Starting cross validation
2026-02-02 12:49:29,377:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:49:46,692:INFO:Calculating mean and std
2026-02-02 12:49:46,693:INFO:Creating metrics dataframe
2026-02-02 12:49:46,695:INFO:Finalizing model
2026-02-02 12:49:54,756:INFO:Uploading results into container
2026-02-02 12:49:54,758:INFO:Uploading model into container now
2026-02-02 12:49:54,758:INFO:_master_model_container: 6
2026-02-02 12:49:54,759:INFO:_display_container: 4
2026-02-02 12:49:54,759:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:49:54,760:INFO:create_model() successfully completed......................................
2026-02-02 12:49:54,913:INFO:SubProcess create_model() end ==================================
2026-02-02 12:49:54,913:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9956
2026-02-02 12:49:54,914:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9648
2026-02-02 12:49:54,914:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 12:49:54,914:INFO:choose_better completed
2026-02-02 12:49:54,915:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:49:54,917:INFO:_master_model_container: 6
2026-02-02 12:49:54,917:INFO:_display_container: 3
2026-02-02 12:49:54,917:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:49:54,917:INFO:tune_model() successfully completed......................................
2026-02-02 12:49:55,056:INFO:Initializing tune_model()
2026-02-02 12:49:55,056:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:49:55,056:INFO:Checking exceptions
2026-02-02 12:49:55,098:INFO:Copying training dataset
2026-02-02 12:49:55,170:INFO:Checking base model
2026-02-02 12:49:55,170:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 12:49:55,171:INFO:Declaring metric variables
2026-02-02 12:49:55,171:INFO:Defining Hyperparameters
2026-02-02 12:49:55,303:INFO:Tuning with n_jobs=-1
2026-02-02 12:49:55,303:INFO:Initializing RandomizedSearchCV
2026-02-02 12:50:25,431:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 12:50:25,433:INFO:Hyperparameter search completed
2026-02-02 12:50:25,433:INFO:SubProcess create_model() called ==================================
2026-02-02 12:50:25,434:INFO:Initializing create_model()
2026-02-02 12:50:25,435:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D0271D50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 12:50:25,435:INFO:Checking exceptions
2026-02-02 12:50:25,436:INFO:Importing libraries
2026-02-02 12:50:25,436:INFO:Copying training dataset
2026-02-02 12:50:25,586:INFO:Defining folds
2026-02-02 12:50:25,587:INFO:Declaring metric variables
2026-02-02 12:50:25,587:INFO:Importing untrained model
2026-02-02 12:50:25,587:INFO:Declaring custom model
2026-02-02 12:50:25,590:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:50:25,590:INFO:Starting cross validation
2026-02-02 12:50:25,592:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:50:32,646:INFO:Calculating mean and std
2026-02-02 12:50:32,648:INFO:Creating metrics dataframe
2026-02-02 12:50:32,650:INFO:Finalizing model
2026-02-02 12:50:32,981:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:50:32,982:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:50:32,982:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:50:33,091:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:50:33,091:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:50:33,091:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:50:33,092:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 12:50:33,102:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003770 seconds.
2026-02-02 12:50:33,102:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 12:50:33,103:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 12:50:33,103:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 12:50:33,104:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 12:50:33,107:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 12:50:33,107:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 12:50:35,158:INFO:Uploading results into container
2026-02-02 12:50:35,160:INFO:Uploading model into container now
2026-02-02 12:50:35,161:INFO:_master_model_container: 7
2026-02-02 12:50:35,161:INFO:_display_container: 4
2026-02-02 12:50:35,163:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:50:35,163:INFO:create_model() successfully completed......................................
2026-02-02 12:50:35,362:INFO:SubProcess create_model() end ==================================
2026-02-02 12:50:35,362:INFO:choose_better activated
2026-02-02 12:50:35,363:INFO:SubProcess create_model() called ==================================
2026-02-02 12:50:35,364:INFO:Initializing create_model()
2026-02-02 12:50:35,364:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:50:35,364:INFO:Checking exceptions
2026-02-02 12:50:35,365:INFO:Importing libraries
2026-02-02 12:50:35,365:INFO:Copying training dataset
2026-02-02 12:50:35,491:INFO:Defining folds
2026-02-02 12:50:35,492:INFO:Declaring metric variables
2026-02-02 12:50:35,492:INFO:Importing untrained model
2026-02-02 12:50:35,492:INFO:Declaring custom model
2026-02-02 12:50:35,493:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:50:35,493:INFO:Starting cross validation
2026-02-02 12:50:35,494:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:50:39,064:INFO:Calculating mean and std
2026-02-02 12:50:39,066:INFO:Creating metrics dataframe
2026-02-02 12:50:39,068:INFO:Finalizing model
2026-02-02 12:50:39,516:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 12:50:39,532:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006130 seconds.
2026-02-02 12:50:39,532:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 12:50:39,533:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 12:50:39,533:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 12:50:39,534:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 12:50:39,536:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 12:50:39,536:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 12:50:40,333:INFO:Uploading results into container
2026-02-02 12:50:40,335:INFO:Uploading model into container now
2026-02-02 12:50:40,335:INFO:_master_model_container: 8
2026-02-02 12:50:40,336:INFO:_display_container: 5
2026-02-02 12:50:40,337:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:50:40,338:INFO:create_model() successfully completed......................................
2026-02-02 12:50:40,575:INFO:SubProcess create_model() end ==================================
2026-02-02 12:50:40,576:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9767
2026-02-02 12:50:40,576:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9928
2026-02-02 12:50:40,577:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 12:50:40,577:INFO:choose_better completed
2026-02-02 12:50:40,580:INFO:_master_model_container: 8
2026-02-02 12:50:40,580:INFO:_display_container: 4
2026-02-02 12:50:40,581:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:50:40,581:INFO:tune_model() successfully completed......................................
2026-02-02 12:50:40,745:INFO:Initializing tune_model()
2026-02-02 12:50:40,746:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:50:40,746:INFO:Checking exceptions
2026-02-02 12:50:40,790:INFO:Copying training dataset
2026-02-02 12:50:40,872:INFO:Checking base model
2026-02-02 12:50:40,872:INFO:Base model : Decision Tree Classifier
2026-02-02 12:50:40,872:INFO:Declaring metric variables
2026-02-02 12:50:40,872:INFO:Defining Hyperparameters
2026-02-02 12:50:41,018:INFO:Tuning with n_jobs=-1
2026-02-02 12:50:41,018:INFO:Initializing RandomizedSearchCV
2026-02-02 12:50:46,283:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-02-02 12:50:46,284:INFO:Hyperparameter search completed
2026-02-02 12:50:46,284:INFO:SubProcess create_model() called ==================================
2026-02-02 12:50:46,286:INFO:Initializing create_model()
2026-02-02 12:50:46,286:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002290A19F590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-02-02 12:50:46,286:INFO:Checking exceptions
2026-02-02 12:50:46,286:INFO:Importing libraries
2026-02-02 12:50:46,286:INFO:Copying training dataset
2026-02-02 12:50:46,397:INFO:Defining folds
2026-02-02 12:50:46,398:INFO:Declaring metric variables
2026-02-02 12:50:46,399:INFO:Importing untrained model
2026-02-02 12:50:46,399:INFO:Declaring custom model
2026-02-02 12:50:46,399:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:50:46,400:INFO:Starting cross validation
2026-02-02 12:50:46,400:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:50:48,165:INFO:Calculating mean and std
2026-02-02 12:50:48,167:INFO:Creating metrics dataframe
2026-02-02 12:50:48,170:INFO:Finalizing model
2026-02-02 12:50:49,294:INFO:Uploading results into container
2026-02-02 12:50:49,295:INFO:Uploading model into container now
2026-02-02 12:50:49,295:INFO:_master_model_container: 9
2026-02-02 12:50:49,295:INFO:_display_container: 5
2026-02-02 12:50:49,296:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:50:49,296:INFO:create_model() successfully completed......................................
2026-02-02 12:50:49,429:INFO:SubProcess create_model() end ==================================
2026-02-02 12:50:49,429:INFO:choose_better activated
2026-02-02 12:50:49,430:INFO:SubProcess create_model() called ==================================
2026-02-02 12:50:49,430:INFO:Initializing create_model()
2026-02-02 12:50:49,430:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:50:49,430:INFO:Checking exceptions
2026-02-02 12:50:49,430:INFO:Importing libraries
2026-02-02 12:50:49,430:INFO:Copying training dataset
2026-02-02 12:50:49,531:INFO:Defining folds
2026-02-02 12:50:49,531:INFO:Declaring metric variables
2026-02-02 12:50:49,531:INFO:Importing untrained model
2026-02-02 12:50:49,531:INFO:Declaring custom model
2026-02-02 12:50:49,531:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:50:49,532:INFO:Starting cross validation
2026-02-02 12:50:49,532:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:50:52,086:INFO:Calculating mean and std
2026-02-02 12:50:52,087:INFO:Creating metrics dataframe
2026-02-02 12:50:52,091:INFO:Finalizing model
2026-02-02 12:50:53,928:INFO:Uploading results into container
2026-02-02 12:50:53,928:INFO:Uploading model into container now
2026-02-02 12:50:53,929:INFO:_master_model_container: 10
2026-02-02 12:50:53,929:INFO:_display_container: 6
2026-02-02 12:50:53,930:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:50:53,930:INFO:create_model() successfully completed......................................
2026-02-02 12:50:54,062:INFO:SubProcess create_model() end ==================================
2026-02-02 12:50:54,063:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9732
2026-02-02 12:50:54,063:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9559
2026-02-02 12:50:54,063:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 12:50:54,063:INFO:choose_better completed
2026-02-02 12:50:54,063:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:50:54,065:INFO:_master_model_container: 10
2026-02-02 12:50:54,065:INFO:_display_container: 5
2026-02-02 12:50:54,065:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:50:54,066:INFO:tune_model() successfully completed......................................
2026-02-02 12:50:54,225:INFO:Initializing predict_model()
2026-02-02 12:50:54,225:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022902822660>)
2026-02-02 12:50:54,225:INFO:Checking exceptions
2026-02-02 12:50:54,225:INFO:Preloading libraries
2026-02-02 12:50:54,225:INFO:Set up data.
2026-02-02 12:50:54,238:INFO:Set up index.
2026-02-02 12:50:54,771:INFO:Initializing predict_model()
2026-02-02 12:50:54,771:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022902822660>)
2026-02-02 12:50:54,771:INFO:Checking exceptions
2026-02-02 12:50:54,771:INFO:Preloading libraries
2026-02-02 12:50:54,771:INFO:Set up data.
2026-02-02 12:50:54,783:INFO:Set up index.
2026-02-02 12:50:55,371:INFO:Initializing predict_model()
2026-02-02 12:50:55,371:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022902822660>)
2026-02-02 12:50:55,371:INFO:Checking exceptions
2026-02-02 12:50:55,371:INFO:Preloading libraries
2026-02-02 12:50:55,371:INFO:Set up data.
2026-02-02 12:50:55,383:INFO:Set up index.
2026-02-02 12:50:55,639:INFO:Initializing predict_model()
2026-02-02 12:50:55,639:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D037D300>)
2026-02-02 12:50:55,639:INFO:Checking exceptions
2026-02-02 12:50:55,639:INFO:Preloading libraries
2026-02-02 12:50:55,639:INFO:Set up data.
2026-02-02 12:50:55,651:INFO:Set up index.
2026-02-02 12:50:56,204:INFO:Initializing plot_model()
2026-02-02 12:50:56,205:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:50:56,205:INFO:Checking exceptions
2026-02-02 12:50:56,279:INFO:Preloading libraries
2026-02-02 12:50:56,381:INFO:Copying training dataset
2026-02-02 12:50:56,381:INFO:Plot type: feature
2026-02-02 12:50:56,381:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:50:56,658:INFO:Visual Rendered Successfully
2026-02-02 12:50:56,789:INFO:plot_model() successfully completed......................................
2026-02-02 12:50:56,796:INFO:Initializing plot_model()
2026-02-02 12:50:56,796:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228D5E0F190>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:50:56,796:INFO:Checking exceptions
2026-02-02 12:50:56,871:INFO:Preloading libraries
2026-02-02 12:50:56,963:INFO:Copying training dataset
2026-02-02 12:50:56,964:INFO:Plot type: feature_all
2026-02-02 12:50:57,066:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:50:57,334:INFO:Visual Rendered Successfully
2026-02-02 12:50:57,468:INFO:plot_model() successfully completed......................................
2026-02-02 12:50:57,480:INFO:Initializing save_model()
2026-02-02 12:50:57,480:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 12:50:57,480:INFO:Adding model into prep_pipe
2026-02-02 12:50:57,622:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-02 12:50:57,626:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-02 12:50:57,626:INFO:save_model() successfully completed......................................
2026-02-02 12:51:55,091:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\1986049682.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 12:51:55,943:INFO:PyCaret ClassificationExperiment
2026-02-02 12:51:55,943:INFO:Logging name: clf-default-name
2026-02-02 12:51:55,943:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 12:51:55,943:INFO:version 3.3.2
2026-02-02 12:51:55,943:INFO:Initializing setup()
2026-02-02 12:51:55,943:INFO:self.USI: cb4f
2026-02-02 12:51:55,943:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 12:51:55,943:INFO:Checking environment
2026-02-02 12:51:55,943:INFO:python_version: 3.11.11
2026-02-02 12:51:55,943:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 12:51:55,943:INFO:machine: AMD64
2026-02-02 12:51:55,943:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 12:51:55,943:INFO:Memory: svmem(total=34009374720, available=11657076736, percent=65.7, used=22352297984, free=11657076736)
2026-02-02 12:51:55,943:INFO:Physical Core: 12
2026-02-02 12:51:55,943:INFO:Logical Core: 16
2026-02-02 12:51:55,944:INFO:Checking libraries
2026-02-02 12:51:55,944:INFO:System:
2026-02-02 12:51:55,944:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 12:51:55,944:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 12:51:55,944:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 12:51:55,944:INFO:PyCaret required dependencies:
2026-02-02 12:51:55,944:INFO:                 pip: 25.0
2026-02-02 12:51:55,944:INFO:          setuptools: 75.8.0
2026-02-02 12:51:55,944:INFO:             pycaret: 3.3.2
2026-02-02 12:51:55,945:INFO:             IPython: 9.9.0
2026-02-02 12:51:55,945:INFO:          ipywidgets: 8.1.8
2026-02-02 12:51:55,945:INFO:                tqdm: 4.67.1
2026-02-02 12:51:55,945:INFO:               numpy: 1.26.4
2026-02-02 12:51:55,945:INFO:              pandas: 2.1.4
2026-02-02 12:51:55,945:INFO:              jinja2: 3.1.6
2026-02-02 12:51:55,945:INFO:               scipy: 1.11.4
2026-02-02 12:51:55,946:INFO:              joblib: 1.3.2
2026-02-02 12:51:55,946:INFO:             sklearn: 1.4.2
2026-02-02 12:51:55,946:INFO:                pyod: 2.0.6
2026-02-02 12:51:55,946:INFO:            imblearn: 0.14.1
2026-02-02 12:51:55,946:INFO:   category_encoders: 2.7.0
2026-02-02 12:51:55,946:INFO:            lightgbm: 4.6.0
2026-02-02 12:51:55,947:INFO:               numba: 0.62.1
2026-02-02 12:51:55,947:INFO:            requests: 2.32.3
2026-02-02 12:51:55,947:INFO:          matplotlib: 3.7.5
2026-02-02 12:51:55,947:INFO:          scikitplot: 0.3.7
2026-02-02 12:51:55,947:INFO:         yellowbrick: 1.5
2026-02-02 12:51:55,947:INFO:              plotly: 5.24.1
2026-02-02 12:51:55,948:INFO:    plotly-resampler: Not installed
2026-02-02 12:51:55,948:INFO:             kaleido: 1.2.0
2026-02-02 12:51:55,948:INFO:           schemdraw: 0.15
2026-02-02 12:51:55,948:INFO:         statsmodels: 0.14.6
2026-02-02 12:51:55,948:INFO:              sktime: 0.26.0
2026-02-02 12:51:55,948:INFO:               tbats: 1.1.3
2026-02-02 12:51:55,948:INFO:            pmdarima: 2.0.4
2026-02-02 12:51:55,948:INFO:              psutil: 7.2.1
2026-02-02 12:51:55,949:INFO:          markupsafe: 3.0.3
2026-02-02 12:51:55,949:INFO:             pickle5: Not installed
2026-02-02 12:51:55,949:INFO:         cloudpickle: 3.0.0
2026-02-02 12:51:55,949:INFO:         deprecation: 2.1.0
2026-02-02 12:51:55,949:INFO:              xxhash: 3.6.0
2026-02-02 12:51:55,949:INFO:           wurlitzer: Not installed
2026-02-02 12:51:55,950:INFO:PyCaret optional dependencies:
2026-02-02 12:51:55,950:INFO:                shap: 0.44.1
2026-02-02 12:51:55,950:INFO:           interpret: 0.7.3
2026-02-02 12:51:55,950:INFO:                umap: 0.5.7
2026-02-02 12:51:55,950:INFO:     ydata_profiling: 4.18.1
2026-02-02 12:51:55,950:INFO:  explainerdashboard: 0.5.1
2026-02-02 12:51:55,950:INFO:             autoviz: Not installed
2026-02-02 12:51:55,950:INFO:           fairlearn: 0.7.0
2026-02-02 12:51:55,951:INFO:          deepchecks: Not installed
2026-02-02 12:51:55,951:INFO:             xgboost: Not installed
2026-02-02 12:51:55,951:INFO:            catboost: 1.2.8
2026-02-02 12:51:55,951:INFO:              kmodes: 0.12.2
2026-02-02 12:51:55,951:INFO:             mlxtend: 0.23.4
2026-02-02 12:51:55,951:INFO:       statsforecast: 1.5.0
2026-02-02 12:51:55,951:INFO:        tune_sklearn: Not installed
2026-02-02 12:51:55,952:INFO:                 ray: Not installed
2026-02-02 12:51:55,952:INFO:            hyperopt: 0.2.7
2026-02-02 12:51:55,952:INFO:              optuna: 4.6.0
2026-02-02 12:51:55,952:INFO:               skopt: 0.10.2
2026-02-02 12:51:55,952:INFO:              mlflow: 3.8.1
2026-02-02 12:51:55,952:INFO:              gradio: 6.3.0
2026-02-02 12:51:55,952:INFO:             fastapi: 0.128.0
2026-02-02 12:51:55,953:INFO:             uvicorn: 0.40.0
2026-02-02 12:51:55,953:INFO:              m2cgen: 0.10.0
2026-02-02 12:51:55,956:INFO:           evidently: 0.4.40
2026-02-02 12:51:55,956:INFO:               fugue: 0.8.7
2026-02-02 12:51:55,957:INFO:           streamlit: Not installed
2026-02-02 12:51:55,957:INFO:             prophet: Not installed
2026-02-02 12:51:55,957:INFO:None
2026-02-02 12:51:55,957:INFO:Set up data.
2026-02-02 12:51:55,963:INFO:Set up folding strategy.
2026-02-02 12:51:55,963:INFO:Set up train/test split.
2026-02-02 12:51:55,970:INFO:Set up index.
2026-02-02 12:51:55,970:INFO:Assigning column types.
2026-02-02 12:51:55,975:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 12:51:56,011:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:51:56,012:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:51:56,037:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:51:56,037:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:51:56,077:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:51:56,112:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:51:56,132:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:51:56,132:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:51:56,133:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 12:51:56,167:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:51:56,188:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:51:56,188:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:51:56,223:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:51:56,245:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:51:56,245:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:51:56,245:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 12:51:56,302:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:51:56,302:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:51:56,361:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:51:56,361:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:51:56,362:INFO:Preparing preprocessing pipeline...
2026-02-02 12:51:56,364:INFO:Set up simple imputation.
2026-02-02 12:51:56,364:INFO:Set up feature normalization.
2026-02-02 12:51:56,395:INFO:Finished creating preprocessing pipeline.
2026-02-02 12:51:56,398:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 12:51:56,398:INFO:Creating final display dataframe.
2026-02-02 12:51:56,503:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 19)
4        Transformed data shape       (10381, 19)
5   Transformed train set shape        (7266, 19)
6    Transformed test set shape        (3115, 19)
7              Numeric features                 8
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              cb4f
2026-02-02 12:51:56,558:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:51:56,558:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:51:56,617:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:51:56,617:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:51:56,620:INFO:setup() successfully completed in 0.69s...............
2026-02-02 12:51:56,621:INFO:Initializing compare_models()
2026-02-02 12:51:56,621:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 12:51:56,621:INFO:Checking exceptions
2026-02-02 12:51:56,625:INFO:Preparing display monitor
2026-02-02 12:51:56,628:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 12:51:56,628:INFO:Total runtime is 0.0 minutes
2026-02-02 12:51:56,628:INFO:SubProcess create_model() called ==================================
2026-02-02 12:51:56,628:INFO:Initializing create_model()
2026-02-02 12:51:56,628:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000229092D2650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:51:56,628:INFO:Checking exceptions
2026-02-02 12:51:56,628:INFO:Importing libraries
2026-02-02 12:51:56,628:INFO:Copying training dataset
2026-02-02 12:51:56,634:INFO:Defining folds
2026-02-02 12:51:56,634:INFO:Declaring metric variables
2026-02-02 12:51:56,634:INFO:Importing untrained model
2026-02-02 12:51:56,634:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:51:56,635:INFO:Starting cross validation
2026-02-02 12:51:56,635:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:51:57,373:INFO:Calculating mean and std
2026-02-02 12:51:57,375:INFO:Creating metrics dataframe
2026-02-02 12:51:57,377:INFO:Uploading results into container
2026-02-02 12:51:57,377:INFO:Uploading model into container now
2026-02-02 12:51:57,378:INFO:_master_model_container: 1
2026-02-02 12:51:57,378:INFO:_display_container: 2
2026-02-02 12:51:57,379:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:51:57,380:INFO:create_model() successfully completed......................................
2026-02-02 12:51:58,084:INFO:SubProcess create_model() end ==================================
2026-02-02 12:51:58,084:INFO:Creating metrics dataframe
2026-02-02 12:51:58,086:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 12:51:58,088:INFO:Initializing create_model()
2026-02-02 12:51:58,088:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:51:58,088:INFO:Checking exceptions
2026-02-02 12:51:58,088:INFO:Importing libraries
2026-02-02 12:51:58,089:INFO:Copying training dataset
2026-02-02 12:51:58,094:INFO:Defining folds
2026-02-02 12:51:58,094:INFO:Declaring metric variables
2026-02-02 12:51:58,094:INFO:Importing untrained model
2026-02-02 12:51:58,095:INFO:Declaring custom model
2026-02-02 12:51:58,095:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:51:58,096:INFO:Cross validation set to False
2026-02-02 12:51:58,096:INFO:Fitting Model
2026-02-02 12:51:58,120:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:51:58,122:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000988 seconds.
2026-02-02 12:51:58,122:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:51:58,122:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 12:51:58,123:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 12:51:58,123:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:51:58,123:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:51:58,299:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:51:58,299:INFO:create_model() successfully completed......................................
2026-02-02 12:51:58,477:INFO:_master_model_container: 1
2026-02-02 12:51:58,477:INFO:_display_container: 2
2026-02-02 12:51:58,477:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:51:58,477:INFO:compare_models() successfully completed......................................
2026-02-02 12:51:58,478:INFO:Initializing tune_model()
2026-02-02 12:51:58,478:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:51:58,478:INFO:Checking exceptions
2026-02-02 12:51:58,482:INFO:Copying training dataset
2026-02-02 12:51:58,486:INFO:Checking base model
2026-02-02 12:51:58,486:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 12:51:58,487:INFO:Declaring metric variables
2026-02-02 12:51:58,487:INFO:Defining Hyperparameters
2026-02-02 12:51:58,622:INFO:Tuning with n_jobs=-1
2026-02-02 12:51:58,624:INFO:Initializing RandomizedSearchCV
2026-02-02 12:52:02,889:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 12:52:02,893:INFO:Hyperparameter search completed
2026-02-02 12:52:02,894:INFO:SubProcess create_model() called ==================================
2026-02-02 12:52:02,896:INFO:Initializing create_model()
2026-02-02 12:52:02,896:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F177EBD0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 12:52:02,896:INFO:Checking exceptions
2026-02-02 12:52:02,897:INFO:Importing libraries
2026-02-02 12:52:02,897:INFO:Copying training dataset
2026-02-02 12:52:02,908:INFO:Defining folds
2026-02-02 12:52:02,908:INFO:Declaring metric variables
2026-02-02 12:52:02,909:INFO:Importing untrained model
2026-02-02 12:52:02,909:INFO:Declaring custom model
2026-02-02 12:52:02,911:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:52:02,911:INFO:Starting cross validation
2026-02-02 12:52:02,913:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:52:03,652:INFO:Calculating mean and std
2026-02-02 12:52:03,654:INFO:Creating metrics dataframe
2026-02-02 12:52:03,657:INFO:Finalizing model
2026-02-02 12:52:03,682:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:52:03,683:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:52:03,683:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:52:03,689:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 12:52:03,690:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 12:52:03,690:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 12:52:03,691:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:52:03,694:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001601 seconds.
2026-02-02 12:52:03,694:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:52:03,694:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 12:52:03,695:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 12:52:03,695:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:52:03,696:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:52:03,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,816:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:52:03,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,883:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 12:52:03,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 12:52:03,924:INFO:Uploading results into container
2026-02-02 12:52:03,925:INFO:Uploading model into container now
2026-02-02 12:52:03,926:INFO:_master_model_container: 2
2026-02-02 12:52:03,926:INFO:_display_container: 3
2026-02-02 12:52:03,927:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:52:03,928:INFO:create_model() successfully completed......................................
2026-02-02 12:52:04,108:INFO:SubProcess create_model() end ==================================
2026-02-02 12:52:04,108:INFO:choose_better activated
2026-02-02 12:52:04,108:INFO:SubProcess create_model() called ==================================
2026-02-02 12:52:04,108:INFO:Initializing create_model()
2026-02-02 12:52:04,108:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:52:04,108:INFO:Checking exceptions
2026-02-02 12:52:04,109:INFO:Importing libraries
2026-02-02 12:52:04,109:INFO:Copying training dataset
2026-02-02 12:52:04,114:INFO:Defining folds
2026-02-02 12:52:04,115:INFO:Declaring metric variables
2026-02-02 12:52:04,115:INFO:Importing untrained model
2026-02-02 12:52:04,115:INFO:Declaring custom model
2026-02-02 12:52:04,116:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:52:04,116:INFO:Starting cross validation
2026-02-02 12:52:04,116:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:52:04,830:INFO:Calculating mean and std
2026-02-02 12:52:04,830:INFO:Creating metrics dataframe
2026-02-02 12:52:04,833:INFO:Finalizing model
2026-02-02 12:52:04,867:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 12:52:04,870:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001956 seconds.
2026-02-02 12:52:04,871:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 12:52:04,871:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 12:52:04,871:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 12:52:04,872:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 12:52:04,872:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 12:52:05,046:INFO:Uploading results into container
2026-02-02 12:52:05,046:INFO:Uploading model into container now
2026-02-02 12:52:05,047:INFO:_master_model_container: 3
2026-02-02 12:52:05,047:INFO:_display_container: 4
2026-02-02 12:52:05,048:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:52:05,048:INFO:create_model() successfully completed......................................
2026-02-02 12:52:05,228:INFO:SubProcess create_model() end ==================================
2026-02-02 12:52:05,229:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9974
2026-02-02 12:52:05,229:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9946
2026-02-02 12:52:05,230:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 12:52:05,230:INFO:choose_better completed
2026-02-02 12:52:05,230:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:52:05,232:INFO:_master_model_container: 3
2026-02-02 12:52:05,232:INFO:_display_container: 3
2026-02-02 12:52:05,233:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:52:05,233:INFO:tune_model() successfully completed......................................
2026-02-02 12:52:05,377:INFO:Initializing predict_model()
2026-02-02 12:52:05,377:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002283AA1D9E0>)
2026-02-02 12:52:05,377:INFO:Checking exceptions
2026-02-02 12:52:05,377:INFO:Preloading libraries
2026-02-02 12:52:05,377:INFO:Set up data.
2026-02-02 12:52:05,380:INFO:Set up index.
2026-02-02 12:52:08,120:INFO:Initializing plot_model()
2026-02-02 12:52:08,121:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:52:08,121:INFO:Checking exceptions
2026-02-02 12:52:08,125:INFO:Preloading libraries
2026-02-02 12:52:08,135:INFO:Copying training dataset
2026-02-02 12:52:08,135:INFO:Plot type: feature
2026-02-02 12:52:08,137:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:52:08,310:INFO:Visual Rendered Successfully
2026-02-02 12:52:08,478:INFO:plot_model() successfully completed......................................
2026-02-02 12:52:08,480:INFO:Initializing plot_model()
2026-02-02 12:52:08,480:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022866F00410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 12:52:08,480:INFO:Checking exceptions
2026-02-02 12:52:08,484:INFO:Preloading libraries
2026-02-02 12:52:08,489:INFO:Copying training dataset
2026-02-02 12:52:08,489:INFO:Plot type: feature_all
2026-02-02 12:52:08,537:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 12:52:08,731:INFO:Visual Rendered Successfully
2026-02-02 12:52:08,873:INFO:plot_model() successfully completed......................................
2026-02-02 12:52:08,877:INFO:Initializing save_model()
2026-02-02 12:52:08,877:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 12:52:08,877:INFO:Adding model into prep_pipe
2026-02-02 12:52:08,887:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-02 12:52:08,895:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_feat...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-02 12:52:08,895:INFO:save_model() successfully completed......................................
2026-02-02 12:55:35,708:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\2258415846.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 12:55:37,743:INFO:PyCaret ClassificationExperiment
2026-02-02 12:55:37,744:INFO:Logging name: clf-default-name
2026-02-02 12:55:37,744:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 12:55:37,744:INFO:version 3.3.2
2026-02-02 12:55:37,745:INFO:Initializing setup()
2026-02-02 12:55:37,745:INFO:self.USI: 31c0
2026-02-02 12:55:37,745:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 12:55:37,745:INFO:Checking environment
2026-02-02 12:55:37,745:INFO:python_version: 3.11.11
2026-02-02 12:55:37,745:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 12:55:37,745:INFO:machine: AMD64
2026-02-02 12:55:37,745:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 12:55:37,745:INFO:Memory: svmem(total=34009374720, available=11154743296, percent=67.2, used=22854631424, free=11154743296)
2026-02-02 12:55:37,745:INFO:Physical Core: 12
2026-02-02 12:55:37,745:INFO:Logical Core: 16
2026-02-02 12:55:37,745:INFO:Checking libraries
2026-02-02 12:55:37,745:INFO:System:
2026-02-02 12:55:37,746:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 12:55:37,746:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 12:55:37,746:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 12:55:37,746:INFO:PyCaret required dependencies:
2026-02-02 12:55:37,746:INFO:                 pip: 25.0
2026-02-02 12:55:37,746:INFO:          setuptools: 75.8.0
2026-02-02 12:55:37,746:INFO:             pycaret: 3.3.2
2026-02-02 12:55:37,746:INFO:             IPython: 9.9.0
2026-02-02 12:55:37,746:INFO:          ipywidgets: 8.1.8
2026-02-02 12:55:37,746:INFO:                tqdm: 4.67.1
2026-02-02 12:55:37,746:INFO:               numpy: 1.26.4
2026-02-02 12:55:37,746:INFO:              pandas: 2.1.4
2026-02-02 12:55:37,746:INFO:              jinja2: 3.1.6
2026-02-02 12:55:37,746:INFO:               scipy: 1.11.4
2026-02-02 12:55:37,746:INFO:              joblib: 1.3.2
2026-02-02 12:55:37,746:INFO:             sklearn: 1.4.2
2026-02-02 12:55:37,746:INFO:                pyod: 2.0.6
2026-02-02 12:55:37,746:INFO:            imblearn: 0.14.1
2026-02-02 12:55:37,746:INFO:   category_encoders: 2.7.0
2026-02-02 12:55:37,746:INFO:            lightgbm: 4.6.0
2026-02-02 12:55:37,746:INFO:               numba: 0.62.1
2026-02-02 12:55:37,746:INFO:            requests: 2.32.3
2026-02-02 12:55:37,746:INFO:          matplotlib: 3.7.5
2026-02-02 12:55:37,746:INFO:          scikitplot: 0.3.7
2026-02-02 12:55:37,746:INFO:         yellowbrick: 1.5
2026-02-02 12:55:37,746:INFO:              plotly: 5.24.1
2026-02-02 12:55:37,746:INFO:    plotly-resampler: Not installed
2026-02-02 12:55:37,746:INFO:             kaleido: 1.2.0
2026-02-02 12:55:37,746:INFO:           schemdraw: 0.15
2026-02-02 12:55:37,746:INFO:         statsmodels: 0.14.6
2026-02-02 12:55:37,746:INFO:              sktime: 0.26.0
2026-02-02 12:55:37,746:INFO:               tbats: 1.1.3
2026-02-02 12:55:37,746:INFO:            pmdarima: 2.0.4
2026-02-02 12:55:37,746:INFO:              psutil: 7.2.1
2026-02-02 12:55:37,746:INFO:          markupsafe: 3.0.3
2026-02-02 12:55:37,746:INFO:             pickle5: Not installed
2026-02-02 12:55:37,746:INFO:         cloudpickle: 3.0.0
2026-02-02 12:55:37,746:INFO:         deprecation: 2.1.0
2026-02-02 12:55:37,746:INFO:              xxhash: 3.6.0
2026-02-02 12:55:37,746:INFO:           wurlitzer: Not installed
2026-02-02 12:55:37,747:INFO:PyCaret optional dependencies:
2026-02-02 12:55:37,747:INFO:                shap: 0.44.1
2026-02-02 12:55:37,747:INFO:           interpret: 0.7.3
2026-02-02 12:55:37,747:INFO:                umap: 0.5.7
2026-02-02 12:55:37,747:INFO:     ydata_profiling: 4.18.1
2026-02-02 12:55:37,747:INFO:  explainerdashboard: 0.5.1
2026-02-02 12:55:37,747:INFO:             autoviz: Not installed
2026-02-02 12:55:37,747:INFO:           fairlearn: 0.7.0
2026-02-02 12:55:37,747:INFO:          deepchecks: Not installed
2026-02-02 12:55:37,747:INFO:             xgboost: Not installed
2026-02-02 12:55:37,747:INFO:            catboost: 1.2.8
2026-02-02 12:55:37,747:INFO:              kmodes: 0.12.2
2026-02-02 12:55:37,747:INFO:             mlxtend: 0.23.4
2026-02-02 12:55:37,747:INFO:       statsforecast: 1.5.0
2026-02-02 12:55:37,747:INFO:        tune_sklearn: Not installed
2026-02-02 12:55:37,747:INFO:                 ray: Not installed
2026-02-02 12:55:37,747:INFO:            hyperopt: 0.2.7
2026-02-02 12:55:37,747:INFO:              optuna: 4.6.0
2026-02-02 12:55:37,747:INFO:               skopt: 0.10.2
2026-02-02 12:55:37,747:INFO:              mlflow: 3.8.1
2026-02-02 12:55:37,747:INFO:              gradio: 6.3.0
2026-02-02 12:55:37,747:INFO:             fastapi: 0.128.0
2026-02-02 12:55:37,747:INFO:             uvicorn: 0.40.0
2026-02-02 12:55:37,747:INFO:              m2cgen: 0.10.0
2026-02-02 12:55:37,747:INFO:           evidently: 0.4.40
2026-02-02 12:55:37,747:INFO:               fugue: 0.8.7
2026-02-02 12:55:37,747:INFO:           streamlit: Not installed
2026-02-02 12:55:37,747:INFO:             prophet: Not installed
2026-02-02 12:55:37,747:INFO:None
2026-02-02 12:55:37,747:INFO:Set up data.
2026-02-02 12:55:37,817:INFO:Set up folding strategy.
2026-02-02 12:55:37,817:INFO:Set up train/test split.
2026-02-02 12:55:37,956:INFO:Set up index.
2026-02-02 12:55:37,963:INFO:Assigning column types.
2026-02-02 12:55:38,035:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 12:55:38,064:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:55:38,065:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:55:38,083:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:55:38,083:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:55:38,110:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 12:55:38,110:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:55:38,128:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:55:38,128:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:55:38,128:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 12:55:38,158:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:55:38,176:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:55:38,177:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:55:38,205:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 12:55:38,223:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:55:38,224:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:55:38,224:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 12:55:38,270:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:55:38,270:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:55:38,316:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:55:38,318:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:55:38,318:INFO:Preparing preprocessing pipeline...
2026-02-02 12:55:38,334:INFO:Set up simple imputation.
2026-02-02 12:55:38,334:INFO:Set up feature normalization.
2026-02-02 12:55:38,530:INFO:Finished creating preprocessing pipeline.
2026-02-02 12:55:38,533:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 12:55:38,534:INFO:Creating final display dataframe.
2026-02-02 12:55:38,873:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 17)
4        Transformed data shape      (373023, 17)
5   Transformed train set shape      (261116, 17)
6    Transformed test set shape      (111907, 17)
7              Numeric features                13
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              31c0
2026-02-02 12:55:38,926:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:55:38,927:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:55:38,973:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 12:55:38,973:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 12:55:38,975:INFO:setup() successfully completed in 1.24s...............
2026-02-02 12:55:38,975:INFO:Initializing compare_models()
2026-02-02 12:55:38,975:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 12:55:38,975:INFO:Checking exceptions
2026-02-02 12:55:39,034:INFO:Preparing display monitor
2026-02-02 12:55:39,036:INFO:Initializing Logistic Regression
2026-02-02 12:55:39,036:INFO:Total runtime is 0.0 minutes
2026-02-02 12:55:39,037:INFO:SubProcess create_model() called ==================================
2026-02-02 12:55:39,037:INFO:Initializing create_model()
2026-02-02 12:55:39,037:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228CAF3A450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:55:39,037:INFO:Checking exceptions
2026-02-02 12:55:39,037:INFO:Importing libraries
2026-02-02 12:55:39,037:INFO:Copying training dataset
2026-02-02 12:55:39,139:INFO:Defining folds
2026-02-02 12:55:39,139:INFO:Declaring metric variables
2026-02-02 12:55:39,139:INFO:Importing untrained model
2026-02-02 12:55:39,139:INFO:Logistic Regression Imported successfully
2026-02-02 12:55:39,140:INFO:Starting cross validation
2026-02-02 12:55:39,140:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:55:41,220:INFO:Calculating mean and std
2026-02-02 12:55:41,224:INFO:Creating metrics dataframe
2026-02-02 12:55:41,226:INFO:Uploading results into container
2026-02-02 12:55:41,227:INFO:Uploading model into container now
2026-02-02 12:55:41,228:INFO:_master_model_container: 1
2026-02-02 12:55:41,228:INFO:_display_container: 2
2026-02-02 12:55:41,229:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 12:55:41,229:INFO:create_model() successfully completed......................................
2026-02-02 12:55:41,400:INFO:SubProcess create_model() end ==================================
2026-02-02 12:55:41,400:INFO:Creating metrics dataframe
2026-02-02 12:55:41,403:INFO:Initializing Decision Tree Classifier
2026-02-02 12:55:41,404:INFO:Total runtime is 0.039462427298227944 minutes
2026-02-02 12:55:41,404:INFO:SubProcess create_model() called ==================================
2026-02-02 12:55:41,404:INFO:Initializing create_model()
2026-02-02 12:55:41,405:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228CAF3A450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:55:41,405:INFO:Checking exceptions
2026-02-02 12:55:41,405:INFO:Importing libraries
2026-02-02 12:55:41,405:INFO:Copying training dataset
2026-02-02 12:55:41,539:INFO:Defining folds
2026-02-02 12:55:41,540:INFO:Declaring metric variables
2026-02-02 12:55:41,540:INFO:Importing untrained model
2026-02-02 12:55:41,540:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:55:41,542:INFO:Starting cross validation
2026-02-02 12:55:41,543:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:55:44,669:INFO:Calculating mean and std
2026-02-02 12:55:44,677:INFO:Creating metrics dataframe
2026-02-02 12:55:44,680:INFO:Uploading results into container
2026-02-02 12:55:44,683:INFO:Uploading model into container now
2026-02-02 12:55:44,684:INFO:_master_model_container: 2
2026-02-02 12:55:44,684:INFO:_display_container: 2
2026-02-02 12:55:44,685:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:55:44,686:INFO:create_model() successfully completed......................................
2026-02-02 12:55:44,852:INFO:SubProcess create_model() end ==================================
2026-02-02 12:55:44,853:INFO:Creating metrics dataframe
2026-02-02 12:55:44,855:INFO:Initializing Random Forest Classifier
2026-02-02 12:55:44,855:INFO:Total runtime is 0.0969805359840393 minutes
2026-02-02 12:55:44,856:INFO:SubProcess create_model() called ==================================
2026-02-02 12:55:44,856:INFO:Initializing create_model()
2026-02-02 12:55:44,856:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228CAF3A450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:55:44,857:INFO:Checking exceptions
2026-02-02 12:55:44,857:INFO:Importing libraries
2026-02-02 12:55:44,857:INFO:Copying training dataset
2026-02-02 12:55:44,962:INFO:Defining folds
2026-02-02 12:55:44,962:INFO:Declaring metric variables
2026-02-02 12:55:44,963:INFO:Importing untrained model
2026-02-02 12:55:44,963:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:55:44,964:INFO:Starting cross validation
2026-02-02 12:55:44,965:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:56:02,668:INFO:Calculating mean and std
2026-02-02 12:56:02,670:INFO:Creating metrics dataframe
2026-02-02 12:56:02,671:INFO:Uploading results into container
2026-02-02 12:56:02,672:INFO:Uploading model into container now
2026-02-02 12:56:02,672:INFO:_master_model_container: 3
2026-02-02 12:56:02,673:INFO:_display_container: 2
2026-02-02 12:56:02,673:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:56:02,674:INFO:create_model() successfully completed......................................
2026-02-02 12:56:02,825:INFO:SubProcess create_model() end ==================================
2026-02-02 12:56:02,826:INFO:Creating metrics dataframe
2026-02-02 12:56:02,828:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 12:56:02,828:INFO:Total runtime is 0.3965240160624186 minutes
2026-02-02 12:56:02,828:INFO:SubProcess create_model() called ==================================
2026-02-02 12:56:02,828:INFO:Initializing create_model()
2026-02-02 12:56:02,828:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228CAF3A450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:56:02,828:INFO:Checking exceptions
2026-02-02 12:56:02,828:INFO:Importing libraries
2026-02-02 12:56:02,829:INFO:Copying training dataset
2026-02-02 12:56:02,944:INFO:Defining folds
2026-02-02 12:56:02,945:INFO:Declaring metric variables
2026-02-02 12:56:02,946:INFO:Importing untrained model
2026-02-02 12:56:02,947:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:56:02,947:INFO:Starting cross validation
2026-02-02 12:56:02,948:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:56:06,694:INFO:Calculating mean and std
2026-02-02 12:56:06,696:INFO:Creating metrics dataframe
2026-02-02 12:56:06,698:INFO:Uploading results into container
2026-02-02 12:56:06,698:INFO:Uploading model into container now
2026-02-02 12:56:06,699:INFO:_master_model_container: 4
2026-02-02 12:56:06,699:INFO:_display_container: 2
2026-02-02 12:56:06,700:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:56:06,700:INFO:create_model() successfully completed......................................
2026-02-02 12:56:06,856:INFO:SubProcess create_model() end ==================================
2026-02-02 12:56:06,856:INFO:Creating metrics dataframe
2026-02-02 12:56:06,859:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 12:56:06,860:INFO:Initializing create_model()
2026-02-02 12:56:06,860:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:56:06,860:INFO:Checking exceptions
2026-02-02 12:56:06,861:INFO:Importing libraries
2026-02-02 12:56:06,861:INFO:Copying training dataset
2026-02-02 12:56:06,966:INFO:Defining folds
2026-02-02 12:56:06,967:INFO:Declaring metric variables
2026-02-02 12:56:06,967:INFO:Importing untrained model
2026-02-02 12:56:06,967:INFO:Declaring custom model
2026-02-02 12:56:06,967:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:56:06,967:INFO:Cross validation set to False
2026-02-02 12:56:06,967:INFO:Fitting Model
2026-02-02 12:56:15,472:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:56:15,472:INFO:create_model() successfully completed......................................
2026-02-02 12:56:15,640:INFO:Initializing create_model()
2026-02-02 12:56:15,640:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:56:15,640:INFO:Checking exceptions
2026-02-02 12:56:15,641:INFO:Importing libraries
2026-02-02 12:56:15,641:INFO:Copying training dataset
2026-02-02 12:56:15,758:INFO:Defining folds
2026-02-02 12:56:15,759:INFO:Declaring metric variables
2026-02-02 12:56:15,759:INFO:Importing untrained model
2026-02-02 12:56:15,759:INFO:Declaring custom model
2026-02-02 12:56:15,759:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 12:56:15,761:INFO:Cross validation set to False
2026-02-02 12:56:15,761:INFO:Fitting Model
2026-02-02 12:56:16,190:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 12:56:16,203:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005028 seconds.
2026-02-02 12:56:16,203:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 12:56:16,204:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 12:56:16,204:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 12:56:16,204:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 12:56:16,206:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 12:56:16,206:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 12:56:16,973:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 12:56:16,973:INFO:create_model() successfully completed......................................
2026-02-02 12:56:17,202:INFO:Initializing create_model()
2026-02-02 12:56:17,203:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:56:17,203:INFO:Checking exceptions
2026-02-02 12:56:17,204:INFO:Importing libraries
2026-02-02 12:56:17,204:INFO:Copying training dataset
2026-02-02 12:56:17,358:INFO:Defining folds
2026-02-02 12:56:17,358:INFO:Declaring metric variables
2026-02-02 12:56:17,358:INFO:Importing untrained model
2026-02-02 12:56:17,358:INFO:Declaring custom model
2026-02-02 12:56:17,359:INFO:Decision Tree Classifier Imported successfully
2026-02-02 12:56:17,359:INFO:Cross validation set to False
2026-02-02 12:56:17,360:INFO:Fitting Model
2026-02-02 12:56:19,825:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 12:56:19,825:INFO:create_model() successfully completed......................................
2026-02-02 12:56:19,977:INFO:_master_model_container: 4
2026-02-02 12:56:19,977:INFO:_display_container: 2
2026-02-02 12:56:19,979:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')]
2026-02-02 12:56:19,979:INFO:compare_models() successfully completed......................................
2026-02-02 12:56:19,991:INFO:Initializing tune_model()
2026-02-02 12:56:19,991:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:56:19,991:INFO:Checking exceptions
2026-02-02 12:56:20,034:INFO:Copying training dataset
2026-02-02 12:56:20,106:INFO:Checking base model
2026-02-02 12:56:20,106:INFO:Base model : Random Forest Classifier
2026-02-02 12:56:20,107:INFO:Declaring metric variables
2026-02-02 12:56:20,107:INFO:Defining Hyperparameters
2026-02-02 12:56:20,248:INFO:Tuning with n_jobs=-1
2026-02-02 12:56:20,248:INFO:Initializing RandomizedSearchCV
2026-02-02 12:58:31,200:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 12:58:31,201:INFO:Hyperparameter search completed
2026-02-02 12:58:31,202:INFO:SubProcess create_model() called ==================================
2026-02-02 12:58:31,202:INFO:Initializing create_model()
2026-02-02 12:58:31,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228CB28FC90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-02 12:58:31,203:INFO:Checking exceptions
2026-02-02 12:58:31,203:INFO:Importing libraries
2026-02-02 12:58:31,203:INFO:Copying training dataset
2026-02-02 12:58:31,412:INFO:Defining folds
2026-02-02 12:58:31,413:INFO:Declaring metric variables
2026-02-02 12:58:31,414:INFO:Importing untrained model
2026-02-02 12:58:31,414:INFO:Declaring custom model
2026-02-02 12:58:31,416:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:58:31,416:INFO:Starting cross validation
2026-02-02 12:58:31,417:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:58:59,877:INFO:Calculating mean and std
2026-02-02 12:58:59,878:INFO:Creating metrics dataframe
2026-02-02 12:58:59,881:INFO:Finalizing model
2026-02-02 12:59:13,104:INFO:Uploading results into container
2026-02-02 12:59:13,105:INFO:Uploading model into container now
2026-02-02 12:59:13,106:INFO:_master_model_container: 5
2026-02-02 12:59:13,106:INFO:_display_container: 3
2026-02-02 12:59:13,107:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:59:13,107:INFO:create_model() successfully completed......................................
2026-02-02 12:59:13,277:INFO:SubProcess create_model() end ==================================
2026-02-02 12:59:13,277:INFO:choose_better activated
2026-02-02 12:59:13,277:INFO:SubProcess create_model() called ==================================
2026-02-02 12:59:13,278:INFO:Initializing create_model()
2026-02-02 12:59:13,278:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 12:59:13,279:INFO:Checking exceptions
2026-02-02 12:59:13,280:INFO:Importing libraries
2026-02-02 12:59:13,280:INFO:Copying training dataset
2026-02-02 12:59:13,420:INFO:Defining folds
2026-02-02 12:59:13,420:INFO:Declaring metric variables
2026-02-02 12:59:13,420:INFO:Importing untrained model
2026-02-02 12:59:13,420:INFO:Declaring custom model
2026-02-02 12:59:13,421:INFO:Random Forest Classifier Imported successfully
2026-02-02 12:59:13,421:INFO:Starting cross validation
2026-02-02 12:59:13,422:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 12:59:31,510:INFO:Calculating mean and std
2026-02-02 12:59:31,511:INFO:Creating metrics dataframe
2026-02-02 12:59:31,515:INFO:Finalizing model
2026-02-02 12:59:40,356:INFO:Uploading results into container
2026-02-02 12:59:40,356:INFO:Uploading model into container now
2026-02-02 12:59:40,362:INFO:_master_model_container: 6
2026-02-02 12:59:40,362:INFO:_display_container: 4
2026-02-02 12:59:40,364:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:59:40,364:INFO:create_model() successfully completed......................................
2026-02-02 12:59:40,550:INFO:SubProcess create_model() end ==================================
2026-02-02 12:59:40,551:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9956
2026-02-02 12:59:40,551:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9648
2026-02-02 12:59:40,552:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 12:59:40,552:INFO:choose_better completed
2026-02-02 12:59:40,552:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 12:59:40,555:INFO:_master_model_container: 6
2026-02-02 12:59:40,555:INFO:_display_container: 3
2026-02-02 12:59:40,555:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 12:59:40,555:INFO:tune_model() successfully completed......................................
2026-02-02 12:59:40,713:INFO:Initializing tune_model()
2026-02-02 12:59:40,713:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 12:59:40,713:INFO:Checking exceptions
2026-02-02 12:59:40,764:INFO:Copying training dataset
2026-02-02 12:59:40,834:INFO:Checking base model
2026-02-02 12:59:40,834:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 12:59:40,834:INFO:Declaring metric variables
2026-02-02 12:59:40,834:INFO:Defining Hyperparameters
2026-02-02 12:59:40,988:INFO:Tuning with n_jobs=-1
2026-02-02 12:59:40,988:INFO:Initializing RandomizedSearchCV
2026-02-02 13:00:10,074:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 13:00:10,074:INFO:Hyperparameter search completed
2026-02-02 13:00:10,078:INFO:SubProcess create_model() called ==================================
2026-02-02 13:00:10,079:INFO:Initializing create_model()
2026-02-02 13:00:10,079:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002285CC7AA90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 13:00:10,080:INFO:Checking exceptions
2026-02-02 13:00:10,080:INFO:Importing libraries
2026-02-02 13:00:10,080:INFO:Copying training dataset
2026-02-02 13:00:10,213:INFO:Defining folds
2026-02-02 13:00:10,213:INFO:Declaring metric variables
2026-02-02 13:00:10,213:INFO:Importing untrained model
2026-02-02 13:00:10,213:INFO:Declaring custom model
2026-02-02 13:00:10,213:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:00:10,213:INFO:Starting cross validation
2026-02-02 13:00:10,213:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:00:17,263:INFO:Calculating mean and std
2026-02-02 13:00:17,264:INFO:Creating metrics dataframe
2026-02-02 13:00:17,264:INFO:Finalizing model
2026-02-02 13:00:17,563:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:00:17,563:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:00:17,563:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:00:17,683:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:00:17,683:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:00:17,683:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:00:17,683:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 13:00:17,693:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004266 seconds.
2026-02-02 13:00:17,693:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:00:17,693:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:00:17,693:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 13:00:17,694:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 13:00:17,697:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 13:00:17,698:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 13:00:19,900:INFO:Uploading results into container
2026-02-02 13:00:19,902:INFO:Uploading model into container now
2026-02-02 13:00:19,903:INFO:_master_model_container: 7
2026-02-02 13:00:19,903:INFO:_display_container: 4
2026-02-02 13:00:19,905:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:00:19,906:INFO:create_model() successfully completed......................................
2026-02-02 13:00:20,114:INFO:SubProcess create_model() end ==================================
2026-02-02 13:00:20,115:INFO:choose_better activated
2026-02-02 13:00:20,116:INFO:SubProcess create_model() called ==================================
2026-02-02 13:00:20,116:INFO:Initializing create_model()
2026-02-02 13:00:20,116:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:00:20,116:INFO:Checking exceptions
2026-02-02 13:00:20,116:INFO:Importing libraries
2026-02-02 13:00:20,116:INFO:Copying training dataset
2026-02-02 13:00:20,246:INFO:Defining folds
2026-02-02 13:00:20,246:INFO:Declaring metric variables
2026-02-02 13:00:20,246:INFO:Importing untrained model
2026-02-02 13:00:20,246:INFO:Declaring custom model
2026-02-02 13:00:20,246:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:00:20,246:INFO:Starting cross validation
2026-02-02 13:00:20,246:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:00:23,624:INFO:Calculating mean and std
2026-02-02 13:00:23,624:INFO:Creating metrics dataframe
2026-02-02 13:00:23,629:INFO:Finalizing model
2026-02-02 13:00:24,048:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 13:00:24,060:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004769 seconds.
2026-02-02 13:00:24,060:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:00:24,060:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:00:24,060:INFO:[LightGBM] [Info] Total Bins 2096
2026-02-02 13:00:24,061:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 16
2026-02-02 13:00:24,062:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 13:00:24,062:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 13:00:24,679:INFO:Uploading results into container
2026-02-02 13:00:24,681:INFO:Uploading model into container now
2026-02-02 13:00:24,681:INFO:_master_model_container: 8
2026-02-02 13:00:24,681:INFO:_display_container: 5
2026-02-02 13:00:24,683:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:00:24,683:INFO:create_model() successfully completed......................................
2026-02-02 13:00:24,879:INFO:SubProcess create_model() end ==================================
2026-02-02 13:00:24,879:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9767
2026-02-02 13:00:24,879:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9928
2026-02-02 13:00:24,879:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 13:00:24,879:INFO:choose_better completed
2026-02-02 13:00:24,879:INFO:_master_model_container: 8
2026-02-02 13:00:24,879:INFO:_display_container: 4
2026-02-02 13:00:24,879:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:00:24,879:INFO:tune_model() successfully completed......................................
2026-02-02 13:00:25,035:INFO:Initializing tune_model()
2026-02-02 13:00:25,035:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:00:25,035:INFO:Checking exceptions
2026-02-02 13:00:25,065:INFO:Copying training dataset
2026-02-02 13:00:25,145:INFO:Checking base model
2026-02-02 13:00:25,145:INFO:Base model : Decision Tree Classifier
2026-02-02 13:00:25,146:INFO:Declaring metric variables
2026-02-02 13:00:25,146:INFO:Defining Hyperparameters
2026-02-02 13:00:25,305:INFO:Tuning with n_jobs=-1
2026-02-02 13:00:25,305:INFO:Initializing RandomizedSearchCV
2026-02-02 13:00:30,022:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-02-02 13:00:30,022:INFO:Hyperparameter search completed
2026-02-02 13:00:30,022:INFO:SubProcess create_model() called ==================================
2026-02-02 13:00:30,022:INFO:Initializing create_model()
2026-02-02 13:00:30,022:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D4272E90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-02-02 13:00:30,022:INFO:Checking exceptions
2026-02-02 13:00:30,022:INFO:Importing libraries
2026-02-02 13:00:30,022:INFO:Copying training dataset
2026-02-02 13:00:30,146:INFO:Defining folds
2026-02-02 13:00:30,146:INFO:Declaring metric variables
2026-02-02 13:00:30,146:INFO:Importing untrained model
2026-02-02 13:00:30,146:INFO:Declaring custom model
2026-02-02 13:00:30,146:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:00:30,146:INFO:Starting cross validation
2026-02-02 13:00:30,146:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:00:32,077:INFO:Calculating mean and std
2026-02-02 13:00:32,080:INFO:Creating metrics dataframe
2026-02-02 13:00:32,083:INFO:Finalizing model
2026-02-02 13:00:33,205:INFO:Uploading results into container
2026-02-02 13:00:33,206:INFO:Uploading model into container now
2026-02-02 13:00:33,206:INFO:_master_model_container: 9
2026-02-02 13:00:33,207:INFO:_display_container: 5
2026-02-02 13:00:33,207:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:00:33,207:INFO:create_model() successfully completed......................................
2026-02-02 13:00:33,329:INFO:SubProcess create_model() end ==================================
2026-02-02 13:00:33,329:INFO:choose_better activated
2026-02-02 13:00:33,329:INFO:SubProcess create_model() called ==================================
2026-02-02 13:00:33,329:INFO:Initializing create_model()
2026-02-02 13:00:33,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:00:33,329:INFO:Checking exceptions
2026-02-02 13:00:33,329:INFO:Importing libraries
2026-02-02 13:00:33,329:INFO:Copying training dataset
2026-02-02 13:00:33,431:INFO:Defining folds
2026-02-02 13:00:33,431:INFO:Declaring metric variables
2026-02-02 13:00:33,431:INFO:Importing untrained model
2026-02-02 13:00:33,431:INFO:Declaring custom model
2026-02-02 13:00:33,431:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:00:33,431:INFO:Starting cross validation
2026-02-02 13:00:33,431:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:00:35,616:INFO:Calculating mean and std
2026-02-02 13:00:35,616:INFO:Creating metrics dataframe
2026-02-02 13:00:35,616:INFO:Finalizing model
2026-02-02 13:00:37,511:INFO:Uploading results into container
2026-02-02 13:00:37,511:INFO:Uploading model into container now
2026-02-02 13:00:37,512:INFO:_master_model_container: 10
2026-02-02 13:00:37,512:INFO:_display_container: 6
2026-02-02 13:00:37,512:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:00:37,512:INFO:create_model() successfully completed......................................
2026-02-02 13:00:37,629:INFO:SubProcess create_model() end ==================================
2026-02-02 13:00:37,629:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9732
2026-02-02 13:00:37,629:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9559
2026-02-02 13:00:37,629:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 13:00:37,629:INFO:choose_better completed
2026-02-02 13:00:37,629:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:00:37,645:INFO:_master_model_container: 10
2026-02-02 13:00:37,646:INFO:_display_container: 5
2026-02-02 13:00:37,646:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:00:37,646:INFO:tune_model() successfully completed......................................
2026-02-02 13:00:37,796:INFO:Initializing predict_model()
2026-02-02 13:00:37,797:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D037D300>)
2026-02-02 13:00:37,797:INFO:Checking exceptions
2026-02-02 13:00:37,797:INFO:Preloading libraries
2026-02-02 13:00:37,797:INFO:Set up data.
2026-02-02 13:00:37,809:INFO:Set up index.
2026-02-02 13:00:38,379:INFO:Initializing predict_model()
2026-02-02 13:00:38,379:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D037D300>)
2026-02-02 13:00:38,379:INFO:Checking exceptions
2026-02-02 13:00:38,379:INFO:Preloading libraries
2026-02-02 13:00:38,379:INFO:Set up data.
2026-02-02 13:00:38,398:INFO:Set up index.
2026-02-02 13:00:39,106:INFO:Initializing predict_model()
2026-02-02 13:00:39,106:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D037D300>)
2026-02-02 13:00:39,106:INFO:Checking exceptions
2026-02-02 13:00:39,106:INFO:Preloading libraries
2026-02-02 13:00:39,106:INFO:Set up data.
2026-02-02 13:00:39,112:INFO:Set up index.
2026-02-02 13:00:39,378:INFO:Initializing predict_model()
2026-02-02 13:00:39,378:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D2EB6B60>)
2026-02-02 13:00:39,379:INFO:Checking exceptions
2026-02-02 13:00:39,379:INFO:Preloading libraries
2026-02-02 13:00:39,379:INFO:Set up data.
2026-02-02 13:00:39,379:INFO:Set up index.
2026-02-02 13:00:40,012:INFO:Initializing plot_model()
2026-02-02 13:00:40,012:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:00:40,012:INFO:Checking exceptions
2026-02-02 13:00:40,103:INFO:Preloading libraries
2026-02-02 13:00:40,210:INFO:Copying training dataset
2026-02-02 13:00:40,210:INFO:Plot type: feature
2026-02-02 13:00:40,210:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:00:40,446:INFO:Visual Rendered Successfully
2026-02-02 13:00:40,579:INFO:plot_model() successfully completed......................................
2026-02-02 13:00:40,596:INFO:Initializing plot_model()
2026-02-02 13:00:40,596:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228E97F1590>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:00:40,596:INFO:Checking exceptions
2026-02-02 13:00:40,646:INFO:Preloading libraries
2026-02-02 13:00:40,744:INFO:Copying training dataset
2026-02-02 13:00:40,744:INFO:Plot type: feature_all
2026-02-02 13:00:40,846:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:00:41,096:INFO:Visual Rendered Successfully
2026-02-02 13:00:41,248:INFO:plot_model() successfully completed......................................
2026-02-02 13:00:41,254:INFO:Initializing save_model()
2026-02-02 13:00:41,254:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 13:00:41,254:INFO:Adding model into prep_pipe
2026-02-02 13:00:41,413:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-02 13:00:41,413:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-02 13:00:41,413:INFO:save_model() successfully completed......................................
2026-02-02 13:01:35,052:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\4190064707.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 13:01:35,903:INFO:PyCaret ClassificationExperiment
2026-02-02 13:01:35,903:INFO:Logging name: clf-default-name
2026-02-02 13:01:35,903:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 13:01:35,903:INFO:version 3.3.2
2026-02-02 13:01:35,903:INFO:Initializing setup()
2026-02-02 13:01:35,903:INFO:self.USI: 6cbe
2026-02-02 13:01:35,903:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 13:01:35,903:INFO:Checking environment
2026-02-02 13:01:35,903:INFO:python_version: 3.11.11
2026-02-02 13:01:35,903:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 13:01:35,903:INFO:machine: AMD64
2026-02-02 13:01:35,903:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 13:01:35,903:INFO:Memory: svmem(total=34009374720, available=11909488640, percent=65.0, used=22099886080, free=11909488640)
2026-02-02 13:01:35,903:INFO:Physical Core: 12
2026-02-02 13:01:35,903:INFO:Logical Core: 16
2026-02-02 13:01:35,903:INFO:Checking libraries
2026-02-02 13:01:35,903:INFO:System:
2026-02-02 13:01:35,903:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 13:01:35,903:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 13:01:35,903:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 13:01:35,903:INFO:PyCaret required dependencies:
2026-02-02 13:01:35,903:INFO:                 pip: 25.0
2026-02-02 13:01:35,903:INFO:          setuptools: 75.8.0
2026-02-02 13:01:35,903:INFO:             pycaret: 3.3.2
2026-02-02 13:01:35,903:INFO:             IPython: 9.9.0
2026-02-02 13:01:35,903:INFO:          ipywidgets: 8.1.8
2026-02-02 13:01:35,903:INFO:                tqdm: 4.67.1
2026-02-02 13:01:35,903:INFO:               numpy: 1.26.4
2026-02-02 13:01:35,903:INFO:              pandas: 2.1.4
2026-02-02 13:01:35,903:INFO:              jinja2: 3.1.6
2026-02-02 13:01:35,903:INFO:               scipy: 1.11.4
2026-02-02 13:01:35,903:INFO:              joblib: 1.3.2
2026-02-02 13:01:35,903:INFO:             sklearn: 1.4.2
2026-02-02 13:01:35,903:INFO:                pyod: 2.0.6
2026-02-02 13:01:35,903:INFO:            imblearn: 0.14.1
2026-02-02 13:01:35,903:INFO:   category_encoders: 2.7.0
2026-02-02 13:01:35,903:INFO:            lightgbm: 4.6.0
2026-02-02 13:01:35,903:INFO:               numba: 0.62.1
2026-02-02 13:01:35,903:INFO:            requests: 2.32.3
2026-02-02 13:01:35,903:INFO:          matplotlib: 3.7.5
2026-02-02 13:01:35,903:INFO:          scikitplot: 0.3.7
2026-02-02 13:01:35,903:INFO:         yellowbrick: 1.5
2026-02-02 13:01:35,903:INFO:              plotly: 5.24.1
2026-02-02 13:01:35,903:INFO:    plotly-resampler: Not installed
2026-02-02 13:01:35,903:INFO:             kaleido: 1.2.0
2026-02-02 13:01:35,903:INFO:           schemdraw: 0.15
2026-02-02 13:01:35,903:INFO:         statsmodels: 0.14.6
2026-02-02 13:01:35,903:INFO:              sktime: 0.26.0
2026-02-02 13:01:35,903:INFO:               tbats: 1.1.3
2026-02-02 13:01:35,903:INFO:            pmdarima: 2.0.4
2026-02-02 13:01:35,903:INFO:              psutil: 7.2.1
2026-02-02 13:01:35,903:INFO:          markupsafe: 3.0.3
2026-02-02 13:01:35,903:INFO:             pickle5: Not installed
2026-02-02 13:01:35,903:INFO:         cloudpickle: 3.0.0
2026-02-02 13:01:35,903:INFO:         deprecation: 2.1.0
2026-02-02 13:01:35,903:INFO:              xxhash: 3.6.0
2026-02-02 13:01:35,903:INFO:           wurlitzer: Not installed
2026-02-02 13:01:35,903:INFO:PyCaret optional dependencies:
2026-02-02 13:01:35,903:INFO:                shap: 0.44.1
2026-02-02 13:01:35,903:INFO:           interpret: 0.7.3
2026-02-02 13:01:35,903:INFO:                umap: 0.5.7
2026-02-02 13:01:35,903:INFO:     ydata_profiling: 4.18.1
2026-02-02 13:01:35,903:INFO:  explainerdashboard: 0.5.1
2026-02-02 13:01:35,903:INFO:             autoviz: Not installed
2026-02-02 13:01:35,903:INFO:           fairlearn: 0.7.0
2026-02-02 13:01:35,903:INFO:          deepchecks: Not installed
2026-02-02 13:01:35,903:INFO:             xgboost: Not installed
2026-02-02 13:01:35,903:INFO:            catboost: 1.2.8
2026-02-02 13:01:35,903:INFO:              kmodes: 0.12.2
2026-02-02 13:01:35,903:INFO:             mlxtend: 0.23.4
2026-02-02 13:01:35,903:INFO:       statsforecast: 1.5.0
2026-02-02 13:01:35,903:INFO:        tune_sklearn: Not installed
2026-02-02 13:01:35,903:INFO:                 ray: Not installed
2026-02-02 13:01:35,903:INFO:            hyperopt: 0.2.7
2026-02-02 13:01:35,903:INFO:              optuna: 4.6.0
2026-02-02 13:01:35,903:INFO:               skopt: 0.10.2
2026-02-02 13:01:35,903:INFO:              mlflow: 3.8.1
2026-02-02 13:01:35,903:INFO:              gradio: 6.3.0
2026-02-02 13:01:35,903:INFO:             fastapi: 0.128.0
2026-02-02 13:01:35,903:INFO:             uvicorn: 0.40.0
2026-02-02 13:01:35,903:INFO:              m2cgen: 0.10.0
2026-02-02 13:01:35,903:INFO:           evidently: 0.4.40
2026-02-02 13:01:35,903:INFO:               fugue: 0.8.7
2026-02-02 13:01:35,903:INFO:           streamlit: Not installed
2026-02-02 13:01:35,903:INFO:             prophet: Not installed
2026-02-02 13:01:35,903:INFO:None
2026-02-02 13:01:35,903:INFO:Set up data.
2026-02-02 13:01:35,919:INFO:Set up folding strategy.
2026-02-02 13:01:35,919:INFO:Set up train/test split.
2026-02-02 13:01:35,919:INFO:Set up index.
2026-02-02 13:01:35,919:INFO:Assigning column types.
2026-02-02 13:01:35,935:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 13:01:35,967:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:01:35,967:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:01:35,977:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:01:35,977:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:01:36,009:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:01:36,009:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:01:36,070:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:01:36,072:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:01:36,072:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 13:01:36,093:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:01:36,113:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:01:36,113:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:01:36,159:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:01:36,178:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:01:36,178:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:01:36,178:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 13:01:36,226:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:01:36,226:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:01:36,292:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:01:36,292:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:01:36,294:INFO:Preparing preprocessing pipeline...
2026-02-02 13:01:36,296:INFO:Set up simple imputation.
2026-02-02 13:01:36,296:INFO:Set up feature normalization.
2026-02-02 13:01:36,322:INFO:Finished creating preprocessing pipeline.
2026-02-02 13:01:36,324:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 13:01:36,324:INFO:Creating final display dataframe.
2026-02-02 13:01:36,411:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 19)
4        Transformed data shape       (10381, 19)
5   Transformed train set shape        (7266, 19)
6    Transformed test set shape        (3115, 19)
7              Numeric features                 8
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              6cbe
2026-02-02 13:01:36,468:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:01:36,468:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:01:36,527:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:01:36,527:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:01:36,527:INFO:setup() successfully completed in 0.64s...............
2026-02-02 13:01:36,527:INFO:Initializing compare_models()
2026-02-02 13:01:36,527:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228CAF5F410>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000228CAF5F410>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 13:01:36,527:INFO:Checking exceptions
2026-02-02 13:01:36,527:INFO:Preparing display monitor
2026-02-02 13:01:36,527:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 13:01:36,527:INFO:Total runtime is 0.0 minutes
2026-02-02 13:01:36,527:INFO:SubProcess create_model() called ==================================
2026-02-02 13:01:36,527:INFO:Initializing create_model()
2026-02-02 13:01:36,527:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228CAF5F410>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022867052450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:01:36,527:INFO:Checking exceptions
2026-02-02 13:01:36,527:INFO:Importing libraries
2026-02-02 13:01:36,527:INFO:Copying training dataset
2026-02-02 13:01:36,543:INFO:Defining folds
2026-02-02 13:01:36,543:INFO:Declaring metric variables
2026-02-02 13:01:36,543:INFO:Importing untrained model
2026-02-02 13:01:36,543:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:01:36,543:INFO:Starting cross validation
2026-02-02 13:01:36,543:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:01:37,243:INFO:Calculating mean and std
2026-02-02 13:01:37,244:INFO:Creating metrics dataframe
2026-02-02 13:01:37,248:INFO:Uploading results into container
2026-02-02 13:01:37,248:INFO:Uploading model into container now
2026-02-02 13:01:37,250:INFO:_master_model_container: 1
2026-02-02 13:01:37,250:INFO:_display_container: 2
2026-02-02 13:01:37,250:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:01:37,250:INFO:create_model() successfully completed......................................
2026-02-02 13:01:37,893:INFO:SubProcess create_model() end ==================================
2026-02-02 13:01:37,893:INFO:Creating metrics dataframe
2026-02-02 13:01:37,893:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 13:01:37,893:INFO:Initializing create_model()
2026-02-02 13:01:37,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228CAF5F410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:01:37,893:INFO:Checking exceptions
2026-02-02 13:01:37,893:INFO:Importing libraries
2026-02-02 13:01:37,893:INFO:Copying training dataset
2026-02-02 13:01:37,893:INFO:Defining folds
2026-02-02 13:01:37,893:INFO:Declaring metric variables
2026-02-02 13:01:37,893:INFO:Importing untrained model
2026-02-02 13:01:37,893:INFO:Declaring custom model
2026-02-02 13:01:37,893:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:01:37,893:INFO:Cross validation set to False
2026-02-02 13:01:37,893:INFO:Fitting Model
2026-02-02 13:01:37,929:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:01:37,931:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001017 seconds.
2026-02-02 13:01:37,931:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:01:37,931:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:01:37,931:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:01:37,931:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:01:37,931:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:01:38,100:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:01:38,100:INFO:create_model() successfully completed......................................
2026-02-02 13:01:38,275:INFO:_master_model_container: 1
2026-02-02 13:01:38,277:INFO:_display_container: 2
2026-02-02 13:01:38,277:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:01:38,277:INFO:compare_models() successfully completed......................................
2026-02-02 13:01:38,277:INFO:Initializing tune_model()
2026-02-02 13:01:38,277:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228CAF5F410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:01:38,277:INFO:Checking exceptions
2026-02-02 13:01:38,277:INFO:Copying training dataset
2026-02-02 13:01:38,277:INFO:Checking base model
2026-02-02 13:01:38,277:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 13:01:38,277:INFO:Declaring metric variables
2026-02-02 13:01:38,277:INFO:Defining Hyperparameters
2026-02-02 13:01:38,417:INFO:Tuning with n_jobs=-1
2026-02-02 13:01:38,417:INFO:Initializing RandomizedSearchCV
2026-02-02 13:01:43,089:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 13:01:43,095:INFO:Hyperparameter search completed
2026-02-02 13:01:43,096:INFO:SubProcess create_model() called ==================================
2026-02-02 13:01:43,098:INFO:Initializing create_model()
2026-02-02 13:01:43,099:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228CAF5F410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228DF3CEC10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 13:01:43,099:INFO:Checking exceptions
2026-02-02 13:01:43,100:INFO:Importing libraries
2026-02-02 13:01:43,100:INFO:Copying training dataset
2026-02-02 13:01:43,113:INFO:Defining folds
2026-02-02 13:01:43,114:INFO:Declaring metric variables
2026-02-02 13:01:43,114:INFO:Importing untrained model
2026-02-02 13:01:43,115:INFO:Declaring custom model
2026-02-02 13:01:43,117:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:01:43,118:INFO:Starting cross validation
2026-02-02 13:01:43,119:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:01:44,044:INFO:Calculating mean and std
2026-02-02 13:01:44,047:INFO:Creating metrics dataframe
2026-02-02 13:01:44,051:INFO:Finalizing model
2026-02-02 13:01:44,084:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:01:44,084:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:01:44,084:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:01:44,092:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:01:44,093:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:01:44,094:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:01:44,094:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:01:44,097:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001914 seconds.
2026-02-02 13:01:44,097:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:01:44,098:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:01:44,098:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:01:44,098:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:01:44,098:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:01:44,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,196:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,207:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,211:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,213:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,215:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,222:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,233:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,248:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,252:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,263:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:01:44,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,267:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,269:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,282:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,368:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:01:44,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:01:44,424:INFO:Uploading results into container
2026-02-02 13:01:44,424:INFO:Uploading model into container now
2026-02-02 13:01:44,426:INFO:_master_model_container: 2
2026-02-02 13:01:44,426:INFO:_display_container: 3
2026-02-02 13:01:44,428:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:01:44,428:INFO:create_model() successfully completed......................................
2026-02-02 13:01:44,611:INFO:SubProcess create_model() end ==================================
2026-02-02 13:01:44,611:INFO:choose_better activated
2026-02-02 13:01:44,611:INFO:SubProcess create_model() called ==================================
2026-02-02 13:01:44,611:INFO:Initializing create_model()
2026-02-02 13:01:44,611:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228CAF5F410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:01:44,611:INFO:Checking exceptions
2026-02-02 13:01:44,611:INFO:Importing libraries
2026-02-02 13:01:44,611:INFO:Copying training dataset
2026-02-02 13:01:44,628:INFO:Defining folds
2026-02-02 13:01:44,628:INFO:Declaring metric variables
2026-02-02 13:01:44,628:INFO:Importing untrained model
2026-02-02 13:01:44,628:INFO:Declaring custom model
2026-02-02 13:01:44,628:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:01:44,628:INFO:Starting cross validation
2026-02-02 13:01:44,628:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:01:45,547:INFO:Calculating mean and std
2026-02-02 13:01:45,547:INFO:Creating metrics dataframe
2026-02-02 13:01:45,549:INFO:Finalizing model
2026-02-02 13:01:45,586:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:01:45,590:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001784 seconds.
2026-02-02 13:01:45,590:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:01:45,590:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:01:45,590:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:01:45,590:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:01:45,590:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:01:45,765:INFO:Uploading results into container
2026-02-02 13:01:45,767:INFO:Uploading model into container now
2026-02-02 13:01:45,767:INFO:_master_model_container: 3
2026-02-02 13:01:45,767:INFO:_display_container: 4
2026-02-02 13:01:45,769:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:01:45,769:INFO:create_model() successfully completed......................................
2026-02-02 13:01:45,945:INFO:SubProcess create_model() end ==================================
2026-02-02 13:01:45,961:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9974
2026-02-02 13:01:45,961:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9946
2026-02-02 13:01:45,961:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 13:01:45,961:INFO:choose_better completed
2026-02-02 13:01:45,961:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:01:45,961:INFO:_master_model_container: 3
2026-02-02 13:01:45,961:INFO:_display_container: 3
2026-02-02 13:01:45,961:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:01:45,961:INFO:tune_model() successfully completed......................................
2026-02-02 13:01:46,132:INFO:Initializing predict_model()
2026-02-02 13:01:46,132:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228CAF5F410>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228E8CBAD40>)
2026-02-02 13:01:46,132:INFO:Checking exceptions
2026-02-02 13:01:46,132:INFO:Preloading libraries
2026-02-02 13:01:46,132:INFO:Set up data.
2026-02-02 13:01:46,136:INFO:Set up index.
2026-02-02 13:03:06,299:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\3893966445.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 13:03:07,002:INFO:PyCaret ClassificationExperiment
2026-02-02 13:03:07,002:INFO:Logging name: clf-default-name
2026-02-02 13:03:07,002:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 13:03:07,002:INFO:version 3.3.2
2026-02-02 13:03:07,002:INFO:Initializing setup()
2026-02-02 13:03:07,002:INFO:self.USI: 3d1e
2026-02-02 13:03:07,002:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 13:03:07,002:INFO:Checking environment
2026-02-02 13:03:07,002:INFO:python_version: 3.11.11
2026-02-02 13:03:07,002:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 13:03:07,002:INFO:machine: AMD64
2026-02-02 13:03:07,002:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 13:03:07,010:INFO:Memory: svmem(total=34009374720, available=11769974784, percent=65.4, used=22239399936, free=11769974784)
2026-02-02 13:03:07,010:INFO:Physical Core: 12
2026-02-02 13:03:07,010:INFO:Logical Core: 16
2026-02-02 13:03:07,011:INFO:Checking libraries
2026-02-02 13:03:07,011:INFO:System:
2026-02-02 13:03:07,012:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 13:03:07,012:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 13:03:07,013:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 13:03:07,013:INFO:PyCaret required dependencies:
2026-02-02 13:03:07,013:INFO:                 pip: 25.0
2026-02-02 13:03:07,013:INFO:          setuptools: 75.8.0
2026-02-02 13:03:07,014:INFO:             pycaret: 3.3.2
2026-02-02 13:03:07,014:INFO:             IPython: 9.9.0
2026-02-02 13:03:07,014:INFO:          ipywidgets: 8.1.8
2026-02-02 13:03:07,014:INFO:                tqdm: 4.67.1
2026-02-02 13:03:07,015:INFO:               numpy: 1.26.4
2026-02-02 13:03:07,015:INFO:              pandas: 2.1.4
2026-02-02 13:03:07,015:INFO:              jinja2: 3.1.6
2026-02-02 13:03:07,015:INFO:               scipy: 1.11.4
2026-02-02 13:03:07,015:INFO:              joblib: 1.3.2
2026-02-02 13:03:07,015:INFO:             sklearn: 1.4.2
2026-02-02 13:03:07,015:INFO:                pyod: 2.0.6
2026-02-02 13:03:07,015:INFO:            imblearn: 0.14.1
2026-02-02 13:03:07,016:INFO:   category_encoders: 2.7.0
2026-02-02 13:03:07,017:INFO:            lightgbm: 4.6.0
2026-02-02 13:03:07,017:INFO:               numba: 0.62.1
2026-02-02 13:03:07,017:INFO:            requests: 2.32.3
2026-02-02 13:03:07,017:INFO:          matplotlib: 3.7.5
2026-02-02 13:03:07,017:INFO:          scikitplot: 0.3.7
2026-02-02 13:03:07,017:INFO:         yellowbrick: 1.5
2026-02-02 13:03:07,017:INFO:              plotly: 5.24.1
2026-02-02 13:03:07,017:INFO:    plotly-resampler: Not installed
2026-02-02 13:03:07,017:INFO:             kaleido: 1.2.0
2026-02-02 13:03:07,017:INFO:           schemdraw: 0.15
2026-02-02 13:03:07,017:INFO:         statsmodels: 0.14.6
2026-02-02 13:03:07,017:INFO:              sktime: 0.26.0
2026-02-02 13:03:07,017:INFO:               tbats: 1.1.3
2026-02-02 13:03:07,017:INFO:            pmdarima: 2.0.4
2026-02-02 13:03:07,017:INFO:              psutil: 7.2.1
2026-02-02 13:03:07,017:INFO:          markupsafe: 3.0.3
2026-02-02 13:03:07,017:INFO:             pickle5: Not installed
2026-02-02 13:03:07,017:INFO:         cloudpickle: 3.0.0
2026-02-02 13:03:07,017:INFO:         deprecation: 2.1.0
2026-02-02 13:03:07,017:INFO:              xxhash: 3.6.0
2026-02-02 13:03:07,017:INFO:           wurlitzer: Not installed
2026-02-02 13:03:07,017:INFO:PyCaret optional dependencies:
2026-02-02 13:03:07,017:INFO:                shap: 0.44.1
2026-02-02 13:03:07,017:INFO:           interpret: 0.7.3
2026-02-02 13:03:07,017:INFO:                umap: 0.5.7
2026-02-02 13:03:07,017:INFO:     ydata_profiling: 4.18.1
2026-02-02 13:03:07,017:INFO:  explainerdashboard: 0.5.1
2026-02-02 13:03:07,017:INFO:             autoviz: Not installed
2026-02-02 13:03:07,017:INFO:           fairlearn: 0.7.0
2026-02-02 13:03:07,017:INFO:          deepchecks: Not installed
2026-02-02 13:03:07,017:INFO:             xgboost: Not installed
2026-02-02 13:03:07,017:INFO:            catboost: 1.2.8
2026-02-02 13:03:07,017:INFO:              kmodes: 0.12.2
2026-02-02 13:03:07,017:INFO:             mlxtend: 0.23.4
2026-02-02 13:03:07,017:INFO:       statsforecast: 1.5.0
2026-02-02 13:03:07,017:INFO:        tune_sklearn: Not installed
2026-02-02 13:03:07,017:INFO:                 ray: Not installed
2026-02-02 13:03:07,017:INFO:            hyperopt: 0.2.7
2026-02-02 13:03:07,017:INFO:              optuna: 4.6.0
2026-02-02 13:03:07,017:INFO:               skopt: 0.10.2
2026-02-02 13:03:07,017:INFO:              mlflow: 3.8.1
2026-02-02 13:03:07,017:INFO:              gradio: 6.3.0
2026-02-02 13:03:07,017:INFO:             fastapi: 0.128.0
2026-02-02 13:03:07,017:INFO:             uvicorn: 0.40.0
2026-02-02 13:03:07,017:INFO:              m2cgen: 0.10.0
2026-02-02 13:03:07,017:INFO:           evidently: 0.4.40
2026-02-02 13:03:07,017:INFO:               fugue: 0.8.7
2026-02-02 13:03:07,017:INFO:           streamlit: Not installed
2026-02-02 13:03:07,017:INFO:             prophet: Not installed
2026-02-02 13:03:07,017:INFO:None
2026-02-02 13:03:07,017:INFO:Set up data.
2026-02-02 13:03:07,029:INFO:Set up folding strategy.
2026-02-02 13:03:07,029:INFO:Set up train/test split.
2026-02-02 13:03:07,033:INFO:Set up index.
2026-02-02 13:03:07,033:INFO:Assigning column types.
2026-02-02 13:03:07,033:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 13:03:07,065:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:03:07,065:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:03:07,083:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:03:07,083:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:03:07,116:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:03:07,116:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:03:07,165:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:03:07,166:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:03:07,166:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 13:03:07,195:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:03:07,214:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:03:07,215:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:03:07,243:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:03:07,264:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:03:07,264:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:03:07,265:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 13:03:07,312:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:03:07,312:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:03:07,350:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:03:07,350:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:03:07,360:INFO:Preparing preprocessing pipeline...
2026-02-02 13:03:07,362:INFO:Set up simple imputation.
2026-02-02 13:03:07,362:INFO:Set up feature normalization.
2026-02-02 13:03:07,383:INFO:Finished creating preprocessing pipeline.
2026-02-02 13:03:07,383:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 13:03:07,383:INFO:Creating final display dataframe.
2026-02-02 13:03:07,483:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 19)
4        Transformed data shape       (10381, 19)
5   Transformed train set shape        (7266, 19)
6    Transformed test set shape        (3115, 19)
7              Numeric features                 8
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              3d1e
2026-02-02 13:03:07,530:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:03:07,530:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:03:07,576:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:03:07,576:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:03:07,577:INFO:setup() successfully completed in 0.59s...............
2026-02-02 13:03:07,577:INFO:Initializing compare_models()
2026-02-02 13:03:07,577:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 13:03:07,577:INFO:Checking exceptions
2026-02-02 13:03:07,577:INFO:Preparing display monitor
2026-02-02 13:03:07,577:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 13:03:07,577:INFO:Total runtime is 0.0 minutes
2026-02-02 13:03:07,577:INFO:SubProcess create_model() called ==================================
2026-02-02 13:03:07,577:INFO:Initializing create_model()
2026-02-02 13:03:07,577:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228CAEEC7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:03:07,577:INFO:Checking exceptions
2026-02-02 13:03:07,577:INFO:Importing libraries
2026-02-02 13:03:07,577:INFO:Copying training dataset
2026-02-02 13:03:07,577:INFO:Defining folds
2026-02-02 13:03:07,577:INFO:Declaring metric variables
2026-02-02 13:03:07,577:INFO:Importing untrained model
2026-02-02 13:03:07,577:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:03:07,577:INFO:Starting cross validation
2026-02-02 13:03:07,577:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:03:08,152:INFO:Calculating mean and std
2026-02-02 13:03:08,152:INFO:Creating metrics dataframe
2026-02-02 13:03:08,154:INFO:Uploading results into container
2026-02-02 13:03:08,156:INFO:Uploading model into container now
2026-02-02 13:03:08,156:INFO:_master_model_container: 1
2026-02-02 13:03:08,156:INFO:_display_container: 2
2026-02-02 13:03:08,156:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:03:08,156:INFO:create_model() successfully completed......................................
2026-02-02 13:03:08,362:INFO:SubProcess create_model() end ==================================
2026-02-02 13:03:08,362:INFO:Creating metrics dataframe
2026-02-02 13:03:08,364:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 13:03:08,364:INFO:Initializing create_model()
2026-02-02 13:03:08,364:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:03:08,364:INFO:Checking exceptions
2026-02-02 13:03:08,364:INFO:Importing libraries
2026-02-02 13:03:08,364:INFO:Copying training dataset
2026-02-02 13:03:08,364:INFO:Defining folds
2026-02-02 13:03:08,364:INFO:Declaring metric variables
2026-02-02 13:03:08,364:INFO:Importing untrained model
2026-02-02 13:03:08,364:INFO:Declaring custom model
2026-02-02 13:03:08,364:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:03:08,364:INFO:Cross validation set to False
2026-02-02 13:03:08,364:INFO:Fitting Model
2026-02-02 13:03:08,396:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:03:08,398:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000841 seconds.
2026-02-02 13:03:08,398:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:03:08,398:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:03:08,398:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:03:08,398:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:03:08,398:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:03:08,513:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:03:08,515:INFO:create_model() successfully completed......................................
2026-02-02 13:03:08,705:INFO:_master_model_container: 1
2026-02-02 13:03:08,705:INFO:_display_container: 2
2026-02-02 13:03:08,705:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:03:08,705:INFO:compare_models() successfully completed......................................
2026-02-02 13:03:08,709:INFO:Initializing tune_model()
2026-02-02 13:03:08,709:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:03:08,709:INFO:Checking exceptions
2026-02-02 13:03:08,710:INFO:Copying training dataset
2026-02-02 13:03:08,717:INFO:Checking base model
2026-02-02 13:03:08,717:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 13:03:08,717:INFO:Declaring metric variables
2026-02-02 13:03:08,717:INFO:Defining Hyperparameters
2026-02-02 13:03:08,877:INFO:Tuning with n_jobs=-1
2026-02-02 13:03:08,877:INFO:Initializing RandomizedSearchCV
2026-02-02 13:03:13,498:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 13:03:13,500:INFO:Hyperparameter search completed
2026-02-02 13:03:13,502:INFO:SubProcess create_model() called ==================================
2026-02-02 13:03:13,502:INFO:Initializing create_model()
2026-02-02 13:03:13,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228E6EEB6D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 13:03:13,504:INFO:Checking exceptions
2026-02-02 13:03:13,504:INFO:Importing libraries
2026-02-02 13:03:13,504:INFO:Copying training dataset
2026-02-02 13:03:13,514:INFO:Defining folds
2026-02-02 13:03:13,514:INFO:Declaring metric variables
2026-02-02 13:03:13,514:INFO:Importing untrained model
2026-02-02 13:03:13,514:INFO:Declaring custom model
2026-02-02 13:03:13,516:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:03:13,516:INFO:Starting cross validation
2026-02-02 13:03:13,516:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:03:14,133:INFO:Calculating mean and std
2026-02-02 13:03:14,133:INFO:Creating metrics dataframe
2026-02-02 13:03:14,135:INFO:Finalizing model
2026-02-02 13:03:14,160:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:03:14,160:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:03:14,160:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:03:14,166:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:03:14,166:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:03:14,166:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:03:14,166:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:03:14,168:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001118 seconds.
2026-02-02 13:03:14,170:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:03:14,170:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:03:14,170:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:03:14,170:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:03:14,170:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:03:14,170:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:03:14,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,241:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,249:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,274:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,279:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,293:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,295:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:03:14,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,300:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,312:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,315:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,327:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,337:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,339:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,344:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,348:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,378:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:03:14,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,381:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,394:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:03:14,423:INFO:Uploading results into container
2026-02-02 13:03:14,426:INFO:Uploading model into container now
2026-02-02 13:03:14,427:INFO:_master_model_container: 2
2026-02-02 13:03:14,427:INFO:_display_container: 3
2026-02-02 13:03:14,428:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:03:14,429:INFO:create_model() successfully completed......................................
2026-02-02 13:03:14,593:INFO:SubProcess create_model() end ==================================
2026-02-02 13:03:14,593:INFO:choose_better activated
2026-02-02 13:03:14,593:INFO:SubProcess create_model() called ==================================
2026-02-02 13:03:14,593:INFO:Initializing create_model()
2026-02-02 13:03:14,593:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:03:14,593:INFO:Checking exceptions
2026-02-02 13:03:14,593:INFO:Importing libraries
2026-02-02 13:03:14,593:INFO:Copying training dataset
2026-02-02 13:03:14,610:INFO:Defining folds
2026-02-02 13:03:14,610:INFO:Declaring metric variables
2026-02-02 13:03:14,610:INFO:Importing untrained model
2026-02-02 13:03:14,611:INFO:Declaring custom model
2026-02-02 13:03:14,611:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:03:14,611:INFO:Starting cross validation
2026-02-02 13:03:14,612:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:03:15,279:INFO:Calculating mean and std
2026-02-02 13:03:15,279:INFO:Creating metrics dataframe
2026-02-02 13:03:15,281:INFO:Finalizing model
2026-02-02 13:03:15,309:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:03:15,310:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.
2026-02-02 13:03:15,310:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:03:15,310:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:03:15,312:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:03:15,312:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:03:15,312:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:03:15,312:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:03:15,434:INFO:Uploading results into container
2026-02-02 13:03:15,436:INFO:Uploading model into container now
2026-02-02 13:03:15,436:INFO:_master_model_container: 3
2026-02-02 13:03:15,436:INFO:_display_container: 4
2026-02-02 13:03:15,436:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:03:15,436:INFO:create_model() successfully completed......................................
2026-02-02 13:03:15,593:INFO:SubProcess create_model() end ==================================
2026-02-02 13:03:15,593:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9974
2026-02-02 13:03:15,593:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9946
2026-02-02 13:03:15,593:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 13:03:15,593:INFO:choose_better completed
2026-02-02 13:03:15,593:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:03:15,593:INFO:_master_model_container: 3
2026-02-02 13:03:15,593:INFO:_display_container: 3
2026-02-02 13:03:15,593:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:03:15,593:INFO:tune_model() successfully completed......................................
2026-02-02 13:03:15,727:INFO:Initializing predict_model()
2026-02-02 13:03:15,727:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022871DD5EE0>)
2026-02-02 13:03:15,727:INFO:Checking exceptions
2026-02-02 13:03:15,727:INFO:Preloading libraries
2026-02-02 13:03:15,727:INFO:Set up data.
2026-02-02 13:03:15,743:INFO:Set up index.
2026-02-02 13:03:17,985:INFO:Initializing plot_model()
2026-02-02 13:03:17,986:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:03:17,986:INFO:Checking exceptions
2026-02-02 13:03:17,988:INFO:Preloading libraries
2026-02-02 13:03:17,994:INFO:Copying training dataset
2026-02-02 13:03:17,994:INFO:Plot type: feature
2026-02-02 13:03:17,994:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:03:18,144:INFO:Visual Rendered Successfully
2026-02-02 13:03:18,310:INFO:plot_model() successfully completed......................................
2026-02-02 13:03:18,310:INFO:Initializing plot_model()
2026-02-02 13:03:18,310:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022902736FD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:03:18,310:INFO:Checking exceptions
2026-02-02 13:03:18,317:INFO:Preloading libraries
2026-02-02 13:03:18,324:INFO:Copying training dataset
2026-02-02 13:03:18,324:INFO:Plot type: feature_all
2026-02-02 13:03:18,365:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:03:18,525:INFO:Visual Rendered Successfully
2026-02-02 13:03:18,664:INFO:plot_model() successfully completed......................................
2026-02-02 13:03:18,664:INFO:Initializing save_model()
2026-02-02 13:03:18,664:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 13:03:18,664:INFO:Adding model into prep_pipe
2026-02-02 13:03:18,679:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-02 13:03:18,686:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_feat...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-02 13:03:18,686:INFO:save_model() successfully completed......................................
2026-02-02 13:05:41,441:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\3874047575.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 13:05:42,123:INFO:PyCaret ClassificationExperiment
2026-02-02 13:05:42,124:INFO:Logging name: clf-default-name
2026-02-02 13:05:42,124:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 13:05:42,125:INFO:version 3.3.2
2026-02-02 13:05:42,125:INFO:Initializing setup()
2026-02-02 13:05:42,125:INFO:self.USI: 6b3d
2026-02-02 13:05:42,125:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 13:05:42,125:INFO:Checking environment
2026-02-02 13:05:42,125:INFO:python_version: 3.11.11
2026-02-02 13:05:42,125:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 13:05:42,125:INFO:machine: AMD64
2026-02-02 13:05:42,125:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 13:05:42,125:INFO:Memory: svmem(total=34009374720, available=11867058176, percent=65.1, used=22142316544, free=11867058176)
2026-02-02 13:05:42,125:INFO:Physical Core: 12
2026-02-02 13:05:42,125:INFO:Logical Core: 16
2026-02-02 13:05:42,125:INFO:Checking libraries
2026-02-02 13:05:42,125:INFO:System:
2026-02-02 13:05:42,125:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 13:05:42,125:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 13:05:42,125:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 13:05:42,125:INFO:PyCaret required dependencies:
2026-02-02 13:05:42,125:INFO:                 pip: 25.0
2026-02-02 13:05:42,125:INFO:          setuptools: 75.8.0
2026-02-02 13:05:42,125:INFO:             pycaret: 3.3.2
2026-02-02 13:05:42,125:INFO:             IPython: 9.9.0
2026-02-02 13:05:42,125:INFO:          ipywidgets: 8.1.8
2026-02-02 13:05:42,125:INFO:                tqdm: 4.67.1
2026-02-02 13:05:42,125:INFO:               numpy: 1.26.4
2026-02-02 13:05:42,125:INFO:              pandas: 2.1.4
2026-02-02 13:05:42,125:INFO:              jinja2: 3.1.6
2026-02-02 13:05:42,125:INFO:               scipy: 1.11.4
2026-02-02 13:05:42,125:INFO:              joblib: 1.3.2
2026-02-02 13:05:42,125:INFO:             sklearn: 1.4.2
2026-02-02 13:05:42,125:INFO:                pyod: 2.0.6
2026-02-02 13:05:42,125:INFO:            imblearn: 0.14.1
2026-02-02 13:05:42,125:INFO:   category_encoders: 2.7.0
2026-02-02 13:05:42,125:INFO:            lightgbm: 4.6.0
2026-02-02 13:05:42,125:INFO:               numba: 0.62.1
2026-02-02 13:05:42,125:INFO:            requests: 2.32.3
2026-02-02 13:05:42,125:INFO:          matplotlib: 3.7.5
2026-02-02 13:05:42,125:INFO:          scikitplot: 0.3.7
2026-02-02 13:05:42,125:INFO:         yellowbrick: 1.5
2026-02-02 13:05:42,125:INFO:              plotly: 5.24.1
2026-02-02 13:05:42,125:INFO:    plotly-resampler: Not installed
2026-02-02 13:05:42,125:INFO:             kaleido: 1.2.0
2026-02-02 13:05:42,125:INFO:           schemdraw: 0.15
2026-02-02 13:05:42,125:INFO:         statsmodels: 0.14.6
2026-02-02 13:05:42,125:INFO:              sktime: 0.26.0
2026-02-02 13:05:42,125:INFO:               tbats: 1.1.3
2026-02-02 13:05:42,125:INFO:            pmdarima: 2.0.4
2026-02-02 13:05:42,125:INFO:              psutil: 7.2.1
2026-02-02 13:05:42,125:INFO:          markupsafe: 3.0.3
2026-02-02 13:05:42,125:INFO:             pickle5: Not installed
2026-02-02 13:05:42,125:INFO:         cloudpickle: 3.0.0
2026-02-02 13:05:42,125:INFO:         deprecation: 2.1.0
2026-02-02 13:05:42,125:INFO:              xxhash: 3.6.0
2026-02-02 13:05:42,125:INFO:           wurlitzer: Not installed
2026-02-02 13:05:42,125:INFO:PyCaret optional dependencies:
2026-02-02 13:05:42,125:INFO:                shap: 0.44.1
2026-02-02 13:05:42,125:INFO:           interpret: 0.7.3
2026-02-02 13:05:42,125:INFO:                umap: 0.5.7
2026-02-02 13:05:42,125:INFO:     ydata_profiling: 4.18.1
2026-02-02 13:05:42,125:INFO:  explainerdashboard: 0.5.1
2026-02-02 13:05:42,125:INFO:             autoviz: Not installed
2026-02-02 13:05:42,125:INFO:           fairlearn: 0.7.0
2026-02-02 13:05:42,125:INFO:          deepchecks: Not installed
2026-02-02 13:05:42,125:INFO:             xgboost: Not installed
2026-02-02 13:05:42,125:INFO:            catboost: 1.2.8
2026-02-02 13:05:42,125:INFO:              kmodes: 0.12.2
2026-02-02 13:05:42,125:INFO:             mlxtend: 0.23.4
2026-02-02 13:05:42,125:INFO:       statsforecast: 1.5.0
2026-02-02 13:05:42,125:INFO:        tune_sklearn: Not installed
2026-02-02 13:05:42,125:INFO:                 ray: Not installed
2026-02-02 13:05:42,125:INFO:            hyperopt: 0.2.7
2026-02-02 13:05:42,125:INFO:              optuna: 4.6.0
2026-02-02 13:05:42,125:INFO:               skopt: 0.10.2
2026-02-02 13:05:42,125:INFO:              mlflow: 3.8.1
2026-02-02 13:05:42,125:INFO:              gradio: 6.3.0
2026-02-02 13:05:42,125:INFO:             fastapi: 0.128.0
2026-02-02 13:05:42,125:INFO:             uvicorn: 0.40.0
2026-02-02 13:05:42,125:INFO:              m2cgen: 0.10.0
2026-02-02 13:05:42,125:INFO:           evidently: 0.4.40
2026-02-02 13:05:42,125:INFO:               fugue: 0.8.7
2026-02-02 13:05:42,125:INFO:           streamlit: Not installed
2026-02-02 13:05:42,140:INFO:             prophet: Not installed
2026-02-02 13:05:42,140:INFO:None
2026-02-02 13:05:42,140:INFO:Set up data.
2026-02-02 13:05:42,146:INFO:Set up folding strategy.
2026-02-02 13:05:42,147:INFO:Set up train/test split.
2026-02-02 13:05:42,152:INFO:Set up index.
2026-02-02 13:05:42,152:INFO:Assigning column types.
2026-02-02 13:05:42,156:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 13:05:42,185:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:05:42,185:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:05:42,194:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:05:42,194:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:05:42,225:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:05:42,258:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:05:42,275:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:05:42,275:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:05:42,275:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 13:05:42,308:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:05:42,324:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:05:42,324:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:05:42,362:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:05:42,380:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:05:42,380:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:05:42,381:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 13:05:42,426:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:05:42,426:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:05:42,461:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:05:42,461:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:05:42,461:INFO:Preparing preprocessing pipeline...
2026-02-02 13:05:42,475:INFO:Set up simple imputation.
2026-02-02 13:05:42,475:INFO:Set up feature normalization.
2026-02-02 13:05:42,491:INFO:Finished creating preprocessing pipeline.
2026-02-02 13:05:42,491:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 13:05:42,491:INFO:Creating final display dataframe.
2026-02-02 13:05:42,590:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 19)
4        Transformed data shape       (10381, 19)
5   Transformed train set shape        (7266, 19)
6    Transformed test set shape        (3115, 19)
7              Numeric features                 8
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              6b3d
2026-02-02 13:05:42,625:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:05:42,625:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:05:42,678:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:05:42,678:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:05:42,678:INFO:setup() successfully completed in 0.57s...............
2026-02-02 13:05:42,678:INFO:Initializing compare_models()
2026-02-02 13:05:42,678:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 13:05:42,678:INFO:Checking exceptions
2026-02-02 13:05:42,678:INFO:Preparing display monitor
2026-02-02 13:05:42,692:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 13:05:42,692:INFO:Total runtime is 0.0 minutes
2026-02-02 13:05:42,692:INFO:SubProcess create_model() called ==================================
2026-02-02 13:05:42,692:INFO:Initializing create_model()
2026-02-02 13:05:42,692:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D4152450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:05:42,692:INFO:Checking exceptions
2026-02-02 13:05:42,692:INFO:Importing libraries
2026-02-02 13:05:42,692:INFO:Copying training dataset
2026-02-02 13:05:42,692:INFO:Defining folds
2026-02-02 13:05:42,692:INFO:Declaring metric variables
2026-02-02 13:05:42,692:INFO:Importing untrained model
2026-02-02 13:05:42,692:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:05:42,692:INFO:Starting cross validation
2026-02-02 13:05:42,692:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:05:43,368:INFO:Calculating mean and std
2026-02-02 13:05:43,368:INFO:Creating metrics dataframe
2026-02-02 13:05:43,370:INFO:Uploading results into container
2026-02-02 13:05:43,372:INFO:Uploading model into container now
2026-02-02 13:05:43,372:INFO:_master_model_container: 1
2026-02-02 13:05:43,372:INFO:_display_container: 2
2026-02-02 13:05:43,374:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:05:43,374:INFO:create_model() successfully completed......................................
2026-02-02 13:05:43,541:INFO:SubProcess create_model() end ==================================
2026-02-02 13:05:43,541:INFO:Creating metrics dataframe
2026-02-02 13:05:43,541:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 13:05:43,541:INFO:Initializing create_model()
2026-02-02 13:05:43,541:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:05:43,541:INFO:Checking exceptions
2026-02-02 13:05:43,541:INFO:Importing libraries
2026-02-02 13:05:43,541:INFO:Copying training dataset
2026-02-02 13:05:43,553:INFO:Defining folds
2026-02-02 13:05:43,553:INFO:Declaring metric variables
2026-02-02 13:05:43,553:INFO:Importing untrained model
2026-02-02 13:05:43,553:INFO:Declaring custom model
2026-02-02 13:05:43,553:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:05:43,553:INFO:Cross validation set to False
2026-02-02 13:05:43,553:INFO:Fitting Model
2026-02-02 13:05:43,578:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:05:43,580:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001065 seconds.
2026-02-02 13:05:43,580:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:05:43,581:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:05:43,581:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:05:43,581:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:05:43,581:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:05:43,716:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:05:43,716:INFO:create_model() successfully completed......................................
2026-02-02 13:05:43,908:INFO:_master_model_container: 1
2026-02-02 13:05:43,908:INFO:_display_container: 2
2026-02-02 13:05:43,908:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:05:43,908:INFO:compare_models() successfully completed......................................
2026-02-02 13:05:43,908:INFO:Initializing tune_model()
2026-02-02 13:05:43,908:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:05:43,908:INFO:Checking exceptions
2026-02-02 13:05:43,908:INFO:Copying training dataset
2026-02-02 13:05:43,921:INFO:Checking base model
2026-02-02 13:05:43,924:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 13:05:43,924:INFO:Declaring metric variables
2026-02-02 13:05:43,924:INFO:Defining Hyperparameters
2026-02-02 13:05:44,061:INFO:Tuning with n_jobs=-1
2026-02-02 13:05:44,061:INFO:Initializing RandomizedSearchCV
2026-02-02 13:05:48,645:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 13:05:48,652:INFO:Hyperparameter search completed
2026-02-02 13:05:48,652:INFO:SubProcess create_model() called ==================================
2026-02-02 13:05:48,654:INFO:Initializing create_model()
2026-02-02 13:05:48,654:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228390C4590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 13:05:48,656:INFO:Checking exceptions
2026-02-02 13:05:48,656:INFO:Importing libraries
2026-02-02 13:05:48,656:INFO:Copying training dataset
2026-02-02 13:05:48,664:INFO:Defining folds
2026-02-02 13:05:48,664:INFO:Declaring metric variables
2026-02-02 13:05:48,664:INFO:Importing untrained model
2026-02-02 13:05:48,664:INFO:Declaring custom model
2026-02-02 13:05:48,666:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:05:48,666:INFO:Starting cross validation
2026-02-02 13:05:48,666:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:05:49,238:INFO:Calculating mean and std
2026-02-02 13:05:49,240:INFO:Creating metrics dataframe
2026-02-02 13:05:49,241:INFO:Finalizing model
2026-02-02 13:05:49,266:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:05:49,266:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:05:49,266:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:05:49,272:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:05:49,272:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:05:49,272:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:05:49,272:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:05:49,276:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001534 seconds.
2026-02-02 13:05:49,276:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:05:49,276:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:05:49,276:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:05:49,277:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:05:49,277:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:05:49,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,332:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,341:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,374:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,384:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:05:49,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,407:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,414:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,416:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,423:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:05:49,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:05:49,491:INFO:Uploading results into container
2026-02-02 13:05:49,491:INFO:Uploading model into container now
2026-02-02 13:05:49,493:INFO:_master_model_container: 2
2026-02-02 13:05:49,493:INFO:_display_container: 3
2026-02-02 13:05:49,493:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:05:49,493:INFO:create_model() successfully completed......................................
2026-02-02 13:05:49,665:INFO:SubProcess create_model() end ==================================
2026-02-02 13:05:49,665:INFO:choose_better activated
2026-02-02 13:05:49,665:INFO:SubProcess create_model() called ==================================
2026-02-02 13:05:49,666:INFO:Initializing create_model()
2026-02-02 13:05:49,666:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:05:49,666:INFO:Checking exceptions
2026-02-02 13:05:49,666:INFO:Importing libraries
2026-02-02 13:05:49,666:INFO:Copying training dataset
2026-02-02 13:05:49,672:INFO:Defining folds
2026-02-02 13:05:49,672:INFO:Declaring metric variables
2026-02-02 13:05:49,672:INFO:Importing untrained model
2026-02-02 13:05:49,672:INFO:Declaring custom model
2026-02-02 13:05:49,673:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:05:49,673:INFO:Starting cross validation
2026-02-02 13:05:49,673:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:05:50,386:INFO:Calculating mean and std
2026-02-02 13:05:50,386:INFO:Creating metrics dataframe
2026-02-02 13:05:50,388:INFO:Finalizing model
2026-02-02 13:05:50,416:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:05:50,418:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000957 seconds.
2026-02-02 13:05:50,418:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:05:50,418:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:05:50,418:INFO:[LightGBM] [Info] Total Bins 1101
2026-02-02 13:05:50,420:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 17
2026-02-02 13:05:50,420:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:05:50,420:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:05:50,585:INFO:Uploading results into container
2026-02-02 13:05:50,587:INFO:Uploading model into container now
2026-02-02 13:05:50,587:INFO:_master_model_container: 3
2026-02-02 13:05:50,587:INFO:_display_container: 4
2026-02-02 13:05:50,587:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:05:50,587:INFO:create_model() successfully completed......................................
2026-02-02 13:05:50,763:INFO:SubProcess create_model() end ==================================
2026-02-02 13:05:50,764:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9974
2026-02-02 13:05:50,764:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9946
2026-02-02 13:05:50,765:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 13:05:50,765:INFO:choose_better completed
2026-02-02 13:05:50,765:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:05:50,768:INFO:_master_model_container: 3
2026-02-02 13:05:50,768:INFO:_display_container: 3
2026-02-02 13:05:50,768:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:05:50,768:INFO:tune_model() successfully completed......................................
2026-02-02 13:05:50,903:INFO:Initializing predict_model()
2026-02-02 13:05:50,903:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228E99AC860>)
2026-02-02 13:05:50,903:INFO:Checking exceptions
2026-02-02 13:05:50,903:INFO:Preloading libraries
2026-02-02 13:05:50,903:INFO:Set up data.
2026-02-02 13:05:50,906:INFO:Set up index.
2026-02-02 13:05:53,041:INFO:Initializing plot_model()
2026-02-02 13:05:53,041:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:05:53,041:INFO:Checking exceptions
2026-02-02 13:05:53,052:INFO:Preloading libraries
2026-02-02 13:05:53,058:INFO:Copying training dataset
2026-02-02 13:05:53,058:INFO:Plot type: feature
2026-02-02 13:05:53,058:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:05:53,206:INFO:Visual Rendered Successfully
2026-02-02 13:05:53,363:INFO:plot_model() successfully completed......................................
2026-02-02 13:05:53,363:INFO:Initializing plot_model()
2026-02-02 13:05:53,363:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022909071A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:05:53,363:INFO:Checking exceptions
2026-02-02 13:05:53,363:INFO:Preloading libraries
2026-02-02 13:05:53,372:INFO:Copying training dataset
2026-02-02 13:05:53,372:INFO:Plot type: feature_all
2026-02-02 13:05:53,414:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:05:53,573:INFO:Visual Rendered Successfully
2026-02-02 13:05:53,707:INFO:plot_model() successfully completed......................................
2026-02-02 13:05:53,711:INFO:Initializing save_model()
2026-02-02 13:05:53,711:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 13:05:53,711:INFO:Adding model into prep_pipe
2026-02-02 13:05:53,720:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-02 13:05:53,727:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_feat...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-02 13:05:53,727:INFO:save_model() successfully completed......................................
2026-02-02 13:07:32,076:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\1950018585.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 13:07:32,750:INFO:PyCaret ClassificationExperiment
2026-02-02 13:07:32,750:INFO:Logging name: clf-default-name
2026-02-02 13:07:32,750:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 13:07:32,750:INFO:version 3.3.2
2026-02-02 13:07:32,750:INFO:Initializing setup()
2026-02-02 13:07:32,750:INFO:self.USI: 1aa4
2026-02-02 13:07:32,755:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 13:07:32,755:INFO:Checking environment
2026-02-02 13:07:32,755:INFO:python_version: 3.11.11
2026-02-02 13:07:32,756:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 13:07:32,756:INFO:machine: AMD64
2026-02-02 13:07:32,756:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 13:07:32,756:INFO:Memory: svmem(total=34009374720, available=11736985600, percent=65.5, used=22272389120, free=11736985600)
2026-02-02 13:07:32,756:INFO:Physical Core: 12
2026-02-02 13:07:32,756:INFO:Logical Core: 16
2026-02-02 13:07:32,756:INFO:Checking libraries
2026-02-02 13:07:32,756:INFO:System:
2026-02-02 13:07:32,756:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 13:07:32,756:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 13:07:32,756:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 13:07:32,756:INFO:PyCaret required dependencies:
2026-02-02 13:07:32,756:INFO:                 pip: 25.0
2026-02-02 13:07:32,756:INFO:          setuptools: 75.8.0
2026-02-02 13:07:32,756:INFO:             pycaret: 3.3.2
2026-02-02 13:07:32,756:INFO:             IPython: 9.9.0
2026-02-02 13:07:32,756:INFO:          ipywidgets: 8.1.8
2026-02-02 13:07:32,756:INFO:                tqdm: 4.67.1
2026-02-02 13:07:32,756:INFO:               numpy: 1.26.4
2026-02-02 13:07:32,756:INFO:              pandas: 2.1.4
2026-02-02 13:07:32,756:INFO:              jinja2: 3.1.6
2026-02-02 13:07:32,756:INFO:               scipy: 1.11.4
2026-02-02 13:07:32,756:INFO:              joblib: 1.3.2
2026-02-02 13:07:32,756:INFO:             sklearn: 1.4.2
2026-02-02 13:07:32,756:INFO:                pyod: 2.0.6
2026-02-02 13:07:32,756:INFO:            imblearn: 0.14.1
2026-02-02 13:07:32,756:INFO:   category_encoders: 2.7.0
2026-02-02 13:07:32,756:INFO:            lightgbm: 4.6.0
2026-02-02 13:07:32,756:INFO:               numba: 0.62.1
2026-02-02 13:07:32,756:INFO:            requests: 2.32.3
2026-02-02 13:07:32,756:INFO:          matplotlib: 3.7.5
2026-02-02 13:07:32,756:INFO:          scikitplot: 0.3.7
2026-02-02 13:07:32,756:INFO:         yellowbrick: 1.5
2026-02-02 13:07:32,756:INFO:              plotly: 5.24.1
2026-02-02 13:07:32,756:INFO:    plotly-resampler: Not installed
2026-02-02 13:07:32,756:INFO:             kaleido: 1.2.0
2026-02-02 13:07:32,756:INFO:           schemdraw: 0.15
2026-02-02 13:07:32,756:INFO:         statsmodels: 0.14.6
2026-02-02 13:07:32,756:INFO:              sktime: 0.26.0
2026-02-02 13:07:32,756:INFO:               tbats: 1.1.3
2026-02-02 13:07:32,756:INFO:            pmdarima: 2.0.4
2026-02-02 13:07:32,756:INFO:              psutil: 7.2.1
2026-02-02 13:07:32,756:INFO:          markupsafe: 3.0.3
2026-02-02 13:07:32,756:INFO:             pickle5: Not installed
2026-02-02 13:07:32,756:INFO:         cloudpickle: 3.0.0
2026-02-02 13:07:32,756:INFO:         deprecation: 2.1.0
2026-02-02 13:07:32,756:INFO:              xxhash: 3.6.0
2026-02-02 13:07:32,756:INFO:           wurlitzer: Not installed
2026-02-02 13:07:32,756:INFO:PyCaret optional dependencies:
2026-02-02 13:07:32,756:INFO:                shap: 0.44.1
2026-02-02 13:07:32,756:INFO:           interpret: 0.7.3
2026-02-02 13:07:32,756:INFO:                umap: 0.5.7
2026-02-02 13:07:32,756:INFO:     ydata_profiling: 4.18.1
2026-02-02 13:07:32,756:INFO:  explainerdashboard: 0.5.1
2026-02-02 13:07:32,756:INFO:             autoviz: Not installed
2026-02-02 13:07:32,756:INFO:           fairlearn: 0.7.0
2026-02-02 13:07:32,756:INFO:          deepchecks: Not installed
2026-02-02 13:07:32,756:INFO:             xgboost: Not installed
2026-02-02 13:07:32,756:INFO:            catboost: 1.2.8
2026-02-02 13:07:32,756:INFO:              kmodes: 0.12.2
2026-02-02 13:07:32,756:INFO:             mlxtend: 0.23.4
2026-02-02 13:07:32,756:INFO:       statsforecast: 1.5.0
2026-02-02 13:07:32,756:INFO:        tune_sklearn: Not installed
2026-02-02 13:07:32,756:INFO:                 ray: Not installed
2026-02-02 13:07:32,756:INFO:            hyperopt: 0.2.7
2026-02-02 13:07:32,756:INFO:              optuna: 4.6.0
2026-02-02 13:07:32,756:INFO:               skopt: 0.10.2
2026-02-02 13:07:32,756:INFO:              mlflow: 3.8.1
2026-02-02 13:07:32,756:INFO:              gradio: 6.3.0
2026-02-02 13:07:32,756:INFO:             fastapi: 0.128.0
2026-02-02 13:07:32,756:INFO:             uvicorn: 0.40.0
2026-02-02 13:07:32,756:INFO:              m2cgen: 0.10.0
2026-02-02 13:07:32,756:INFO:           evidently: 0.4.40
2026-02-02 13:07:32,756:INFO:               fugue: 0.8.7
2026-02-02 13:07:32,756:INFO:           streamlit: Not installed
2026-02-02 13:07:32,756:INFO:             prophet: Not installed
2026-02-02 13:07:32,756:INFO:None
2026-02-02 13:07:32,756:INFO:Set up data.
2026-02-02 13:07:32,756:INFO:Set up folding strategy.
2026-02-02 13:07:32,771:INFO:Set up train/test split.
2026-02-02 13:07:32,773:INFO:Set up index.
2026-02-02 13:07:32,773:INFO:Assigning column types.
2026-02-02 13:07:32,773:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 13:07:32,808:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:07:32,808:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:07:32,825:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:07:32,828:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:07:32,856:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:07:32,857:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:07:32,906:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:07:32,907:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:07:32,908:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 13:07:32,928:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:07:32,954:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:07:32,955:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:07:32,983:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:07:32,992:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:07:32,992:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:07:32,992:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 13:07:33,040:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:07:33,040:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:07:33,089:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:07:33,089:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:07:33,090:INFO:Preparing preprocessing pipeline...
2026-02-02 13:07:33,090:INFO:Set up simple imputation.
2026-02-02 13:07:33,090:INFO:Set up feature normalization.
2026-02-02 13:07:33,107:INFO:Finished creating preprocessing pipeline.
2026-02-02 13:07:33,107:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fil...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 13:07:33,107:INFO:Creating final display dataframe.
2026-02-02 13:07:33,190:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 17)
4        Transformed data shape       (10381, 17)
5   Transformed train set shape        (7266, 17)
6    Transformed test set shape        (3115, 17)
7              Numeric features                 6
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              1aa4
2026-02-02 13:07:33,240:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:07:33,240:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:07:33,276:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:07:33,276:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:07:33,276:INFO:setup() successfully completed in 0.55s...............
2026-02-02 13:07:33,276:INFO:Initializing compare_models()
2026-02-02 13:07:33,276:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 13:07:33,276:INFO:Checking exceptions
2026-02-02 13:07:33,290:INFO:Preparing display monitor
2026-02-02 13:07:33,290:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 13:07:33,290:INFO:Total runtime is 0.0 minutes
2026-02-02 13:07:33,290:INFO:SubProcess create_model() called ==================================
2026-02-02 13:07:33,290:INFO:Initializing create_model()
2026-02-02 13:07:33,290:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D02715D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:07:33,290:INFO:Checking exceptions
2026-02-02 13:07:33,290:INFO:Importing libraries
2026-02-02 13:07:33,290:INFO:Copying training dataset
2026-02-02 13:07:33,290:INFO:Defining folds
2026-02-02 13:07:33,290:INFO:Declaring metric variables
2026-02-02 13:07:33,290:INFO:Importing untrained model
2026-02-02 13:07:33,290:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:07:33,290:INFO:Starting cross validation
2026-02-02 13:07:33,290:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:07:33,792:INFO:Calculating mean and std
2026-02-02 13:07:33,794:INFO:Creating metrics dataframe
2026-02-02 13:07:33,796:INFO:Uploading results into container
2026-02-02 13:07:33,796:INFO:Uploading model into container now
2026-02-02 13:07:33,797:INFO:_master_model_container: 1
2026-02-02 13:07:33,797:INFO:_display_container: 2
2026-02-02 13:07:33,798:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:07:33,798:INFO:create_model() successfully completed......................................
2026-02-02 13:07:33,960:INFO:SubProcess create_model() end ==================================
2026-02-02 13:07:33,960:INFO:Creating metrics dataframe
2026-02-02 13:07:33,960:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 13:07:33,960:INFO:Initializing create_model()
2026-02-02 13:07:33,960:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:07:33,960:INFO:Checking exceptions
2026-02-02 13:07:33,960:INFO:Importing libraries
2026-02-02 13:07:33,960:INFO:Copying training dataset
2026-02-02 13:07:33,973:INFO:Defining folds
2026-02-02 13:07:33,973:INFO:Declaring metric variables
2026-02-02 13:07:33,973:INFO:Importing untrained model
2026-02-02 13:07:33,973:INFO:Declaring custom model
2026-02-02 13:07:33,973:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:07:33,973:INFO:Cross validation set to False
2026-02-02 13:07:33,973:INFO:Fitting Model
2026-02-02 13:07:33,992:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:07:33,994:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000394 seconds.
2026-02-02 13:07:33,994:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:07:33,994:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:07:33,994:INFO:[LightGBM] [Info] Total Bins 591
2026-02-02 13:07:33,994:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 15
2026-02-02 13:07:33,994:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:07:33,994:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:07:34,072:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:07:34,073:INFO:create_model() successfully completed......................................
2026-02-02 13:07:34,239:INFO:_master_model_container: 1
2026-02-02 13:07:34,239:INFO:_display_container: 2
2026-02-02 13:07:34,239:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:07:34,239:INFO:compare_models() successfully completed......................................
2026-02-02 13:07:34,239:INFO:Initializing tune_model()
2026-02-02 13:07:34,239:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:07:34,239:INFO:Checking exceptions
2026-02-02 13:07:34,239:INFO:Copying training dataset
2026-02-02 13:07:34,239:INFO:Checking base model
2026-02-02 13:07:34,239:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 13:07:34,239:INFO:Declaring metric variables
2026-02-02 13:07:34,239:INFO:Defining Hyperparameters
2026-02-02 13:07:34,395:INFO:Tuning with n_jobs=-1
2026-02-02 13:07:34,395:INFO:Initializing RandomizedSearchCV
2026-02-02 13:07:37,847:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 13:07:37,853:INFO:Hyperparameter search completed
2026-02-02 13:07:37,855:INFO:SubProcess create_model() called ==================================
2026-02-02 13:07:37,856:INFO:Initializing create_model()
2026-02-02 13:07:37,857:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022833994410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 13:07:37,857:INFO:Checking exceptions
2026-02-02 13:07:37,858:INFO:Importing libraries
2026-02-02 13:07:37,858:INFO:Copying training dataset
2026-02-02 13:07:37,866:INFO:Defining folds
2026-02-02 13:07:37,866:INFO:Declaring metric variables
2026-02-02 13:07:37,866:INFO:Importing untrained model
2026-02-02 13:07:37,866:INFO:Declaring custom model
2026-02-02 13:07:37,868:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:07:37,868:INFO:Starting cross validation
2026-02-02 13:07:37,868:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:07:38,406:INFO:Calculating mean and std
2026-02-02 13:07:38,406:INFO:Creating metrics dataframe
2026-02-02 13:07:38,408:INFO:Finalizing model
2026-02-02 13:07:38,431:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:07:38,431:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:07:38,431:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:07:38,436:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:07:38,436:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:07:38,436:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:07:38,436:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:07:38,440:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001508 seconds.
2026-02-02 13:07:38,440:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:07:38,440:INFO:[LightGBM] [Info] Total Bins 591
2026-02-02 13:07:38,440:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 15
2026-02-02 13:07:38,440:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:07:38,441:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:07:38,483:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,502:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,514:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,518:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,526:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,532:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,549:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,551:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,553:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,572:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:07:38,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,585:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,612:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,616:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,654:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:07:38,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:07:38,675:INFO:Uploading results into container
2026-02-02 13:07:38,676:INFO:Uploading model into container now
2026-02-02 13:07:38,676:INFO:_master_model_container: 2
2026-02-02 13:07:38,676:INFO:_display_container: 3
2026-02-02 13:07:38,678:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:07:38,678:INFO:create_model() successfully completed......................................
2026-02-02 13:07:38,842:INFO:SubProcess create_model() end ==================================
2026-02-02 13:07:38,842:INFO:choose_better activated
2026-02-02 13:07:38,842:INFO:SubProcess create_model() called ==================================
2026-02-02 13:07:38,842:INFO:Initializing create_model()
2026-02-02 13:07:38,842:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:07:38,842:INFO:Checking exceptions
2026-02-02 13:07:38,842:INFO:Importing libraries
2026-02-02 13:07:38,842:INFO:Copying training dataset
2026-02-02 13:07:38,842:INFO:Defining folds
2026-02-02 13:07:38,842:INFO:Declaring metric variables
2026-02-02 13:07:38,842:INFO:Importing untrained model
2026-02-02 13:07:38,842:INFO:Declaring custom model
2026-02-02 13:07:38,842:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:07:38,842:INFO:Starting cross validation
2026-02-02 13:07:38,842:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:07:39,538:INFO:Calculating mean and std
2026-02-02 13:07:39,538:INFO:Creating metrics dataframe
2026-02-02 13:07:39,540:INFO:Finalizing model
2026-02-02 13:07:39,564:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:07:39,568:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001047 seconds.
2026-02-02 13:07:39,568:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:07:39,568:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:07:39,568:INFO:[LightGBM] [Info] Total Bins 591
2026-02-02 13:07:39,568:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 15
2026-02-02 13:07:39,568:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:07:39,568:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:07:39,690:INFO:Uploading results into container
2026-02-02 13:07:39,690:INFO:Uploading model into container now
2026-02-02 13:07:39,690:INFO:_master_model_container: 3
2026-02-02 13:07:39,690:INFO:_display_container: 4
2026-02-02 13:07:39,692:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:07:39,692:INFO:create_model() successfully completed......................................
2026-02-02 13:07:39,856:INFO:SubProcess create_model() end ==================================
2026-02-02 13:07:39,856:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9971
2026-02-02 13:07:39,856:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9955
2026-02-02 13:07:39,856:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 13:07:39,856:INFO:choose_better completed
2026-02-02 13:07:39,856:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:07:39,856:INFO:_master_model_container: 3
2026-02-02 13:07:39,856:INFO:_display_container: 3
2026-02-02 13:07:39,856:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:07:39,856:INFO:tune_model() successfully completed......................................
2026-02-02 13:07:39,989:INFO:Initializing predict_model()
2026-02-02 13:07:39,989:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000022839200B80>)
2026-02-02 13:07:39,989:INFO:Checking exceptions
2026-02-02 13:07:39,989:INFO:Preloading libraries
2026-02-02 13:07:39,989:INFO:Set up data.
2026-02-02 13:07:39,989:INFO:Set up index.
2026-02-02 13:07:42,012:INFO:Initializing plot_model()
2026-02-02 13:07:42,012:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:07:42,012:INFO:Checking exceptions
2026-02-02 13:07:42,012:INFO:Preloading libraries
2026-02-02 13:07:42,025:INFO:Copying training dataset
2026-02-02 13:07:42,025:INFO:Plot type: feature
2026-02-02 13:07:42,026:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:07:42,162:INFO:Visual Rendered Successfully
2026-02-02 13:07:42,329:INFO:plot_model() successfully completed......................................
2026-02-02 13:07:42,329:INFO:Initializing plot_model()
2026-02-02 13:07:42,329:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F17B4450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:07:42,329:INFO:Checking exceptions
2026-02-02 13:07:42,329:INFO:Preloading libraries
2026-02-02 13:07:42,342:INFO:Copying training dataset
2026-02-02 13:07:42,342:INFO:Plot type: feature_all
2026-02-02 13:07:42,392:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:07:42,553:INFO:Visual Rendered Successfully
2026-02-02 13:07:42,695:INFO:plot_model() successfully completed......................................
2026-02-02 13:07:42,695:INFO:Initializing save_model()
2026-02-02 13:07:42,695:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fil...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 13:07:42,695:INFO:Adding model into prep_pipe
2026-02-02 13:07:42,714:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-02 13:07:42,722:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              mis...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-02 13:07:42,722:INFO:save_model() successfully completed......................................
2026-02-02 13:08:19,489:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\3912965107.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 13:08:21,574:INFO:PyCaret ClassificationExperiment
2026-02-02 13:08:21,574:INFO:Logging name: clf-default-name
2026-02-02 13:08:21,574:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 13:08:21,574:INFO:version 3.3.2
2026-02-02 13:08:21,574:INFO:Initializing setup()
2026-02-02 13:08:21,574:INFO:self.USI: 632a
2026-02-02 13:08:21,574:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 13:08:21,574:INFO:Checking environment
2026-02-02 13:08:21,574:INFO:python_version: 3.11.11
2026-02-02 13:08:21,574:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 13:08:21,574:INFO:machine: AMD64
2026-02-02 13:08:21,574:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 13:08:21,574:INFO:Memory: svmem(total=34009374720, available=11230306304, percent=67.0, used=22779068416, free=11230306304)
2026-02-02 13:08:21,574:INFO:Physical Core: 12
2026-02-02 13:08:21,574:INFO:Logical Core: 16
2026-02-02 13:08:21,574:INFO:Checking libraries
2026-02-02 13:08:21,574:INFO:System:
2026-02-02 13:08:21,574:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 13:08:21,574:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 13:08:21,574:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 13:08:21,574:INFO:PyCaret required dependencies:
2026-02-02 13:08:21,574:INFO:                 pip: 25.0
2026-02-02 13:08:21,574:INFO:          setuptools: 75.8.0
2026-02-02 13:08:21,574:INFO:             pycaret: 3.3.2
2026-02-02 13:08:21,574:INFO:             IPython: 9.9.0
2026-02-02 13:08:21,574:INFO:          ipywidgets: 8.1.8
2026-02-02 13:08:21,574:INFO:                tqdm: 4.67.1
2026-02-02 13:08:21,574:INFO:               numpy: 1.26.4
2026-02-02 13:08:21,574:INFO:              pandas: 2.1.4
2026-02-02 13:08:21,574:INFO:              jinja2: 3.1.6
2026-02-02 13:08:21,574:INFO:               scipy: 1.11.4
2026-02-02 13:08:21,574:INFO:              joblib: 1.3.2
2026-02-02 13:08:21,574:INFO:             sklearn: 1.4.2
2026-02-02 13:08:21,574:INFO:                pyod: 2.0.6
2026-02-02 13:08:21,574:INFO:            imblearn: 0.14.1
2026-02-02 13:08:21,574:INFO:   category_encoders: 2.7.0
2026-02-02 13:08:21,574:INFO:            lightgbm: 4.6.0
2026-02-02 13:08:21,574:INFO:               numba: 0.62.1
2026-02-02 13:08:21,574:INFO:            requests: 2.32.3
2026-02-02 13:08:21,574:INFO:          matplotlib: 3.7.5
2026-02-02 13:08:21,574:INFO:          scikitplot: 0.3.7
2026-02-02 13:08:21,574:INFO:         yellowbrick: 1.5
2026-02-02 13:08:21,574:INFO:              plotly: 5.24.1
2026-02-02 13:08:21,574:INFO:    plotly-resampler: Not installed
2026-02-02 13:08:21,574:INFO:             kaleido: 1.2.0
2026-02-02 13:08:21,574:INFO:           schemdraw: 0.15
2026-02-02 13:08:21,574:INFO:         statsmodels: 0.14.6
2026-02-02 13:08:21,574:INFO:              sktime: 0.26.0
2026-02-02 13:08:21,574:INFO:               tbats: 1.1.3
2026-02-02 13:08:21,574:INFO:            pmdarima: 2.0.4
2026-02-02 13:08:21,574:INFO:              psutil: 7.2.1
2026-02-02 13:08:21,574:INFO:          markupsafe: 3.0.3
2026-02-02 13:08:21,574:INFO:             pickle5: Not installed
2026-02-02 13:08:21,574:INFO:         cloudpickle: 3.0.0
2026-02-02 13:08:21,574:INFO:         deprecation: 2.1.0
2026-02-02 13:08:21,574:INFO:              xxhash: 3.6.0
2026-02-02 13:08:21,574:INFO:           wurlitzer: Not installed
2026-02-02 13:08:21,574:INFO:PyCaret optional dependencies:
2026-02-02 13:08:21,574:INFO:                shap: 0.44.1
2026-02-02 13:08:21,574:INFO:           interpret: 0.7.3
2026-02-02 13:08:21,574:INFO:                umap: 0.5.7
2026-02-02 13:08:21,574:INFO:     ydata_profiling: 4.18.1
2026-02-02 13:08:21,574:INFO:  explainerdashboard: 0.5.1
2026-02-02 13:08:21,574:INFO:             autoviz: Not installed
2026-02-02 13:08:21,574:INFO:           fairlearn: 0.7.0
2026-02-02 13:08:21,574:INFO:          deepchecks: Not installed
2026-02-02 13:08:21,574:INFO:             xgboost: Not installed
2026-02-02 13:08:21,574:INFO:            catboost: 1.2.8
2026-02-02 13:08:21,574:INFO:              kmodes: 0.12.2
2026-02-02 13:08:21,574:INFO:             mlxtend: 0.23.4
2026-02-02 13:08:21,574:INFO:       statsforecast: 1.5.0
2026-02-02 13:08:21,574:INFO:        tune_sklearn: Not installed
2026-02-02 13:08:21,574:INFO:                 ray: Not installed
2026-02-02 13:08:21,574:INFO:            hyperopt: 0.2.7
2026-02-02 13:08:21,574:INFO:              optuna: 4.6.0
2026-02-02 13:08:21,574:INFO:               skopt: 0.10.2
2026-02-02 13:08:21,574:INFO:              mlflow: 3.8.1
2026-02-02 13:08:21,574:INFO:              gradio: 6.3.0
2026-02-02 13:08:21,574:INFO:             fastapi: 0.128.0
2026-02-02 13:08:21,574:INFO:             uvicorn: 0.40.0
2026-02-02 13:08:21,574:INFO:              m2cgen: 0.10.0
2026-02-02 13:08:21,574:INFO:           evidently: 0.4.40
2026-02-02 13:08:21,574:INFO:               fugue: 0.8.7
2026-02-02 13:08:21,574:INFO:           streamlit: Not installed
2026-02-02 13:08:21,574:INFO:             prophet: Not installed
2026-02-02 13:08:21,574:INFO:None
2026-02-02 13:08:21,574:INFO:Set up data.
2026-02-02 13:08:21,639:INFO:Set up folding strategy.
2026-02-02 13:08:21,639:INFO:Set up train/test split.
2026-02-02 13:08:21,774:INFO:Set up index.
2026-02-02 13:08:21,774:INFO:Assigning column types.
2026-02-02 13:08:21,839:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 13:08:21,874:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:08:21,874:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:08:21,892:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:08:21,892:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:08:21,920:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:08:21,926:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:08:21,944:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:08:21,944:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:08:21,945:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 13:08:21,976:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:08:21,996:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:08:21,996:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:08:22,025:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:08:22,039:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:08:22,039:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:08:22,039:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 13:08:22,089:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:08:22,089:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:08:22,138:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:08:22,138:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:08:22,140:INFO:Preparing preprocessing pipeline...
2026-02-02 13:08:22,142:INFO:Set up simple imputation.
2026-02-02 13:08:22,142:INFO:Set up feature normalization.
2026-02-02 13:08:22,359:INFO:Finished creating preprocessing pipeline.
2026-02-02 13:08:22,372:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 13:08:22,372:INFO:Creating final display dataframe.
2026-02-02 13:08:23,092:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 15)
4        Transformed data shape      (373023, 15)
5   Transformed train set shape      (261116, 15)
6    Transformed test set shape      (111907, 15)
7              Numeric features                11
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              632a
2026-02-02 13:08:23,139:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:08:23,139:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:08:23,189:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:08:23,189:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:08:23,189:INFO:setup() successfully completed in 1.62s...............
2026-02-02 13:08:23,189:INFO:Initializing compare_models()
2026-02-02 13:08:23,189:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 13:08:23,189:INFO:Checking exceptions
2026-02-02 13:08:23,239:INFO:Preparing display monitor
2026-02-02 13:08:23,239:INFO:Initializing Logistic Regression
2026-02-02 13:08:23,239:INFO:Total runtime is 0.0 minutes
2026-02-02 13:08:23,239:INFO:SubProcess create_model() called ==================================
2026-02-02 13:08:23,239:INFO:Initializing create_model()
2026-02-02 13:08:23,239:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F4CDC750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:08:23,239:INFO:Checking exceptions
2026-02-02 13:08:23,239:INFO:Importing libraries
2026-02-02 13:08:23,239:INFO:Copying training dataset
2026-02-02 13:08:23,322:INFO:Defining folds
2026-02-02 13:08:23,322:INFO:Declaring metric variables
2026-02-02 13:08:23,322:INFO:Importing untrained model
2026-02-02 13:08:23,322:INFO:Logistic Regression Imported successfully
2026-02-02 13:08:23,322:INFO:Starting cross validation
2026-02-02 13:08:23,338:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:08:24,547:INFO:Calculating mean and std
2026-02-02 13:08:24,547:INFO:Creating metrics dataframe
2026-02-02 13:08:24,547:INFO:Uploading results into container
2026-02-02 13:08:24,555:INFO:Uploading model into container now
2026-02-02 13:08:24,556:INFO:_master_model_container: 1
2026-02-02 13:08:24,556:INFO:_display_container: 2
2026-02-02 13:08:24,557:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 13:08:24,557:INFO:create_model() successfully completed......................................
2026-02-02 13:08:24,722:INFO:SubProcess create_model() end ==================================
2026-02-02 13:08:24,722:INFO:Creating metrics dataframe
2026-02-02 13:08:24,724:INFO:Initializing Decision Tree Classifier
2026-02-02 13:08:24,724:INFO:Total runtime is 0.024743942419687907 minutes
2026-02-02 13:08:24,724:INFO:SubProcess create_model() called ==================================
2026-02-02 13:08:24,724:INFO:Initializing create_model()
2026-02-02 13:08:24,724:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F4CDC750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:08:24,724:INFO:Checking exceptions
2026-02-02 13:08:24,724:INFO:Importing libraries
2026-02-02 13:08:24,724:INFO:Copying training dataset
2026-02-02 13:08:24,845:INFO:Defining folds
2026-02-02 13:08:24,845:INFO:Declaring metric variables
2026-02-02 13:08:24,845:INFO:Importing untrained model
2026-02-02 13:08:24,845:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:08:24,845:INFO:Starting cross validation
2026-02-02 13:08:24,855:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:08:26,756:INFO:Calculating mean and std
2026-02-02 13:08:26,756:INFO:Creating metrics dataframe
2026-02-02 13:08:26,756:INFO:Uploading results into container
2026-02-02 13:08:26,756:INFO:Uploading model into container now
2026-02-02 13:08:26,756:INFO:_master_model_container: 2
2026-02-02 13:08:26,756:INFO:_display_container: 2
2026-02-02 13:08:26,756:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:08:26,756:INFO:create_model() successfully completed......................................
2026-02-02 13:08:26,906:INFO:SubProcess create_model() end ==================================
2026-02-02 13:08:26,907:INFO:Creating metrics dataframe
2026-02-02 13:08:26,908:INFO:Initializing Random Forest Classifier
2026-02-02 13:08:26,909:INFO:Total runtime is 0.06116692622502645 minutes
2026-02-02 13:08:26,909:INFO:SubProcess create_model() called ==================================
2026-02-02 13:08:26,909:INFO:Initializing create_model()
2026-02-02 13:08:26,909:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F4CDC750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:08:26,909:INFO:Checking exceptions
2026-02-02 13:08:26,909:INFO:Importing libraries
2026-02-02 13:08:26,909:INFO:Copying training dataset
2026-02-02 13:08:27,010:INFO:Defining folds
2026-02-02 13:08:27,010:INFO:Declaring metric variables
2026-02-02 13:08:27,010:INFO:Importing untrained model
2026-02-02 13:08:27,010:INFO:Random Forest Classifier Imported successfully
2026-02-02 13:08:27,010:INFO:Starting cross validation
2026-02-02 13:08:27,010:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:08:35,178:INFO:Calculating mean and std
2026-02-02 13:08:35,178:INFO:Creating metrics dataframe
2026-02-02 13:08:35,178:INFO:Uploading results into container
2026-02-02 13:08:35,178:INFO:Uploading model into container now
2026-02-02 13:08:35,178:INFO:_master_model_container: 3
2026-02-02 13:08:35,178:INFO:_display_container: 2
2026-02-02 13:08:35,178:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:08:35,178:INFO:create_model() successfully completed......................................
2026-02-02 13:08:35,305:INFO:SubProcess create_model() end ==================================
2026-02-02 13:08:35,305:INFO:Creating metrics dataframe
2026-02-02 13:08:35,305:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 13:08:35,305:INFO:Total runtime is 0.20109709103902182 minutes
2026-02-02 13:08:35,305:INFO:SubProcess create_model() called ==================================
2026-02-02 13:08:35,305:INFO:Initializing create_model()
2026-02-02 13:08:35,305:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F4CDC750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:08:35,305:INFO:Checking exceptions
2026-02-02 13:08:35,305:INFO:Importing libraries
2026-02-02 13:08:35,305:INFO:Copying training dataset
2026-02-02 13:08:35,405:INFO:Defining folds
2026-02-02 13:08:35,405:INFO:Declaring metric variables
2026-02-02 13:08:35,405:INFO:Importing untrained model
2026-02-02 13:08:35,405:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:08:35,405:INFO:Starting cross validation
2026-02-02 13:08:35,405:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:08:37,730:INFO:Calculating mean and std
2026-02-02 13:08:37,730:INFO:Creating metrics dataframe
2026-02-02 13:08:37,730:INFO:Uploading results into container
2026-02-02 13:08:37,730:INFO:Uploading model into container now
2026-02-02 13:08:37,730:INFO:_master_model_container: 4
2026-02-02 13:08:37,730:INFO:_display_container: 2
2026-02-02 13:08:37,730:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:08:37,730:INFO:create_model() successfully completed......................................
2026-02-02 13:08:37,855:INFO:SubProcess create_model() end ==================================
2026-02-02 13:08:37,855:INFO:Creating metrics dataframe
2026-02-02 13:08:37,870:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 13:08:37,870:INFO:Initializing create_model()
2026-02-02 13:08:37,870:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:08:37,870:INFO:Checking exceptions
2026-02-02 13:08:37,870:INFO:Importing libraries
2026-02-02 13:08:37,870:INFO:Copying training dataset
2026-02-02 13:08:37,967:INFO:Defining folds
2026-02-02 13:08:37,967:INFO:Declaring metric variables
2026-02-02 13:08:37,967:INFO:Importing untrained model
2026-02-02 13:08:37,967:INFO:Declaring custom model
2026-02-02 13:08:37,967:INFO:Random Forest Classifier Imported successfully
2026-02-02 13:08:37,967:INFO:Cross validation set to False
2026-02-02 13:08:37,967:INFO:Fitting Model
2026-02-02 13:08:41,505:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:08:41,505:INFO:create_model() successfully completed......................................
2026-02-02 13:08:41,638:INFO:Initializing create_model()
2026-02-02 13:08:41,638:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:08:41,638:INFO:Checking exceptions
2026-02-02 13:08:41,638:INFO:Importing libraries
2026-02-02 13:08:41,638:INFO:Copying training dataset
2026-02-02 13:08:41,736:INFO:Defining folds
2026-02-02 13:08:41,736:INFO:Declaring metric variables
2026-02-02 13:08:41,736:INFO:Importing untrained model
2026-02-02 13:08:41,736:INFO:Declaring custom model
2026-02-02 13:08:41,736:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:08:41,736:INFO:Cross validation set to False
2026-02-02 13:08:41,736:INFO:Fitting Model
2026-02-02 13:08:42,799:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:08:42,799:INFO:create_model() successfully completed......................................
2026-02-02 13:08:42,953:INFO:Initializing create_model()
2026-02-02 13:08:42,953:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:08:42,953:INFO:Checking exceptions
2026-02-02 13:08:42,955:INFO:Importing libraries
2026-02-02 13:08:42,955:INFO:Copying training dataset
2026-02-02 13:08:43,074:INFO:Defining folds
2026-02-02 13:08:43,074:INFO:Declaring metric variables
2026-02-02 13:08:43,074:INFO:Importing untrained model
2026-02-02 13:08:43,074:INFO:Declaring custom model
2026-02-02 13:08:43,074:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:08:43,074:INFO:Cross validation set to False
2026-02-02 13:08:43,074:INFO:Fitting Model
2026-02-02 13:08:43,451:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 13:08:43,462:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005864 seconds.
2026-02-02 13:08:43,462:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:08:43,464:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:08:43,464:INFO:[LightGBM] [Info] Total Bins 1586
2026-02-02 13:08:43,464:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 14
2026-02-02 13:08:43,467:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 13:08:43,467:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 13:08:43,924:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:08:43,924:INFO:create_model() successfully completed......................................
2026-02-02 13:08:44,102:INFO:_master_model_container: 4
2026-02-02 13:08:44,102:INFO:_display_container: 2
2026-02-02 13:08:44,102:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2026-02-02 13:08:44,102:INFO:compare_models() successfully completed......................................
2026-02-02 13:08:44,111:INFO:Initializing tune_model()
2026-02-02 13:08:44,111:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:08:44,111:INFO:Checking exceptions
2026-02-02 13:08:44,144:INFO:Copying training dataset
2026-02-02 13:08:44,210:INFO:Checking base model
2026-02-02 13:08:44,210:INFO:Base model : Random Forest Classifier
2026-02-02 13:08:44,210:INFO:Declaring metric variables
2026-02-02 13:08:44,210:INFO:Defining Hyperparameters
2026-02-02 13:08:44,340:INFO:Tuning with n_jobs=-1
2026-02-02 13:08:44,341:INFO:Initializing RandomizedSearchCV
2026-02-02 13:09:39,218:INFO:best_params: {'actual_estimator__n_estimators': 120, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 13:09:39,219:INFO:Hyperparameter search completed
2026-02-02 13:09:39,220:INFO:SubProcess create_model() called ==================================
2026-02-02 13:09:39,221:INFO:Initializing create_model()
2026-02-02 13:09:39,221:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002283A827A50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': True})
2026-02-02 13:09:39,222:INFO:Checking exceptions
2026-02-02 13:09:39,222:INFO:Importing libraries
2026-02-02 13:09:39,222:INFO:Copying training dataset
2026-02-02 13:09:39,389:INFO:Defining folds
2026-02-02 13:09:39,389:INFO:Declaring metric variables
2026-02-02 13:09:39,390:INFO:Importing untrained model
2026-02-02 13:09:39,390:INFO:Declaring custom model
2026-02-02 13:09:39,391:INFO:Random Forest Classifier Imported successfully
2026-02-02 13:09:39,392:INFO:Starting cross validation
2026-02-02 13:09:39,393:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:09:47,206:INFO:Calculating mean and std
2026-02-02 13:09:47,208:INFO:Creating metrics dataframe
2026-02-02 13:09:47,211:INFO:Finalizing model
2026-02-02 13:09:51,161:INFO:Uploading results into container
2026-02-02 13:09:51,162:INFO:Uploading model into container now
2026-02-02 13:09:51,163:INFO:_master_model_container: 5
2026-02-02 13:09:51,164:INFO:_display_container: 3
2026-02-02 13:09:51,165:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:09:51,165:INFO:create_model() successfully completed......................................
2026-02-02 13:09:51,365:INFO:SubProcess create_model() end ==================================
2026-02-02 13:09:51,365:INFO:choose_better activated
2026-02-02 13:09:51,365:INFO:SubProcess create_model() called ==================================
2026-02-02 13:09:51,366:INFO:Initializing create_model()
2026-02-02 13:09:51,366:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:09:51,366:INFO:Checking exceptions
2026-02-02 13:09:51,367:INFO:Importing libraries
2026-02-02 13:09:51,367:INFO:Copying training dataset
2026-02-02 13:09:51,581:INFO:Defining folds
2026-02-02 13:09:51,581:INFO:Declaring metric variables
2026-02-02 13:09:51,582:INFO:Importing untrained model
2026-02-02 13:09:51,582:INFO:Declaring custom model
2026-02-02 13:09:51,583:INFO:Random Forest Classifier Imported successfully
2026-02-02 13:09:51,584:INFO:Starting cross validation
2026-02-02 13:09:51,585:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:10:02,749:INFO:Calculating mean and std
2026-02-02 13:10:02,751:INFO:Creating metrics dataframe
2026-02-02 13:10:02,753:INFO:Finalizing model
2026-02-02 13:10:07,859:INFO:Uploading results into container
2026-02-02 13:10:07,859:INFO:Uploading model into container now
2026-02-02 13:10:07,860:INFO:_master_model_container: 6
2026-02-02 13:10:07,860:INFO:_display_container: 4
2026-02-02 13:10:07,860:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:10:07,861:INFO:create_model() successfully completed......................................
2026-02-02 13:10:08,009:INFO:SubProcess create_model() end ==================================
2026-02-02 13:10:08,010:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9946
2026-02-02 13:10:08,010:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9444
2026-02-02 13:10:08,011:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 13:10:08,011:INFO:choose_better completed
2026-02-02 13:10:08,011:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:10:08,014:INFO:_master_model_container: 6
2026-02-02 13:10:08,014:INFO:_display_container: 3
2026-02-02 13:10:08,015:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:10:08,015:INFO:tune_model() successfully completed......................................
2026-02-02 13:10:08,169:INFO:Initializing tune_model()
2026-02-02 13:10:08,169:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:10:08,169:INFO:Checking exceptions
2026-02-02 13:10:08,216:INFO:Copying training dataset
2026-02-02 13:10:08,296:INFO:Checking base model
2026-02-02 13:10:08,296:INFO:Base model : Decision Tree Classifier
2026-02-02 13:10:08,297:INFO:Declaring metric variables
2026-02-02 13:10:08,297:INFO:Defining Hyperparameters
2026-02-02 13:10:08,451:INFO:Tuning with n_jobs=-1
2026-02-02 13:10:08,451:INFO:Initializing RandomizedSearchCV
2026-02-02 13:10:11,923:INFO:best_params: {'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 11, 'actual_estimator__criterion': 'gini'}
2026-02-02 13:10:11,925:INFO:Hyperparameter search completed
2026-02-02 13:10:11,926:INFO:SubProcess create_model() called ==================================
2026-02-02 13:10:11,927:INFO:Initializing create_model()
2026-02-02 13:10:11,929:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228D5180850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 10, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 11, 'criterion': 'gini'})
2026-02-02 13:10:11,929:INFO:Checking exceptions
2026-02-02 13:10:11,929:INFO:Importing libraries
2026-02-02 13:10:11,930:INFO:Copying training dataset
2026-02-02 13:10:12,070:INFO:Defining folds
2026-02-02 13:10:12,070:INFO:Declaring metric variables
2026-02-02 13:10:12,071:INFO:Importing untrained model
2026-02-02 13:10:12,072:INFO:Declaring custom model
2026-02-02 13:10:12,073:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:10:12,073:INFO:Starting cross validation
2026-02-02 13:10:12,074:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:10:13,579:INFO:Calculating mean and std
2026-02-02 13:10:13,582:INFO:Creating metrics dataframe
2026-02-02 13:10:13,584:INFO:Finalizing model
2026-02-02 13:10:14,324:INFO:Uploading results into container
2026-02-02 13:10:14,325:INFO:Uploading model into container now
2026-02-02 13:10:14,326:INFO:_master_model_container: 7
2026-02-02 13:10:14,326:INFO:_display_container: 4
2026-02-02 13:10:14,326:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=11, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=3,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:10:14,327:INFO:create_model() successfully completed......................................
2026-02-02 13:10:14,481:INFO:SubProcess create_model() end ==================================
2026-02-02 13:10:14,481:INFO:choose_better activated
2026-02-02 13:10:14,482:INFO:SubProcess create_model() called ==================================
2026-02-02 13:10:14,482:INFO:Initializing create_model()
2026-02-02 13:10:14,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:10:14,482:INFO:Checking exceptions
2026-02-02 13:10:14,483:INFO:Importing libraries
2026-02-02 13:10:14,483:INFO:Copying training dataset
2026-02-02 13:10:14,592:INFO:Defining folds
2026-02-02 13:10:14,593:INFO:Declaring metric variables
2026-02-02 13:10:14,593:INFO:Importing untrained model
2026-02-02 13:10:14,593:INFO:Declaring custom model
2026-02-02 13:10:14,593:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:10:14,593:INFO:Starting cross validation
2026-02-02 13:10:14,594:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:10:16,531:INFO:Calculating mean and std
2026-02-02 13:10:16,532:INFO:Creating metrics dataframe
2026-02-02 13:10:16,534:INFO:Finalizing model
2026-02-02 13:10:17,809:INFO:Uploading results into container
2026-02-02 13:10:17,809:INFO:Uploading model into container now
2026-02-02 13:10:17,810:INFO:_master_model_container: 8
2026-02-02 13:10:17,810:INFO:_display_container: 5
2026-02-02 13:10:17,810:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:10:17,810:INFO:create_model() successfully completed......................................
2026-02-02 13:10:17,990:INFO:SubProcess create_model() end ==================================
2026-02-02 13:10:17,990:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9931
2026-02-02 13:10:17,991:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=11, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=3,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9306
2026-02-02 13:10:17,991:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 13:10:17,991:INFO:choose_better completed
2026-02-02 13:10:17,991:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:10:17,994:INFO:_master_model_container: 8
2026-02-02 13:10:17,994:INFO:_display_container: 4
2026-02-02 13:10:17,994:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:10:17,994:INFO:tune_model() successfully completed......................................
2026-02-02 13:10:18,136:INFO:Initializing tune_model()
2026-02-02 13:10:18,136:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:10:18,136:INFO:Checking exceptions
2026-02-02 13:10:18,195:INFO:Copying training dataset
2026-02-02 13:10:18,273:INFO:Checking base model
2026-02-02 13:10:18,273:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 13:10:18,274:INFO:Declaring metric variables
2026-02-02 13:10:18,274:INFO:Defining Hyperparameters
2026-02-02 13:10:18,423:INFO:Tuning with n_jobs=-1
2026-02-02 13:10:18,423:INFO:Initializing RandomizedSearchCV
2026-02-02 13:10:42,043:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 13:10:42,045:INFO:Hyperparameter search completed
2026-02-02 13:10:42,045:INFO:SubProcess create_model() called ==================================
2026-02-02 13:10:42,046:INFO:Initializing create_model()
2026-02-02 13:10:42,047:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022908D9D290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 13:10:42,047:INFO:Checking exceptions
2026-02-02 13:10:42,047:INFO:Importing libraries
2026-02-02 13:10:42,047:INFO:Copying training dataset
2026-02-02 13:10:42,167:INFO:Defining folds
2026-02-02 13:10:42,168:INFO:Declaring metric variables
2026-02-02 13:10:42,168:INFO:Importing untrained model
2026-02-02 13:10:42,168:INFO:Declaring custom model
2026-02-02 13:10:42,170:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:10:42,170:INFO:Starting cross validation
2026-02-02 13:10:42,172:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:10:48,131:INFO:Calculating mean and std
2026-02-02 13:10:48,133:INFO:Creating metrics dataframe
2026-02-02 13:10:48,136:INFO:Finalizing model
2026-02-02 13:10:48,449:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:10:48,449:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:10:48,449:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:10:48,554:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:10:48,554:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:10:48,554:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:10:48,555:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 13:10:48,569:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008252 seconds.
2026-02-02 13:10:48,569:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:10:48,569:INFO:[LightGBM] [Info] Total Bins 1586
2026-02-02 13:10:48,570:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 14
2026-02-02 13:10:48,573:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 13:10:48,574:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 13:10:50,266:INFO:Uploading results into container
2026-02-02 13:10:50,268:INFO:Uploading model into container now
2026-02-02 13:10:50,269:INFO:_master_model_container: 9
2026-02-02 13:10:50,269:INFO:_display_container: 5
2026-02-02 13:10:50,270:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:10:50,270:INFO:create_model() successfully completed......................................
2026-02-02 13:10:50,453:INFO:SubProcess create_model() end ==================================
2026-02-02 13:10:50,453:INFO:choose_better activated
2026-02-02 13:10:50,454:INFO:SubProcess create_model() called ==================================
2026-02-02 13:10:50,455:INFO:Initializing create_model()
2026-02-02 13:10:50,455:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:10:50,455:INFO:Checking exceptions
2026-02-02 13:10:50,456:INFO:Importing libraries
2026-02-02 13:10:50,456:INFO:Copying training dataset
2026-02-02 13:10:50,587:INFO:Defining folds
2026-02-02 13:10:50,587:INFO:Declaring metric variables
2026-02-02 13:10:50,587:INFO:Importing untrained model
2026-02-02 13:10:50,587:INFO:Declaring custom model
2026-02-02 13:10:50,588:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:10:50,588:INFO:Starting cross validation
2026-02-02 13:10:50,589:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:10:53,473:INFO:Calculating mean and std
2026-02-02 13:10:53,474:INFO:Creating metrics dataframe
2026-02-02 13:10:53,476:INFO:Finalizing model
2026-02-02 13:10:53,882:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 13:10:53,897:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008572 seconds.
2026-02-02 13:10:53,897:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-02 13:10:53,898:INFO:[LightGBM] [Info] Total Bins 1586
2026-02-02 13:10:53,898:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 14
2026-02-02 13:10:53,900:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 13:10:53,900:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 13:10:54,572:INFO:Uploading results into container
2026-02-02 13:10:54,573:INFO:Uploading model into container now
2026-02-02 13:10:54,573:INFO:_master_model_container: 10
2026-02-02 13:10:54,573:INFO:_display_container: 6
2026-02-02 13:10:54,574:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:10:54,574:INFO:create_model() successfully completed......................................
2026-02-02 13:10:54,747:INFO:SubProcess create_model() end ==================================
2026-02-02 13:10:54,748:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9625
2026-02-02 13:10:54,749:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.989
2026-02-02 13:10:54,749:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 13:10:54,749:INFO:choose_better completed
2026-02-02 13:10:54,751:INFO:_master_model_container: 10
2026-02-02 13:10:54,751:INFO:_display_container: 5
2026-02-02 13:10:54,752:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:10:54,752:INFO:tune_model() successfully completed......................................
2026-02-02 13:10:54,906:INFO:Initializing predict_model()
2026-02-02 13:10:54,906:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228CBAA11C0>)
2026-02-02 13:10:54,906:INFO:Checking exceptions
2026-02-02 13:10:54,906:INFO:Preloading libraries
2026-02-02 13:10:54,906:INFO:Set up data.
2026-02-02 13:10:54,916:INFO:Set up index.
2026-02-02 13:10:55,641:INFO:Initializing predict_model()
2026-02-02 13:10:55,641:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D5D50EA0>)
2026-02-02 13:10:55,641:INFO:Checking exceptions
2026-02-02 13:10:55,642:INFO:Preloading libraries
2026-02-02 13:10:55,642:INFO:Set up data.
2026-02-02 13:10:55,652:INFO:Set up index.
2026-02-02 13:10:56,015:INFO:Initializing predict_model()
2026-02-02 13:10:56,015:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D5D50EA0>)
2026-02-02 13:10:56,015:INFO:Checking exceptions
2026-02-02 13:10:56,015:INFO:Preloading libraries
2026-02-02 13:10:56,015:INFO:Set up data.
2026-02-02 13:10:56,027:INFO:Set up index.
2026-02-02 13:10:56,756:INFO:Initializing predict_model()
2026-02-02 13:10:56,756:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D5D52700>)
2026-02-02 13:10:56,757:INFO:Checking exceptions
2026-02-02 13:10:56,757:INFO:Preloading libraries
2026-02-02 13:10:56,757:INFO:Set up data.
2026-02-02 13:10:56,769:INFO:Set up index.
2026-02-02 13:10:57,501:INFO:Initializing plot_model()
2026-02-02 13:10:57,501:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:10:57,501:INFO:Checking exceptions
2026-02-02 13:10:57,572:INFO:Preloading libraries
2026-02-02 13:10:57,694:INFO:Copying training dataset
2026-02-02 13:10:57,694:INFO:Plot type: feature
2026-02-02 13:10:57,695:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:10:57,986:INFO:Visual Rendered Successfully
2026-02-02 13:10:58,123:INFO:plot_model() successfully completed......................................
2026-02-02 13:10:58,134:INFO:Initializing plot_model()
2026-02-02 13:10:58,134:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000022839089790>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:10:58,134:INFO:Checking exceptions
2026-02-02 13:10:58,207:INFO:Preloading libraries
2026-02-02 13:10:58,327:INFO:Copying training dataset
2026-02-02 13:10:58,327:INFO:Plot type: feature_all
2026-02-02 13:10:58,441:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:10:58,766:INFO:Visual Rendered Successfully
2026-02-02 13:10:58,919:INFO:plot_model() successfully completed......................................
2026-02-02 13:10:58,934:INFO:Initializing save_model()
2026-02-02 13:10:58,934:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 13:10:58,934:INFO:Adding model into prep_pipe
2026-02-02 13:10:59,133:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-02 13:10:59,137:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-02 13:10:59,137:INFO:save_model() successfully completed......................................
2026-02-02 13:46:11,473:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\1358270520.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 13:46:13,844:INFO:PyCaret ClassificationExperiment
2026-02-02 13:46:13,844:INFO:Logging name: clf-default-name
2026-02-02 13:46:13,844:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 13:46:13,844:INFO:version 3.3.2
2026-02-02 13:46:13,845:INFO:Initializing setup()
2026-02-02 13:46:13,845:INFO:self.USI: 90d0
2026-02-02 13:46:13,845:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 13:46:13,845:INFO:Checking environment
2026-02-02 13:46:13,845:INFO:python_version: 3.11.11
2026-02-02 13:46:13,846:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 13:46:13,848:INFO:machine: AMD64
2026-02-02 13:46:13,848:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 13:46:13,849:INFO:Memory: svmem(total=34009374720, available=13224828928, percent=61.1, used=20784545792, free=13224828928)
2026-02-02 13:46:13,849:INFO:Physical Core: 12
2026-02-02 13:46:13,850:INFO:Logical Core: 16
2026-02-02 13:46:13,850:INFO:Checking libraries
2026-02-02 13:46:13,851:INFO:System:
2026-02-02 13:46:13,851:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 13:46:13,851:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 13:46:13,851:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 13:46:13,851:INFO:PyCaret required dependencies:
2026-02-02 13:46:13,851:INFO:                 pip: 25.0
2026-02-02 13:46:13,851:INFO:          setuptools: 75.8.0
2026-02-02 13:46:13,851:INFO:             pycaret: 3.3.2
2026-02-02 13:46:13,852:INFO:             IPython: 9.9.0
2026-02-02 13:46:13,852:INFO:          ipywidgets: 8.1.8
2026-02-02 13:46:13,852:INFO:                tqdm: 4.67.1
2026-02-02 13:46:13,852:INFO:               numpy: 1.26.4
2026-02-02 13:46:13,852:INFO:              pandas: 2.1.4
2026-02-02 13:46:13,853:INFO:              jinja2: 3.1.6
2026-02-02 13:46:13,853:INFO:               scipy: 1.11.4
2026-02-02 13:46:13,853:INFO:              joblib: 1.3.2
2026-02-02 13:46:13,853:INFO:             sklearn: 1.4.2
2026-02-02 13:46:13,854:INFO:                pyod: 2.0.6
2026-02-02 13:46:13,854:INFO:            imblearn: 0.14.1
2026-02-02 13:46:13,854:INFO:   category_encoders: 2.7.0
2026-02-02 13:46:13,854:INFO:            lightgbm: 4.6.0
2026-02-02 13:46:13,854:INFO:               numba: 0.62.1
2026-02-02 13:46:13,854:INFO:            requests: 2.32.3
2026-02-02 13:46:13,854:INFO:          matplotlib: 3.7.5
2026-02-02 13:46:13,855:INFO:          scikitplot: 0.3.7
2026-02-02 13:46:13,855:INFO:         yellowbrick: 1.5
2026-02-02 13:46:13,856:INFO:              plotly: 5.24.1
2026-02-02 13:46:13,857:INFO:    plotly-resampler: Not installed
2026-02-02 13:46:13,858:INFO:             kaleido: 1.2.0
2026-02-02 13:46:13,858:INFO:           schemdraw: 0.15
2026-02-02 13:46:13,858:INFO:         statsmodels: 0.14.6
2026-02-02 13:46:13,859:INFO:              sktime: 0.26.0
2026-02-02 13:46:13,859:INFO:               tbats: 1.1.3
2026-02-02 13:46:13,859:INFO:            pmdarima: 2.0.4
2026-02-02 13:46:13,859:INFO:              psutil: 7.2.1
2026-02-02 13:46:13,860:INFO:          markupsafe: 3.0.3
2026-02-02 13:46:13,860:INFO:             pickle5: Not installed
2026-02-02 13:46:13,862:INFO:         cloudpickle: 3.0.0
2026-02-02 13:46:13,862:INFO:         deprecation: 2.1.0
2026-02-02 13:46:13,862:INFO:              xxhash: 3.6.0
2026-02-02 13:46:13,863:INFO:           wurlitzer: Not installed
2026-02-02 13:46:13,863:INFO:PyCaret optional dependencies:
2026-02-02 13:46:13,863:INFO:                shap: 0.44.1
2026-02-02 13:46:13,864:INFO:           interpret: 0.7.3
2026-02-02 13:46:13,864:INFO:                umap: 0.5.7
2026-02-02 13:46:13,864:INFO:     ydata_profiling: 4.18.1
2026-02-02 13:46:13,864:INFO:  explainerdashboard: 0.5.1
2026-02-02 13:46:13,865:INFO:             autoviz: Not installed
2026-02-02 13:46:13,865:INFO:           fairlearn: 0.7.0
2026-02-02 13:46:13,865:INFO:          deepchecks: Not installed
2026-02-02 13:46:13,865:INFO:             xgboost: Not installed
2026-02-02 13:46:13,865:INFO:            catboost: 1.2.8
2026-02-02 13:46:13,865:INFO:              kmodes: 0.12.2
2026-02-02 13:46:13,866:INFO:             mlxtend: 0.23.4
2026-02-02 13:46:13,866:INFO:       statsforecast: 1.5.0
2026-02-02 13:46:13,867:INFO:        tune_sklearn: Not installed
2026-02-02 13:46:13,868:INFO:                 ray: Not installed
2026-02-02 13:46:13,868:INFO:            hyperopt: 0.2.7
2026-02-02 13:46:13,868:INFO:              optuna: 4.6.0
2026-02-02 13:46:13,868:INFO:               skopt: 0.10.2
2026-02-02 13:46:13,868:INFO:              mlflow: 3.8.1
2026-02-02 13:46:13,869:INFO:              gradio: 6.3.0
2026-02-02 13:46:13,869:INFO:             fastapi: 0.128.0
2026-02-02 13:46:13,869:INFO:             uvicorn: 0.40.0
2026-02-02 13:46:13,869:INFO:              m2cgen: 0.10.0
2026-02-02 13:46:13,870:INFO:           evidently: 0.4.40
2026-02-02 13:46:13,870:INFO:               fugue: 0.8.7
2026-02-02 13:46:13,870:INFO:           streamlit: Not installed
2026-02-02 13:46:13,870:INFO:             prophet: Not installed
2026-02-02 13:46:13,870:INFO:None
2026-02-02 13:46:13,871:INFO:Set up data.
2026-02-02 13:46:13,946:INFO:Set up folding strategy.
2026-02-02 13:46:13,947:INFO:Set up train/test split.
2026-02-02 13:46:14,092:INFO:Set up index.
2026-02-02 13:46:14,101:INFO:Assigning column types.
2026-02-02 13:46:14,171:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 13:46:14,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:46:14,202:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:46:14,223:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:46:14,223:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:46:14,253:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:46:14,254:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:46:14,274:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:46:14,275:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:46:14,275:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 13:46:14,304:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:46:14,325:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:46:14,325:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:46:14,355:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:46:14,379:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:46:14,380:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:46:14,380:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 13:46:14,430:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:46:14,431:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:46:14,485:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:46:14,485:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:46:14,486:INFO:Preparing preprocessing pipeline...
2026-02-02 13:46:14,501:INFO:Set up simple imputation.
2026-02-02 13:46:14,501:INFO:Set up feature normalization.
2026-02-02 13:46:14,753:INFO:Finished creating preprocessing pipeline.
2026-02-02 13:46:14,757:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 13:46:14,757:INFO:Creating final display dataframe.
2026-02-02 13:46:15,519:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 15)
4        Transformed data shape      (373023, 15)
5   Transformed train set shape      (261116, 15)
6    Transformed test set shape      (111907, 15)
7              Numeric features                11
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              90d0
2026-02-02 13:46:15,611:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:46:15,611:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:46:15,704:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:46:15,706:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:46:15,709:INFO:setup() successfully completed in 1.88s...............
2026-02-02 13:46:15,709:INFO:Initializing compare_models()
2026-02-02 13:46:15,709:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 13:46:15,710:INFO:Checking exceptions
2026-02-02 13:46:15,819:INFO:Preparing display monitor
2026-02-02 13:46:15,824:INFO:Initializing Logistic Regression
2026-02-02 13:46:15,825:INFO:Total runtime is 1.6637643178304036e-05 minutes
2026-02-02 13:46:15,827:INFO:SubProcess create_model() called ==================================
2026-02-02 13:46:15,827:INFO:Initializing create_model()
2026-02-02 13:46:15,827:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F1792450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:46:15,827:INFO:Checking exceptions
2026-02-02 13:46:15,827:INFO:Importing libraries
2026-02-02 13:46:15,828:INFO:Copying training dataset
2026-02-02 13:46:16,047:INFO:Defining folds
2026-02-02 13:46:16,048:INFO:Declaring metric variables
2026-02-02 13:46:16,050:INFO:Importing untrained model
2026-02-02 13:46:16,051:INFO:Logistic Regression Imported successfully
2026-02-02 13:46:16,051:INFO:Starting cross validation
2026-02-02 13:46:16,054:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:46:25,019:INFO:Calculating mean and std
2026-02-02 13:46:25,020:INFO:Creating metrics dataframe
2026-02-02 13:46:25,023:INFO:Uploading results into container
2026-02-02 13:46:25,025:INFO:Uploading model into container now
2026-02-02 13:46:25,025:INFO:_master_model_container: 1
2026-02-02 13:46:25,025:INFO:_display_container: 2
2026-02-02 13:46:25,026:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-02 13:46:25,026:INFO:create_model() successfully completed......................................
2026-02-02 13:46:25,680:INFO:SubProcess create_model() end ==================================
2026-02-02 13:46:25,680:INFO:Creating metrics dataframe
2026-02-02 13:46:25,682:INFO:Initializing Decision Tree Classifier
2026-02-02 13:46:25,682:INFO:Total runtime is 0.16429317394892373 minutes
2026-02-02 13:46:25,682:INFO:SubProcess create_model() called ==================================
2026-02-02 13:46:25,682:INFO:Initializing create_model()
2026-02-02 13:46:25,682:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F1792450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:46:25,682:INFO:Checking exceptions
2026-02-02 13:46:25,682:INFO:Importing libraries
2026-02-02 13:46:25,683:INFO:Copying training dataset
2026-02-02 13:46:25,776:INFO:Defining folds
2026-02-02 13:46:25,776:INFO:Declaring metric variables
2026-02-02 13:46:25,776:INFO:Importing untrained model
2026-02-02 13:46:25,777:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:46:25,777:INFO:Starting cross validation
2026-02-02 13:46:25,778:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:46:31,812:INFO:Calculating mean and std
2026-02-02 13:46:31,813:INFO:Creating metrics dataframe
2026-02-02 13:46:31,814:INFO:Uploading results into container
2026-02-02 13:46:31,815:INFO:Uploading model into container now
2026-02-02 13:46:31,815:INFO:_master_model_container: 2
2026-02-02 13:46:31,815:INFO:_display_container: 2
2026-02-02 13:46:31,815:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:46:31,816:INFO:create_model() successfully completed......................................
2026-02-02 13:46:31,959:INFO:SubProcess create_model() end ==================================
2026-02-02 13:46:31,959:INFO:Creating metrics dataframe
2026-02-02 13:46:31,961:INFO:Initializing Random Forest Classifier
2026-02-02 13:46:31,961:INFO:Total runtime is 0.26893630425135295 minutes
2026-02-02 13:46:31,961:INFO:SubProcess create_model() called ==================================
2026-02-02 13:46:31,961:INFO:Initializing create_model()
2026-02-02 13:46:31,962:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F1792450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:46:31,962:INFO:Checking exceptions
2026-02-02 13:46:31,962:INFO:Importing libraries
2026-02-02 13:46:31,962:INFO:Copying training dataset
2026-02-02 13:46:32,050:INFO:Defining folds
2026-02-02 13:46:32,050:INFO:Declaring metric variables
2026-02-02 13:46:32,050:INFO:Importing untrained model
2026-02-02 13:46:32,051:INFO:Random Forest Classifier Imported successfully
2026-02-02 13:46:32,051:INFO:Starting cross validation
2026-02-02 13:46:32,052:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:46:45,858:INFO:Calculating mean and std
2026-02-02 13:46:45,860:INFO:Creating metrics dataframe
2026-02-02 13:46:45,863:INFO:Uploading results into container
2026-02-02 13:46:45,864:INFO:Uploading model into container now
2026-02-02 13:46:45,865:INFO:_master_model_container: 3
2026-02-02 13:46:45,865:INFO:_display_container: 2
2026-02-02 13:46:45,866:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:46:45,866:INFO:create_model() successfully completed......................................
2026-02-02 13:46:46,009:INFO:SubProcess create_model() end ==================================
2026-02-02 13:46:46,009:INFO:Creating metrics dataframe
2026-02-02 13:46:46,011:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 13:46:46,011:INFO:Total runtime is 0.5031083345413208 minutes
2026-02-02 13:46:46,011:INFO:SubProcess create_model() called ==================================
2026-02-02 13:46:46,011:INFO:Initializing create_model()
2026-02-02 13:46:46,011:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F1792450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:46:46,011:INFO:Checking exceptions
2026-02-02 13:46:46,011:INFO:Importing libraries
2026-02-02 13:46:46,011:INFO:Copying training dataset
2026-02-02 13:46:46,107:INFO:Defining folds
2026-02-02 13:46:46,108:INFO:Declaring metric variables
2026-02-02 13:46:46,108:INFO:Importing untrained model
2026-02-02 13:46:46,108:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:46:46,108:INFO:Starting cross validation
2026-02-02 13:46:46,109:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:46:53,407:INFO:Calculating mean and std
2026-02-02 13:46:53,411:INFO:Creating metrics dataframe
2026-02-02 13:46:53,416:INFO:Uploading results into container
2026-02-02 13:46:53,418:INFO:Uploading model into container now
2026-02-02 13:46:53,418:INFO:_master_model_container: 4
2026-02-02 13:46:53,419:INFO:_display_container: 2
2026-02-02 13:46:53,420:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:46:53,420:INFO:create_model() successfully completed......................................
2026-02-02 13:46:53,579:INFO:SubProcess create_model() end ==================================
2026-02-02 13:46:53,579:INFO:Creating metrics dataframe
2026-02-02 13:46:53,584:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 13:46:53,586:INFO:Initializing create_model()
2026-02-02 13:46:53,586:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:46:53,586:INFO:Checking exceptions
2026-02-02 13:46:53,587:INFO:Importing libraries
2026-02-02 13:46:53,587:INFO:Copying training dataset
2026-02-02 13:46:53,671:INFO:Defining folds
2026-02-02 13:46:53,671:INFO:Declaring metric variables
2026-02-02 13:46:53,671:INFO:Importing untrained model
2026-02-02 13:46:53,671:INFO:Declaring custom model
2026-02-02 13:46:53,672:INFO:Random Forest Classifier Imported successfully
2026-02-02 13:46:53,673:INFO:Cross validation set to False
2026-02-02 13:46:53,673:INFO:Fitting Model
2026-02-02 13:46:58,199:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:46:58,199:INFO:create_model() successfully completed......................................
2026-02-02 13:46:58,349:INFO:Initializing create_model()
2026-02-02 13:46:58,350:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:46:58,350:INFO:Checking exceptions
2026-02-02 13:46:58,351:INFO:Importing libraries
2026-02-02 13:46:58,351:INFO:Copying training dataset
2026-02-02 13:46:58,448:INFO:Defining folds
2026-02-02 13:46:58,448:INFO:Declaring metric variables
2026-02-02 13:46:58,449:INFO:Importing untrained model
2026-02-02 13:46:58,449:INFO:Declaring custom model
2026-02-02 13:46:58,449:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:46:58,450:INFO:Cross validation set to False
2026-02-02 13:46:58,450:INFO:Fitting Model
2026-02-02 13:46:59,663:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:46:59,663:INFO:create_model() successfully completed......................................
2026-02-02 13:46:59,816:INFO:Initializing create_model()
2026-02-02 13:46:59,816:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:46:59,817:INFO:Checking exceptions
2026-02-02 13:46:59,818:INFO:Importing libraries
2026-02-02 13:46:59,818:INFO:Copying training dataset
2026-02-02 13:46:59,910:INFO:Defining folds
2026-02-02 13:46:59,910:INFO:Declaring metric variables
2026-02-02 13:46:59,910:INFO:Importing untrained model
2026-02-02 13:46:59,910:INFO:Declaring custom model
2026-02-02 13:46:59,911:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:46:59,911:INFO:Cross validation set to False
2026-02-02 13:46:59,911:INFO:Fitting Model
2026-02-02 13:47:00,288:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 13:47:00,298:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004414 seconds.
2026-02-02 13:47:00,298:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:47:00,299:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:47:00,299:INFO:[LightGBM] [Info] Total Bins 1586
2026-02-02 13:47:00,299:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 14
2026-02-02 13:47:00,301:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 13:47:00,301:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 13:47:01,231:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:47:01,231:INFO:create_model() successfully completed......................................
2026-02-02 13:47:01,454:INFO:_master_model_container: 4
2026-02-02 13:47:01,455:INFO:_display_container: 2
2026-02-02 13:47:01,456:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2026-02-02 13:47:01,456:INFO:compare_models() successfully completed......................................
2026-02-02 13:47:01,467:INFO:Initializing tune_model()
2026-02-02 13:47:01,468:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:47:01,468:INFO:Checking exceptions
2026-02-02 13:47:01,513:INFO:Copying training dataset
2026-02-02 13:47:01,593:INFO:Checking base model
2026-02-02 13:47:01,593:INFO:Base model : Random Forest Classifier
2026-02-02 13:47:01,594:INFO:Declaring metric variables
2026-02-02 13:47:01,594:INFO:Defining Hyperparameters
2026-02-02 13:47:01,784:INFO:Tuning with n_jobs=-1
2026-02-02 13:47:01,784:INFO:Initializing RandomizedSearchCV
2026-02-02 13:48:11,758:INFO:best_params: {'actual_estimator__n_estimators': 120, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-02 13:48:11,760:INFO:Hyperparameter search completed
2026-02-02 13:48:11,760:INFO:SubProcess create_model() called ==================================
2026-02-02 13:48:11,762:INFO:Initializing create_model()
2026-02-02 13:48:11,762:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228391F4710>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 120, 'min_samples_split': 5, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': True})
2026-02-02 13:48:11,762:INFO:Checking exceptions
2026-02-02 13:48:11,762:INFO:Importing libraries
2026-02-02 13:48:11,762:INFO:Copying training dataset
2026-02-02 13:48:11,928:INFO:Defining folds
2026-02-02 13:48:11,928:INFO:Declaring metric variables
2026-02-02 13:48:11,928:INFO:Importing untrained model
2026-02-02 13:48:11,928:INFO:Declaring custom model
2026-02-02 13:48:11,929:INFO:Random Forest Classifier Imported successfully
2026-02-02 13:48:11,930:INFO:Starting cross validation
2026-02-02 13:48:11,930:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:48:20,075:INFO:Calculating mean and std
2026-02-02 13:48:20,077:INFO:Creating metrics dataframe
2026-02-02 13:48:20,080:INFO:Finalizing model
2026-02-02 13:48:23,909:INFO:Uploading results into container
2026-02-02 13:48:23,911:INFO:Uploading model into container now
2026-02-02 13:48:23,912:INFO:_master_model_container: 5
2026-02-02 13:48:23,912:INFO:_display_container: 3
2026-02-02 13:48:23,913:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:48:23,913:INFO:create_model() successfully completed......................................
2026-02-02 13:48:24,091:INFO:SubProcess create_model() end ==================================
2026-02-02 13:48:24,092:INFO:choose_better activated
2026-02-02 13:48:24,092:INFO:SubProcess create_model() called ==================================
2026-02-02 13:48:24,092:INFO:Initializing create_model()
2026-02-02 13:48:24,092:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:48:24,092:INFO:Checking exceptions
2026-02-02 13:48:24,093:INFO:Importing libraries
2026-02-02 13:48:24,093:INFO:Copying training dataset
2026-02-02 13:48:24,199:INFO:Defining folds
2026-02-02 13:48:24,199:INFO:Declaring metric variables
2026-02-02 13:48:24,199:INFO:Importing untrained model
2026-02-02 13:48:24,199:INFO:Declaring custom model
2026-02-02 13:48:24,199:INFO:Random Forest Classifier Imported successfully
2026-02-02 13:48:24,200:INFO:Starting cross validation
2026-02-02 13:48:24,200:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:48:36,225:INFO:Calculating mean and std
2026-02-02 13:48:36,226:INFO:Creating metrics dataframe
2026-02-02 13:48:36,227:INFO:Finalizing model
2026-02-02 13:48:41,547:INFO:Uploading results into container
2026-02-02 13:48:41,548:INFO:Uploading model into container now
2026-02-02 13:48:41,549:INFO:_master_model_container: 6
2026-02-02 13:48:41,549:INFO:_display_container: 4
2026-02-02 13:48:41,550:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:48:41,550:INFO:create_model() successfully completed......................................
2026-02-02 13:48:41,707:INFO:SubProcess create_model() end ==================================
2026-02-02 13:48:41,708:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9946
2026-02-02 13:48:41,708:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=5, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=120, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9444
2026-02-02 13:48:41,708:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-02 13:48:41,709:INFO:choose_better completed
2026-02-02 13:48:41,709:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:48:41,711:INFO:_master_model_container: 6
2026-02-02 13:48:41,712:INFO:_display_container: 3
2026-02-02 13:48:41,712:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-02 13:48:41,712:INFO:tune_model() successfully completed......................................
2026-02-02 13:48:41,858:INFO:Initializing tune_model()
2026-02-02 13:48:41,858:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:48:41,858:INFO:Checking exceptions
2026-02-02 13:48:41,893:INFO:Copying training dataset
2026-02-02 13:48:41,952:INFO:Checking base model
2026-02-02 13:48:41,953:INFO:Base model : Decision Tree Classifier
2026-02-02 13:48:41,953:INFO:Declaring metric variables
2026-02-02 13:48:41,953:INFO:Defining Hyperparameters
2026-02-02 13:48:42,087:INFO:Tuning with n_jobs=-1
2026-02-02 13:48:42,087:INFO:Initializing RandomizedSearchCV
2026-02-02 13:48:46,035:INFO:best_params: {'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 11, 'actual_estimator__criterion': 'gini'}
2026-02-02 13:48:46,037:INFO:Hyperparameter search completed
2026-02-02 13:48:46,038:INFO:SubProcess create_model() called ==================================
2026-02-02 13:48:46,039:INFO:Initializing create_model()
2026-02-02 13:48:46,039:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228F877D8D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 10, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 11, 'criterion': 'gini'})
2026-02-02 13:48:46,039:INFO:Checking exceptions
2026-02-02 13:48:46,039:INFO:Importing libraries
2026-02-02 13:48:46,039:INFO:Copying training dataset
2026-02-02 13:48:46,153:INFO:Defining folds
2026-02-02 13:48:46,153:INFO:Declaring metric variables
2026-02-02 13:48:46,153:INFO:Importing untrained model
2026-02-02 13:48:46,154:INFO:Declaring custom model
2026-02-02 13:48:46,155:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:48:46,156:INFO:Starting cross validation
2026-02-02 13:48:46,157:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:48:47,509:INFO:Calculating mean and std
2026-02-02 13:48:47,510:INFO:Creating metrics dataframe
2026-02-02 13:48:47,512:INFO:Finalizing model
2026-02-02 13:48:48,112:INFO:Uploading results into container
2026-02-02 13:48:48,113:INFO:Uploading model into container now
2026-02-02 13:48:48,113:INFO:_master_model_container: 7
2026-02-02 13:48:48,114:INFO:_display_container: 4
2026-02-02 13:48:48,114:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=11, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=3,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:48:48,114:INFO:create_model() successfully completed......................................
2026-02-02 13:48:48,251:INFO:SubProcess create_model() end ==================================
2026-02-02 13:48:48,251:INFO:choose_better activated
2026-02-02 13:48:48,252:INFO:SubProcess create_model() called ==================================
2026-02-02 13:48:48,252:INFO:Initializing create_model()
2026-02-02 13:48:48,252:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:48:48,252:INFO:Checking exceptions
2026-02-02 13:48:48,254:INFO:Importing libraries
2026-02-02 13:48:48,254:INFO:Copying training dataset
2026-02-02 13:48:48,339:INFO:Defining folds
2026-02-02 13:48:48,340:INFO:Declaring metric variables
2026-02-02 13:48:48,340:INFO:Importing untrained model
2026-02-02 13:48:48,340:INFO:Declaring custom model
2026-02-02 13:48:48,340:INFO:Decision Tree Classifier Imported successfully
2026-02-02 13:48:48,340:INFO:Starting cross validation
2026-02-02 13:48:48,341:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:48:50,049:INFO:Calculating mean and std
2026-02-02 13:48:50,050:INFO:Creating metrics dataframe
2026-02-02 13:48:50,052:INFO:Finalizing model
2026-02-02 13:48:51,050:INFO:Uploading results into container
2026-02-02 13:48:51,050:INFO:Uploading model into container now
2026-02-02 13:48:51,051:INFO:_master_model_container: 8
2026-02-02 13:48:51,051:INFO:_display_container: 5
2026-02-02 13:48:51,051:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:48:51,051:INFO:create_model() successfully completed......................................
2026-02-02 13:48:51,186:INFO:SubProcess create_model() end ==================================
2026-02-02 13:48:51,186:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9931
2026-02-02 13:48:51,186:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=11, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=3,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9306
2026-02-02 13:48:51,186:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-02 13:48:51,187:INFO:choose_better completed
2026-02-02 13:48:51,187:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:48:51,189:INFO:_master_model_container: 8
2026-02-02 13:48:51,190:INFO:_display_container: 4
2026-02-02 13:48:51,190:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-02 13:48:51,190:INFO:tune_model() successfully completed......................................
2026-02-02 13:48:51,328:INFO:Initializing tune_model()
2026-02-02 13:48:51,328:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:48:51,328:INFO:Checking exceptions
2026-02-02 13:48:51,365:INFO:Copying training dataset
2026-02-02 13:48:51,421:INFO:Checking base model
2026-02-02 13:48:51,422:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 13:48:51,422:INFO:Declaring metric variables
2026-02-02 13:48:51,422:INFO:Defining Hyperparameters
2026-02-02 13:48:51,586:INFO:Tuning with n_jobs=-1
2026-02-02 13:48:51,586:INFO:Initializing RandomizedSearchCV
2026-02-02 13:49:19,357:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 13:49:19,358:INFO:Hyperparameter search completed
2026-02-02 13:49:19,359:INFO:SubProcess create_model() called ==================================
2026-02-02 13:49:19,362:INFO:Initializing create_model()
2026-02-02 13:49:19,362:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000022867051D10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 13:49:19,362:INFO:Checking exceptions
2026-02-02 13:49:19,362:INFO:Importing libraries
2026-02-02 13:49:19,362:INFO:Copying training dataset
2026-02-02 13:49:19,522:INFO:Defining folds
2026-02-02 13:49:19,523:INFO:Declaring metric variables
2026-02-02 13:49:19,523:INFO:Importing untrained model
2026-02-02 13:49:19,523:INFO:Declaring custom model
2026-02-02 13:49:19,524:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:49:19,525:INFO:Starting cross validation
2026-02-02 13:49:19,526:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:49:26,415:INFO:Calculating mean and std
2026-02-02 13:49:26,416:INFO:Creating metrics dataframe
2026-02-02 13:49:26,417:INFO:Finalizing model
2026-02-02 13:49:26,722:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:49:26,723:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:49:26,723:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:49:26,818:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:49:26,818:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:49:26,818:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:49:26,819:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 13:49:26,828:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003869 seconds.
2026-02-02 13:49:26,828:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:49:26,828:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:49:26,828:INFO:[LightGBM] [Info] Total Bins 1586
2026-02-02 13:49:26,829:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 14
2026-02-02 13:49:26,832:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 13:49:26,832:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 13:49:28,713:INFO:Uploading results into container
2026-02-02 13:49:28,715:INFO:Uploading model into container now
2026-02-02 13:49:28,716:INFO:_master_model_container: 9
2026-02-02 13:49:28,716:INFO:_display_container: 5
2026-02-02 13:49:28,718:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:49:28,718:INFO:create_model() successfully completed......................................
2026-02-02 13:49:28,950:INFO:SubProcess create_model() end ==================================
2026-02-02 13:49:28,950:INFO:choose_better activated
2026-02-02 13:49:28,950:INFO:SubProcess create_model() called ==================================
2026-02-02 13:49:28,951:INFO:Initializing create_model()
2026-02-02 13:49:28,952:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:49:28,952:INFO:Checking exceptions
2026-02-02 13:49:28,953:INFO:Importing libraries
2026-02-02 13:49:28,954:INFO:Copying training dataset
2026-02-02 13:49:29,073:INFO:Defining folds
2026-02-02 13:49:29,073:INFO:Declaring metric variables
2026-02-02 13:49:29,073:INFO:Importing untrained model
2026-02-02 13:49:29,073:INFO:Declaring custom model
2026-02-02 13:49:29,074:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:49:29,074:INFO:Starting cross validation
2026-02-02 13:49:29,075:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:49:32,918:INFO:Calculating mean and std
2026-02-02 13:49:32,920:INFO:Creating metrics dataframe
2026-02-02 13:49:32,923:INFO:Finalizing model
2026-02-02 13:49:33,411:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-02 13:49:33,425:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005064 seconds.
2026-02-02 13:49:33,425:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:49:33,425:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:49:33,426:INFO:[LightGBM] [Info] Total Bins 1586
2026-02-02 13:49:33,426:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 14
2026-02-02 13:49:33,428:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-02 13:49:33,428:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-02 13:49:34,343:INFO:Uploading results into container
2026-02-02 13:49:34,345:INFO:Uploading model into container now
2026-02-02 13:49:34,345:INFO:_master_model_container: 10
2026-02-02 13:49:34,345:INFO:_display_container: 6
2026-02-02 13:49:34,346:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:49:34,347:INFO:create_model() successfully completed......................................
2026-02-02 13:49:34,581:INFO:SubProcess create_model() end ==================================
2026-02-02 13:49:34,582:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9625
2026-02-02 13:49:34,583:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.989
2026-02-02 13:49:34,584:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 13:49:34,584:INFO:choose_better completed
2026-02-02 13:49:34,588:INFO:_master_model_container: 10
2026-02-02 13:49:34,589:INFO:_display_container: 5
2026-02-02 13:49:34,590:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:49:34,590:INFO:tune_model() successfully completed......................................
2026-02-02 13:49:34,785:INFO:Initializing predict_model()
2026-02-02 13:49:34,785:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228393AB7E0>)
2026-02-02 13:49:34,785:INFO:Checking exceptions
2026-02-02 13:49:34,785:INFO:Preloading libraries
2026-02-02 13:49:34,785:INFO:Set up data.
2026-02-02 13:49:34,800:INFO:Set up index.
2026-02-02 13:49:35,798:INFO:Initializing predict_model()
2026-02-02 13:49:35,798:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228D03BF380>)
2026-02-02 13:49:35,798:INFO:Checking exceptions
2026-02-02 13:49:35,799:INFO:Preloading libraries
2026-02-02 13:49:35,799:INFO:Set up data.
2026-02-02 13:49:35,811:INFO:Set up index.
2026-02-02 13:49:36,608:INFO:Initializing plot_model()
2026-02-02 13:49:36,609:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:49:36,609:INFO:Checking exceptions
2026-02-02 13:49:36,701:INFO:Preloading libraries
2026-02-02 13:49:36,850:INFO:Copying training dataset
2026-02-02 13:49:36,850:INFO:Plot type: feature
2026-02-02 13:49:36,852:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:49:37,247:INFO:Visual Rendered Successfully
2026-02-02 13:49:37,539:INFO:plot_model() successfully completed......................................
2026-02-02 13:49:37,553:INFO:Initializing plot_model()
2026-02-02 13:49:37,554:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228F877D910>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:49:37,554:INFO:Checking exceptions
2026-02-02 13:49:37,740:INFO:Preloading libraries
2026-02-02 13:49:37,926:INFO:Copying training dataset
2026-02-02 13:49:37,926:INFO:Plot type: feature_all
2026-02-02 13:49:38,068:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:49:38,397:INFO:Visual Rendered Successfully
2026-02-02 13:49:38,571:INFO:plot_model() successfully completed......................................
2026-02-02 13:49:38,584:INFO:Initializing save_model()
2026-02-02 13:49:38,584:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 13:49:38,584:INFO:Adding model into prep_pipe
2026-02-02 13:49:38,763:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-02 13:49:38,768:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_solicitudes_acum',...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-02 13:49:38,768:INFO:save_model() successfully completed......................................
2026-02-02 13:50:34,275:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_16556\1950018585.py:15: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-02 13:50:35,282:INFO:PyCaret ClassificationExperiment
2026-02-02 13:50:35,282:INFO:Logging name: clf-default-name
2026-02-02 13:50:35,282:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-02 13:50:35,282:INFO:version 3.3.2
2026-02-02 13:50:35,282:INFO:Initializing setup()
2026-02-02 13:50:35,282:INFO:self.USI: a4c8
2026-02-02 13:50:35,282:INFO:self._variable_keys: {'USI', 'data', 'memory', 'pipeline', 'exp_name_log', 'gpu_param', 'fold_generator', 'exp_id', 'n_jobs_param', 'gpu_n_jobs_param', 'html_param', 'X_test', 'y_test', 'fix_imbalance', 'fold_groups_param', 'target_param', '_available_plots', 'idx', '_ml_usecase', 'fold_shuffle_param', 'X', 'is_multiclass', 'logging_param', 'y_train', 'X_train', 'log_plots_param', 'y', 'seed'}
2026-02-02 13:50:35,282:INFO:Checking environment
2026-02-02 13:50:35,282:INFO:python_version: 3.11.11
2026-02-02 13:50:35,282:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-02 13:50:35,282:INFO:machine: AMD64
2026-02-02 13:50:35,282:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-02 13:50:35,283:INFO:Memory: svmem(total=34009374720, available=11035152384, percent=67.6, used=22974222336, free=11035152384)
2026-02-02 13:50:35,283:INFO:Physical Core: 12
2026-02-02 13:50:35,283:INFO:Logical Core: 16
2026-02-02 13:50:35,283:INFO:Checking libraries
2026-02-02 13:50:35,283:INFO:System:
2026-02-02 13:50:35,283:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-02 13:50:35,283:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-02 13:50:35,283:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-02 13:50:35,283:INFO:PyCaret required dependencies:
2026-02-02 13:50:35,283:INFO:                 pip: 25.0
2026-02-02 13:50:35,283:INFO:          setuptools: 75.8.0
2026-02-02 13:50:35,283:INFO:             pycaret: 3.3.2
2026-02-02 13:50:35,283:INFO:             IPython: 9.9.0
2026-02-02 13:50:35,283:INFO:          ipywidgets: 8.1.8
2026-02-02 13:50:35,283:INFO:                tqdm: 4.67.1
2026-02-02 13:50:35,283:INFO:               numpy: 1.26.4
2026-02-02 13:50:35,284:INFO:              pandas: 2.1.4
2026-02-02 13:50:35,284:INFO:              jinja2: 3.1.6
2026-02-02 13:50:35,285:INFO:               scipy: 1.11.4
2026-02-02 13:50:35,285:INFO:              joblib: 1.3.2
2026-02-02 13:50:35,285:INFO:             sklearn: 1.4.2
2026-02-02 13:50:35,285:INFO:                pyod: 2.0.6
2026-02-02 13:50:35,285:INFO:            imblearn: 0.14.1
2026-02-02 13:50:35,285:INFO:   category_encoders: 2.7.0
2026-02-02 13:50:35,285:INFO:            lightgbm: 4.6.0
2026-02-02 13:50:35,286:INFO:               numba: 0.62.1
2026-02-02 13:50:35,286:INFO:            requests: 2.32.3
2026-02-02 13:50:35,286:INFO:          matplotlib: 3.7.5
2026-02-02 13:50:35,286:INFO:          scikitplot: 0.3.7
2026-02-02 13:50:35,286:INFO:         yellowbrick: 1.5
2026-02-02 13:50:35,286:INFO:              plotly: 5.24.1
2026-02-02 13:50:35,286:INFO:    plotly-resampler: Not installed
2026-02-02 13:50:35,287:INFO:             kaleido: 1.2.0
2026-02-02 13:50:35,287:INFO:           schemdraw: 0.15
2026-02-02 13:50:35,287:INFO:         statsmodels: 0.14.6
2026-02-02 13:50:35,287:INFO:              sktime: 0.26.0
2026-02-02 13:50:35,287:INFO:               tbats: 1.1.3
2026-02-02 13:50:35,287:INFO:            pmdarima: 2.0.4
2026-02-02 13:50:35,287:INFO:              psutil: 7.2.1
2026-02-02 13:50:35,287:INFO:          markupsafe: 3.0.3
2026-02-02 13:50:35,287:INFO:             pickle5: Not installed
2026-02-02 13:50:35,287:INFO:         cloudpickle: 3.0.0
2026-02-02 13:50:35,287:INFO:         deprecation: 2.1.0
2026-02-02 13:50:35,287:INFO:              xxhash: 3.6.0
2026-02-02 13:50:35,288:INFO:           wurlitzer: Not installed
2026-02-02 13:50:35,288:INFO:PyCaret optional dependencies:
2026-02-02 13:50:35,288:INFO:                shap: 0.44.1
2026-02-02 13:50:35,288:INFO:           interpret: 0.7.3
2026-02-02 13:50:35,289:INFO:                umap: 0.5.7
2026-02-02 13:50:35,289:INFO:     ydata_profiling: 4.18.1
2026-02-02 13:50:35,289:INFO:  explainerdashboard: 0.5.1
2026-02-02 13:50:35,289:INFO:             autoviz: Not installed
2026-02-02 13:50:35,289:INFO:           fairlearn: 0.7.0
2026-02-02 13:50:35,289:INFO:          deepchecks: Not installed
2026-02-02 13:50:35,290:INFO:             xgboost: Not installed
2026-02-02 13:50:35,290:INFO:            catboost: 1.2.8
2026-02-02 13:50:35,290:INFO:              kmodes: 0.12.2
2026-02-02 13:50:35,291:INFO:             mlxtend: 0.23.4
2026-02-02 13:50:35,291:INFO:       statsforecast: 1.5.0
2026-02-02 13:50:35,291:INFO:        tune_sklearn: Not installed
2026-02-02 13:50:35,291:INFO:                 ray: Not installed
2026-02-02 13:50:35,291:INFO:            hyperopt: 0.2.7
2026-02-02 13:50:35,291:INFO:              optuna: 4.6.0
2026-02-02 13:50:35,291:INFO:               skopt: 0.10.2
2026-02-02 13:50:35,291:INFO:              mlflow: 3.8.1
2026-02-02 13:50:35,292:INFO:              gradio: 6.3.0
2026-02-02 13:50:35,292:INFO:             fastapi: 0.128.0
2026-02-02 13:50:35,292:INFO:             uvicorn: 0.40.0
2026-02-02 13:50:35,292:INFO:              m2cgen: 0.10.0
2026-02-02 13:50:35,292:INFO:           evidently: 0.4.40
2026-02-02 13:50:35,292:INFO:               fugue: 0.8.7
2026-02-02 13:50:35,292:INFO:           streamlit: Not installed
2026-02-02 13:50:35,293:INFO:             prophet: Not installed
2026-02-02 13:50:35,293:INFO:None
2026-02-02 13:50:35,293:INFO:Set up data.
2026-02-02 13:50:35,298:INFO:Set up folding strategy.
2026-02-02 13:50:35,298:INFO:Set up train/test split.
2026-02-02 13:50:35,306:INFO:Set up index.
2026-02-02 13:50:35,306:INFO:Assigning column types.
2026-02-02 13:50:35,313:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-02 13:50:35,349:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:50:35,350:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:50:35,372:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:50:35,373:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:50:35,411:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-02 13:50:35,412:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:50:35,473:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:50:35,474:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:50:35,475:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-02 13:50:35,511:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:50:35,534:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:50:35,534:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:50:35,572:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-02 13:50:35,596:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:50:35,596:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:50:35,597:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-02 13:50:35,659:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:50:35,659:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:50:35,721:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:50:35,721:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:50:35,723:INFO:Preparing preprocessing pipeline...
2026-02-02 13:50:35,726:INFO:Set up simple imputation.
2026-02-02 13:50:35,726:INFO:Set up feature normalization.
2026-02-02 13:50:35,759:INFO:Finished creating preprocessing pipeline.
2026-02-02 13:50:35,762:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fil...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-02 13:50:35,762:INFO:Creating final display dataframe.
2026-02-02 13:50:35,881:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 17)
4        Transformed data shape       (10381, 17)
5   Transformed train set shape        (7266, 17)
6    Transformed test set shape        (3115, 17)
7              Numeric features                 6
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              a4c8
2026-02-02 13:50:35,943:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:50:35,944:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:50:36,011:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-02 13:50:36,011:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-02 13:50:36,014:INFO:setup() successfully completed in 0.75s...............
2026-02-02 13:50:36,014:INFO:Initializing compare_models()
2026-02-02 13:50:36,014:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-02 13:50:36,014:INFO:Checking exceptions
2026-02-02 13:50:36,019:INFO:Preparing display monitor
2026-02-02 13:50:36,022:INFO:Initializing Light Gradient Boosting Machine
2026-02-02 13:50:36,022:INFO:Total runtime is 0.0 minutes
2026-02-02 13:50:36,023:INFO:SubProcess create_model() called ==================================
2026-02-02 13:50:36,023:INFO:Initializing create_model()
2026-02-02 13:50:36,023:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228C8C12450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:50:36,023:INFO:Checking exceptions
2026-02-02 13:50:36,023:INFO:Importing libraries
2026-02-02 13:50:36,023:INFO:Copying training dataset
2026-02-02 13:50:36,029:INFO:Defining folds
2026-02-02 13:50:36,030:INFO:Declaring metric variables
2026-02-02 13:50:36,030:INFO:Importing untrained model
2026-02-02 13:50:36,030:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:50:36,030:INFO:Starting cross validation
2026-02-02 13:50:36,031:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:50:37,207:INFO:Calculating mean and std
2026-02-02 13:50:37,209:INFO:Creating metrics dataframe
2026-02-02 13:50:37,213:INFO:Uploading results into container
2026-02-02 13:50:37,214:INFO:Uploading model into container now
2026-02-02 13:50:37,215:INFO:_master_model_container: 1
2026-02-02 13:50:37,216:INFO:_display_container: 2
2026-02-02 13:50:37,218:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:50:37,218:INFO:create_model() successfully completed......................................
2026-02-02 13:50:38,194:INFO:SubProcess create_model() end ==================================
2026-02-02 13:50:38,194:INFO:Creating metrics dataframe
2026-02-02 13:50:38,198:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-02 13:50:38,200:INFO:Initializing create_model()
2026-02-02 13:50:38,201:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:50:38,201:INFO:Checking exceptions
2026-02-02 13:50:38,202:INFO:Importing libraries
2026-02-02 13:50:38,203:INFO:Copying training dataset
2026-02-02 13:50:38,212:INFO:Defining folds
2026-02-02 13:50:38,212:INFO:Declaring metric variables
2026-02-02 13:50:38,212:INFO:Importing untrained model
2026-02-02 13:50:38,212:INFO:Declaring custom model
2026-02-02 13:50:38,213:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:50:38,214:INFO:Cross validation set to False
2026-02-02 13:50:38,214:INFO:Fitting Model
2026-02-02 13:50:38,244:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:50:38,246:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000936 seconds.
2026-02-02 13:50:38,246:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:50:38,246:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:50:38,246:INFO:[LightGBM] [Info] Total Bins 591
2026-02-02 13:50:38,247:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 15
2026-02-02 13:50:38,247:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:50:38,247:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:50:38,546:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:50:38,547:INFO:create_model() successfully completed......................................
2026-02-02 13:50:38,759:INFO:_master_model_container: 1
2026-02-02 13:50:38,760:INFO:_display_container: 2
2026-02-02 13:50:38,762:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:50:38,762:INFO:compare_models() successfully completed......................................
2026-02-02 13:50:38,763:INFO:Initializing tune_model()
2026-02-02 13:50:38,763:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-02 13:50:38,763:INFO:Checking exceptions
2026-02-02 13:50:38,769:INFO:Copying training dataset
2026-02-02 13:50:38,775:INFO:Checking base model
2026-02-02 13:50:38,775:INFO:Base model : Light Gradient Boosting Machine
2026-02-02 13:50:38,776:INFO:Declaring metric variables
2026-02-02 13:50:38,776:INFO:Defining Hyperparameters
2026-02-02 13:50:38,954:INFO:Tuning with n_jobs=-1
2026-02-02 13:50:38,954:INFO:Initializing RandomizedSearchCV
2026-02-02 13:50:45,449:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-02 13:50:45,450:INFO:Hyperparameter search completed
2026-02-02 13:50:45,451:INFO:SubProcess create_model() called ==================================
2026-02-02 13:50:45,452:INFO:Initializing create_model()
2026-02-02 13:50:45,453:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000228C8BFFE50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-02 13:50:45,453:INFO:Checking exceptions
2026-02-02 13:50:45,453:INFO:Importing libraries
2026-02-02 13:50:45,454:INFO:Copying training dataset
2026-02-02 13:50:45,470:INFO:Defining folds
2026-02-02 13:50:45,470:INFO:Declaring metric variables
2026-02-02 13:50:45,470:INFO:Importing untrained model
2026-02-02 13:50:45,471:INFO:Declaring custom model
2026-02-02 13:50:45,473:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:50:45,473:INFO:Starting cross validation
2026-02-02 13:50:45,474:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:50:46,851:INFO:Calculating mean and std
2026-02-02 13:50:46,854:INFO:Creating metrics dataframe
2026-02-02 13:50:46,858:INFO:Finalizing model
2026-02-02 13:50:46,895:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:50:46,896:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:50:46,896:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:50:46,925:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-02 13:50:46,925:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-02 13:50:46,925:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-02 13:50:46,927:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:50:46,931:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001472 seconds.
2026-02-02 13:50:46,931:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:50:46,931:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:50:46,931:INFO:[LightGBM] [Info] Total Bins 591
2026-02-02 13:50:46,931:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 15
2026-02-02 13:50:46,933:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:50:46,933:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:50:46,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:46,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,227:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,229:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,238:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,247:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,266:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,375:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,382:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,400:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,404:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,415:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,431:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,447:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,449:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:50:47,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,481:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,505:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,515:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,517:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,531:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,533:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,535:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,582:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,599:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,649:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,664:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,688:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,733:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-02 13:50:47,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-02 13:50:47,778:INFO:Uploading results into container
2026-02-02 13:50:47,780:INFO:Uploading model into container now
2026-02-02 13:50:47,781:INFO:_master_model_container: 2
2026-02-02 13:50:47,781:INFO:_display_container: 3
2026-02-02 13:50:47,784:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:50:47,784:INFO:create_model() successfully completed......................................
2026-02-02 13:50:48,018:INFO:SubProcess create_model() end ==================================
2026-02-02 13:50:48,018:INFO:choose_better activated
2026-02-02 13:50:48,019:INFO:SubProcess create_model() called ==================================
2026-02-02 13:50:48,020:INFO:Initializing create_model()
2026-02-02 13:50:48,020:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-02 13:50:48,020:INFO:Checking exceptions
2026-02-02 13:50:48,021:INFO:Importing libraries
2026-02-02 13:50:48,021:INFO:Copying training dataset
2026-02-02 13:50:48,028:INFO:Defining folds
2026-02-02 13:50:48,029:INFO:Declaring metric variables
2026-02-02 13:50:48,029:INFO:Importing untrained model
2026-02-02 13:50:48,029:INFO:Declaring custom model
2026-02-02 13:50:48,030:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-02 13:50:48,030:INFO:Starting cross validation
2026-02-02 13:50:48,031:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-02 13:50:48,980:INFO:Calculating mean and std
2026-02-02 13:50:48,981:INFO:Creating metrics dataframe
2026-02-02 13:50:48,985:INFO:Finalizing model
2026-02-02 13:50:49,027:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-02 13:50:49,031:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001936 seconds.
2026-02-02 13:50:49,031:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-02 13:50:49,031:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-02 13:50:49,031:INFO:[LightGBM] [Info] Total Bins 591
2026-02-02 13:50:49,031:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 15
2026-02-02 13:50:49,032:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-02 13:50:49,032:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-02 13:50:49,475:INFO:Uploading results into container
2026-02-02 13:50:49,476:INFO:Uploading model into container now
2026-02-02 13:50:49,476:INFO:_master_model_container: 3
2026-02-02 13:50:49,477:INFO:_display_container: 4
2026-02-02 13:50:49,478:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:50:49,478:INFO:create_model() successfully completed......................................
2026-02-02 13:50:49,693:INFO:SubProcess create_model() end ==================================
2026-02-02 13:50:49,694:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9971
2026-02-02 13:50:49,694:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9955
2026-02-02 13:50:49,695:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-02 13:50:49,695:INFO:choose_better completed
2026-02-02 13:50:49,695:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-02 13:50:49,698:INFO:_master_model_container: 3
2026-02-02 13:50:49,699:INFO:_display_container: 3
2026-02-02 13:50:49,699:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-02 13:50:49,699:INFO:tune_model() successfully completed......................................
2026-02-02 13:50:49,880:INFO:Initializing predict_model()
2026-02-02 13:50:49,881:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x00000228F94AEB60>)
2026-02-02 13:50:49,881:INFO:Checking exceptions
2026-02-02 13:50:49,881:INFO:Preloading libraries
2026-02-02 13:50:49,881:INFO:Set up data.
2026-02-02 13:50:49,884:INFO:Set up index.
2026-02-02 13:50:52,346:INFO:Initializing plot_model()
2026-02-02 13:50:52,346:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:50:52,346:INFO:Checking exceptions
2026-02-02 13:50:52,349:INFO:Preloading libraries
2026-02-02 13:50:52,357:INFO:Copying training dataset
2026-02-02 13:50:52,357:INFO:Plot type: feature
2026-02-02 13:50:52,357:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:50:52,546:INFO:Visual Rendered Successfully
2026-02-02 13:50:52,753:INFO:plot_model() successfully completed......................................
2026-02-02 13:50:52,754:INFO:Initializing plot_model()
2026-02-02 13:50:52,754:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000228551F7A10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-02 13:50:52,754:INFO:Checking exceptions
2026-02-02 13:50:52,757:INFO:Preloading libraries
2026-02-02 13:50:52,762:INFO:Copying training dataset
2026-02-02 13:50:52,762:INFO:Plot type: feature_all
2026-02-02 13:50:52,810:WARNING:No coef_ found. Trying feature_importances_
2026-02-02 13:50:53,029:INFO:Visual Rendered Successfully
2026-02-02 13:50:53,189:INFO:plot_model() successfully completed......................................
2026-02-02 13:50:53,193:INFO:Initializing save_model()
2026-02-02 13:50:53,193:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fil...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-02 13:50:53,194:INFO:Adding model into prep_pipe
2026-02-02 13:50:53,207:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-02 13:50:53,215:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              mis...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-02 13:50:53,215:INFO:save_model() successfully completed......................................
2026-02-03 09:59:15,962:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-03 09:59:15,962:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-03 09:59:15,962:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-03 09:59:15,962:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2026-02-03 09:59:25,091:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_30444\4130107628.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.
  df = pd.read_csv(ruta_dataset, sep=";")

2026-02-03 09:59:27,352:INFO:PyCaret ClassificationExperiment
2026-02-03 09:59:27,352:INFO:Logging name: clf-default-name
2026-02-03 09:59:27,353:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-03 09:59:27,353:INFO:version 3.3.2
2026-02-03 09:59:27,353:INFO:Initializing setup()
2026-02-03 09:59:27,354:INFO:self.USI: ba38
2026-02-03 09:59:27,354:INFO:self._variable_keys: {'X', 'memory', 'USI', 'data', '_ml_usecase', 'X_test', 'idx', 'gpu_n_jobs_param', 'is_multiclass', 'logging_param', '_available_plots', 'seed', 'fold_generator', 'fold_shuffle_param', 'y_test', 'exp_id', 'pipeline', 'log_plots_param', 'fold_groups_param', 'fix_imbalance', 'html_param', 'y', 'X_train', 'gpu_param', 'target_param', 'n_jobs_param', 'exp_name_log', 'y_train'}
2026-02-03 09:59:27,354:INFO:Checking environment
2026-02-03 09:59:27,355:INFO:python_version: 3.11.11
2026-02-03 09:59:27,355:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-03 09:59:27,355:INFO:machine: AMD64
2026-02-03 09:59:27,355:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-03 09:59:27,355:INFO:Memory: svmem(total=34009374720, available=17473990656, percent=48.6, used=16535384064, free=17473990656)
2026-02-03 09:59:27,356:INFO:Physical Core: 12
2026-02-03 09:59:27,356:INFO:Logical Core: 16
2026-02-03 09:59:27,356:INFO:Checking libraries
2026-02-03 09:59:27,356:INFO:System:
2026-02-03 09:59:27,356:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-03 09:59:27,356:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-03 09:59:27,356:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-03 09:59:27,356:INFO:PyCaret required dependencies:
2026-02-03 09:59:28,395:INFO:                 pip: 25.0
2026-02-03 09:59:28,395:INFO:          setuptools: 75.8.0
2026-02-03 09:59:28,395:INFO:             pycaret: 3.3.2
2026-02-03 09:59:28,395:INFO:             IPython: 9.9.0
2026-02-03 09:59:28,395:INFO:          ipywidgets: 8.1.8
2026-02-03 09:59:28,395:INFO:                tqdm: 4.67.1
2026-02-03 09:59:28,395:INFO:               numpy: 1.26.4
2026-02-03 09:59:28,395:INFO:              pandas: 2.1.4
2026-02-03 09:59:28,395:INFO:              jinja2: 3.1.6
2026-02-03 09:59:28,395:INFO:               scipy: 1.11.4
2026-02-03 09:59:28,395:INFO:              joblib: 1.3.2
2026-02-03 09:59:28,395:INFO:             sklearn: 1.4.2
2026-02-03 09:59:28,395:INFO:                pyod: 2.0.6
2026-02-03 09:59:28,395:INFO:            imblearn: 0.14.1
2026-02-03 09:59:28,395:INFO:   category_encoders: 2.7.0
2026-02-03 09:59:28,395:INFO:            lightgbm: 4.6.0
2026-02-03 09:59:28,395:INFO:               numba: 0.62.1
2026-02-03 09:59:28,395:INFO:            requests: 2.32.3
2026-02-03 09:59:28,395:INFO:          matplotlib: 3.7.5
2026-02-03 09:59:28,395:INFO:          scikitplot: 0.3.7
2026-02-03 09:59:28,395:INFO:         yellowbrick: 1.5
2026-02-03 09:59:28,395:INFO:              plotly: 5.24.1
2026-02-03 09:59:28,395:INFO:    plotly-resampler: Not installed
2026-02-03 09:59:28,395:INFO:             kaleido: 1.2.0
2026-02-03 09:59:28,395:INFO:           schemdraw: 0.15
2026-02-03 09:59:28,395:INFO:         statsmodels: 0.14.6
2026-02-03 09:59:28,395:INFO:              sktime: 0.26.0
2026-02-03 09:59:28,395:INFO:               tbats: 1.1.3
2026-02-03 09:59:28,395:INFO:            pmdarima: 2.0.4
2026-02-03 09:59:28,395:INFO:              psutil: 7.2.1
2026-02-03 09:59:28,395:INFO:          markupsafe: 3.0.3
2026-02-03 09:59:28,395:INFO:             pickle5: Not installed
2026-02-03 09:59:28,395:INFO:         cloudpickle: 3.0.0
2026-02-03 09:59:28,395:INFO:         deprecation: 2.1.0
2026-02-03 09:59:28,395:INFO:              xxhash: 3.6.0
2026-02-03 09:59:28,395:INFO:           wurlitzer: Not installed
2026-02-03 09:59:28,395:INFO:PyCaret optional dependencies:
2026-02-03 09:59:32,279:INFO:                shap: 0.44.1
2026-02-03 09:59:32,279:INFO:           interpret: 0.7.3
2026-02-03 09:59:32,279:INFO:                umap: 0.5.7
2026-02-03 09:59:32,279:INFO:     ydata_profiling: 4.18.1
2026-02-03 09:59:32,279:INFO:  explainerdashboard: 0.5.1
2026-02-03 09:59:32,279:INFO:             autoviz: Not installed
2026-02-03 09:59:32,279:INFO:           fairlearn: 0.7.0
2026-02-03 09:59:32,279:INFO:          deepchecks: Not installed
2026-02-03 09:59:32,279:INFO:             xgboost: Not installed
2026-02-03 09:59:32,279:INFO:            catboost: 1.2.8
2026-02-03 09:59:32,279:INFO:              kmodes: 0.12.2
2026-02-03 09:59:32,279:INFO:             mlxtend: 0.23.4
2026-02-03 09:59:32,279:INFO:       statsforecast: 1.5.0
2026-02-03 09:59:32,279:INFO:        tune_sklearn: Not installed
2026-02-03 09:59:32,279:INFO:                 ray: Not installed
2026-02-03 09:59:32,279:INFO:            hyperopt: 0.2.7
2026-02-03 09:59:32,279:INFO:              optuna: 4.6.0
2026-02-03 09:59:32,279:INFO:               skopt: 0.10.2
2026-02-03 09:59:32,279:INFO:              mlflow: 3.8.1
2026-02-03 09:59:32,279:INFO:              gradio: 6.3.0
2026-02-03 09:59:32,279:INFO:             fastapi: 0.128.0
2026-02-03 09:59:32,279:INFO:             uvicorn: 0.40.0
2026-02-03 09:59:32,279:INFO:              m2cgen: 0.10.0
2026-02-03 09:59:32,279:INFO:           evidently: 0.4.40
2026-02-03 09:59:32,279:INFO:               fugue: 0.8.7
2026-02-03 09:59:32,279:INFO:           streamlit: Not installed
2026-02-03 09:59:32,279:INFO:             prophet: Not installed
2026-02-03 09:59:32,279:INFO:None
2026-02-03 09:59:32,279:INFO:Set up data.
2026-02-03 09:59:32,345:INFO:Set up folding strategy.
2026-02-03 09:59:32,345:INFO:Set up train/test split.
2026-02-03 09:59:32,478:INFO:Set up index.
2026-02-03 09:59:32,495:INFO:Assigning column types.
2026-02-03 09:59:32,562:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-03 09:59:32,579:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-03 09:59:32,595:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 09:59:32,615:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 09:59:32,615:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 09:59:32,745:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-03 09:59:32,745:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 09:59:32,762:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 09:59:32,762:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 09:59:32,762:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-03 09:59:32,795:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 09:59:32,812:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 09:59:32,812:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 09:59:32,845:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 09:59:32,862:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 09:59:32,862:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 09:59:32,862:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-03 09:59:32,911:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 09:59:32,911:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 09:59:32,945:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 09:59:32,945:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 09:59:32,945:INFO:Preparing preprocessing pipeline...
2026-02-03 09:59:32,962:INFO:Set up simple imputation.
2026-02-03 09:59:32,962:INFO:Set up feature normalization.
2026-02-03 09:59:33,195:INFO:Finished creating preprocessing pipeline.
2026-02-03 09:59:33,212:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_et...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-03 09:59:33,212:INFO:Creating final display dataframe.
2026-02-03 09:59:33,912:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (373023, 16)
4        Transformed data shape      (373023, 16)
5   Transformed train set shape      (261116, 16)
6    Transformed test set shape      (111907, 16)
7              Numeric features                12
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              ba38
2026-02-03 09:59:33,945:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 09:59:33,945:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 09:59:33,995:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 09:59:33,995:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 09:59:33,995:INFO:setup() successfully completed in 6.68s...............
2026-02-03 09:59:33,995:INFO:Initializing compare_models()
2026-02-03 09:59:33,995:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-03 09:59:33,995:INFO:Checking exceptions
2026-02-03 09:59:34,062:INFO:Preparing display monitor
2026-02-03 09:59:34,062:INFO:Initializing Logistic Regression
2026-02-03 09:59:34,062:INFO:Total runtime is 0.0 minutes
2026-02-03 09:59:34,062:INFO:SubProcess create_model() called ==================================
2026-02-03 09:59:34,062:INFO:Initializing create_model()
2026-02-03 09:59:34,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED22AE3F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 09:59:34,062:INFO:Checking exceptions
2026-02-03 09:59:34,062:INFO:Importing libraries
2026-02-03 09:59:34,062:INFO:Copying training dataset
2026-02-03 09:59:34,162:INFO:Defining folds
2026-02-03 09:59:34,162:INFO:Declaring metric variables
2026-02-03 09:59:34,162:INFO:Importing untrained model
2026-02-03 09:59:34,162:INFO:Logistic Regression Imported successfully
2026-02-03 09:59:34,162:INFO:Starting cross validation
2026-02-03 09:59:34,162:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 09:59:39,362:INFO:Calculating mean and std
2026-02-03 09:59:39,362:INFO:Creating metrics dataframe
2026-02-03 09:59:39,362:INFO:Uploading results into container
2026-02-03 09:59:39,362:INFO:Uploading model into container now
2026-02-03 09:59:39,362:INFO:_master_model_container: 1
2026-02-03 09:59:39,362:INFO:_display_container: 2
2026-02-03 09:59:39,362:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-03 09:59:39,362:INFO:create_model() successfully completed......................................
2026-02-03 09:59:39,478:INFO:SubProcess create_model() end ==================================
2026-02-03 09:59:39,478:INFO:Creating metrics dataframe
2026-02-03 09:59:39,478:INFO:Initializing Decision Tree Classifier
2026-02-03 09:59:39,478:INFO:Total runtime is 0.09027154048283895 minutes
2026-02-03 09:59:39,478:INFO:SubProcess create_model() called ==================================
2026-02-03 09:59:39,478:INFO:Initializing create_model()
2026-02-03 09:59:39,478:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED22AE3F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 09:59:39,478:INFO:Checking exceptions
2026-02-03 09:59:39,478:INFO:Importing libraries
2026-02-03 09:59:39,478:INFO:Copying training dataset
2026-02-03 09:59:39,578:INFO:Defining folds
2026-02-03 09:59:39,578:INFO:Declaring metric variables
2026-02-03 09:59:39,578:INFO:Importing untrained model
2026-02-03 09:59:39,578:INFO:Decision Tree Classifier Imported successfully
2026-02-03 09:59:39,578:INFO:Starting cross validation
2026-02-03 09:59:39,578:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 09:59:44,194:INFO:Calculating mean and std
2026-02-03 09:59:44,195:INFO:Creating metrics dataframe
2026-02-03 09:59:44,196:INFO:Uploading results into container
2026-02-03 09:59:44,196:INFO:Uploading model into container now
2026-02-03 09:59:44,196:INFO:_master_model_container: 2
2026-02-03 09:59:44,196:INFO:_display_container: 2
2026-02-03 09:59:44,196:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-03 09:59:44,196:INFO:create_model() successfully completed......................................
2026-02-03 09:59:44,295:INFO:SubProcess create_model() end ==================================
2026-02-03 09:59:44,295:INFO:Creating metrics dataframe
2026-02-03 09:59:44,295:INFO:Initializing Random Forest Classifier
2026-02-03 09:59:44,295:INFO:Total runtime is 0.17055134773254393 minutes
2026-02-03 09:59:44,295:INFO:SubProcess create_model() called ==================================
2026-02-03 09:59:44,295:INFO:Initializing create_model()
2026-02-03 09:59:44,295:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED22AE3F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 09:59:44,295:INFO:Checking exceptions
2026-02-03 09:59:44,295:INFO:Importing libraries
2026-02-03 09:59:44,295:INFO:Copying training dataset
2026-02-03 09:59:44,395:INFO:Defining folds
2026-02-03 09:59:44,395:INFO:Declaring metric variables
2026-02-03 09:59:44,395:INFO:Importing untrained model
2026-02-03 09:59:44,395:INFO:Random Forest Classifier Imported successfully
2026-02-03 09:59:44,395:INFO:Starting cross validation
2026-02-03 09:59:44,395:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 09:59:55,099:INFO:Calculating mean and std
2026-02-03 09:59:55,099:INFO:Creating metrics dataframe
2026-02-03 09:59:55,099:INFO:Uploading results into container
2026-02-03 09:59:55,099:INFO:Uploading model into container now
2026-02-03 09:59:55,099:INFO:_master_model_container: 3
2026-02-03 09:59:55,099:INFO:_display_container: 2
2026-02-03 09:59:55,099:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 09:59:55,099:INFO:create_model() successfully completed......................................
2026-02-03 09:59:55,211:INFO:SubProcess create_model() end ==================================
2026-02-03 09:59:55,211:INFO:Creating metrics dataframe
2026-02-03 09:59:55,211:INFO:Initializing Light Gradient Boosting Machine
2026-02-03 09:59:55,211:INFO:Total runtime is 0.35248648325602217 minutes
2026-02-03 09:59:55,211:INFO:SubProcess create_model() called ==================================
2026-02-03 09:59:55,211:INFO:Initializing create_model()
2026-02-03 09:59:55,211:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED22AE3F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 09:59:55,211:INFO:Checking exceptions
2026-02-03 09:59:55,211:INFO:Importing libraries
2026-02-03 09:59:55,211:INFO:Copying training dataset
2026-02-03 09:59:55,312:INFO:Defining folds
2026-02-03 09:59:55,312:INFO:Declaring metric variables
2026-02-03 09:59:55,312:INFO:Importing untrained model
2026-02-03 09:59:55,312:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 09:59:55,312:INFO:Starting cross validation
2026-02-03 09:59:55,312:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:00:01,473:INFO:Calculating mean and std
2026-02-03 10:00:01,477:INFO:Creating metrics dataframe
2026-02-03 10:00:01,479:INFO:Uploading results into container
2026-02-03 10:00:01,480:INFO:Uploading model into container now
2026-02-03 10:00:01,480:INFO:_master_model_container: 4
2026-02-03 10:00:01,480:INFO:_display_container: 2
2026-02-03 10:00:01,481:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:00:01,481:INFO:create_model() successfully completed......................................
2026-02-03 10:00:01,578:INFO:SubProcess create_model() end ==================================
2026-02-03 10:00:01,578:INFO:Creating metrics dataframe
2026-02-03 10:00:01,594:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-03 10:00:01,595:INFO:Initializing create_model()
2026-02-03 10:00:01,595:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:00:01,595:INFO:Checking exceptions
2026-02-03 10:00:01,597:INFO:Importing libraries
2026-02-03 10:00:01,597:INFO:Copying training dataset
2026-02-03 10:00:01,698:INFO:Defining folds
2026-02-03 10:00:01,698:INFO:Declaring metric variables
2026-02-03 10:00:01,698:INFO:Importing untrained model
2026-02-03 10:00:01,698:INFO:Declaring custom model
2026-02-03 10:00:01,698:INFO:Random Forest Classifier Imported successfully
2026-02-03 10:00:01,698:INFO:Cross validation set to False
2026-02-03 10:00:01,698:INFO:Fitting Model
2026-02-03 10:00:05,261:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:00:05,261:INFO:create_model() successfully completed......................................
2026-02-03 10:00:05,395:INFO:Initializing create_model()
2026-02-03 10:00:05,395:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:00:05,395:INFO:Checking exceptions
2026-02-03 10:00:05,395:INFO:Importing libraries
2026-02-03 10:00:05,395:INFO:Copying training dataset
2026-02-03 10:00:05,495:INFO:Defining folds
2026-02-03 10:00:05,495:INFO:Declaring metric variables
2026-02-03 10:00:05,495:INFO:Importing untrained model
2026-02-03 10:00:05,495:INFO:Declaring custom model
2026-02-03 10:00:05,495:INFO:Decision Tree Classifier Imported successfully
2026-02-03 10:00:05,495:INFO:Cross validation set to False
2026-02-03 10:00:05,495:INFO:Fitting Model
2026-02-03 10:00:06,561:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-03 10:00:06,561:INFO:create_model() successfully completed......................................
2026-02-03 10:00:06,678:INFO:Initializing create_model()
2026-02-03 10:00:06,678:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:00:06,678:INFO:Checking exceptions
2026-02-03 10:00:06,678:INFO:Importing libraries
2026-02-03 10:00:06,678:INFO:Copying training dataset
2026-02-03 10:00:06,778:INFO:Defining folds
2026-02-03 10:00:06,778:INFO:Declaring metric variables
2026-02-03 10:00:06,778:INFO:Importing untrained model
2026-02-03 10:00:06,778:INFO:Declaring custom model
2026-02-03 10:00:06,778:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:00:06,778:INFO:Cross validation set to False
2026-02-03 10:00:06,778:INFO:Fitting Model
2026-02-03 10:00:07,178:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-03 10:00:07,190:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004183 seconds.
2026-02-03 10:00:07,190:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-03 10:00:07,191:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-03 10:00:07,191:INFO:[LightGBM] [Info] Total Bins 1841
2026-02-03 10:00:07,191:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 15
2026-02-03 10:00:07,193:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-03 10:00:07,194:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-03 10:00:07,663:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:00:07,663:INFO:create_model() successfully completed......................................
2026-02-03 10:00:07,811:INFO:_master_model_container: 4
2026-02-03 10:00:07,811:INFO:_display_container: 2
2026-02-03 10:00:07,811:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)]
2026-02-03 10:00:07,811:INFO:compare_models() successfully completed......................................
2026-02-03 10:00:07,811:INFO:Initializing tune_model()
2026-02-03 10:00:07,811:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-03 10:00:07,811:INFO:Checking exceptions
2026-02-03 10:00:07,861:INFO:Copying training dataset
2026-02-03 10:00:07,925:INFO:Checking base model
2026-02-03 10:00:07,925:INFO:Base model : Random Forest Classifier
2026-02-03 10:00:07,926:INFO:Declaring metric variables
2026-02-03 10:00:07,926:INFO:Defining Hyperparameters
2026-02-03 10:00:08,028:INFO:Tuning with n_jobs=-1
2026-02-03 10:00:08,028:INFO:Initializing RandomizedSearchCV
2026-02-03 10:01:10,476:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-03 10:01:10,477:INFO:Hyperparameter search completed
2026-02-03 10:01:10,478:INFO:SubProcess create_model() called ==================================
2026-02-03 10:01:10,478:INFO:Initializing create_model()
2026-02-03 10:01:10,478:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED22B39B50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-03 10:01:10,478:INFO:Checking exceptions
2026-02-03 10:01:10,478:INFO:Importing libraries
2026-02-03 10:01:10,478:INFO:Copying training dataset
2026-02-03 10:01:10,615:INFO:Defining folds
2026-02-03 10:01:10,615:INFO:Declaring metric variables
2026-02-03 10:01:10,615:INFO:Importing untrained model
2026-02-03 10:01:10,616:INFO:Declaring custom model
2026-02-03 10:01:10,616:INFO:Random Forest Classifier Imported successfully
2026-02-03 10:01:10,616:INFO:Starting cross validation
2026-02-03 10:01:10,618:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:01:24,066:INFO:Calculating mean and std
2026-02-03 10:01:24,068:INFO:Creating metrics dataframe
2026-02-03 10:01:24,071:INFO:Finalizing model
2026-02-03 10:01:30,712:INFO:Uploading results into container
2026-02-03 10:01:30,712:INFO:Uploading model into container now
2026-02-03 10:01:30,712:INFO:_master_model_container: 5
2026-02-03 10:01:30,712:INFO:_display_container: 3
2026-02-03 10:01:30,712:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:01:30,712:INFO:create_model() successfully completed......................................
2026-02-03 10:01:30,861:INFO:SubProcess create_model() end ==================================
2026-02-03 10:01:30,861:INFO:choose_better activated
2026-02-03 10:01:30,861:INFO:SubProcess create_model() called ==================================
2026-02-03 10:01:30,861:INFO:Initializing create_model()
2026-02-03 10:01:30,861:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:01:30,861:INFO:Checking exceptions
2026-02-03 10:01:30,863:INFO:Importing libraries
2026-02-03 10:01:30,863:INFO:Copying training dataset
2026-02-03 10:01:30,980:INFO:Defining folds
2026-02-03 10:01:30,980:INFO:Declaring metric variables
2026-02-03 10:01:30,980:INFO:Importing untrained model
2026-02-03 10:01:30,980:INFO:Declaring custom model
2026-02-03 10:01:30,980:INFO:Random Forest Classifier Imported successfully
2026-02-03 10:01:30,980:INFO:Starting cross validation
2026-02-03 10:01:30,980:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:01:41,011:INFO:Calculating mean and std
2026-02-03 10:01:41,011:INFO:Creating metrics dataframe
2026-02-03 10:01:41,011:INFO:Finalizing model
2026-02-03 10:01:45,626:INFO:Uploading results into container
2026-02-03 10:01:45,627:INFO:Uploading model into container now
2026-02-03 10:01:45,627:INFO:_master_model_container: 6
2026-02-03 10:01:45,627:INFO:_display_container: 4
2026-02-03 10:01:45,627:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:01:45,627:INFO:create_model() successfully completed......................................
2026-02-03 10:01:45,777:INFO:SubProcess create_model() end ==================================
2026-02-03 10:01:45,793:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9971
2026-02-03 10:01:45,793:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9733
2026-02-03 10:01:45,793:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-03 10:01:45,793:INFO:choose_better completed
2026-02-03 10:01:45,793:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-03 10:01:45,793:INFO:_master_model_container: 6
2026-02-03 10:01:45,793:INFO:_display_container: 3
2026-02-03 10:01:45,793:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:01:45,793:INFO:tune_model() successfully completed......................................
2026-02-03 10:01:45,943:INFO:Initializing tune_model()
2026-02-03 10:01:45,943:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-03 10:01:45,943:INFO:Checking exceptions
2026-02-03 10:01:46,027:INFO:Copying training dataset
2026-02-03 10:01:46,165:INFO:Checking base model
2026-02-03 10:01:46,165:INFO:Base model : Decision Tree Classifier
2026-02-03 10:01:46,165:INFO:Declaring metric variables
2026-02-03 10:01:46,165:INFO:Defining Hyperparameters
2026-02-03 10:01:46,294:INFO:Tuning with n_jobs=-1
2026-02-03 10:01:46,294:INFO:Initializing RandomizedSearchCV
2026-02-03 10:01:50,427:INFO:best_params: {'actual_estimator__min_samples_split': 2, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 15, 'actual_estimator__criterion': 'gini'}
2026-02-03 10:01:50,427:INFO:Hyperparameter search completed
2026-02-03 10:01:50,427:INFO:SubProcess create_model() called ==================================
2026-02-03 10:01:50,427:INFO:Initializing create_model()
2026-02-03 10:01:50,427:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED2B07E690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'min_samples_split': 2, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.0001, 'max_features': 1.0, 'max_depth': 15, 'criterion': 'gini'})
2026-02-03 10:01:50,427:INFO:Checking exceptions
2026-02-03 10:01:50,427:INFO:Importing libraries
2026-02-03 10:01:50,427:INFO:Copying training dataset
2026-02-03 10:01:50,543:INFO:Defining folds
2026-02-03 10:01:50,543:INFO:Declaring metric variables
2026-02-03 10:01:50,543:INFO:Importing untrained model
2026-02-03 10:01:50,543:INFO:Declaring custom model
2026-02-03 10:01:50,543:INFO:Decision Tree Classifier Imported successfully
2026-02-03 10:01:50,543:INFO:Starting cross validation
2026-02-03 10:01:50,543:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:01:51,838:INFO:Calculating mean and std
2026-02-03 10:01:51,842:INFO:Creating metrics dataframe
2026-02-03 10:01:51,846:INFO:Finalizing model
2026-02-03 10:01:52,610:INFO:Uploading results into container
2026-02-03 10:01:52,626:INFO:Uploading model into container now
2026-02-03 10:01:52,626:INFO:_master_model_container: 7
2026-02-03 10:01:52,627:INFO:_display_container: 4
2026-02-03 10:01:52,627:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-03 10:01:52,627:INFO:create_model() successfully completed......................................
2026-02-03 10:01:52,743:INFO:SubProcess create_model() end ==================================
2026-02-03 10:01:52,743:INFO:choose_better activated
2026-02-03 10:01:52,743:INFO:SubProcess create_model() called ==================================
2026-02-03 10:01:52,743:INFO:Initializing create_model()
2026-02-03 10:01:52,743:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best'), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:01:52,743:INFO:Checking exceptions
2026-02-03 10:01:52,743:INFO:Importing libraries
2026-02-03 10:01:52,743:INFO:Copying training dataset
2026-02-03 10:01:52,845:INFO:Defining folds
2026-02-03 10:01:52,845:INFO:Declaring metric variables
2026-02-03 10:01:52,845:INFO:Importing untrained model
2026-02-03 10:01:52,845:INFO:Declaring custom model
2026-02-03 10:01:52,845:INFO:Decision Tree Classifier Imported successfully
2026-02-03 10:01:52,845:INFO:Starting cross validation
2026-02-03 10:01:52,845:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:01:54,388:INFO:Calculating mean and std
2026-02-03 10:01:54,388:INFO:Creating metrics dataframe
2026-02-03 10:01:54,393:INFO:Finalizing model
2026-02-03 10:01:55,560:INFO:Uploading results into container
2026-02-03 10:01:55,560:INFO:Uploading model into container now
2026-02-03 10:01:55,560:INFO:_master_model_container: 8
2026-02-03 10:01:55,560:INFO:_display_container: 5
2026-02-03 10:01:55,560:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-03 10:01:55,560:INFO:create_model() successfully completed......................................
2026-02-03 10:01:55,680:INFO:SubProcess create_model() end ==================================
2026-02-03 10:01:55,680:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.9953
2026-02-03 10:01:55,680:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=15, max_features=1.0, max_leaf_nodes=None,
                       min_impurity_decrease=0.0001, min_samples_leaf=6,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') result for AUC is 0.967
2026-02-03 10:01:55,680:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best') is best model
2026-02-03 10:01:55,680:INFO:choose_better completed
2026-02-03 10:01:55,680:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-03 10:01:55,680:INFO:_master_model_container: 8
2026-02-03 10:01:55,680:INFO:_display_container: 4
2026-02-03 10:01:55,680:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-03 10:01:55,680:INFO:tune_model() successfully completed......................................
2026-02-03 10:01:55,795:INFO:Initializing tune_model()
2026-02-03 10:01:55,795:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-03 10:01:55,795:INFO:Checking exceptions
2026-02-03 10:01:55,827:INFO:Copying training dataset
2026-02-03 10:01:55,896:INFO:Checking base model
2026-02-03 10:01:55,896:INFO:Base model : Light Gradient Boosting Machine
2026-02-03 10:01:55,910:INFO:Declaring metric variables
2026-02-03 10:01:55,910:INFO:Defining Hyperparameters
2026-02-03 10:01:56,010:INFO:Tuning with n_jobs=-1
2026-02-03 10:01:56,010:INFO:Initializing RandomizedSearchCV
2026-02-03 10:02:21,576:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-03 10:02:21,579:INFO:Hyperparameter search completed
2026-02-03 10:02:21,579:INFO:SubProcess create_model() called ==================================
2026-02-03 10:02:21,581:INFO:Initializing create_model()
2026-02-03 10:02:21,582:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED22B38210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-03 10:02:21,582:INFO:Checking exceptions
2026-02-03 10:02:21,582:INFO:Importing libraries
2026-02-03 10:02:21,582:INFO:Copying training dataset
2026-02-03 10:02:21,768:INFO:Defining folds
2026-02-03 10:02:21,768:INFO:Declaring metric variables
2026-02-03 10:02:21,768:INFO:Importing untrained model
2026-02-03 10:02:21,768:INFO:Declaring custom model
2026-02-03 10:02:21,770:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:02:21,772:INFO:Starting cross validation
2026-02-03 10:02:21,772:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:02:28,647:INFO:Calculating mean and std
2026-02-03 10:02:28,649:INFO:Creating metrics dataframe
2026-02-03 10:02:28,651:INFO:Finalizing model
2026-02-03 10:02:28,984:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-03 10:02:28,984:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-03 10:02:28,984:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-03 10:02:29,091:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-03 10:02:29,092:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-03 10:02:29,093:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-03 10:02:29,093:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-03 10:02:29,106:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005558 seconds.
2026-02-03 10:02:29,107:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-03 10:02:29,107:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-03 10:02:29,107:INFO:[LightGBM] [Info] Total Bins 1841
2026-02-03 10:02:29,108:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 15
2026-02-03 10:02:29,112:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-03 10:02:29,112:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-03 10:02:31,591:INFO:Uploading results into container
2026-02-03 10:02:31,592:INFO:Uploading model into container now
2026-02-03 10:02:31,593:INFO:_master_model_container: 9
2026-02-03 10:02:31,593:INFO:_display_container: 5
2026-02-03 10:02:31,595:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:02:31,595:INFO:create_model() successfully completed......................................
2026-02-03 10:02:31,761:INFO:SubProcess create_model() end ==================================
2026-02-03 10:02:31,761:INFO:choose_better activated
2026-02-03 10:02:31,761:INFO:SubProcess create_model() called ==================================
2026-02-03 10:02:31,761:INFO:Initializing create_model()
2026-02-03 10:02:31,761:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:02:31,761:INFO:Checking exceptions
2026-02-03 10:02:31,761:INFO:Importing libraries
2026-02-03 10:02:31,761:INFO:Copying training dataset
2026-02-03 10:02:31,879:INFO:Defining folds
2026-02-03 10:02:31,879:INFO:Declaring metric variables
2026-02-03 10:02:31,879:INFO:Importing untrained model
2026-02-03 10:02:31,879:INFO:Declaring custom model
2026-02-03 10:02:31,879:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:02:31,879:INFO:Starting cross validation
2026-02-03 10:02:31,879:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:02:35,022:INFO:Calculating mean and std
2026-02-03 10:02:35,022:INFO:Creating metrics dataframe
2026-02-03 10:02:35,025:INFO:Finalizing model
2026-02-03 10:02:35,414:INFO:[LightGBM] [Info] Number of positive: 110231, number of negative: 150885
2026-02-03 10:02:35,424:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006146 seconds.
2026-02-03 10:02:35,425:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-03 10:02:35,425:INFO:[LightGBM] [Info] Total Bins 1841
2026-02-03 10:02:35,426:INFO:[LightGBM] [Info] Number of data points in the train set: 261116, number of used features: 15
2026-02-03 10:02:35,428:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.422153 -> initscore=-0.313940
2026-02-03 10:02:35,428:INFO:[LightGBM] [Info] Start training from score -0.313940
2026-02-03 10:02:36,199:INFO:Uploading results into container
2026-02-03 10:02:36,202:INFO:Uploading model into container now
2026-02-03 10:02:36,202:INFO:_master_model_container: 10
2026-02-03 10:02:36,202:INFO:_display_container: 6
2026-02-03 10:02:36,203:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:02:36,203:INFO:create_model() successfully completed......................................
2026-02-03 10:02:36,376:INFO:SubProcess create_model() end ==================================
2026-02-03 10:02:36,376:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9811
2026-02-03 10:02:36,376:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9958
2026-02-03 10:02:36,376:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-03 10:02:36,376:INFO:choose_better completed
2026-02-03 10:02:36,376:INFO:_master_model_container: 10
2026-02-03 10:02:36,376:INFO:_display_container: 5
2026-02-03 10:02:36,376:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:02:36,376:INFO:tune_model() successfully completed......................................
2026-02-03 10:02:36,520:INFO:Initializing predict_model()
2026-02-03 10:02:36,520:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ED55EC2520>)
2026-02-03 10:02:36,520:INFO:Checking exceptions
2026-02-03 10:02:36,520:INFO:Preloading libraries
2026-02-03 10:02:36,520:INFO:Set up data.
2026-02-03 10:02:36,526:INFO:Set up index.
2026-02-03 10:03:46,075:INFO:Initializing plot_model()
2026-02-03 10:03:46,075:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-03 10:03:46,075:INFO:Checking exceptions
2026-02-03 10:03:46,196:INFO:Preloading libraries
2026-02-03 10:03:46,259:INFO:Copying training dataset
2026-02-03 10:03:46,259:INFO:Plot type: feature
2026-02-03 10:03:46,259:WARNING:No coef_ found. Trying feature_importances_
2026-02-03 10:03:46,572:INFO:Visual Rendered Successfully
2026-02-03 10:03:47,445:INFO:plot_model() successfully completed......................................
2026-02-03 10:03:47,461:INFO:Initializing plot_model()
2026-02-03 10:03:47,461:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7BECC110>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-03 10:03:47,461:INFO:Checking exceptions
2026-02-03 10:03:47,540:INFO:Preloading libraries
2026-02-03 10:03:47,641:INFO:Copying training dataset
2026-02-03 10:03:47,641:INFO:Plot type: feature_all
2026-02-03 10:03:47,761:WARNING:No coef_ found. Trying feature_importances_
2026-02-03 10:03:48,058:INFO:Visual Rendered Successfully
2026-02-03 10:03:48,195:INFO:plot_model() successfully completed......................................
2026-02-03 10:03:48,211:INFO:Initializing save_model()
2026-02-03 10:03:48,211:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_explicable, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_et...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-03 10:03:48,211:INFO:Adding model into prep_pipe
2026-02-03 10:03:48,316:INFO:..\datos\04. Modelos\modelo_final_explicable.pkl saved in current working directory
2026-02-03 10:03:48,324:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_sol...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-03 10:03:48,324:INFO:save_model() successfully completed......................................
2026-02-03 10:05:56,442:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_30444\3688405148.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-03 10:05:57,164:INFO:PyCaret ClassificationExperiment
2026-02-03 10:05:57,164:INFO:Logging name: clf-default-name
2026-02-03 10:05:57,164:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-03 10:05:57,164:INFO:version 3.3.2
2026-02-03 10:05:57,164:INFO:Initializing setup()
2026-02-03 10:05:57,164:INFO:self.USI: f9c0
2026-02-03 10:05:57,164:INFO:self._variable_keys: {'X', 'memory', 'USI', 'data', '_ml_usecase', 'X_test', 'idx', 'gpu_n_jobs_param', 'is_multiclass', 'logging_param', '_available_plots', 'seed', 'fold_generator', 'fold_shuffle_param', 'y_test', 'exp_id', 'pipeline', 'log_plots_param', 'fold_groups_param', 'fix_imbalance', 'html_param', 'y', 'X_train', 'gpu_param', 'target_param', 'n_jobs_param', 'exp_name_log', 'y_train'}
2026-02-03 10:05:57,164:INFO:Checking environment
2026-02-03 10:05:57,164:INFO:python_version: 3.11.11
2026-02-03 10:05:57,164:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-03 10:05:57,164:INFO:machine: AMD64
2026-02-03 10:05:57,164:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-03 10:05:57,164:INFO:Memory: svmem(total=34009374720, available=14030381056, percent=58.7, used=19978993664, free=14030381056)
2026-02-03 10:05:57,164:INFO:Physical Core: 12
2026-02-03 10:05:57,164:INFO:Logical Core: 16
2026-02-03 10:05:57,164:INFO:Checking libraries
2026-02-03 10:05:57,164:INFO:System:
2026-02-03 10:05:57,164:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-03 10:05:57,164:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-03 10:05:57,164:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-03 10:05:57,164:INFO:PyCaret required dependencies:
2026-02-03 10:05:57,164:INFO:                 pip: 25.0
2026-02-03 10:05:57,164:INFO:          setuptools: 75.8.0
2026-02-03 10:05:57,164:INFO:             pycaret: 3.3.2
2026-02-03 10:05:57,164:INFO:             IPython: 9.9.0
2026-02-03 10:05:57,164:INFO:          ipywidgets: 8.1.8
2026-02-03 10:05:57,164:INFO:                tqdm: 4.67.1
2026-02-03 10:05:57,164:INFO:               numpy: 1.26.4
2026-02-03 10:05:57,164:INFO:              pandas: 2.1.4
2026-02-03 10:05:57,164:INFO:              jinja2: 3.1.6
2026-02-03 10:05:57,164:INFO:               scipy: 1.11.4
2026-02-03 10:05:57,164:INFO:              joblib: 1.3.2
2026-02-03 10:05:57,164:INFO:             sklearn: 1.4.2
2026-02-03 10:05:57,164:INFO:                pyod: 2.0.6
2026-02-03 10:05:57,164:INFO:            imblearn: 0.14.1
2026-02-03 10:05:57,164:INFO:   category_encoders: 2.7.0
2026-02-03 10:05:57,164:INFO:            lightgbm: 4.6.0
2026-02-03 10:05:57,164:INFO:               numba: 0.62.1
2026-02-03 10:05:57,164:INFO:            requests: 2.32.3
2026-02-03 10:05:57,164:INFO:          matplotlib: 3.7.5
2026-02-03 10:05:57,164:INFO:          scikitplot: 0.3.7
2026-02-03 10:05:57,167:INFO:         yellowbrick: 1.5
2026-02-03 10:05:57,167:INFO:              plotly: 5.24.1
2026-02-03 10:05:57,167:INFO:    plotly-resampler: Not installed
2026-02-03 10:05:57,167:INFO:             kaleido: 1.2.0
2026-02-03 10:05:57,167:INFO:           schemdraw: 0.15
2026-02-03 10:05:57,167:INFO:         statsmodels: 0.14.6
2026-02-03 10:05:57,167:INFO:              sktime: 0.26.0
2026-02-03 10:05:57,167:INFO:               tbats: 1.1.3
2026-02-03 10:05:57,167:INFO:            pmdarima: 2.0.4
2026-02-03 10:05:57,167:INFO:              psutil: 7.2.1
2026-02-03 10:05:57,167:INFO:          markupsafe: 3.0.3
2026-02-03 10:05:57,167:INFO:             pickle5: Not installed
2026-02-03 10:05:57,167:INFO:         cloudpickle: 3.0.0
2026-02-03 10:05:57,167:INFO:         deprecation: 2.1.0
2026-02-03 10:05:57,167:INFO:              xxhash: 3.6.0
2026-02-03 10:05:57,167:INFO:           wurlitzer: Not installed
2026-02-03 10:05:57,167:INFO:PyCaret optional dependencies:
2026-02-03 10:05:57,167:INFO:                shap: 0.44.1
2026-02-03 10:05:57,167:INFO:           interpret: 0.7.3
2026-02-03 10:05:57,167:INFO:                umap: 0.5.7
2026-02-03 10:05:57,167:INFO:     ydata_profiling: 4.18.1
2026-02-03 10:05:57,167:INFO:  explainerdashboard: 0.5.1
2026-02-03 10:05:57,167:INFO:             autoviz: Not installed
2026-02-03 10:05:57,167:INFO:           fairlearn: 0.7.0
2026-02-03 10:05:57,167:INFO:          deepchecks: Not installed
2026-02-03 10:05:57,167:INFO:             xgboost: Not installed
2026-02-03 10:05:57,167:INFO:            catboost: 1.2.8
2026-02-03 10:05:57,167:INFO:              kmodes: 0.12.2
2026-02-03 10:05:57,167:INFO:             mlxtend: 0.23.4
2026-02-03 10:05:57,167:INFO:       statsforecast: 1.5.0
2026-02-03 10:05:57,167:INFO:        tune_sklearn: Not installed
2026-02-03 10:05:57,167:INFO:                 ray: Not installed
2026-02-03 10:05:57,167:INFO:            hyperopt: 0.2.7
2026-02-03 10:05:57,167:INFO:              optuna: 4.6.0
2026-02-03 10:05:57,167:INFO:               skopt: 0.10.2
2026-02-03 10:05:57,167:INFO:              mlflow: 3.8.1
2026-02-03 10:05:57,167:INFO:              gradio: 6.3.0
2026-02-03 10:05:57,167:INFO:             fastapi: 0.128.0
2026-02-03 10:05:57,167:INFO:             uvicorn: 0.40.0
2026-02-03 10:05:57,167:INFO:              m2cgen: 0.10.0
2026-02-03 10:05:57,167:INFO:           evidently: 0.4.40
2026-02-03 10:05:57,167:INFO:               fugue: 0.8.7
2026-02-03 10:05:57,167:INFO:           streamlit: Not installed
2026-02-03 10:05:57,167:INFO:             prophet: Not installed
2026-02-03 10:05:57,167:INFO:None
2026-02-03 10:05:57,167:INFO:Set up data.
2026-02-03 10:05:57,205:INFO:Set up folding strategy.
2026-02-03 10:05:57,205:INFO:Set up train/test split.
2026-02-03 10:05:57,206:INFO:Set up index.
2026-02-03 10:05:57,206:INFO:Assigning column types.
2026-02-03 10:05:57,206:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-03 10:05:57,240:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-03 10:05:57,240:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:05:57,261:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:05:57,261:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:05:57,290:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-03 10:05:57,290:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:05:57,306:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:05:57,306:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:05:57,306:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-03 10:05:57,340:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:05:57,361:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:05:57,361:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:05:57,393:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:05:57,412:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:05:57,413:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:05:57,413:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-03 10:05:57,457:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:05:57,457:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:05:57,492:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:05:57,492:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:05:57,507:INFO:Preparing preprocessing pipeline...
2026-02-03 10:05:57,507:INFO:Set up simple imputation.
2026-02-03 10:05:57,507:INFO:Set up feature normalization.
2026-02-03 10:05:57,523:INFO:Finished creating preprocessing pipeline.
2026-02-03 10:05:57,523:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=Fals...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-03 10:05:57,523:INFO:Creating final display dataframe.
2026-02-03 10:05:57,610:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (10381, 18)
4        Transformed data shape       (10381, 18)
5   Transformed train set shape        (7266, 18)
6    Transformed test set shape        (3115, 18)
7              Numeric features                 7
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              f9c0
2026-02-03 10:05:57,662:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:05:57,662:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:05:57,709:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:05:57,709:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:05:57,712:INFO:setup() successfully completed in 0.57s...............
2026-02-03 10:05:57,712:INFO:Initializing compare_models()
2026-02-03 10:05:57,712:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-03 10:05:57,712:INFO:Checking exceptions
2026-02-03 10:05:57,714:INFO:Preparing display monitor
2026-02-03 10:05:57,716:INFO:Initializing Light Gradient Boosting Machine
2026-02-03 10:05:57,716:INFO:Total runtime is 0.0 minutes
2026-02-03 10:05:57,716:INFO:SubProcess create_model() called ==================================
2026-02-03 10:05:57,716:INFO:Initializing create_model()
2026-02-03 10:05:57,716:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED7D294F50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:05:57,716:INFO:Checking exceptions
2026-02-03 10:05:57,716:INFO:Importing libraries
2026-02-03 10:05:57,716:INFO:Copying training dataset
2026-02-03 10:05:57,723:INFO:Defining folds
2026-02-03 10:05:57,723:INFO:Declaring metric variables
2026-02-03 10:05:57,723:INFO:Importing untrained model
2026-02-03 10:05:57,723:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:05:57,723:INFO:Starting cross validation
2026-02-03 10:05:57,723:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:05:58,417:INFO:Calculating mean and std
2026-02-03 10:05:58,417:INFO:Creating metrics dataframe
2026-02-03 10:05:58,419:INFO:Uploading results into container
2026-02-03 10:05:58,421:INFO:Uploading model into container now
2026-02-03 10:05:58,421:INFO:_master_model_container: 1
2026-02-03 10:05:58,421:INFO:_display_container: 2
2026-02-03 10:05:58,422:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:05:58,422:INFO:create_model() successfully completed......................................
2026-02-03 10:05:58,573:INFO:SubProcess create_model() end ==================================
2026-02-03 10:05:58,574:INFO:Creating metrics dataframe
2026-02-03 10:05:58,574:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-03 10:05:58,574:INFO:Initializing create_model()
2026-02-03 10:05:58,574:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:05:58,574:INFO:Checking exceptions
2026-02-03 10:05:58,578:INFO:Importing libraries
2026-02-03 10:05:58,578:INFO:Copying training dataset
2026-02-03 10:05:58,582:INFO:Defining folds
2026-02-03 10:05:58,582:INFO:Declaring metric variables
2026-02-03 10:05:58,582:INFO:Importing untrained model
2026-02-03 10:05:58,582:INFO:Declaring custom model
2026-02-03 10:05:58,582:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:05:58,584:INFO:Cross validation set to False
2026-02-03 10:05:58,584:INFO:Fitting Model
2026-02-03 10:05:58,600:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-03 10:05:58,602:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000598 seconds.
2026-02-03 10:05:58,602:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-03 10:05:58,602:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-03 10:05:58,602:INFO:[LightGBM] [Info] Total Bins 759
2026-02-03 10:05:58,602:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 16
2026-02-03 10:05:58,602:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-03 10:05:58,602:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-03 10:05:58,675:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:05:58,675:INFO:create_model() successfully completed......................................
2026-02-03 10:05:58,823:INFO:_master_model_container: 1
2026-02-03 10:05:58,823:INFO:_display_container: 2
2026-02-03 10:05:58,823:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:05:58,823:INFO:compare_models() successfully completed......................................
2026-02-03 10:05:58,823:INFO:Initializing tune_model()
2026-02-03 10:05:58,823:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-03 10:05:58,823:INFO:Checking exceptions
2026-02-03 10:05:58,832:INFO:Copying training dataset
2026-02-03 10:05:58,832:INFO:Checking base model
2026-02-03 10:05:58,832:INFO:Base model : Light Gradient Boosting Machine
2026-02-03 10:05:58,832:INFO:Declaring metric variables
2026-02-03 10:05:58,832:INFO:Defining Hyperparameters
2026-02-03 10:05:58,944:INFO:Tuning with n_jobs=-1
2026-02-03 10:05:58,944:INFO:Initializing RandomizedSearchCV
2026-02-03 10:06:01,794:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 30, 'actual_estimator__n_estimators': 250, 'actual_estimator__min_split_gain': 0.3, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.5, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.5}
2026-02-03 10:06:01,796:INFO:Hyperparameter search completed
2026-02-03 10:06:01,796:INFO:SubProcess create_model() called ==================================
2026-02-03 10:06:01,800:INFO:Initializing create_model()
2026-02-03 10:06:01,800:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EDCDFCEED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.7, 'num_leaves': 30, 'n_estimators': 250, 'min_split_gain': 0.3, 'min_child_samples': 11, 'learning_rate': 0.5, 'feature_fraction': 0.8, 'bagging_freq': 1, 'bagging_fraction': 0.5})
2026-02-03 10:06:01,800:INFO:Checking exceptions
2026-02-03 10:06:01,800:INFO:Importing libraries
2026-02-03 10:06:01,802:INFO:Copying training dataset
2026-02-03 10:06:01,815:INFO:Defining folds
2026-02-03 10:06:01,815:INFO:Declaring metric variables
2026-02-03 10:06:01,817:INFO:Importing untrained model
2026-02-03 10:06:01,817:INFO:Declaring custom model
2026-02-03 10:06:01,819:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:06:01,821:INFO:Starting cross validation
2026-02-03 10:06:01,822:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:06:02,507:INFO:Calculating mean and std
2026-02-03 10:06:02,509:INFO:Creating metrics dataframe
2026-02-03 10:06:02,513:INFO:Finalizing model
2026-02-03 10:06:02,547:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-03 10:06:02,547:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-03 10:06:02,548:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-03 10:06:02,554:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2026-02-03 10:06:02,555:INFO:[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5
2026-02-03 10:06:02,555:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2026-02-03 10:06:02,555:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-03 10:06:02,556:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001362 seconds.
2026-02-03 10:06:02,559:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-03 10:06:02,559:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-03 10:06:02,559:INFO:[LightGBM] [Info] Total Bins 759
2026-02-03 10:06:02,559:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 16
2026-02-03 10:06:02,559:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-03 10:06:02,559:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-03 10:06:02,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,574:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,619:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,631:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,633:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,652:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,652:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,659:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,659:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,666:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,668:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,675:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,691:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,694:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,699:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,703:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,703:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,707:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,715:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,719:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,729:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,739:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,744:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,748:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,752:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,765:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,770:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,775:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,779:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,790:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,791:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,801:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,801:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,803:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,803:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,805:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,805:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,817:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,823:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,825:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,835:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,840:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,846:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,848:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,852:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,857:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,861:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,866:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2026-02-03 10:06:02,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:06:02,888:INFO:Uploading results into container
2026-02-03 10:06:02,890:INFO:Uploading model into container now
2026-02-03 10:06:02,890:INFO:_master_model_container: 2
2026-02-03 10:06:02,890:INFO:_display_container: 3
2026-02-03 10:06:02,892:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:06:02,892:INFO:create_model() successfully completed......................................
2026-02-03 10:06:03,072:INFO:SubProcess create_model() end ==================================
2026-02-03 10:06:03,073:INFO:choose_better activated
2026-02-03 10:06:03,073:INFO:SubProcess create_model() called ==================================
2026-02-03 10:06:03,073:INFO:Initializing create_model()
2026-02-03 10:06:03,073:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:06:03,073:INFO:Checking exceptions
2026-02-03 10:06:03,073:INFO:Importing libraries
2026-02-03 10:06:03,073:INFO:Copying training dataset
2026-02-03 10:06:03,073:INFO:Defining folds
2026-02-03 10:06:03,073:INFO:Declaring metric variables
2026-02-03 10:06:03,073:INFO:Importing untrained model
2026-02-03 10:06:03,073:INFO:Declaring custom model
2026-02-03 10:06:03,073:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:06:03,073:INFO:Starting cross validation
2026-02-03 10:06:03,073:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:06:04,049:INFO:Calculating mean and std
2026-02-03 10:06:04,049:INFO:Creating metrics dataframe
2026-02-03 10:06:04,051:INFO:Finalizing model
2026-02-03 10:06:04,073:INFO:[LightGBM] [Info] Number of positive: 4405, number of negative: 2861
2026-02-03 10:06:04,075:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000972 seconds.
2026-02-03 10:06:04,075:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-03 10:06:04,075:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-03 10:06:04,075:INFO:[LightGBM] [Info] Total Bins 759
2026-02-03 10:06:04,075:INFO:[LightGBM] [Info] Number of data points in the train set: 7266, number of used features: 16
2026-02-03 10:06:04,076:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.606248 -> initscore=0.431569
2026-02-03 10:06:04,076:INFO:[LightGBM] [Info] Start training from score 0.431569
2026-02-03 10:06:04,305:INFO:Uploading results into container
2026-02-03 10:06:04,306:INFO:Uploading model into container now
2026-02-03 10:06:04,306:INFO:_master_model_container: 3
2026-02-03 10:06:04,306:INFO:_display_container: 4
2026-02-03 10:06:04,306:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:06:04,306:INFO:create_model() successfully completed......................................
2026-02-03 10:06:04,457:INFO:SubProcess create_model() end ==================================
2026-02-03 10:06:04,457:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9994
2026-02-03 10:06:04,457:INFO:LGBMClassifier(bagging_fraction=0.5, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.5, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.3,
               n_estimators=250, n_jobs=-1, num_leaves=30, objective=None,
               random_state=42, reg_alpha=0.7, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9991
2026-02-03 10:06:04,457:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-03 10:06:04,457:INFO:choose_better completed
2026-02-03 10:06:04,457:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-03 10:06:04,457:INFO:_master_model_container: 3
2026-02-03 10:06:04,457:INFO:_display_container: 3
2026-02-03 10:06:04,457:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:06:04,457:INFO:tune_model() successfully completed......................................
2026-02-03 10:06:04,583:INFO:Initializing predict_model()
2026-02-03 10:06:04,583:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ED357E5EE0>)
2026-02-03 10:06:04,583:INFO:Checking exceptions
2026-02-03 10:06:04,583:INFO:Preloading libraries
2026-02-03 10:06:04,583:INFO:Set up data.
2026-02-03 10:06:04,583:INFO:Set up index.
2026-02-03 10:06:06,773:INFO:Initializing plot_model()
2026-02-03 10:06:06,773:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-03 10:06:06,773:INFO:Checking exceptions
2026-02-03 10:06:06,782:INFO:Preloading libraries
2026-02-03 10:06:06,789:INFO:Copying training dataset
2026-02-03 10:06:06,789:INFO:Plot type: feature
2026-02-03 10:06:06,789:WARNING:No coef_ found. Trying feature_importances_
2026-02-03 10:06:06,943:INFO:Visual Rendered Successfully
2026-02-03 10:06:07,073:INFO:plot_model() successfully completed......................................
2026-02-03 10:06:07,073:INFO:Initializing plot_model()
2026-02-03 10:06:07,073:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED22CD9B10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-03 10:06:07,073:INFO:Checking exceptions
2026-02-03 10:06:07,089:INFO:Preloading libraries
2026-02-03 10:06:07,093:INFO:Copying training dataset
2026-02-03 10:06:07,093:INFO:Plot type: feature_all
2026-02-03 10:06:07,135:WARNING:No coef_ found. Trying feature_importances_
2026-02-03 10:06:07,305:INFO:Visual Rendered Successfully
2026-02-03 10:06:07,406:INFO:plot_model() successfully completed......................................
2026-02-03 10:06:07,417:INFO:Initializing save_model()
2026-02-03 10:06:07,417:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=Fals...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-03 10:06:07,417:INFO:Adding model into prep_pipe
2026-02-03 10:06:07,444:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-03 10:06:07,452:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_fea...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-03 10:06:07,452:INFO:save_model() successfully completed......................................
2026-02-03 10:13:47,917:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_30444\3728923736.py:18: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-03 10:13:50,796:INFO:PyCaret ClassificationExperiment
2026-02-03 10:13:50,799:INFO:Logging name: clf-default-name
2026-02-03 10:13:50,800:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-03 10:13:50,800:INFO:version 3.3.2
2026-02-03 10:13:50,801:INFO:Initializing setup()
2026-02-03 10:13:50,801:INFO:self.USI: ec7b
2026-02-03 10:13:50,802:INFO:self._variable_keys: {'X', 'memory', 'USI', 'data', '_ml_usecase', 'X_test', 'idx', 'gpu_n_jobs_param', 'is_multiclass', 'logging_param', '_available_plots', 'seed', 'fold_generator', 'fold_shuffle_param', 'y_test', 'exp_id', 'pipeline', 'log_plots_param', 'fold_groups_param', 'fix_imbalance', 'html_param', 'y', 'X_train', 'gpu_param', 'target_param', 'n_jobs_param', 'exp_name_log', 'y_train'}
2026-02-03 10:13:50,802:INFO:Checking environment
2026-02-03 10:13:50,802:INFO:python_version: 3.11.11
2026-02-03 10:13:50,803:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-03 10:13:50,803:INFO:machine: AMD64
2026-02-03 10:13:50,803:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-03 10:13:50,804:INFO:Memory: svmem(total=34009374720, available=16676577280, percent=51.0, used=17332797440, free=16676577280)
2026-02-03 10:13:50,804:INFO:Physical Core: 12
2026-02-03 10:13:50,805:INFO:Logical Core: 16
2026-02-03 10:13:50,805:INFO:Checking libraries
2026-02-03 10:13:50,805:INFO:System:
2026-02-03 10:13:50,805:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-03 10:13:50,805:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-03 10:13:50,805:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-03 10:13:50,805:INFO:PyCaret required dependencies:
2026-02-03 10:13:50,805:INFO:                 pip: 25.0
2026-02-03 10:13:50,805:INFO:          setuptools: 75.8.0
2026-02-03 10:13:50,805:INFO:             pycaret: 3.3.2
2026-02-03 10:13:50,805:INFO:             IPython: 9.9.0
2026-02-03 10:13:50,805:INFO:          ipywidgets: 8.1.8
2026-02-03 10:13:50,805:INFO:                tqdm: 4.67.1
2026-02-03 10:13:50,806:INFO:               numpy: 1.26.4
2026-02-03 10:13:50,806:INFO:              pandas: 2.1.4
2026-02-03 10:13:50,806:INFO:              jinja2: 3.1.6
2026-02-03 10:13:50,806:INFO:               scipy: 1.11.4
2026-02-03 10:13:50,806:INFO:              joblib: 1.3.2
2026-02-03 10:13:50,806:INFO:             sklearn: 1.4.2
2026-02-03 10:13:50,806:INFO:                pyod: 2.0.6
2026-02-03 10:13:50,806:INFO:            imblearn: 0.14.1
2026-02-03 10:13:50,806:INFO:   category_encoders: 2.7.0
2026-02-03 10:13:50,806:INFO:            lightgbm: 4.6.0
2026-02-03 10:13:50,806:INFO:               numba: 0.62.1
2026-02-03 10:13:50,806:INFO:            requests: 2.32.3
2026-02-03 10:13:50,806:INFO:          matplotlib: 3.7.5
2026-02-03 10:13:50,806:INFO:          scikitplot: 0.3.7
2026-02-03 10:13:50,806:INFO:         yellowbrick: 1.5
2026-02-03 10:13:50,806:INFO:              plotly: 5.24.1
2026-02-03 10:13:50,806:INFO:    plotly-resampler: Not installed
2026-02-03 10:13:50,806:INFO:             kaleido: 1.2.0
2026-02-03 10:13:50,806:INFO:           schemdraw: 0.15
2026-02-03 10:13:50,806:INFO:         statsmodels: 0.14.6
2026-02-03 10:13:50,806:INFO:              sktime: 0.26.0
2026-02-03 10:13:50,806:INFO:               tbats: 1.1.3
2026-02-03 10:13:50,806:INFO:            pmdarima: 2.0.4
2026-02-03 10:13:50,806:INFO:              psutil: 7.2.1
2026-02-03 10:13:50,806:INFO:          markupsafe: 3.0.3
2026-02-03 10:13:50,806:INFO:             pickle5: Not installed
2026-02-03 10:13:50,806:INFO:         cloudpickle: 3.0.0
2026-02-03 10:13:50,806:INFO:         deprecation: 2.1.0
2026-02-03 10:13:50,806:INFO:              xxhash: 3.6.0
2026-02-03 10:13:50,806:INFO:           wurlitzer: Not installed
2026-02-03 10:13:50,806:INFO:PyCaret optional dependencies:
2026-02-03 10:13:50,806:INFO:                shap: 0.44.1
2026-02-03 10:13:50,806:INFO:           interpret: 0.7.3
2026-02-03 10:13:50,806:INFO:                umap: 0.5.7
2026-02-03 10:13:50,806:INFO:     ydata_profiling: 4.18.1
2026-02-03 10:13:50,806:INFO:  explainerdashboard: 0.5.1
2026-02-03 10:13:50,806:INFO:             autoviz: Not installed
2026-02-03 10:13:50,806:INFO:           fairlearn: 0.7.0
2026-02-03 10:13:50,806:INFO:          deepchecks: Not installed
2026-02-03 10:13:50,806:INFO:             xgboost: Not installed
2026-02-03 10:13:50,806:INFO:            catboost: 1.2.8
2026-02-03 10:13:50,806:INFO:              kmodes: 0.12.2
2026-02-03 10:13:50,806:INFO:             mlxtend: 0.23.4
2026-02-03 10:13:50,806:INFO:       statsforecast: 1.5.0
2026-02-03 10:13:50,806:INFO:        tune_sklearn: Not installed
2026-02-03 10:13:50,806:INFO:                 ray: Not installed
2026-02-03 10:13:50,806:INFO:            hyperopt: 0.2.7
2026-02-03 10:13:50,806:INFO:              optuna: 4.6.0
2026-02-03 10:13:50,806:INFO:               skopt: 0.10.2
2026-02-03 10:13:50,806:INFO:              mlflow: 3.8.1
2026-02-03 10:13:50,806:INFO:              gradio: 6.3.0
2026-02-03 10:13:50,806:INFO:             fastapi: 0.128.0
2026-02-03 10:13:50,806:INFO:             uvicorn: 0.40.0
2026-02-03 10:13:50,806:INFO:              m2cgen: 0.10.0
2026-02-03 10:13:50,806:INFO:           evidently: 0.4.40
2026-02-03 10:13:50,806:INFO:               fugue: 0.8.7
2026-02-03 10:13:50,806:INFO:           streamlit: Not installed
2026-02-03 10:13:50,806:INFO:             prophet: Not installed
2026-02-03 10:13:50,806:INFO:None
2026-02-03 10:13:50,806:INFO:Set up data.
2026-02-03 10:13:50,917:INFO:Set up folding strategy.
2026-02-03 10:13:50,917:INFO:Set up train/test split.
2026-02-03 10:13:51,121:INFO:Set up index.
2026-02-03 10:13:51,134:INFO:Assigning column types.
2026-02-03 10:13:51,233:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-03 10:13:51,267:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-03 10:13:51,267:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:13:51,300:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:13:51,300:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:13:51,333:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-03 10:13:51,333:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:13:51,367:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:13:51,367:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:13:51,367:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-03 10:13:51,400:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:13:51,422:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:13:51,422:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:13:51,467:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:13:51,483:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:13:51,483:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:13:51,483:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-03 10:13:51,550:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:13:51,550:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:13:51,600:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:13:51,600:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:13:51,600:INFO:Preparing preprocessing pipeline...
2026-02-03 10:13:51,616:INFO:Set up simple imputation.
2026-02-03 10:13:51,616:INFO:Set up feature normalization.
2026-02-03 10:13:51,883:INFO:Finished creating preprocessing pipeline.
2026-02-03 10:13:51,883:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_et...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-03 10:13:51,883:INFO:Creating final display dataframe.
2026-02-03 10:13:52,719:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape      (362642, 16)
4        Transformed data shape      (362642, 16)
5   Transformed train set shape      (253849, 16)
6    Transformed test set shape      (108793, 16)
7              Numeric features                12
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              ec7b
2026-02-03 10:13:52,769:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:13:52,769:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:13:52,836:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:13:52,836:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:13:52,836:INFO:setup() successfully completed in 2.07s...............
2026-02-03 10:13:52,836:INFO:Initializing compare_models()
2026-02-03 10:13:52,836:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, include=['lr', 'dt', 'rf', 'lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, 'include': ['lr', 'dt', 'rf', 'lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-03 10:13:52,836:INFO:Checking exceptions
2026-02-03 10:13:52,903:INFO:Preparing display monitor
2026-02-03 10:13:52,917:INFO:Initializing Logistic Regression
2026-02-03 10:13:52,917:INFO:Total runtime is 0.0 minutes
2026-02-03 10:13:52,917:INFO:SubProcess create_model() called ==================================
2026-02-03 10:13:52,917:INFO:Initializing create_model()
2026-02-03 10:13:52,917:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=lr, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED7DB46690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:13:52,917:INFO:Checking exceptions
2026-02-03 10:13:52,917:INFO:Importing libraries
2026-02-03 10:13:52,917:INFO:Copying training dataset
2026-02-03 10:13:53,038:INFO:Defining folds
2026-02-03 10:13:53,038:INFO:Declaring metric variables
2026-02-03 10:13:53,038:INFO:Importing untrained model
2026-02-03 10:13:53,039:INFO:Logistic Regression Imported successfully
2026-02-03 10:13:53,039:INFO:Starting cross validation
2026-02-03 10:13:53,040:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:13:58,709:INFO:Calculating mean and std
2026-02-03 10:13:58,709:INFO:Creating metrics dataframe
2026-02-03 10:13:58,709:INFO:Uploading results into container
2026-02-03 10:13:58,716:INFO:Uploading model into container now
2026-02-03 10:13:58,717:INFO:_master_model_container: 1
2026-02-03 10:13:58,717:INFO:_display_container: 2
2026-02-03 10:13:58,718:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2026-02-03 10:13:58,718:INFO:create_model() successfully completed......................................
2026-02-03 10:13:58,836:INFO:SubProcess create_model() end ==================================
2026-02-03 10:13:58,837:INFO:Creating metrics dataframe
2026-02-03 10:13:58,838:INFO:Initializing Decision Tree Classifier
2026-02-03 10:13:58,838:INFO:Total runtime is 0.09868465264638265 minutes
2026-02-03 10:13:58,838:INFO:SubProcess create_model() called ==================================
2026-02-03 10:13:58,838:INFO:Initializing create_model()
2026-02-03 10:13:58,838:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=dt, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED7DB46690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:13:58,838:INFO:Checking exceptions
2026-02-03 10:13:58,838:INFO:Importing libraries
2026-02-03 10:13:58,838:INFO:Copying training dataset
2026-02-03 10:13:58,932:INFO:Defining folds
2026-02-03 10:13:58,932:INFO:Declaring metric variables
2026-02-03 10:13:58,932:INFO:Importing untrained model
2026-02-03 10:13:58,932:INFO:Decision Tree Classifier Imported successfully
2026-02-03 10:13:58,934:INFO:Starting cross validation
2026-02-03 10:13:58,935:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:14:04,161:INFO:Calculating mean and std
2026-02-03 10:14:04,168:INFO:Creating metrics dataframe
2026-02-03 10:14:04,170:INFO:Uploading results into container
2026-02-03 10:14:04,171:INFO:Uploading model into container now
2026-02-03 10:14:04,171:INFO:_master_model_container: 2
2026-02-03 10:14:04,172:INFO:_display_container: 2
2026-02-03 10:14:04,172:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=42, splitter='best')
2026-02-03 10:14:04,172:INFO:create_model() successfully completed......................................
2026-02-03 10:14:04,289:INFO:SubProcess create_model() end ==================================
2026-02-03 10:14:04,289:INFO:Creating metrics dataframe
2026-02-03 10:14:04,291:INFO:Initializing Random Forest Classifier
2026-02-03 10:14:04,291:INFO:Total runtime is 0.18957314093907673 minutes
2026-02-03 10:14:04,292:INFO:SubProcess create_model() called ==================================
2026-02-03 10:14:04,292:INFO:Initializing create_model()
2026-02-03 10:14:04,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=rf, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED7DB46690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:14:04,292:INFO:Checking exceptions
2026-02-03 10:14:04,292:INFO:Importing libraries
2026-02-03 10:14:04,293:INFO:Copying training dataset
2026-02-03 10:14:04,380:INFO:Defining folds
2026-02-03 10:14:04,380:INFO:Declaring metric variables
2026-02-03 10:14:04,380:INFO:Importing untrained model
2026-02-03 10:14:04,381:INFO:Random Forest Classifier Imported successfully
2026-02-03 10:14:04,381:INFO:Starting cross validation
2026-02-03 10:14:04,382:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:14:14,735:INFO:Calculating mean and std
2026-02-03 10:14:14,737:INFO:Creating metrics dataframe
2026-02-03 10:14:14,737:INFO:Uploading results into container
2026-02-03 10:14:14,740:INFO:Uploading model into container now
2026-02-03 10:14:14,740:INFO:_master_model_container: 3
2026-02-03 10:14:14,740:INFO:_display_container: 2
2026-02-03 10:14:14,740:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:14:14,740:INFO:create_model() successfully completed......................................
2026-02-03 10:14:14,849:INFO:SubProcess create_model() end ==================================
2026-02-03 10:14:14,849:INFO:Creating metrics dataframe
2026-02-03 10:14:14,849:INFO:Initializing Light Gradient Boosting Machine
2026-02-03 10:14:14,849:INFO:Total runtime is 0.36554062366485596 minutes
2026-02-03 10:14:14,849:INFO:SubProcess create_model() called ==================================
2026-02-03 10:14:14,849:INFO:Initializing create_model()
2026-02-03 10:14:14,849:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED7DB46690>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:14:14,849:INFO:Checking exceptions
2026-02-03 10:14:14,849:INFO:Importing libraries
2026-02-03 10:14:14,849:INFO:Copying training dataset
2026-02-03 10:14:14,935:INFO:Defining folds
2026-02-03 10:14:14,935:INFO:Declaring metric variables
2026-02-03 10:14:14,935:INFO:Importing untrained model
2026-02-03 10:14:14,936:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:14:14,936:INFO:Starting cross validation
2026-02-03 10:14:14,937:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:14:20,667:INFO:Calculating mean and std
2026-02-03 10:14:20,667:INFO:Creating metrics dataframe
2026-02-03 10:14:20,667:INFO:Uploading results into container
2026-02-03 10:14:20,667:INFO:Uploading model into container now
2026-02-03 10:14:20,667:INFO:_master_model_container: 4
2026-02-03 10:14:20,667:INFO:_display_container: 2
2026-02-03 10:14:20,667:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:14:20,667:INFO:create_model() successfully completed......................................
2026-02-03 10:14:20,783:INFO:SubProcess create_model() end ==================================
2026-02-03 10:14:20,783:INFO:Creating metrics dataframe
2026-02-03 10:14:20,784:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-03 10:14:20,786:INFO:Initializing create_model()
2026-02-03 10:14:20,786:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:14:20,786:INFO:Checking exceptions
2026-02-03 10:14:20,786:INFO:Importing libraries
2026-02-03 10:14:20,786:INFO:Copying training dataset
2026-02-03 10:14:20,866:INFO:Defining folds
2026-02-03 10:14:20,866:INFO:Declaring metric variables
2026-02-03 10:14:20,866:INFO:Importing untrained model
2026-02-03 10:14:20,866:INFO:Declaring custom model
2026-02-03 10:14:20,866:INFO:Random Forest Classifier Imported successfully
2026-02-03 10:14:20,866:INFO:Cross validation set to False
2026-02-03 10:14:20,866:INFO:Fitting Model
2026-02-03 10:14:24,282:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:14:24,282:INFO:create_model() successfully completed......................................
2026-02-03 10:14:24,416:INFO:_master_model_container: 4
2026-02-03 10:14:24,416:INFO:_display_container: 2
2026-02-03 10:14:24,416:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:14:24,416:INFO:compare_models() successfully completed......................................
2026-02-03 10:14:24,416:INFO:Initializing tune_model()
2026-02-03 10:14:24,416:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-03 10:14:24,416:INFO:Checking exceptions
2026-02-03 10:14:24,449:INFO:Copying training dataset
2026-02-03 10:14:24,504:INFO:Checking base model
2026-02-03 10:14:24,504:INFO:Base model : Random Forest Classifier
2026-02-03 10:14:24,504:INFO:Declaring metric variables
2026-02-03 10:14:24,515:INFO:Defining Hyperparameters
2026-02-03 10:14:24,616:INFO:Tuning with n_jobs=-1
2026-02-03 10:14:24,616:INFO:Initializing RandomizedSearchCV
2026-02-03 10:15:19,722:INFO:best_params: {'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 9, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2026-02-03 10:15:19,722:INFO:Hyperparameter search completed
2026-02-03 10:15:19,722:INFO:SubProcess create_model() called ==================================
2026-02-03 10:15:19,722:INFO:Initializing create_model()
2026-02-03 10:15:19,722:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED22DB1310>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 230, 'min_samples_split': 10, 'min_samples_leaf': 6, 'min_impurity_decrease': 0, 'max_features': 'sqrt', 'max_depth': 9, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2026-02-03 10:15:19,722:INFO:Checking exceptions
2026-02-03 10:15:19,722:INFO:Importing libraries
2026-02-03 10:15:19,722:INFO:Copying training dataset
2026-02-03 10:15:19,867:INFO:Defining folds
2026-02-03 10:15:19,867:INFO:Declaring metric variables
2026-02-03 10:15:19,867:INFO:Importing untrained model
2026-02-03 10:15:19,867:INFO:Declaring custom model
2026-02-03 10:15:19,867:INFO:Random Forest Classifier Imported successfully
2026-02-03 10:15:19,867:INFO:Starting cross validation
2026-02-03 10:15:19,867:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:15:31,974:INFO:Calculating mean and std
2026-02-03 10:15:31,974:INFO:Creating metrics dataframe
2026-02-03 10:15:31,974:INFO:Finalizing model
2026-02-03 10:15:37,812:INFO:Uploading results into container
2026-02-03 10:15:37,812:INFO:Uploading model into container now
2026-02-03 10:15:37,816:INFO:_master_model_container: 5
2026-02-03 10:15:37,816:INFO:_display_container: 3
2026-02-03 10:15:37,816:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:15:37,816:INFO:create_model() successfully completed......................................
2026-02-03 10:15:37,963:INFO:SubProcess create_model() end ==================================
2026-02-03 10:15:37,963:INFO:choose_better activated
2026-02-03 10:15:37,963:INFO:SubProcess create_model() called ==================================
2026-02-03 10:15:37,963:INFO:Initializing create_model()
2026-02-03 10:15:37,963:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:15:37,963:INFO:Checking exceptions
2026-02-03 10:15:37,964:INFO:Importing libraries
2026-02-03 10:15:37,964:INFO:Copying training dataset
2026-02-03 10:15:38,065:INFO:Defining folds
2026-02-03 10:15:38,065:INFO:Declaring metric variables
2026-02-03 10:15:38,065:INFO:Importing untrained model
2026-02-03 10:15:38,065:INFO:Declaring custom model
2026-02-03 10:15:38,065:INFO:Random Forest Classifier Imported successfully
2026-02-03 10:15:38,065:INFO:Starting cross validation
2026-02-03 10:15:38,065:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:15:46,506:INFO:Calculating mean and std
2026-02-03 10:15:46,506:INFO:Creating metrics dataframe
2026-02-03 10:15:46,508:INFO:Finalizing model
2026-02-03 10:15:50,534:INFO:Uploading results into container
2026-02-03 10:15:50,534:INFO:Uploading model into container now
2026-02-03 10:15:50,534:INFO:_master_model_container: 6
2026-02-03 10:15:50,534:INFO:_display_container: 4
2026-02-03 10:15:50,534:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:15:50,534:INFO:create_model() successfully completed......................................
2026-02-03 10:15:50,647:INFO:SubProcess create_model() end ==================================
2026-02-03 10:15:50,647:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9971
2026-02-03 10:15:50,647:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=9, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0, min_samples_leaf=6,
                       min_samples_split=10, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=230, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) result for AUC is 0.9728
2026-02-03 10:15:50,647:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False) is best model
2026-02-03 10:15:50,647:INFO:choose_better completed
2026-02-03 10:15:50,647:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-03 10:15:50,663:INFO:_master_model_container: 6
2026-02-03 10:15:50,663:INFO:_display_container: 3
2026-02-03 10:15:50,663:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
2026-02-03 10:15:50,663:INFO:tune_model() successfully completed......................................
2026-02-03 10:15:50,797:INFO:Initializing predict_model()
2026-02-03 10:15:50,797:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ED357E5EE0>)
2026-02-03 10:15:50,797:INFO:Checking exceptions
2026-02-03 10:15:50,797:INFO:Preloading libraries
2026-02-03 10:15:50,797:INFO:Set up data.
2026-02-03 10:15:50,813:INFO:Set up index.
2026-02-03 10:16:48,788:INFO:Initializing plot_model()
2026-02-03 10:16:48,788:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-03 10:16:48,788:INFO:Checking exceptions
2026-02-03 10:16:48,830:INFO:Preloading libraries
2026-02-03 10:16:48,915:INFO:Copying training dataset
2026-02-03 10:16:48,915:INFO:Plot type: feature
2026-02-03 10:16:48,915:WARNING:No coef_ found. Trying feature_importances_
2026-02-03 10:16:49,146:INFO:Visual Rendered Successfully
2026-02-03 10:16:49,989:INFO:plot_model() successfully completed......................................
2026-02-03 10:16:49,997:INFO:Initializing plot_model()
2026-02-03 10:16:49,997:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001ED7C112B90>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), plot=feature_all, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-03 10:16:49,997:INFO:Checking exceptions
2026-02-03 10:16:50,097:INFO:Preloading libraries
2026-02-03 10:16:50,197:INFO:Copying training dataset
2026-02-03 10:16:50,197:INFO:Plot type: feature_all
2026-02-03 10:16:50,314:WARNING:No coef_ found. Trying feature_importances_
2026-02-03 10:16:50,550:INFO:Visual Rendered Successfully
2026-02-03 10:16:50,664:INFO:plot_model() successfully completed......................................
2026-02-03 10:16:50,690:INFO:Initializing save_model()
2026-02-03 10:16:50,690:INFO:save_model(model=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=42, verbose=0,
                       warm_start=False), model_name=..\datos\04. Modelos\modelo_final_grado, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_et...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-03 10:16:50,690:INFO:Adding model into prep_pipe
2026-02-03 10:16:50,762:INFO:..\datos\04. Modelos\modelo_final_grado.pkl saved in current working directory
2026-02-03 10:16:50,762:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_ADMISION',
                                             'NU_NOTA_MEDIA_1_BACH__PC',
                                             'NU_RESULTADO_ADMISION_PUNTOS',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias',
                                             'num_asistencias_acum',
                                             'num_sol...
                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,
                                        class_weight=None, criterion='gini',
                                        max_depth=None, max_features='sqrt',
                                        max_leaf_nodes=None, max_samples=None,
                                        min_impurity_decrease=0.0,
                                        min_samples_leaf=1, min_samples_split=2,
                                        min_weight_fraction_leaf=0.0,
                                        monotonic_cst=None, n_estimators=100,
                                        n_jobs=-1, oob_score=False,
                                        random_state=42, verbose=0,
                                        warm_start=False))],
         verbose=False)
2026-02-03 10:16:50,762:INFO:save_model() successfully completed......................................
2026-02-03 10:16:53,100:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_30444\2801537822.py:19: DtypeWarning: Columns (6,9,17,18,19,21,22,27,28,29,30,33) have mixed types. Specify dtype option on import or set low_memory=False.

2026-02-03 10:16:53,897:WARNING:C:\Users\0021755\AppData\Local\Temp\ipykernel_30444\2801537822.py:88: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

2026-02-03 10:16:53,930:INFO:PyCaret ClassificationExperiment
2026-02-03 10:16:53,930:INFO:Logging name: clf-default-name
2026-02-03 10:16:53,930:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2026-02-03 10:16:53,930:INFO:version 3.3.2
2026-02-03 10:16:53,930:INFO:Initializing setup()
2026-02-03 10:16:53,930:INFO:self.USI: 7b73
2026-02-03 10:16:53,930:INFO:self._variable_keys: {'X', 'memory', 'USI', 'data', '_ml_usecase', 'X_test', 'idx', 'gpu_n_jobs_param', 'is_multiclass', 'logging_param', '_available_plots', 'seed', 'fold_generator', 'fold_shuffle_param', 'y_test', 'exp_id', 'pipeline', 'log_plots_param', 'fold_groups_param', 'fix_imbalance', 'html_param', 'y', 'X_train', 'gpu_param', 'target_param', 'n_jobs_param', 'exp_name_log', 'y_train'}
2026-02-03 10:16:53,930:INFO:Checking environment
2026-02-03 10:16:53,930:INFO:python_version: 3.11.11
2026-02-03 10:16:53,930:INFO:python_build: ('main', 'Dec 11 2024 16:34:19')
2026-02-03 10:16:53,930:INFO:machine: AMD64
2026-02-03 10:16:53,930:INFO:platform: Windows-10-10.0.26100-SP0
2026-02-03 10:16:53,930:INFO:Memory: svmem(total=34009374720, available=13616652288, percent=60.0, used=20392722432, free=13616652288)
2026-02-03 10:16:53,930:INFO:Physical Core: 12
2026-02-03 10:16:53,930:INFO:Logical Core: 16
2026-02-03 10:16:53,930:INFO:Checking libraries
2026-02-03 10:16:53,930:INFO:System:
2026-02-03 10:16:53,930:INFO:    python: 3.11.11 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:34:19) [MSC v.1929 64 bit (AMD64)]
2026-02-03 10:16:53,930:INFO:executable: c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\python.exe
2026-02-03 10:16:53,930:INFO:   machine: Windows-10-10.0.26100-SP0
2026-02-03 10:16:53,930:INFO:PyCaret required dependencies:
2026-02-03 10:16:53,930:INFO:                 pip: 25.0
2026-02-03 10:16:53,930:INFO:          setuptools: 75.8.0
2026-02-03 10:16:53,930:INFO:             pycaret: 3.3.2
2026-02-03 10:16:53,930:INFO:             IPython: 9.9.0
2026-02-03 10:16:53,930:INFO:          ipywidgets: 8.1.8
2026-02-03 10:16:53,930:INFO:                tqdm: 4.67.1
2026-02-03 10:16:53,930:INFO:               numpy: 1.26.4
2026-02-03 10:16:53,930:INFO:              pandas: 2.1.4
2026-02-03 10:16:53,930:INFO:              jinja2: 3.1.6
2026-02-03 10:16:53,930:INFO:               scipy: 1.11.4
2026-02-03 10:16:53,930:INFO:              joblib: 1.3.2
2026-02-03 10:16:53,930:INFO:             sklearn: 1.4.2
2026-02-03 10:16:53,930:INFO:                pyod: 2.0.6
2026-02-03 10:16:53,930:INFO:            imblearn: 0.14.1
2026-02-03 10:16:53,930:INFO:   category_encoders: 2.7.0
2026-02-03 10:16:53,930:INFO:            lightgbm: 4.6.0
2026-02-03 10:16:53,930:INFO:               numba: 0.62.1
2026-02-03 10:16:53,930:INFO:            requests: 2.32.3
2026-02-03 10:16:53,930:INFO:          matplotlib: 3.7.5
2026-02-03 10:16:53,930:INFO:          scikitplot: 0.3.7
2026-02-03 10:16:53,930:INFO:         yellowbrick: 1.5
2026-02-03 10:16:53,930:INFO:              plotly: 5.24.1
2026-02-03 10:16:53,930:INFO:    plotly-resampler: Not installed
2026-02-03 10:16:53,930:INFO:             kaleido: 1.2.0
2026-02-03 10:16:53,930:INFO:           schemdraw: 0.15
2026-02-03 10:16:53,930:INFO:         statsmodels: 0.14.6
2026-02-03 10:16:53,930:INFO:              sktime: 0.26.0
2026-02-03 10:16:53,930:INFO:               tbats: 1.1.3
2026-02-03 10:16:53,930:INFO:            pmdarima: 2.0.4
2026-02-03 10:16:53,930:INFO:              psutil: 7.2.1
2026-02-03 10:16:53,930:INFO:          markupsafe: 3.0.3
2026-02-03 10:16:53,930:INFO:             pickle5: Not installed
2026-02-03 10:16:53,930:INFO:         cloudpickle: 3.0.0
2026-02-03 10:16:53,930:INFO:         deprecation: 2.1.0
2026-02-03 10:16:53,930:INFO:              xxhash: 3.6.0
2026-02-03 10:16:53,930:INFO:           wurlitzer: Not installed
2026-02-03 10:16:53,930:INFO:PyCaret optional dependencies:
2026-02-03 10:16:53,930:INFO:                shap: 0.44.1
2026-02-03 10:16:53,930:INFO:           interpret: 0.7.3
2026-02-03 10:16:53,930:INFO:                umap: 0.5.7
2026-02-03 10:16:53,930:INFO:     ydata_profiling: 4.18.1
2026-02-03 10:16:53,930:INFO:  explainerdashboard: 0.5.1
2026-02-03 10:16:53,946:INFO:             autoviz: Not installed
2026-02-03 10:16:53,946:INFO:           fairlearn: 0.7.0
2026-02-03 10:16:53,946:INFO:          deepchecks: Not installed
2026-02-03 10:16:53,946:INFO:             xgboost: Not installed
2026-02-03 10:16:53,946:INFO:            catboost: 1.2.8
2026-02-03 10:16:53,946:INFO:              kmodes: 0.12.2
2026-02-03 10:16:53,946:INFO:             mlxtend: 0.23.4
2026-02-03 10:16:53,946:INFO:       statsforecast: 1.5.0
2026-02-03 10:16:53,947:INFO:        tune_sklearn: Not installed
2026-02-03 10:16:53,947:INFO:                 ray: Not installed
2026-02-03 10:16:53,947:INFO:            hyperopt: 0.2.7
2026-02-03 10:16:53,947:INFO:              optuna: 4.6.0
2026-02-03 10:16:53,948:INFO:               skopt: 0.10.2
2026-02-03 10:16:53,948:INFO:              mlflow: 3.8.1
2026-02-03 10:16:53,948:INFO:              gradio: 6.3.0
2026-02-03 10:16:53,948:INFO:             fastapi: 0.128.0
2026-02-03 10:16:53,948:INFO:             uvicorn: 0.40.0
2026-02-03 10:16:53,948:INFO:              m2cgen: 0.10.0
2026-02-03 10:16:53,948:INFO:           evidently: 0.4.40
2026-02-03 10:16:53,948:INFO:               fugue: 0.8.7
2026-02-03 10:16:53,948:INFO:           streamlit: Not installed
2026-02-03 10:16:53,949:INFO:             prophet: Not installed
2026-02-03 10:16:53,949:INFO:None
2026-02-03 10:16:53,949:INFO:Set up data.
2026-02-03 10:16:53,949:INFO:Set up folding strategy.
2026-02-03 10:16:53,949:INFO:Set up train/test split.
2026-02-03 10:16:53,949:INFO:Set up index.
2026-02-03 10:16:53,963:INFO:Assigning column types.
2026-02-03 10:16:53,964:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2026-02-03 10:16:53,997:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-03 10:16:53,997:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:16:54,014:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:16:54,014:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:16:54,047:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2026-02-03 10:16:54,047:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:16:54,067:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:16:54,067:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:16:54,067:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2026-02-03 10:16:54,097:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:16:54,131:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:16:54,131:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:16:54,147:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2026-02-03 10:16:54,181:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:16:54,181:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:16:54,181:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2026-02-03 10:16:54,230:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:16:54,230:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:16:54,281:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:16:54,281:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:16:54,281:INFO:Preparing preprocessing pipeline...
2026-02-03 10:16:54,281:INFO:Set up simple imputation.
2026-02-03 10:16:54,281:INFO:Set up feature normalization.
2026-02-03 10:16:54,314:INFO:Finished creating preprocessing pipeline.
2026-02-03 10:16:54,314:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False)
2026-02-03 10:16:54,314:INFO:Creating final display dataframe.
2026-02-03 10:16:54,414:INFO:Setup _display_container:                     Description             Value
0                    Session id                42
1                        Target            target
2                   Target type            Binary
3           Original data shape       (11822, 20)
4        Transformed data shape       (11822, 20)
5   Transformed train set shape        (8275, 20)
6    Transformed test set shape        (3547, 20)
7              Numeric features                 9
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12                    Normalize              True
13             Normalize method            zscore
14               Fold Generator   StratifiedKFold
15                  Fold Number                 3
16                     CPU Jobs                -1
17                      Use GPU             False
18               Log Experiment             False
19              Experiment Name  clf-default-name
20                          USI              7b73
2026-02-03 10:16:54,480:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:16:54,480:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:16:54,531:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2026-02-03 10:16:54,531:INFO:Soft dependency imported: catboost: 1.2.8
2026-02-03 10:16:54,531:INFO:setup() successfully completed in 0.62s...............
2026-02-03 10:16:54,531:INFO:Initializing compare_models()
2026-02-03 10:16:54,531:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, include=['lightgbm'], exclude=None, fold=None, round=4, cross_validation=True, sort=AUC, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, 'include': ['lightgbm'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'AUC', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2026-02-03 10:16:54,531:INFO:Checking exceptions
2026-02-03 10:16:54,531:INFO:Preparing display monitor
2026-02-03 10:16:54,531:INFO:Initializing Light Gradient Boosting Machine
2026-02-03 10:16:54,531:INFO:Total runtime is 0.0 minutes
2026-02-03 10:16:54,531:INFO:SubProcess create_model() called ==================================
2026-02-03 10:16:54,531:INFO:Initializing create_model()
2026-02-03 10:16:54,531:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, estimator=lightgbm, fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001EDEC58BD10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:16:54,531:INFO:Checking exceptions
2026-02-03 10:16:54,531:INFO:Importing libraries
2026-02-03 10:16:54,531:INFO:Copying training dataset
2026-02-03 10:16:54,547:INFO:Defining folds
2026-02-03 10:16:54,547:INFO:Declaring metric variables
2026-02-03 10:16:54,547:INFO:Importing untrained model
2026-02-03 10:16:54,547:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:16:54,547:INFO:Starting cross validation
2026-02-03 10:16:54,547:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:16:55,192:INFO:Calculating mean and std
2026-02-03 10:16:55,192:INFO:Creating metrics dataframe
2026-02-03 10:16:55,196:INFO:Uploading results into container
2026-02-03 10:16:55,196:INFO:Uploading model into container now
2026-02-03 10:16:55,197:INFO:_master_model_container: 1
2026-02-03 10:16:55,197:INFO:_display_container: 2
2026-02-03 10:16:55,198:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:16:55,198:INFO:create_model() successfully completed......................................
2026-02-03 10:16:55,358:INFO:SubProcess create_model() end ==================================
2026-02-03 10:16:55,358:INFO:Creating metrics dataframe
2026-02-03 10:16:55,362:WARNING:c:\Users\0021755\AppData\Local\anaconda3\envs\my_python311_env\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.

2026-02-03 10:16:55,364:INFO:Initializing create_model()
2026-02-03 10:16:55,364:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:16:55,364:INFO:Checking exceptions
2026-02-03 10:16:55,364:INFO:Importing libraries
2026-02-03 10:16:55,364:INFO:Copying training dataset
2026-02-03 10:16:55,364:INFO:Defining folds
2026-02-03 10:16:55,364:INFO:Declaring metric variables
2026-02-03 10:16:55,364:INFO:Importing untrained model
2026-02-03 10:16:55,364:INFO:Declaring custom model
2026-02-03 10:16:55,364:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:16:55,364:INFO:Cross validation set to False
2026-02-03 10:16:55,364:INFO:Fitting Model
2026-02-03 10:16:55,407:INFO:[LightGBM] [Info] Number of positive: 4406, number of negative: 3869
2026-02-03 10:16:55,409:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000963 seconds.
2026-02-03 10:16:55,409:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-03 10:16:55,409:INFO:[LightGBM] [Info] Total Bins 1319
2026-02-03 10:16:55,409:INFO:[LightGBM] [Info] Number of data points in the train set: 8275, number of used features: 18
2026-02-03 10:16:55,410:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.532447 -> initscore=0.129971
2026-02-03 10:16:55,410:INFO:[LightGBM] [Info] Start training from score 0.129971
2026-02-03 10:16:55,693:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:16:55,693:INFO:create_model() successfully completed......................................
2026-02-03 10:16:55,846:INFO:_master_model_container: 1
2026-02-03 10:16:55,847:INFO:_display_container: 2
2026-02-03 10:16:55,847:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:16:55,848:INFO:compare_models() successfully completed......................................
2026-02-03 10:16:55,848:INFO:Initializing tune_model()
2026-02-03 10:16:55,848:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=AUC, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2026-02-03 10:16:55,848:INFO:Checking exceptions
2026-02-03 10:16:55,849:INFO:Copying training dataset
2026-02-03 10:16:55,849:INFO:Checking base model
2026-02-03 10:16:55,849:INFO:Base model : Light Gradient Boosting Machine
2026-02-03 10:16:55,849:INFO:Declaring metric variables
2026-02-03 10:16:55,849:INFO:Defining Hyperparameters
2026-02-03 10:16:55,964:INFO:Tuning with n_jobs=-1
2026-02-03 10:16:55,964:INFO:Initializing RandomizedSearchCV
2026-02-03 10:16:59,416:INFO:best_params: {'actual_estimator__reg_lambda': 0.7, 'actual_estimator__reg_alpha': 0.0005, 'actual_estimator__num_leaves': 40, 'actual_estimator__n_estimators': 90, 'actual_estimator__min_split_gain': 0.1, 'actual_estimator__min_child_samples': 81, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.7, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.6}
2026-02-03 10:16:59,421:INFO:Hyperparameter search completed
2026-02-03 10:16:59,423:INFO:SubProcess create_model() called ==================================
2026-02-03 10:16:59,423:INFO:Initializing create_model()
2026-02-03 10:16:59,425:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ED35748F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.7, 'reg_alpha': 0.0005, 'num_leaves': 40, 'n_estimators': 90, 'min_split_gain': 0.1, 'min_child_samples': 81, 'learning_rate': 0.2, 'feature_fraction': 0.7, 'bagging_freq': 3, 'bagging_fraction': 0.6})
2026-02-03 10:16:59,425:INFO:Checking exceptions
2026-02-03 10:16:59,425:INFO:Importing libraries
2026-02-03 10:16:59,427:INFO:Copying training dataset
2026-02-03 10:16:59,436:INFO:Defining folds
2026-02-03 10:16:59,436:INFO:Declaring metric variables
2026-02-03 10:16:59,436:INFO:Importing untrained model
2026-02-03 10:16:59,438:INFO:Declaring custom model
2026-02-03 10:16:59,438:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:16:59,438:INFO:Starting cross validation
2026-02-03 10:16:59,440:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:16:59,940:INFO:Calculating mean and std
2026-02-03 10:16:59,942:INFO:Creating metrics dataframe
2026-02-03 10:16:59,944:INFO:Finalizing model
2026-02-03 10:16:59,973:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2026-02-03 10:16:59,973:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2026-02-03 10:16:59,973:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2026-02-03 10:16:59,980:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2026-02-03 10:16:59,980:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2026-02-03 10:16:59,980:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2026-02-03 10:16:59,980:INFO:[LightGBM] [Info] Number of positive: 4406, number of negative: 3869
2026-02-03 10:16:59,983:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001707 seconds.
2026-02-03 10:16:59,984:INFO:You can set `force_col_wise=true` to remove the overhead.
2026-02-03 10:16:59,984:INFO:[LightGBM] [Info] Total Bins 1319
2026-02-03 10:16:59,984:INFO:[LightGBM] [Info] Number of data points in the train set: 8275, number of used features: 18
2026-02-03 10:16:59,984:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.532447 -> initscore=0.129971
2026-02-03 10:16:59,985:INFO:[LightGBM] [Info] Start training from score 0.129971
2026-02-03 10:16:59,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:16:59,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:16:59,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,118:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,130:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,132:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,134:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,136:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,140:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,146:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2026-02-03 10:17:00,184:INFO:Uploading results into container
2026-02-03 10:17:00,186:INFO:Uploading model into container now
2026-02-03 10:17:00,187:INFO:_master_model_container: 2
2026-02-03 10:17:00,187:INFO:_display_container: 3
2026-02-03 10:17:00,188:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=81, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=90, n_jobs=-1, num_leaves=40, objective=None,
               random_state=42, reg_alpha=0.0005, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:17:00,188:INFO:create_model() successfully completed......................................
2026-02-03 10:17:00,345:INFO:SubProcess create_model() end ==================================
2026-02-03 10:17:00,345:INFO:choose_better activated
2026-02-03 10:17:00,345:INFO:SubProcess create_model() called ==================================
2026-02-03 10:17:00,347:INFO:Initializing create_model()
2026-02-03 10:17:00,347:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=3, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2026-02-03 10:17:00,347:INFO:Checking exceptions
2026-02-03 10:17:00,347:INFO:Importing libraries
2026-02-03 10:17:00,347:INFO:Copying training dataset
2026-02-03 10:17:00,347:INFO:Defining folds
2026-02-03 10:17:00,347:INFO:Declaring metric variables
2026-02-03 10:17:00,347:INFO:Importing untrained model
2026-02-03 10:17:00,347:INFO:Declaring custom model
2026-02-03 10:17:00,347:INFO:Light Gradient Boosting Machine Imported successfully
2026-02-03 10:17:00,347:INFO:Starting cross validation
2026-02-03 10:17:00,347:INFO:Cross validating with StratifiedKFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2026-02-03 10:17:01,091:INFO:Calculating mean and std
2026-02-03 10:17:01,091:INFO:Creating metrics dataframe
2026-02-03 10:17:01,093:INFO:Finalizing model
2026-02-03 10:17:01,126:INFO:[LightGBM] [Info] Number of positive: 4406, number of negative: 3869
2026-02-03 10:17:01,128:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000840 seconds.
2026-02-03 10:17:01,128:INFO:You can set `force_row_wise=true` to remove the overhead.
2026-02-03 10:17:01,128:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2026-02-03 10:17:01,129:INFO:[LightGBM] [Info] Total Bins 1319
2026-02-03 10:17:01,129:INFO:[LightGBM] [Info] Number of data points in the train set: 8275, number of used features: 18
2026-02-03 10:17:01,129:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.532447 -> initscore=0.129971
2026-02-03 10:17:01,129:INFO:[LightGBM] [Info] Start training from score 0.129971
2026-02-03 10:17:01,314:INFO:Uploading results into container
2026-02-03 10:17:01,314:INFO:Uploading model into container now
2026-02-03 10:17:01,316:INFO:_master_model_container: 3
2026-02-03 10:17:01,316:INFO:_display_container: 4
2026-02-03 10:17:01,316:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:17:01,316:INFO:create_model() successfully completed......................................
2026-02-03 10:17:01,463:INFO:SubProcess create_model() end ==================================
2026-02-03 10:17:01,463:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.9993
2026-02-03 10:17:01,463:INFO:LGBMClassifier(bagging_fraction=0.6, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=81, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=90, n_jobs=-1, num_leaves=40, objective=None,
               random_state=42, reg_alpha=0.0005, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for AUC is 0.999
2026-02-03 10:17:01,463:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2026-02-03 10:17:01,463:INFO:choose_better completed
2026-02-03 10:17:01,463:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2026-02-03 10:17:01,463:INFO:_master_model_container: 3
2026-02-03 10:17:01,463:INFO:_display_container: 3
2026-02-03 10:17:01,463:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2026-02-03 10:17:01,463:INFO:tune_model() successfully completed......................................
2026-02-03 10:17:01,600:INFO:Initializing predict_model()
2026-02-03 10:17:01,600:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ED327F37E0>)
2026-02-03 10:17:01,600:INFO:Checking exceptions
2026-02-03 10:17:01,600:INFO:Preloading libraries
2026-02-03 10:17:01,600:INFO:Set up data.
2026-02-03 10:17:01,600:INFO:Set up index.
2026-02-03 10:17:05,315:INFO:Initializing plot_model()
2026-02-03 10:17:05,315:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001EDEF9DC210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=None, fit_kwargs=None, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=True, system=True, display=None, display_format=None)
2026-02-03 10:17:05,315:INFO:Checking exceptions
2026-02-03 10:17:05,315:INFO:Preloading libraries
2026-02-03 10:17:05,326:INFO:Copying training dataset
2026-02-03 10:17:05,326:INFO:Plot type: feature
2026-02-03 10:17:05,326:WARNING:No coef_ found. Trying feature_importances_
2026-02-03 10:17:05,495:INFO:Visual Rendered Successfully
2026-02-03 10:17:05,632:INFO:plot_model() successfully completed......................................
2026-02-03 10:17:05,646:INFO:Initializing save_model()
2026-02-03 10:17:05,646:INFO:save_model(model=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=42, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), model_name=..\datos\04. Modelos\modelo_final_master, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\0021755\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('normalize',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=StandardScaler(copy=True,
                                                               with_mean=True,
                                                               with_std=True)))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2026-02-03 10:17:05,646:INFO:Adding model into prep_pipe
2026-02-03 10:17:05,655:INFO:..\datos\04. Modelos\modelo_final_master.pkl saved in current working directory
2026-02-03 10:17:05,664:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['NU_NOTA_MEDIA_1_BACH__PC',
                                             'PAID_PERCENT', 'CU_IMPORTE_TOTAL',
                                             'NU_PREFERENCIA',
                                             'PL_SITUACION_SOCIO_ECONOMICA',
                                             'tiempo_etapa_dias',
                                             'tiempo_entre_etapas_dias', 'PCA1',
                                             'PCA2'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,...
                 LGBMClassifier(boosting_type='gbdt', class_weight=None,
                                colsample_bytree=1.0, importance_type='split',
                                learning_rate=0.1, max_depth=-1,
                                min_child_samples=20, min_child_weight=0.001,
                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,
                                num_leaves=31, objective=None, random_state=42,
                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
                                subsample_for_bin=200000, subsample_freq=0))],
         verbose=False)
2026-02-03 10:17:05,664:INFO:save_model() successfully completed......................................
