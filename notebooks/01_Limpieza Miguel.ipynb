{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š UniÃ³n y Limpieza de datos del Dataset  \n",
    "\n",
    "---\n",
    "\n",
    "**Objetivo del Notebook**  \n",
    "Limpieza de datos, columnas innecesarias y valores nulos/blancos \n",
    "\n",
    "**Contexto del anÃ¡lisis**  \n",
    "- Dataset de muestra proporcionado + csv proporcionado unido en un Ãºnico excel dataset\n",
    "- Enfoque en aprendizaje, validaciÃ³n del pipeline y comprensiÃ³n del proceso\n",
    "\n",
    "**Valor devuelto**  \n",
    "- Copia del Dataset de muestra proporcionado completamente limpio y Ãºtil \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import utils\n",
    "# ===============================\n",
    "# LEER EL ARCHIVO LIMPIO\n",
    "# ===============================\n",
    "dfs = pd.read_excel(r'..\\datos\\01. Datos originales\\DataSET_SF.xlsx', sheet_name=None)\n",
    "\n",
    "# Ver la primera hoja\n",
    "oportunidad = list(dfs.values())[0]\n",
    "cuenta = list(dfs.values())[1]\n",
    "ecb = list(dfs.values())[2]\n",
    "solicitud_ban = list(dfs.values())[3]\n",
    "casos = list(dfs.values())[4]\n",
    "correos = list(dfs.values())[5]\n",
    "historial_actividad = list(dfs.values())[6]\n",
    "historial_etapas = list(dfs.values())[7]\n",
    "\n",
    "\n",
    "print(historial_etapas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import analisis_na_por_columna\n",
    "\n",
    "# ===============================\n",
    "# ANALISIS POR COLUMNA DE VALORES NA\n",
    "# ===============================\n",
    "analisis_na_por_columna(oportunidad)\n",
    "analisis_na_por_columna(cuenta)\n",
    "analisis_na_por_columna(ecb)\n",
    "analisis_na_por_columna(solicitud_ban)\n",
    "analisis_na_por_columna(casos)\n",
    "analisis_na_por_columna(correos)\n",
    "analisis_na_por_columna(historial_actividad)\n",
    "analisis_na_por_columna(historial_etapas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import analisis_na_por_columna, eliminar_columnas_na, crear_target\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# LIMPIEZA DE NAS\n",
    "# ===============================\n",
    "\n",
    "oportunidad = eliminar_columnas_na(oportunidad)\n",
    "cuenta = eliminar_columnas_na(cuenta)\n",
    "ecb = eliminar_columnas_na(ecb)\n",
    "solicitud_ban = eliminar_columnas_na(solicitud_ban)\n",
    "casos = eliminar_columnas_na(casos)\n",
    "correos = eliminar_columnas_na(correos)\n",
    "historial_actvidad = eliminar_columnas_na(historial_actividad)\n",
    "historial_etapas = eliminar_columnas_na(historial_etapas)\n",
    "\n",
    "# ===============================\n",
    "# CREACION DEL TARJET\n",
    "# ===============================\n",
    "\n",
    "target = crear_target(oportunidad, historial_etapas)\n",
    "target.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# COLUMNAS A TRATAR\n",
    "# ===============================\n",
    "\n",
    "columnas_seleccionadas = [\n",
    "    # Identificadores y Target\n",
    "    'ACCOUNTID', 'ID','ID18__PC', 'target', 'PL_CURSO_ACADEMICO',\n",
    "    \n",
    "    # Rendimiento AcadÃ©mico\n",
    "    'NU_NOTA_MEDIA_ADMISION', 'NU_NOTA_MEDIA_1_BACH__PC', 'CH_PRUEBAS_CALIFICADAS', \n",
    "    'NU_RESULTADO_ADMISION_PUNTOS', 'PL_RESOLUCION_DEFINITIVA', 'TITULACION',\n",
    "    \n",
    "    # Compromiso EconÃ³mico\n",
    "    'MINIMUMPAYMENTPAYED', 'PAID_AMOUNT', 'PAID_PERCENT', 'CH_PAGO_SUPERIOR', \n",
    "    'CH_MATRICULA_SUJETA_BECA', 'CH_AYUDA_FINANCIACION', 'CU_IMPORTE_TOTAL',\n",
    "    \n",
    "    # Engagement y Actividad\n",
    "    'CH_VISITACAMPUS__PC', 'CH_ENTREVISTA_PERSONAL__PC', 'ACC_DTT_FECHAULTIMAACTIVIDAD', \n",
    "    'NU_PREFERENCIA', 'STAGENAME', 'PL_SUBETAPA',\n",
    "    \n",
    "    # Perfil SociodemogrÃ¡fico y Fidelidad\n",
    "    'CH_HIJO_EMPLEADO__PC', 'CH_HIJO_ANTIGUO_ALUMNO__PC', 'CH_HERMANOS_ESTUDIANDO_UNAV__P', \n",
    "    'YEARPERSONBIRTHDATE', 'NAMEX', 'CH_FAMILIA_NUMEROSA__PC', 'PL_SITUACION_SOCIO_ECONOMICA',\n",
    "    \n",
    "    # Origen y Canal\n",
    "    'LEADSOURCE', 'PL_ORIGEN_DE_SOLICITUD', 'LK_CENTROENSENANZA', 'PL_PLAZO_ADMISION'\n",
    "]\n",
    "\n",
    "# =================================\n",
    "# Unir la tabla con cuenta y obtener filtrar los datos que nos interesen\n",
    "# ===============================\n",
    "df_unido = pd.merge(\n",
    "    target, \n",
    "    cuenta, \n",
    "    left_on='ACCOUNTID', \n",
    "    right_on='ID18', \n",
    "    how='left',\n",
    "    suffixes=('', '_cuenta')\n",
    ")\t\t\n",
    "columnas_finales =  [col for col in columnas_seleccionadas if col in df_unido.columns]\n",
    "df_unido_filtrado =  df_unido[columnas_finales].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calcular_tiempos_etapas, limpiar_historial_por_hitos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "historial_etapas_tiempo = calcular_tiempos_etapas(historial_etapas)\n",
    "\n",
    "# Unir target con historial_etapas y calcular el tiempo medio por etapa\n",
    "df_unido_filtrado \n",
    "\n",
    "\n",
    "df_final = limpiar_historial_por_hitos(historial_etapas_tiempo, df_unido_filtrado)\n",
    "# Unir la tabla con ECB y pagos\n",
    "\n",
    "\n",
    "ejemplo_id = '0066900001k7yTgAAI'\n",
    "\n",
    "columnas_comprobacion = [\n",
    "    'LK_Oportunidad__c', 'CreatedDate', 'PL_Etapa__c', 'PL_Subetapa__c',\n",
    "    'fecha_pruebas_calificadas', 'NU_NOTA_MEDIA_ADMISION',\n",
    "    'fecha_matricula_iniciada', 'PAID_AMOUNT','NU_NOTA_MEDIA_ADMISION', 'CH_PRUEBAS_CALIFICADAS', \n",
    "        'NU_RESULTADO_ADMISION_PUNTOS', 'PL_RESOLUCION_DEFINITIVA'\n",
    "]\n",
    "\n",
    "print(\"--- COMPROBACIÃ“N DE LÃ“GICA TEMPORAL ---\")\n",
    "df_final[df_final['LK_Oportunidad__c'] == ejemplo_id][columnas_comprobacion].sort_values('CreatedDate')\n",
    "df_final.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import integrar_actividades_progresivo_por_curso\n",
    "\n",
    "# EjecuciÃ³n\n",
    "df_final_v3 = integrar_actividades_progresivo_por_curso(df_final, historial_actividad)\n",
    "# ==========================================\n",
    "# EJECUCIÃ“N\n",
    "# ==========================================\n",
    "# Print de comprobaciÃ³n para ver la evoluciÃ³n de un contacto\n",
    "ejemplo_acc = df_final_v3[df_final_v3['num_asistencias_acum'] > 0]['ID18__PC'].iloc[1]\n",
    "cols_print = ['ID','ID18__PC','ACCOUNTID', 'CreatedDate', 'PL_Etapa__c', 'num_asistencias_acum', 'num_solicitudes_acum']\n",
    "\n",
    "print(\"\\n--- COMPROBACIÃ“N DE EVOLUCIÃ“N DE ACTIVIDADES ---\")\n",
    "df_final_v3[df_final_v3['ID18__PC'] == ejemplo_acc][cols_print].sort_values('CreatedDate')\n",
    "historial_actividad.loc[historial_actividad['ContactId']=='003690000312P6pAAE',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def contar_asistencias_por_fecha(df_actividad, id_col='ContactId'):\n",
    "    # 1. Asegurar formato datetime\n",
    "    df_actividad['CreatedDate'] = pd.to_datetime(df_actividad['CreatedDate'], errors='coerce')\n",
    "    \n",
    "    # 2. Crear columna de fecha sin hora\n",
    "    df_actividad['Fecha'] = df_actividad['CreatedDate'].dt.date\n",
    "    \n",
    "    # 3. Contar asistencias y no asistencias por alumno y fecha\n",
    "    resumen = df_actividad.groupby([id_col, 'Fecha']).agg(\n",
    "        num_asistencias = ('FO_Asiste__c', lambda x: (x == True).sum()),\n",
    "        num_no_asistencias = ('FO_Asiste__c', lambda x: (x == False).sum())\n",
    "    ).reset_index()\n",
    "    \n",
    "    return resumen\n",
    "contar_asistencias_por_fecha(historial_actividad).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCION PARA COMPROBAR LOS DATOS DE LOS CRUCES Y DATAFRAMES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "\n",
    "def comprobar_cruce_generico(df, clave_principal):\n",
    "    # Obtener el nombre de la variable que se pasÃ³ como argumento\n",
    "    nombre_df = None\n",
    "    frame = inspect.currentframe().f_back\n",
    "    for var_name, var_val in frame.f_locals.items():\n",
    "        if var_val is df:\n",
    "            nombre_df = var_name\n",
    "            break\n",
    "    if nombre_df is None:\n",
    "        nombre_df = \"DataFrame\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ðŸ” ComprobaciÃ³n de cruce para {nombre_df} ({len(df)} filas, {df.shape[1]} columnas)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Duplicados y nulos en clave principal\n",
    "    if clave_principal in df.columns:\n",
    "        duplicados = int(df[clave_principal].duplicated().sum())\n",
    "        nulos_clave = int(df[clave_principal].isna().sum())\n",
    "        print(f\"ðŸ“Œ Clave principal: {clave_principal}\")\n",
    "        print(f\"   âž¡ Duplicados en clave: {duplicados}\")\n",
    "        print(f\"   âž¡ Nulos en clave: {nulos_clave}\")\n",
    "    else:\n",
    "        print(f\"âš  La columna clave '{clave_principal}' no existe en el DataFrame\")\n",
    "        return\n",
    "    \n",
    "    # 2. Nulos por columna (solo las que tienen nulos)\n",
    "    nulos_por_col = df.isna().sum()\n",
    "    nulos_por_col = nulos_por_col[nulos_por_col > 0]\n",
    "    if not nulos_por_col.empty:\n",
    "        print(\"\\nðŸ“Š Columnas con valores nulos:\")\n",
    "        display(pd.DataFrame(nulos_por_col, columns=['Nulos']))\n",
    "    else:\n",
    "        print(\"\\nâœ… No hay columnas con valores nulos\")\n",
    "    \n",
    "    # 3. Incoherencias en columnas numÃ©ricas (ej. porcentajes fuera de 0-100)\n",
    "    incoherencias = {}\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        serie = df[col].dropna()\n",
    "        if not serie.empty and serie.between(0, 100).sum() != len(serie):\n",
    "            incoherencias[col] = int(((serie < 0) | (serie > 100)).sum())\n",
    "    if incoherencias:\n",
    "        print(\"\\nâš  Incoherencias en columnas numÃ©ricas (fuera de 0-100):\")\n",
    "        display(pd.DataFrame.from_dict(incoherencias, orient='index', columns=['Valores fuera de rango']))\n",
    "    else:\n",
    "        print(\"\\nâœ… No hay incoherencias numÃ©ricas detectadas\")\n",
    "    \n",
    "    # 4. Fechas incoherentes (si hay al menos 2 columnas datetime)\n",
    "    columnas_fecha = df.select_dtypes(include=['datetime64[ns]']).columns.tolist()\n",
    "    if len(columnas_fecha) >= 2:\n",
    "        fecha_inicio = columnas_fecha[0]\n",
    "        fecha_fin = columnas_fecha[1]\n",
    "        incoherentes_fechas = int((df[fecha_fin] < df[fecha_inicio]).sum())\n",
    "        print(f\"\\nðŸ“… ComparaciÃ³n de fechas ({fecha_inicio} vs {fecha_fin}):\")\n",
    "        print(f\"   âž¡ Registros con fecha fin < fecha inicio: {incoherentes_fechas}\")\n",
    "    else:\n",
    "        print(\"\\nâ„¹ No hay suficientes columnas de fecha para comparar\")\n",
    "    \n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# Comprobar cualquier DataFrame (modificar a mano)\n",
    "resultado_df_unido = comprobar_cruce_generico(df_unido, clave_principal='ID')\n",
    "resultado_df_account = comprobar_cruce_generico(df_unido_account, clave_principal='ID')\n",
    "resultado_df_ecb_ban = comprobar_cruce_generico(df_unido_ecb_ban, clave_principal='ID')\n",
    "\n",
    "print(resultado_df_unido)\n",
    "print(resultado_df_account)\n",
    "print(resultado_df_ecb_ban)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Tarea 1: Unir historial de etapas y calcular la \"Velocidad de Embudo\"\n",
    "# ===============================\n",
    "\n",
    "# 1. Unir por Id de oportunidad\n",
    "df_unido = pd.merge(\n",
    "    target,\n",
    "    historial_etapas,\n",
    "    left_on='ID',                 # Id de oportunidad en target\n",
    "    right_on='LK_Oportunidad__c', # Id de oportunidad en historial_etapas\n",
    "    how='left',\n",
    "    suffixes=('', '_historial_etapas')\n",
    ")\n",
    "\n",
    "# 2. Eliminar columnas con demasiados NA\n",
    "df_unido = eliminar_columnas_na(df_unido)\n",
    "\n",
    "# 3. Convertir fechas a datetime\n",
    "df_unido['CreatedDate'] = pd.to_datetime(df_unido['CreatedDate'], errors='coerce')\n",
    "df_unido['Fecha_fin_etapa__c'] = pd.to_datetime(df_unido['Fecha_fin_etapa__c'], errors='coerce')\n",
    "\n",
    "# 4. Ordenar por oportunidad y fecha de inicio de etapa\n",
    "df_unido = df_unido.sort_values(by=['LK_Oportunidad__c', 'CreatedDate'])\n",
    "\n",
    "# 5. Calcular duraciÃ³n de cada etapa\n",
    "df_unido['DuracionEtapaDias'] = (df_unido['Fecha_fin_etapa__c'] - df_unido['CreatedDate']).dt.days\n",
    "\n",
    "# 6. Calcular velocidad de embudo (dÃ­as entre inicio de una etapa y la siguiente)\n",
    "df_unido['VelocidadEmbudo'] = df_unido.groupby('LK_Oportunidad__c')['CreatedDate'].diff().dt.days\n",
    "\n",
    "# 7. Resultado\n",
    "print(df_unido[['LK_Oportunidad__c', 'PL_Etapa__c', 'CreatedDate', 'Fecha_fin_etapa__c', 'DuracionEtapaDias', 'VelocidadEmbudo']].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Tarea 2: Unir Account y extraer Procedencia, Notas y RelaciÃ³n UNAV\n",
    "# ===============================\n",
    "\n",
    "# Columnas necesarias segÃºn el listado real\n",
    "columnas_cuenta = [\n",
    "    'ID18',  # Id de cuenta para unir\n",
    "    # Procedencia\n",
    "    'CH_NACIONAL__PC', 'CH_INTERNACIONAL__PC', 'FO_PAIS__PC', 'FO_PROVINCIA__PC', 'TX_OTRALOCALIDAD__PC',\n",
    "    # Notas\n",
    "    'NU_NOTA_MEDIA_1_BACH__PC', 'NU_NOTA_MEDIA_2_BACH__PC', 'NU_NOTA_MEDIA_DEFINITIVA__PC',\n",
    "    # RelaciÃ³n UNAV\n",
    "    'CH_ANTIGUO_ALUMNO__PC', 'CH_EMPLEADO__PC', 'CH_PROFESOR_ASOCIADO__PC',\n",
    "    'CH_HIJO_EMPLEADO__PC', 'CH_HIJO_ANTIGUO_ALUMNO__PC', 'CH_HERMANOS_ESTUDIANDO_UNAV__P', 'CH_ALUMNI__PC'\n",
    "]\n",
    "\n",
    "# Filtrar solo las columnas que existen\n",
    "columnas_existentes = [col for col in columnas_cuenta if col in cuenta.columns]\n",
    "cuenta_filtrada = cuenta[columnas_existentes]\n",
    "\n",
    "# Unir con target usando el Id de cuenta\n",
    "df_unido_account = pd.merge(\n",
    "    target,\n",
    "    cuenta_filtrada,\n",
    "    left_on='ACCOUNTID',  # Id de cuenta en target\n",
    "    right_on='ID18',      # Id de cuenta en cuenta\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Resultado\n",
    "print(df_unido_account.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Tarea 3: Unir ECB/BAN y calcular mÃ©tricas econÃ³micas\n",
    "# ===============================\n",
    "\n",
    "# 1. Seleccionar columnas de oportunidad\n",
    "columnas_oportunidad = [\n",
    "    'ID',  # Id de oportunidad para unir\n",
    "    'CU_IMPORTE_TOTAL', 'PAID_AMOUNT', 'PAID_PERCENT',\n",
    "    'LK_DESCUENTO_PRONTO_PAGO', 'NU_IMPORTE_DESCUENTO_PRONTO_PA', 'NU_PORCENTAJE_DESCUENTO_PRONTO'\n",
    "]\n",
    "oportunidad_filtrada = oportunidad[[col for col in columnas_oportunidad if col in oportunidad.columns]]\n",
    "\n",
    "# 2. Seleccionar columnas de solicitud BAN\n",
    "columnas_ban = [\n",
    "    'SOL_LK_oportunidad__c', 'SOL_MON_importeConcedido__c'\n",
    "]\n",
    "solicitud_ban_filtrada = solicitud_ban[columnas_ban]\n",
    "\n",
    "# 3. Unir oportunidad con solicitud BAN\n",
    "df_unido_ecb_ban = pd.merge(\n",
    "    oportunidad_filtrada,\n",
    "    solicitud_ban_filtrada,\n",
    "    left_on='ID',\n",
    "    right_on='SOL_LK_oportunidad__c',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4. Calcular mÃ©tricas\n",
    "df_unido_ecb_ban['Total_a_pagar'] = df_unido_ecb_ban['CU_IMPORTE_TOTAL']\n",
    "df_unido_ecb_ban['Porcentaje_pagado'] = df_unido_ecb_ban['PAID_PERCENT']\n",
    "df_unido_ecb_ban['Beca_concedida'] = df_unido_ecb_ban['SOL_MON_importeConcedido__c']\n",
    "df_unido_ecb_ban['Importe_descuento'] = df_unido_ecb_ban['NU_IMPORTE_DESCUENTO_PRONTO_PA']\n",
    "df_unido_ecb_ban['Porcentaje_descuento'] = df_unido_ecb_ban['NU_PORCENTAJE_DESCUENTO_PRONTO']\n",
    "\n",
    "# 5. Resultado\n",
    "print(df_unido_ecb_ban[['ID', 'Total_a_pagar', 'PAID_AMOUNT', 'Porcentaje_pagado', 'Beca_concedida', 'Importe_descuento', 'Porcentaje_descuento']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Tarea 4: Cruzar Actividades: nÂº de eventos asistidos por etapa (excluyendo futuros).\n",
    "# ===============================\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Filtrar actividades excluyendo futuros\n",
    "actividades_filtradas = historial_actividad.copy()\n",
    "actividades_filtradas['CreatedDate'] = pd.to_datetime(actividades_filtradas['CreatedDate'], errors='coerce')\n",
    "actividades_filtradas['CreatedDate'] = actividades_filtradas['CreatedDate'].dt.tz_localize(None)\n",
    "\n",
    "hoy = pd.Timestamp(datetime.today().date())\n",
    "actividades_filtradas = actividades_filtradas[actividades_filtradas['CreatedDate'] <= hoy]\n",
    "\n",
    "# Filtrar solo asistencias reales\n",
    "actividades_filtradas = actividades_filtradas[actividades_filtradas['FO_Asiste__c'] == True]\n",
    "\n",
    "# 2. Unir actividades con oportunidades usando CampaignId\n",
    "df_act_op = pd.merge(\n",
    "    actividades_filtradas,\n",
    "    oportunidad[['ID', 'CAMPAIGNID']],\n",
    "    left_on='CampaignId',\n",
    "    right_on='CAMPAIGNID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. Unir con historial de etapas para traer la etapa\n",
    "df_actividades_etapas = pd.merge(\n",
    "    df_act_op,\n",
    "    historial_etapas[['LK_Oportunidad__c', 'PL_Etapa__c']],\n",
    "    left_on='ID',\n",
    "    right_on='LK_Oportunidad__c',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 4. Contar nÂº de eventos asistidos por etapa\n",
    "eventos_por_etapa = df_actividades_etapas.groupby(['LK_Oportunidad__c', 'PL_Etapa__c']).size().reset_index(name='Num_eventos_asistidos')\n",
    "\n",
    "print(eventos_por_etapa.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Tarea 5: Extraer motivos de baja de la descripciÃ³n y vinculaciÃ³n con tipos de beca.\n",
    "# ===============================\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Filtrar columnas necesarias de casos\n",
    "columnas_casos = [\n",
    "    'LK_Oportunidad__c', 'Description', 'TX_Motivo_cancelacion_beca__c', \n",
    "    'TX_MotivoDenegacion__c', 'LK_Beca__c', 'PL_Resolucion_beca__c', 'NU_Importe_Beca_Concedido__c'\n",
    "]\n",
    "casos_filtrados = casos[columnas_casos]\n",
    "\n",
    "# 2. FunciÃ³n para clasificar motivos de baja\n",
    "def clasificar_motivo(texto):\n",
    "    if pd.isna(texto):\n",
    "        return 'Desconocido'\n",
    "    texto = str(texto).lower()\n",
    "    if 'econ' in texto:\n",
    "        return 'Motivo econÃ³mico'\n",
    "    elif 'traslad' in texto:\n",
    "        return 'Traslado a otra universidad'\n",
    "    elif 'no interes' in texto or 'desist' in texto:\n",
    "        return 'Falta de interÃ©s'\n",
    "    elif 'aprob' in texto and 'otra' in texto:\n",
    "        return 'Admitido en otra universidad'\n",
    "    else:\n",
    "        return 'Otro'\n",
    "\n",
    "# 3. Crear columna de motivo de baja a partir de Description\n",
    "casos_filtrados['Motivo_baja'] = casos_filtrados['Description'].apply(clasificar_motivo)\n",
    "\n",
    "# 4. Si quieres, tambiÃ©n podemos priorizar el motivo estructurado si existe\n",
    "casos_filtrados['Motivo_baja_final'] = casos_filtrados['TX_Motivo_cancelacion_beca__c'].fillna(casos_filtrados['Motivo_baja'])\n",
    "\n",
    "# 5. Resultado final\n",
    "print(casos_filtrados[['LK_Oportunidad__c', 'Motivo_baja_final', 'LK_Beca__c', 'PL_Resolucion_beca__c', 'NU_Importe_Beca_Concedido__c']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Tarea 6: Comprobar que en los cruces no se generan valores que no deben\n",
    "# ===============================\n",
    "\n",
    "def comprobar_cruces(df, clave_principal):\n",
    "    print(\"=== ComprobaciÃ³n de cruces ===\")\n",
    "    \n",
    "    # 1. Duplicados en clave principal\n",
    "    duplicados = df[df[clave_principal].duplicated()]\n",
    "    if not duplicados.empty:\n",
    "        print(f\"âš  Hay {len(duplicados)} duplicados en la clave {clave_principal}\")\n",
    "    else:\n",
    "        print(\"âœ… No hay duplicados en la clave principal\")\n",
    "    \n",
    "    # 2. Nulos en clave principal\n",
    "    nulos_clave = df[df[clave_principal].isna()]\n",
    "    if not nulos_clave.empty:\n",
    "        print(f\"âš  Hay {len(nulos_clave)} registros con clave principal nula\")\n",
    "    else:\n",
    "        print(\"âœ… No hay nulos en la clave principal\")\n",
    "    \n",
    "    # 3. Valores incoherentes en mÃ©tricas econÃ³micas\n",
    "    if 'Porcentaje_pagado' in df.columns:\n",
    "        incoherentes_pago = df[(df['Porcentaje_pagado'] < 0) | (df['Porcentaje_pagado'] > 100)]\n",
    "        if not incoherentes_pago.empty:\n",
    "            print(f\"âš  Hay {len(incoherentes_pago)} registros con % pagado fuera de rango\")\n",
    "        else:\n",
    "            print(\"âœ… % pagado dentro de rango\")\n",
    "    \n",
    "    if 'Beca_concedida' in df.columns and 'Total_a_pagar' in df.columns:\n",
    "        beca_mayor_total = df[df['Beca_concedida'] > df['Total_a_pagar']]\n",
    "        if not beca_mayor_total.empty:\n",
    "            print(f\"âš  Hay {len(beca_mayor_total)} registros con beca mayor que el total a pagar\")\n",
    "        else:\n",
    "            print(\"âœ… Becas dentro de rango\")\n",
    "    \n",
    "    # 4. Fechas incoherentes\n",
    "    if 'CreatedDate' in df.columns and 'Fecha_fin_etapa__c' in df.columns:\n",
    "        fechas_incoherentes = df[df['Fecha_fin_etapa__c'] < df['CreatedDate']]\n",
    "        if not fechas_incoherentes.empty:\n",
    "            print(f\"âš  Hay {len(fechas_incoherentes)} registros con fecha fin anterior a fecha inicio\")\n",
    "        else:\n",
    "            print(\"âœ… Fechas coherentes\")\n",
    "\n",
    "# Ejemplo de uso con tu DataFrame final\n",
    "# Comprobar cruces sobre el DataFrame que quieras\n",
    "# 1. target + historial_etapas\n",
    "comprobar_cruces(df_unido, clave_principal='ID')\n",
    "\n",
    "# 2. target + cuenta\n",
    "comprobar_cruces(df_unido_account, clave_principal='ID')\n",
    "\n",
    "# 3. oportunidad + solicitud BAN\n",
    "comprobar_cruces(df_unido_ecb_ban, clave_principal='ID')\n",
    "\n",
    "# 4. actividades + oportunidad (CampaignId)\n",
    "comprobar_cruces(df_act_op, clave_principal='ID')\n",
    "\n",
    "# 5. actividades + oportunidad + historial_etapas\n",
    "comprobar_cruces(df_actividades_etapas, clave_principal='ID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reescribir y crear el dataset nuevo completamente optimizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3. GUARDAR EXCEL LIMPIO\n",
    "# ===============================\n",
    "with pd.ExcelWriter(\n",
    "    r\"..\\datos\\02. Datos tratamiento preliminar\\DataSet Probabilidad MatrÃ­culas_Completo.xlsx\",\n",
    "    engine='xlsxwriter'\n",
    ") as writer:\n",
    "    for nombre_hoja, df in hojas_limpias.items():\n",
    "        df_final_v3.to_excel(writer, sheet_name=nombre_hoja, index=False)\n",
    "\n",
    "print(\"Archivo limpio guardado como 'DataSet Probabilidad MatrÃ­culas_Completo.xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Proyecto Matriculas)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
